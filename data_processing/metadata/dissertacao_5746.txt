Universidade Regional do Cariri - URCA
Departamento de Matemática
Programa de Mestrado Proﬁssional
em Matemática em Rede Nacional

Matrizes e algumas aplicações

Francisco Genilson dos Santos Silva

Juazeiro do Norte - CE
2021

Matrizes e algumas aplicações

Francisco Genilson dos Santos Silva

Dissertação de Mestrado apresentada à
Comissão Acadêmica
do
PROFMAT-URCA como requisito parcial
para obtenção do título de Mestre em
Matemática.

Institucional

Orientador

Prof. Dr. José Tiago Nogueira Cruz

Juazeiro do Norte - CE
2021

                  Ficha Catalográfica elaborada pela Biblioteca Central da Universidade Regional do Cariri – URCA 

     Bibliotecária: Ana Paula Saraiva de Sousa CRB: 3/1000 

       Silva, Francisco Genilson dos Santos. 
S586m Matrizes e algumas aplicações/ Francisco Genilson dos Santos 
Silva. – Juazeiro do Norte-CE, 2021 
       177p. 

       Dissertação  apresentada  ao  Programa  de  Mestrado  Profissional 
em Matemática em Redes - PROFMAT          
       Orientador: Prof. Dr. José Tiago Nogueira Cruz. 

 1.  Matrizes,  2.  Ensino  Médio,  3.  Aplicações,  4.  Rotação  de 

cônicas, 5. Teorema espectral; I. Título. 

                                                                                  CDD: 512.943 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
                         
                     
 
 
 
 
 
 
 
 
 
Matrizes e algumas aplicações

Francisco Genilson dos Santos Silva

Dissertação de Mestrado apresentada à
Comissão Acadêmica Institucional do
PROFMAT-URCA como requisito par-
cial para obtenção do título de Mestre
em Matemática.

BANCA EXAMINADORA

Prof. Dr José Tiago Nogueira Cruz (Orientador)
Universidade Regional do Cariri (URCA)

Prof. Dr. Júnio Moreira de Alencar
Instituto Federal de Educação e Tecnologia do Ceará (IFCE)

Prof. Dr. Flávio França Cruz
Universidade Regional do Cariri (URCA)

Dedicado à minha esposa Simone, a qual
acreditou em mim e me incentivou nos mo-
mentos em que mais precisei.

Agradecimentos

À minha esposa Simone, pois esta me apoiou nos momentos mais difícieis e me incen-

tivou a não desisitir quando desanimei.

Aos meus pais, que sempre me apoiaram e me ensinaram que a educação é o melhor

caminho.

Ao meu orientador, Professor Doutor José Tiago Nogueira Cruz, por acreditar em

meu potencial e se mostrar sempre disponível quando as dúvidas apareciam.

Ao Professor Doutor Júnio Moreira de Alencar que, durante toda a graduação me
incentivou bastante e por inúmeras vezes, reservou horários para discutirmos os mais
variados assuntos de Álgebra Linear.

A todos os professores que tive, desde o início da minha carreira escolar até hoje, pois

cada um destes contribuiu, à sua maneira, para a formação da pessoa que sou hoje.

À Banca Examinadora deste trabalho, composta pelos professores Júnio Moreira de

Alencar e Fávio França Cruz, pode ter aceito o convite para analisá-la.

A todos que, de alguma maneira contribuíram para a realização deste trabalho.

“— Qual é o melhor caminho para
eu subir a montanha?
— Apenas
nisso!”
(Friederich Nietzsche in, ‘A Gaia
Ciência’.)

e não pense

suba,

“Para os precavidos não existe o
azar; nada de apuros para os pre-
parados”.
(Baltasar Gracián in, ‘A Arte da
Prudência’)

Resumo

O presente trabalho, o qual foi desenvolvido por meio de pesquisa cientíﬁca do tipo
qualitativa, visa mostrar algumas aplicações que as matrizes possuem, tanto dentro da
própria Matemática, como também em outras áreas, tendo a preocupação de esboçar
os detalhes de forma acessível, evitando portanto a utilização de muitas deﬁnições e
teoremas, mas sem perder o rigor matemático necessário. A pesquisa é direcionada tanto
ao professor do ensino médio, para que este possa vir a aprofundar o assunto em sala
de aula, bem como para o aluno de ensino médio que tenha interesse em entender a
funcionalidade das matrizes na prática. Parte-se então de uma simples organização de
uma lista de compras com seus respectivos preços em uma tabela, passando por problemas
que envolvem rotas de voo, criptograﬁa, crescimento populacional, dentre outros. Tais
temáticas foram inseridas como exemplos e/ou comentários ao longo do desenvolvimento
do texto. Uma delas recebe maior enfoque, a saber, a rotação de cônicas, a qual é discutida
tanto com o uso de diagonalização de matrizes, bem como sem a utilização da mesma.
Outro ponto abordado é a exponencial de matrizes, tópico este que possui inúmeras
aplicações em diversos ramos do conhecimento. Por ﬁm, é realizada a explanação sobre a
relação entre números complexos e matrizes, tais como involuções, matrizes hermitianas,
quatérnios até chegar ao Teorema Espectral.

Palavras-chave: Matrizes. Ensino Médio. Aplicações. Rotação de cônicas. Teorema
Espectral.

Abstract

The present work aims to show the most varied applications that the matrices have,
both within mathematics itself, as well as in other areas, with the concern to outline the
details in a simple way, so that a high school student can read it completely, but without
losing the necessary mathematical rigor. It then departs from a simple organization of
a shopping list with their respective prices in a table, going through problems involving
ﬂight routes, cryptography, population growth, among others. These themes were inserted
as examples and/or comments throughout the development of the text. One of them
receives a greater focus, that is, the rotation of conics, which is discussed both with
the use of diagonalization of matrices, as well as without the use of it. Another point
addressed is the exponential of matrices, a topic that has numerous applications in various
branches of knowledge. Finally, an explanation is made about the relationship between
complex numbers and matrices, such as involutions, hermitian matrices, quaternions until
then reaching the Spectral Theorem.

Key-words: Matrices. High School. Applications. Rotation of conics. Spectral theorem.

Sumário

Agradecimentos

Resumo

Abstract

Introdução

1 Matrizes

1.1 Matrizes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
1.1.1 Adição e subtração de matrizes
1.1.2 Multiplicação de matrizes
. . . . . . . . . . . . . . . . . . . . . .
1.1.3 Matriz Transposta . . . . . . . . . . . . . . . . . . . . . . . . . .
1.1.4 Matriz Inversa . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2 Matrizes de blocos
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3 Matrizes elementares . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.4 Algumas aplicações matriciais . . . . . . . . . . . . . . . . . . . . . . . .
1.4.1 Matrizes de Leslie . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.4.2 Criptograﬁa . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 Determinantes e Sistemas Lineares

2.1 Determinantes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.1.1 Propriedades dos determinantes . . . . . . . . . . . . . . . . . . .
2.1.2 Matriz Adjunta e a Regra de Cramer . . . . . . . . . . . . . . . .
2.2 Sistemas Lineares . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3 Transformações Lineares e Matrizes

3.1 O plano R2 e o espaço tridimensional R3
. . . . . . . . . . . . . . . . . .
3.2 Transformações Lineares . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2.1 Conceito e exemplos
. . . . . . . . . . . . . . . . . . . . . . . . .
3.2.2 Translação, rotação e reﬂexão de cônicas . . . . . . . . . . . . . .
Identiﬁcando formas quadráticas . . . . . . . . . . . . . . . . . . .
3.2.3

4 Diagonalização de matrizes

4.1 O conceito . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 A diagonalização de matrizes na rotação de cônicas . . . . . . . . . . . .
4.3 Exponencial de Matriz . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5 Sobre números complexos e quatérnios

5.1 Números complexos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.2 Quatérnios . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5

7

8

11

13
13
16
18
23
30
34
40
52
52
59

66
66
70
75
78

87
87
104
104
114
123

130
130
139
149

156
156
158

Considerações Finais

Referências

Apêndice

163

164

169

Introdução

A Álgebra Linear é, de acordo com BOURBAKI [10], um dos mais velhos ramos
da Matemática e, ao mesmo tempo, um dos mais novos. Enquanto que, por um lado,
conforme MEYER [46], a análise mais antiga de equações simultâneas é encontrada no
livro chinês Chiu-chang Suan-shu (Nove Capítulos sobre Aritmética) por volta de 200
a.C., é aﬁrmado por RICARDO [57] que, devido à disponibilidade de computadores e
calculadoras de grande manuseio, a Álgebra Linear tem emergido como pré-requisito para
muitas áreas de aplicações. Como se pode observar em [3], tais aplicações se estendem
desde tópicos como Programação linear geométrica, até Tomograﬁa Computadorizada.
Dessa forma, é possível perceber o quão importante e atual é o estudo deste ramo da
Matemática, sendo necessário então que seu ensino esteja sempre em aperfeiçoamento.

O processo de ensino e aprendizagem de Matemática ainda possui muitos obstáculos
a serem superados. Um deles, como aponta FIORENTINI [23], é o fato do aluno não
entendê-la como a escola lhe ensina e, na maioria das vezes, sentem diﬁculdades em uti-
lizar o conhecimento “adquirido”, até mesmo quando aprovados na disciplina. Com base
nisso, buscou-se apresentar um dos inúméros tópicos da matemática, a saber, as matrizes,
de uma maneira interdisciplinar, apontando aplicações que estas possuem em diferentes
campos do conhecimento, respeitando assim a importância da interdisciplinaridade, onde
esta, de acordo com TERRADAS [60], é uma metodologia de ensino inovadora e impor-
tante, não somente para a Educação Matemática, mas também para os diversos âmbitos
do saber.

Um dos principais objetivos do presente trabalho é fornecer uma abordagem de tó-
picos matriciais, de modo que estes possam ser entendidos por alunos de Ensino Médio.
Nessa perspectiva, todos os resultados apresentados foram obtidos utilizando proprieda-
des de matrizes, as quais são apresentada no Capítulo 1, juntamente com as operações
relacionadas às mesmas. Os exemplos de aplicações das matrizes aqui utilizados são in-
tercalados à medida que é desenvolvida a teoria, a ﬁm de alinhar a interdisciplinaridade
e, consequentemente despertar a curiosidade do leitor quanto ao assunto, além de estabe-
lecer uma conexão entre o conteúdo e a realidade, o que está de acordo com o pensamento
de DENCKER [17], segundo o qual a interdisciplinaridade surge nos anos 70 como uma
resposta às necessidades de um tratamento mais integrador da realidade.

A abordagem descrita acima também levanta a possibilidade do professor de En-
sino Médio discutir tópicos relacionados às matrizes de uma forma mais dinâmica, não
se atendo apenas à resolução de exercícios, mas também a situações práticas em que
tais temas surgem. O mesmo se aplica ao ensino superior, pois como aponta FAVA-
RÃO e ARAÚJO [22], os professores acabam dando atenção ao conteúdo em si e não
à sua interligação com a situação da qual emerge, gerando assim a clássica dissociação
entre teoria e prática. Vale ressaltar que não está sendo colocada de lado a impor-
tância da teoria, pelo contrário, as aplicações não só realçam o embasamento teórico,
bem como lhes acrescenta beleza. Outrossim, principalmente para os alunos que pre-
tendem ingressar no campo acadêmico voltado à áreas exatadas do conhecimento, é de

fundamental importância que a teoria seja consolidada. Todavia, como ressalta SOA-
RES e ALMEIDA [58], os alunos que pretendem ingressar no Ensino Superior acabam
por criar expectativas que não condizem com a realidade acadêmica, gerando assim con-
ﬂitos que acabam por aumentar os níveis de desistência. Por este motivo, é defendido que,

Num mundo extremamente competitivo, a universidade precisa se preocupar
com o estudante universitário, promovendo condições para o seu desenvolvi-
mento integral, tentando desenvolver suas potencialidades ao máximo para que
possa atingir seu nível de excelência pessoal e estar preparado para um papel
atuante na sociedade (SANTOS apud CUNHA e CARRILHO [15], p. 216).

Dessa maneira, buscou-se escrever a teoria de maneira simples e clara, porém sem
fugir do rigor matemático, com exceção de raros casos de demonstrações que exigem
indução matemática, as quais, quando possível, foram provadas por uma ideia intuitiva
de indução, isto é, monstrando casos iniciais de modo a encontrar um padrão e, em
seguida, generalizando a ideia. O motivo de não ter sido abordada a indução matemática
deve-se à busca de evitar possíveis digressões, mantendo o foco sobre as matrizes e suas
propriedades. Porém, a discussão de tal assunto direcionada a alunos de Ensino Médio
pode ser encontrada em [16] e [51].

O Capítulo 1 é direcionado ao desenvolvimento da teoria acerca das matrizes, apon-
tando suas diferentes propriedades e classiﬁcações. Entre elas, destacam-se duas que não
são comuns ao Ensino Médio: Matrizes de blocos e Matrizes elementares. Além disso, a
última seção de tal capítulo trata exclusivamente do crescimento populacional por meio
de matrizes de Leslie. Já o Capítulo 2 versa a respeito dos determinantes das matrizes e
das propriedades dos mesmos, onde a justiﬁcativa de algumas destas ﬁcarão mais claras
devido à matrizes elementares.

O plano e o espaço tridimensional são discutidos no Capítulo 3, porém seu principal
foco são as transformações lineares, pois estas serão utilizadas para exempliﬁcar situa-
ções em que as cônicas aparecem em posições incomuns, como por exemplo rotacionadas
ou reﬂexionadas em torno de uma reta, além da identiﬁcação de formas quadráticas.
Cabe lembrar que será admitido o conhecimento prévio das cônicas por parte do leitor,
omitindo-se assim os detalhes relativos às mesmas. Mais exemplos destes podem ser
encontrados em [12].

A Diagonalização de matrizes é também um tema incomum ao Ensino Médio, sendo
portanto tratado no Capítulo 4, tanto em sua deﬁnição, como em sua aplicação à rotação
de cônicas, fornecendo assim uma segunda maneira de realizar o processo descrito no
Capítulo 3. Ademais, é realizada um breve exposição sobre a exponencial de uma matriz,
tema este também incomum ao Ensino Médio, mas que possui vasta aplicabilidade.

O Capítulo 5 é destinado à relação estabelecida entre números complexos e matri-
zes, mais precisamente a representação de um número complexo por uma matriz e os
quatérnios, onde estes são uma extensão dos números complexos. O complemento deste
Capítulo é deixado como Apêndice, onde são tratadas as matrizes hermitianas e suas
propriedades, além de apresentar um teorema de bastante valor na Álgebra Linear, o
Teorema Espectral.

1 Matrizes

Iniciaremos este capítulo com a deﬁnição de matrizes, conceito este que será de extrema

importância para o desenvolvimento de todos os capítulos posteriores a este.

1.1 Matrizes

Deﬁnição 1.1 Dados m, n ∈ N, chama-se matriz A, toda tabela retangular A, composta
por números reais aij, distribuídos em m linhas e n colunas, onde i, j ∈ N são os índices
que permitem identiﬁcar um dado elemento na matriz que está na i-ésima linha e j-ésima
coluna. A representação de uma tal matriz é dada por

A =








a12
a11
a22
a21
...
...
am1 am2

a1n
· · ·
a2n
· · ·
...
...
· · · amn








Exemplo 1.1 A ﬁm de melhor compreender a deﬁnição acima, vejamos alguns exemplos
de matrizes a seguir:

A =

(cid:21)

(cid:20) −7 9
16 0






B =

1
2 −1
1

1

− 1
2
0





D =

32
10 12 −3 77
−4 86
16 99 −74
13 38 −9 15 −35










C =









1 2 3
4 5 6
7 8 9

E =







0 0 0 0
0 0 0 0
0 0 0 0
0 0 0 0







As matrizes A, B, C, D e E acima são do tipo 2 × 2, 3 × 2, 3 × 3, 3 × 5 e 4 × 4,
respectivamente. Na matriz A, a entrada a12, isto é, o número que está na primeira
linha e na segunda coluna, é 9, enquanto que b12 = −1 na matriz B. Já na matriz D,
podemos observar que d31 = 13 e d25 = −74.

Vale lembrar que a igualdade entre duas matrizes A = [aij]m×n

ocorre
se, e somente se, aij = bij, para todo i = 1, 2, . . . , m e todo j = 1, 2, . . . , n. Por exemplo,
as matrizes

e B = [bij]m×n

A =

(cid:21)

(cid:20) 1 2
3 4

B =

(cid:21)

(cid:20) 0 2
3 4

são diferentes, tendo em vista que a11 = 1 (cid:54)= 0 = b11.

13

A seguir, veremos algumas matrizes que recebem nomes especiais devido às diferentes

propriedades que elas podem apresentar:

a) Matriz linha: é toda matriz que possui n colunas e uma única linha, isto é, uma matriz
da forma

A = (cid:2) a11 a12

· · · a1n

(cid:3)

b) Matriz coluna: é toda matriz que possui m linhas e uma única coluna, tendo então a
seguinte forma:



B =













b11
b21
...
bm1

c) Matriz quadrada: é toda matriz A = [aij]m×n
, tal que m = n, ou seja, o número
de linhas é igual ao número de colunas. Dizemos neste caso que a matriz A possui
ordem n. As matrizes A, C e E do Exemplo 1.1 são quadradas de ordem 2, 3 e 4,
respectivamente.Vale lembrar que, nas matrizes quadradas A = [aij]n×n, as entradas aij,
tais que i = j constituem um conjunto chamado de diagonal principal , ao passo que
as entradas com i + j = n + 1 formam um conjunto denominado diagonal secundária.
Vejamos, por exemplo, quais são as diagonais principal e secundária da matriz C do

Exemplo 1.1, observando que

• a11 = 1; a22 = 5; a33 = 9 ⇒ Diagonal principal é dada por {1, 5, 9}.

• Como C possui ordem 3, devemos ter i + j = 4. Assim, as entradas que atendem
a essa exigência são: a13 = 3; a22 = 5; a31 = 7. Portanto, temos que a diagonal
secundária é dada por {3, 5, 7}.

Agora podemos visualizar abaixo as duas diagonais citadas anteriormente:

C =









1 2 3
4 5 6
7 8 9

C =









1 2 3
4 5 6
7 8 9

Diagonal principal de C

Diagonal secundária de C

Vale lembrar que a soma das entradas de uma matriz quadrada A é chamado de
traço da matriz A e é representado por tr(A). Mais formalmente, dada uma matriz
A = [aij]n×n, tem-se

tr(A) = a11 + a22 + · · · + ann.

d) Matriz nula: é chamada assim, qualquer matriz A = [aij]m×n
, tal que aij = 0 para todo
i = 1, 2, . . . m e todo j = 1, 2, . . . n, o que quer dizer que todas as suas entradas são iguais
a 0. A matriz E do Exemplo 1.1 é um exemplo de matriz desse tipo. Representaremos
por O uma matriz nula de ordem n × n.

e) Matriz Identidade: São assim denominadas todas as matrizes quadradas A = [aij]n×n,

14

tais que todos os elementos da diagonal principal são iguais a 1. Observemos logo abaixo
alguns exemplos de matrizes identidade:

(cid:21)

(cid:20) 1 0
0 1







1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1

















1 0 0 0 0 0
0 1 0 0 0 0
0 0 1 0 0 0
0 0 0 1 0 0
0 0 0 0 1 0
0 0 0 0 0 1











Por simplicidade, nos referiremos a uma matriz identidade de ordem n por meio da
expressão In×n, ou simplesmente por I, nos casos em que não houver dúvida sobre qual
é a ordem dessa matriz.

f) Matriz diagonal: Essa é a generalização de uma matriz identidade, de modo que ao invés
de possuir todos os elementos de sua diagonal principal iguais a 1, estes são constantes
reais não necessariamente todas iguais. Quanto às demais entradas (não pertencentes
à diagonal principal) de uma tal matriz, são todas iguais a zero. Vejamos logo abaixo
alguns exemplos de matrizes diagonais:

(cid:21)

(cid:20) a 0
0 b







a 0 0 0
0 b 0 0
0 0 c 0
0 0 0 d

















a 0 0 0 0 0
0 b 0 0 0 0
0 0 c 0 0 0
0 0 0 d 0 0
0 0 0 0 e 0
0 0 0 0 0 f











,

onde a, b, c, d, e e f são constantes reais quaisquer.

g) Matriz triangular: As matrizes triangulares podem ser classiﬁcadas como superiores,
quando todas as entradas abaixo da diagonal principal são iguais a zero, ou inferiores,
quando todas os entradas acima da diagonal principal são iguais a zero. Vejamos abaixo
um exemplo de cada caso:







4
1 2 3
7
0 5 6
9
0 0 8
0 0 0 10













0
1 0 0
0
2 3 0
0
4 5 6
7 8 9 10







Matriz triangular superior

Matriz triangular inferior

h) Matriz de Vandermonde1: São matrizes m × n da forma




V =

1

· · · xn−1
x2
1 x1
1
· · · xn−1
x2
1 x2
2
...
...
...
...
m · · · xn−1
1 xm x2

· · ·

m

2






,






1Este nome é em homenagem a Alexandre Theóphile Vandermonde (1735-1796), um matemático
francês que fez uma variedade de contribuições à matemática, mas que talvez seja melhor conhecido
como o primeiro europeu a fornecer uma exposição lógica completa da teoria dos determinates [46].

15

onde xi (cid:54)= xj, para todo i (cid:54)= j.

A seguir deﬁniremos as operações de adição/subtração e multiplicação de matrizes.
Quanto à divisão de matrizes, veremos posteriormente que esta não é deﬁnida, devido ao
fato de que nem toda matriz possui uma matriz inversa.

1.1.1 Adição e subtração de matrizes

Deﬁnição 1.2. Dadas duas matrizes A = [aij]m×n e B = [bij]m×n a soma A + B é igual
a uma matriz C = [cij]m×n tal que

[aij]m×n + [bij]m×n = [cij]m×n .

Podemos concluir, a partir da deﬁnição acima, que a soma de duas (ou mais) matrizes
é ainda uma matriz, a qual está deﬁnida apenas para o caso em que A e B possuem o
mesmo número de linhas e o mesmo número de colunas.

Exemplo 1.2. Ana, Bruna e Carla ﬁzeram compras de arroz, feijão e carne durante o
mês de janeiro nos estabelecimentos A, B e C, respectivamente. A tabela abaixo mostra
as quantidades totais de cada um desses alimentos comprados por elas:

Arroz Feijão Carne
10 kg
10 kg
20 kg
13 kg
12 kg
25 kg
15 kg
18 kg
30 kg

Ana
Bruna
Carla

No mês seguinte, elas realizaram compras dos mesmos alimentos nos mesmos estabeleci-
mentos, porém em quantidades diferentes, conforme mostra o próximo quadro:

Arroz Feijão Carne
12 kg
23 kg
8 kg
11 kg
13 kg
20 kg
17 kg
15 kg
27 kg

Ana
Bruna
Carla

Quanto aos preços, em reais, de cada quilo de alimento nos respectivos estabelecimentos,
estes estão organizados no seguinte quadro:

A
2,50
4,30
25,30

B
2,30
4,00
28,90

C
3,20
4,80
36,60

Arroz
Feijão
Carne

Para saber o total de quilos de cada alimento que cada uma delas comprou durante
esses dois meses, podemos representar as compras realizadas em janeiro e fevereiro pelas
respectivas matrizes J e F abaixo:





J =

20 10 10
25 12 13
30 18 15









F =

8
23 12
20 13 11
27 15 17





16

Agora, realizando a soma das duas últimas matrizes, decorre

J + F =

=

=













20 10 10
25 12 13
30 18 15





 +



23 12
8
20 13 11
27 15 17





10 + 8
20 + 23 10 + 12
25 + 20 12 + 13 13 + 11
30 + 27 18 + 15 15 + 17





43 22 18
45 25 24
57 33 32





Logo, podemos concluir que, durante esses dois meses

• Ana comprou um total de 43 kg de arroz, 22 kg de feijão e 18 kg de carne.

• Bruna comprou um total de 45 kg de arroz, 25 kg de feijão e 24 kg de carne.

• Carla comprou um total de 57 kg de arroz, 33 kg de feijão e 32 kg de carne.

A subtração de matrizes é deﬁnida de forma análoga, de modo que, dadas duas ma-
, tal

então A − B é igual a uma matriz D = [dij]mn

e B = [bij]m×n

trizes A = [aij]m×n
que

[aij]m×n − [bij]m×n = [dij]m×n .

e C = [cij]m×n

Com estas deﬁnições, passemos às propriedades da soma de matrizes. As propriedades
, B =

para a subtração são análogas. Consideremos então as matrizes A = [aij]m×n
. São válidas as seguintes aﬁrmações:
[bij]m×n
a) Comutatividade: A + B = B + A.
Sabemos que, pela comutatividade dos números reais, se a, b ∈ R, então vale a+b = b+a.
Daí, como as entradas das matrizes adotadas até aqui são números reais, temos que
aij + bij = bij + aij, e portanto

A + B = [aij] + [bij] = [aij + bij] = [bij + aij] = [bij] + [aij] = B + A

b) Associatividade: (A + B) + C = A + (B + C).

Segue diretamente da associatividade dos números reais, isto é, dados três números reais
a, b e c, tem-se que (a + b) + c = a + (b + c). Dessa forma, decorre que

(A + B) + C = ([aij] + [bij]) + [cij]

= [aij + bij] + [cij]

= [(aij + bij) + cij]

= [aij + (bij + cij)]

= [aij] + [bij + cij]

= [aij] + ([bij] + [cij])

= A + (B + C)

17

c) Elemento neutro: Para toda matriz A = [aij]m×n
que A + O = A.

, existe uma matriz O = [oij]mn

, tal

Povemos inicialmente a existência. se ocorre A + O = A, então segue que

[aij + oij] = [aij] ⇒ aij + oij = aij ⇒ oij = aij − aij = 0.

Como i e j são arbitrários, decorre que oij = 0 para todo i e todo j. Logo a matriz O
existe e é uma matriz nula com o mesmo número de linhas e colunas de A. Quanto à
(cid:3), tal que A + O(cid:48) = A.
unicidade, suponhamos que exista uma outra matriz O(cid:48) = (cid:2)o(cid:48)
Decorre a partir daí que

ij

A + O = A + O(cid:48) ⇒ aij + oij = aij + o(cid:48)

ij ⇒ oij = o(cid:48)

ij ⇒ O = O(cid:48).

Portanto, o elemento neutro de qualquer matriz é único.

d) Elemento oposto: Para toda matriz A, existe uma única matriz A(cid:48), tal que A+A(cid:48) = O.

De fato, se A + A(cid:48) = O, então

aij + a(cid:48)

ij = 0 ⇒ a(cid:48)

ij = −aij,

ou seja, a matriz A(cid:48) é formada pelo oposto de cada entrada da matriz A. Quanto à
sua unicidade, esta é provada de modo análogo à unicidade do elemento neutro. Por
simplicidade notacional, nos referiremos à matriz oposta A(cid:48) da matriz A apenas por
−A.

1.1.2 Multiplicação de matrizes

Antes de fornecermos a deﬁnição de multiplicação de matrizes, utilizemos o Exemplo

1.2 para auxiliar na compreensão de como as matrizes são multiplicadas.

Se quiséssemos saber quanto Ana gastou em janeiro com as compras de arroz, feijão

e carne, deveremos multiplicar cada quantidade por seu respectivo preço. Assim,

20 · 2, 50 + 10 · 4, 30 + 10 · 25, 30 = 346

(∗)

o que mostra que ela gastou R$ 346, 00.

Agora representando as quantidades de alimento compradas por ela em janeiro pela
matriz linha QA = (cid:2) 20 10 10 (cid:3) ,os preços desses alimentos (por quilo) pela matriz


 e analizando (∗), podemos observar que a entrada q11 de QA

coluna PA =





2, 50
4, 30
25, 30

multiplica a entrada p11 de PA e da mesma forma q12 multiplica p21 e q13 multiplica p31,
o que nos aponta para uma soma de produtos da forma qij · pji, isto é,

q11 · p11 + q12 · p21 + q13 · p31.

Notemos que se alguma das matrizes QA ou PA possuísse mais entradas que a outra, a
expressão (∗) não poderia ser calculada.

Podemos assim fornecer a deﬁnição de multiplicação de matrizes da seguinte forma

18

Deﬁnição 1.3. Dadas duas matrizes A = [aij]m×n e B = [bjk]n×p, tem-se que o produto
A · B é igual a uma matriz C = [cik]m×p, tal que

cik = ai1 · b1k + ai2 · b2k + ai3 · b3k + . . . ain · bnk

Exemplo 1.3. O produto entre as matrizes QA e PA, de acordo com a deﬁnição acima
é dado por

QA · PA = (cid:2) 20 10 10 (cid:3) ·









2,50
4,30
25,30

= (cid:2) 20 · 2,50 + 10 · 4,30 + 10 · 25,30 (cid:3)

= (cid:2) 346 (cid:3)

Note que a expressão obtida na segunda igualdade coincide com (∗).

Exemplo 1.4. Vamos utilizar a deﬁnição acima para calcular o produto entre as matrizes
J e a matriz dos preços dos alimentos do Exemplo 1.2, a qual representaremos por





Q =

2,3
4

3,2
2,5
4,8
4,3
25,3 28,9 36,6



 .

20 10 10
25 12 13
30 18 15





 ·



2,3
4

3,2
2,5
4,3
4,8
25,3 28,9 36,6





50 + 43 + 253
46 + 40 + 289
62,5 + 51,6 + 328,9 57,5 + 48 + 375,7 80 + 57,6 + 475,8
69 + 72 + 433,5
75 + 77,4 + 379,5

96 + 86,4 + 549

64 + 48 + 366





478

375
346
443
481,2 613,4
531,9 574,5 731,4





Daí,

J · Q =

=

=













A interpretação da matriz J · Q nos fornece tanto o valor que Ana, Bruna e Carla
pagaram nas compras de janeiro (valores da diagonal principal), bem como fornece os
valores de suas compras, caso estas tivessem sido realizadas nos outros estabelecimentos.
Por exemplo, Bruna pagou R$ 481, 20 comprando no estabelecimento 2, ao passo que
pagaria R$ 443, 00, caso tivesse comprado no estabelecimento 1.

A próxima deﬁnição nos permitirá conhecer as propriedades da multiplicação em re-

lação às matrizes.

Deﬁnição 1.4. Dado um número real c e uma matriz A = [aij]m×n
produto c · A é uma matriz, cujas entradas são iguais a c · aij. Em termos

, tem-se que o

c · A = c · [aij] = [c · aij] .

19

Exemplo 1.5. Suponhamos que Ana, Bruna e Carla comprem durante três meses segui-
dos a mesma quantidade de alimentos que compraram em fevereiro. Quantos quilos de
alimento terão comprado no total, ao ﬁm desse período?

Basta multiplicar a matriz F do Exemplo 1.2. por 3, de modo que









23 12
8
20 13 11
27 15 17

 =



3 · 23 3 · 12
3 · 8
3 · 20 3 · 13 3 · 11
3 · 27 3 · 15 3 · 17

 =



69 36 24
60 39 33
81 45 51



 .

3 · F = 3 ·



Logo, temos que

• Ana comprou 69 kg de arroz, 36 kg de feijão e 24 kg de carne.

• Bruna comprou 60 kg de arroz, 39 kg de feijão e 33 kg de carne.

• Carla comprou 81 kg de arroz, 45 kg de feijão e 51 kg de carne.

Passemos desde já às propriedades da multiplicação de escalar por matriz, conside-
.
e B = [bij]m×n

rando para tanto, dois números reais c, d e duas matrizes A = [aij]m×n
Assim

a) Associatividade: c · (d · A) = (c · d) · A

Basta aplicar a associatividade dos números reais e a Deﬁnição 1.4, de modo que

c · (d · A) = [c · (d · aij)] = [(c · d) · aij] = (c · d) · [aij] = (c · d) · A

b) Distributividade:

• (c + d) · A = c · A + d · A

Pela distributividade de números reais em relação à soma, temos que:

(c + d) · A = [(c + d)aij]

= [c · aij + d · aij]
= [c · aij] + [d · aij]
= c · [aij] + d · [aij]
= c · A + d · A

• c · (A + B) = c · A + c · B

Novamente iremos aplicar a distributividade dos números reais em relação à soma,
de tal modo que

c · (A + B) = c · ([aij] + [bij])

= c · [aij + bij]
= [c · (aij + bij)]
= [c · aij + c · bij]
= [c · aij] + [c · bij]
= c · [aij] + c · [bij]
= c · A + c · B

20

A seguir, listamos as propriedades principais de multiplicação entre matrizes, as quais
não serão provadas, uma vez que suas demonstrações decorrem da Deﬁnição 1.3 e das
propriedades de produto de um escalar por uma matriz. Além disso, a partir de agora
suprimiremos o ponto “·” de multiplicação, de tal modo que, por exemplo, uma expressão
que aparecia da forma A · B agora será escrita apenas como AB.

a) Associatividade: (AB)C = A(BC)

b) Distributividade à direita em relação à soma: (A + B)C = AC + BC

c) Distributividade à esquerda em relação à soma: C(A + B) = CA + CB

d) Dada uma constante real c, tem-se que

(cA)B = A(cB) = c(AB)

, k ∈ N.
e) Potência de uma matriz: Ak = A · A · A · · · A
(cid:125)

(cid:124)

(cid:123)(cid:122)
k termos

Em relação aos itens b) e c) logo acima, cabe a seguinte pergunta: “a multiplicação
de matrizes não é comutativa assim como a multiplicação de números reais?” A resposta
para essa pergunta é: talvez. Consideremos, por exemplo, as matrizes a seguir:

X =

(cid:21)

(cid:20) 1 3
0 2

Y =

(cid:21)

(cid:20) 5 1
3 4

Pode-se facilmente veriﬁcar que

XY =

(cid:20) 14 13
8
6

(cid:21)

(cid:54)=

(cid:20) 5 17
3 17

(cid:21)

= YX

Por outro lado, existem casos em que a comutatividade é válida, como por exemplo,
pela matriz identidade In×n.

na multiplicação de uma matriz quadrada A = [aij]n×n
Temos abaixo um caso particular de tal fato:

e

XI =

(cid:20) 14 13
8
6

(cid:21) (cid:20) 1 0
0 1

(cid:21)

=

(cid:20) 14 13
8
6

(cid:21)

IX =

(cid:20) 1 0
0 1

(cid:21) (cid:20) 14 13
8
6

(cid:21)

=

(cid:20) 14 13
8
6

(cid:21)

Exemplo 1.6. Algumas matrizes apresentam cacterísticas interessantes quando multipli-
cadas por si mesma. Uma delas é a matriz idempotente, isto é, uma matriz quadrada
A de ordem n, tal que, A2 = A. O exemplo mais comum de tais matrizes é I, porém
podemos listar outras a seguir

(cid:21)

(cid:20) 1 0
1 0

(cid:20) 2
3
2
3

(cid:21)

1
3
1
3

21






1
4
1
4
1
2

1
4
1
4
1
2




 .

1
4
1
4
1
2

Existem também matrizes quadradas A, tais que A2 = I. Tais matrizes são denominadas
involuções. As matrizes abaixo são exemplos de tais matrizes

(cid:20) cos θ

sen θ
sen θ − cos θ

(cid:21)









1 0 0
0 0 1
0 1 0





1
0
0 −1
0

0
0
0 −1



 .

Temos ainda as matrizes nilpotentes, onde estas, por sua vez, são matrizes quadradas
A com Ak = O, onde k é algum número natural. Temos, por exemplo, que

(cid:21)

(cid:20) 0 1
0 0





−1 −1 −3
−5 −2 −3
3
1

2











0 1 0 0
0 0 1 0
0 0 0 1
0 0 0 0







são matrizes nilpotentes.

Exemplo 1.7. Desde que A = [aij]n×n

seja uma involução, as matrizes

B =

1
2

(I + A)

e

C =

1
2

(I − A)

são idempotentes e BC = O. De fato, veriﬁquemos a idempotência de B, sendo a de C
veriﬁcada de modo análogo. Temos assim que

B2 = 1
4
= 1
4
= 1
4
= 1
2

(I + A)2

(I2 + A2 + 2AI)

(2I + 2A)

(I + A),

onde na terceira igualdade acima foi utilizado o fato de I2 = I, A2 = I e AI = A. Quanto
a BC, temos

(I + A)

1
2

(I − A)

BC = 1
2
= 1
4
= 1
4
= O.

(I2 − A2)

(I2 − I2)

Exemplo 1.8. Uma área equivalente a 85 km2 de certo país é ocupada de forma comercial
(62%), industrial (23%) e residencial (15%). Vamos assumir que as probabilidades de
transição de um setor para o outro a cada 5 anos são dadas pela matriz

22

Note que as entradas de T indicam a probabilidade de uma determinada passar a ser
utilizada por outro setor ou não ser alterada, num período de 5 anos. Por exemplo,
a entrada t11 aponta que há 72% de chance de uma área comercial continuar sendo
comercial após 5 anos, enquanto que t23 indica que existe apenas 5% de chance de uma
área industrial passar a ser utilizada de forma residencial após o mesmo período. Note
também que a soma das entradas de qualquer uma das colunas de T é igual a 1 e nenhuma
das entradas é negativa. Uma tal matriz é denominada matriz estocástica. Daí, seja

A =









62
23
15

a matriz das porcentagens de cada área ocupada. Para então determinar quais serão as
porcentagens de ocupação após cinco anos, basta então calcular TA, isto é

TA =





0.72 0.06 0.03
0.19 0.91 0.05
0.09 0.03 0.92









62
23
15





 =





 .

46.47
33.46
20.07

Temos, portanto, que após 5 anos, as áreas de ocupação comercial, industrial e residencial
serão, respectivamente, 46,47%, 33,46% e 20,07%. Como a matriz T é utilizada em cada
intervalo de 5 anos, podemos determinar as porcentagens de ocupação em cada um desses
intervalos, apenas utilizando a matriz de ocupações atual e a matriz T. Por exemplo, se
quiséssemos saber qual seriam as porcentagens de ocupação em 10 anos, bastaria calcular
T · TA = T2A, em 15 anos, T3A, e assim sucessivamente. Um tal processo é chamado
processo de Markov 2. Em 50 anos, isto é, 10 intervalos de 5 anos, por exemplo, as
áreas de ocupação seriam dadas pela matriz

T10A =





0.17 0.16 0.12
0.52 0.60 0.35
0.31 0.24 0.53









62
23
15





 =





 ,

16.02
51.29
32.69

o que nos permite concluir que a área predominante após esse período será a industrial,
com 51,29% do território. Perceba que a soma das entradas da matriz T10A continua
sendo igual a 100%.

1.1.3 Matriz Transposta

Apresentaremos agora um dos tipos de matrizes que possuem muitas aplicações, tanto

na Matemática, como em outros campos de estudo.

Deﬁnição 1.5. Dada uma matriz A = [aij]m×n
de A é tal que

, diz-se que a matriz transposta AT

tal que aji = aij, para todo i e j.

AT = [aji]n×m ,

2Andrei Andreevich Markov foi um talentoso matemático russo, reconhecido pelos seus trabalhos em

Teoria dos Números, Análise e Teoria das Probabilidades [8].

23

De modo resumido, a trasposta de uma matriz tem suas linhas transformadas em
colunas, o que por sua vez faz com que as colunas passem a ser linhas. Vejamos a seguir
alguns exemplos esclarecedores a respeito deste fato:

• A =





1
2
4


 ⇒ AT = (cid:2) 1 2 4 (cid:3)

• B =

• C =

(cid:21)

(cid:20) 10 17
−5 12





1
1 1
2 3
4
4 9 16

⇒ BT =

(cid:21)

(cid:20) 10 −5
12
17


 ⇒ CT =









4
1 2
1 3
9
1 4 16

• D =

(cid:20) a b

c d
e f g h

(cid:21)

⇒ DT =







a e
b f
c g
d h







Observando o segundo e o terceiro exemplo acima, pode-se imaginar que a diagonal
principal de uma matriz quadrada e a diagonal principal de sua transposta são iguais.
De fato, isso ocorre diretamente da deﬁnição, uma vez que que aii = aii, quando i = j.

Vamos agora tomar conhecimento das principais propriedades das matrizes transpos-
tas. Para tanto, consideremos duas matrizes A, B e uma constante real arbitrária c.
Daí:

• (AT )T = A

Decorre diretamente da deﬁnição, pois se aij são as entradas de A, então aji serão
as entradas de AT e, portanto, aij serão as entradas de (AT )T .

• (A + B)T = AT + BT

Tomemos, A + B = C = [cij]m×n
AT = [aji]n×n

e BT = (cid:2)bji

(cid:3)

n×n

, segue que

e (A + B)T = CT = [cji]n×m

. Daí, como

cji = cij = aij + bij = aji + bji,

e portanto, temos CT = AT + BT .

• (cA)T = cAT

Seja (cA)T = (cid:2)pji

(cid:3)

n×m

. Daí, vem que

pji = caij = caji,

donde segue o resultado.

• (AB)T = BT AT

Tomemos AB = C = [cik]m×p
Deﬁnição 1.3 que

e (AB)T = CT = [cki]p×m

. Assim, decorre da

24

cki = cik = ai1b1k + · · · + ainbnk
= b1kai1 + · · · + bnkain
= bk1a1i + · · · + bknani,

o que prova o resultado desejado.

Quanto a esta última propriedade, vejamos um exemplo numérico para dar ênfase à

compreensão. Consideremos assim as matrizes

A =





1
0
4 11

3
5
2 −7
0





B =





8 −9
4
3
3
5





Daí, temos que

AB =





42

18
−29 −13
8

65

Por outro lado, temos que

AT =





1
3
5 −7

0
4
2 11
0





que por sua vez, nos fornece


 ⇒ (AB)T =

(cid:20) 42 −29 65
8
18 −13

(cid:21)

.

(cid:20)

BT =

(cid:21)

8 3 5
−9 4 3

BT AT =

(cid:20) 42 −29 65
8
18 −13

(cid:21)

Voltando agora à observação feita sobre a transposta de uma matriz quadrada pre-
servar a diagonal principal, pode-se então indagar: “existem matrizes cuja trasposta é
exatamente igual à matriz original?” A resposta para essa pergunta é “sim”, e tais matri-
zes são chamadas de simétricas.

Deﬁnição 1.6. Seja A uma matriz quadrada de ordem n. Diz-se que A é uma matriz
simétrica quando AT = A, e nesse caso tem-se aij = aji, para todo i e j. Já se for
AT = −A, então diz-se que A é antissimétrica, e portanto aij = −aji.

Vejamos alguns exemplos de matrizes simétricas e antissimétricas.









1 2 3
2 4 5
3 5 6







−17 26 12 74
26 34
3 17
8 13
3
12
74 17 13 10







Matrizes simétricas









b
c d e
a
b
l m n f
c m k o g
j h
o
d n
g h i
f
e









25





2 3
0
−2
0 5
−3 −5 0











0 −26 −12 −74
0 −3 −17
26
0 −13
3
12
0
13
17
74







Matrizes antissimétricas









c

d e
b
0
0 m n f
−b
o g
−c −m 0
0 h
−d −n −o
−e −f −g −h 0









Note que a diagonal principal de uma matriz antissimétrica de números reais possui
todas suas entradas iguais a zero. Tal fato deriva diretamente da deﬁnição, pois quando
i = j, temos aii = −aii ⇒ 2aii = 0 ⇒ aii = 0.
Teorema 1.1. Toda matriz quadrada A pode ser escrita como soma de uma matriz
simétrica S com uma matriz antissimétrica S(cid:48).
Demonstração: Provemos, inicialmente, que as matrizes A + AT e A − AT são simétrica
e antissimétrica, respectivamente. Com efeito, temos que

(A + AT )T = AT + (AT )T = AT + A = A + AT ,

ou seja, A + AT é simétrica de acordo com a deﬁnição. A veriﬁcação para a antissimetria
de A − AT é realizada de modo análogo. Segue então que

2A = A + A = A + AT − AT + A = (A + AT ) + (A − AT ) = S + S(cid:48),

o que equivale a

onde S = A + AT e S(cid:48) = A − AT .

A =

1
2

(S + S(cid:48)),

Exemplo 1.9. Uma certa companhia aérea faz voos para as cidades A, B e C, de acordo
com o seguinte diagrama:

Figura 1.1: Rotas de viagem ABC

Fonte: O autor

As setas ←→ indicam que há voos de uma cidade para a outra e vice-versa. Podemos
, seguindo a condição de que vij = 1, se houverem

assim criar uma matriz V = [vij]3×3
voos da cidade i para a cidade j e vij = 0, caso contrário. Temos assim que

26

Daí, pode-se observar que as entradas de V representam a quantidade de maneiras
de chegar de uma cidade a outra com apenas um voo. Pode-se notar também que a
diagonal principal de V possui todas as entradas iguais a zero porque de acordo com a
Figura 1.1, não são permitidos voos de uma cidade para ela mesma. Ora, mas isto pode
ser constatado apenas olhando o diagrama da Figura 1.1. Suponhamos então que a rota
A ←→ C esteja sob forte tempestade. Quantos voos serão necesárias para chegar em
C, partindo de A? Nesse caso, obviamente serão realizados dois voos, sendo um de A
para B e outro de B para C. Até então, não haveria necessidade nenhuma de construir a
matriz V. Porém, sabe-se que as rotas de avião não funcionam apenas em três cidades,
mas sim em várias, inclusive de países diferentes. Dessa forma, consideremos agora que
essa mesma companhia aérea atenda a cinco cidades A1, A2, A3, A4 e A5, conforme o
diagrama a seguir:

Figura 1.2: Rotas de viagem ABCDE

Fonte: O autor

Vejamos que neste caso, há mais cidades e mais rotas, logo existem mais combinações
de voos partindo de uma cidade para outra. É possível determinar visualmente quantas
rotas de 2 voos existem para chegar em A1, partindo de A3, ou quantas rotas de 3 voos
permitem partir de A4 e chegar em A1. Porém, pensando matematicamente, vamos criar,
de modo análogo à matriz V, uma matriz W = [wij]5×5

, de modo que

Agora observemos um fato interessante: note que se multiplicarmos, por exemplo, w21
por w13, obtemos 1 como resultado, isto é, existe uma rota de dois voos que vai de A2
para A3 passando por A1. Já os produtos w22w23, w23w33 e w24w43 são todos iguais a
zero, isto porque não existem rotas de dois voos indo de A2 para A3 passando por A2,
nem de A2 para A3 passando por A3 (nesse caso, se fosse possível, seria um voo e não
dois como desejado), nem de A2 para A3 passando por A4. Já w25w53 = 1, pois existe
uma rota de dois voos que vai de A2 para A3 passando por A5. Generalizando então esta
ideia, temos que o produto wijwjk nos fornece o número de rotas de dois voos que vão

27

de uma cidade Ai para uma cidade Ak passando pela cidade Aj. Logo a soma desses
produtos nos fornece o número total de rotas de dois voos que vão de uma cidade Ai
para uma cidade Aj. Com sorte, essa ideia coincide com a multiplicação de matrizes (e.g.
w21w13 + w22w23 + w23w33 + w24w43 + w25w53 = 2 é o número total de rotas de dois voos
que vão de A2 para A3). Temos assim que a matriz simétrica

W2 =









1 1 1 2 1
1 1 2 1 1
1 2 1 1 1
2 1 1 1 1
1 1 1 1 4









fornece o número exato de todas as rotas de dois voos que vão de uma cidade Ai para
uma cidade Aj, onde i e j variam de 1 a 5. Seguindo esse pensamento, temos que a
matriz (também simétrica) que fornece o número de rotas de 3 voos de uma cidade Ai
para uma cidade Aj é dada por

W3 =









2 3 2 2 5
2 2 2 3 5
3 2 2 2 5
2 2 3 2 5
5 5 5 5 4









.

Se quisermos saber quantas rotas existem com no máximo n, n ∈ N, voos para ir de uma
cidade a outra, basta então somar as matrizes W, W2, W3, . . . , Wn.

Exemplo 1.10. Note que, do exemplo anterior, temos

W =









0 0 1 0 1
1 0 0 0 1
0 0 0 1 1
0 1 0 0 1
1 1 1 1 0









(cid:54)=









0 1 0 0 1
0 0 0 1 1
1 0 0 0 1
0 0 1 0 1
1 1 1 1 0









= WT ,

porém

W2 =

W4 =

















1 1 1 2 1
1 1 2 1 1
1 2 1 1 1
2 1 1 1 1
1 1 1 1 4

8 7 7 7
9
7 8 7 7
9
7 7 8 7
9
9
7 7 7 8
9 9 9 9 20

















= (WT )2

= (WT )4

e assim por diante. Observando que as potências obtidas são matrizes simétricas, podemos
concluir que

Wk é simétrica (cid:59) W é simétrica.

28

Exemplo 1.11. Note como identiﬁcar um determinado número de rotas de n voos de
uma cidade a outra pode não ser um trabalho tão simples quando o número de cidades
aumenta.

Figura 1.3: Rotas de viagem A1 . . . A12

Fonte: O autor

Porém, se considermos a matriz das rotas deste caso abaixo

podemos determinar, por exemplo, a matriz que informa quantas rotas de 4 voos existem

29

entre a cidade Ai e a cidade Aj, com i, j = 1, 2, . . . , 12, como podemos ver a seguir:























Z4 =

1
7
7

6 0 5
2 2 3 1 1
1 6 1
4 5 3 1 0
6 6 4 2 0
5 2 8
2 7 1 20 6 7 9 1 1
5 0 7
1 5 0
3 0 2
0 0 1
1 1 0
2 1 6
1 1 0
2 0 6

6
2
9
3
5 3 3 2 1 11
2
2 6 5 3 1
4
1 1 3 1 2
3
1 1 0 2 0
1 1 4 0 2
1
5 5 2 4 0 10
1
1 1 3 0 1
9
5 3 1 2 0

1
8
1
0
6
2
5
1























0
2
3
1
5
3
11 1
5
0
0
5
1
1
1
0
0
4
5
1
0
3
5
0

Os exemplos 1.9 a 1.11 estão diretamente ligados à Teoria dos Grafos. A demonstração
do caso geral de tais exemplos, bem como os detalhes adicionais sobre as matrizes de
adjacência podem ser encontrados em [7] e [18]. Vale ressaltar que o número de voos não
necessariamente implica maior ou menor distância, pois para isso, é necessário que hajam
valores para as distâncias. Uma das maneiras de determinar a menor distância entre duas
cidades é utilizando o Algoritmo de Dijkstra 3, o qual pode ser visto em [62].

1.1.4 Matriz Inversa

Como havíamos mencionado no ﬁnal da seção 1.1, a divisão de matrizes não está
deﬁnida devido ao fato de que nem toda matriz é invertível e, além disso, para que uma
matriz possa admitir sua inversa, quando é possível, é necessário que ela seja uma matriz
quadrada. Formalizando a ideia, temos a seguinte

Deﬁnição 1.7. Dada uma matriz A = [aij]n×n, se existir uma matriz B, tal que AB =
In×n = BA, então essa matriz é chamada de inversa de A. Nesse caso, diz-se que A é
não singular.

Exemplo 1.12. Seguem abaixo alguns exemplos de matrizes e suas respectivas inversas

A =

(cid:21)

(cid:20)1 2
3 4

B =



−1 1

3
2 0 13
6

−1 2





C =

1
−1
1
1 −2

0 −1 −1
0 −3 −1
3 −2 −1
0 −1















A−1 = − 1
2


B−1 = 1
13



(cid:21)
(cid:20) 4 −2
1
−3

−26
0
−25 −3
4

13
19
1 −2





C−1 = − 1
2







−11 1

4

6
4 0 −2 −2
10 0 −4 −6
12

8

−19 1







3Edsger W. Dijkstra (1930-2002), vencedor do Prêmio Turing em 1972, é bem conhecido por suas
contribuições à ciência computacional [24]. Dijkstra nasceu em Rotterdam (Holanda) e, em 1959, ele
publicou um artigo de três páginas “A note on two problems in connexion with graphs”, o célebre e
extremamaente simples algoritmo para encontrar o caminho mais curto em um gráﬁco, agora conhecido
como Algoritmo de Dijkstra [4].

30

Pode-se notar que a Deﬁnição 1.5 diz apenas quando a matriz possui ou não uma
inversa, porém não informa como encontrá-la. Existem vários métodos para determinar
a inversa de uma matriz, entre eles estão aqueles em que se utiliza sistemas lineares,
determinantes e eliminação gaussiana. Este último nos fornece um algoritmo prático
para determinar a inversa de uma matriz dada, o qual podemos utilizar, mesmo sem ter
estudado a eliminação gaussiana.
Dada uma matriz quadrada

A =








a11 a12
a21 a22
...
...
an1 an2








a1n
· · ·
a2n
· · ·
...
...
· · · ann

o algoritmo consiste em adicionar uma matriz identidade de mesma ordem de A à direita
desta, da seguinte forma

A =








a11 a12
a21 a22
...
...
an1 an2

a1n 1 0 · · ·
· · ·
a2n 0 1 · · ·
· · ·
...
...
...
...
...
· · · ann 0 0 · · ·


0
0


...


1

e, a partir daí, realizando-se somas e\ou subtrações de múltiplos das linhas de A às linhas
da mesma, chega-se a matriz

A =








1 0 · · ·
0 1 · · ·
...
...
...
0 0 · · ·

0 y11 y12
0 y21 y22
...
...
...
1 yn1 yn2

· · ·
· · ·
...
· · ·








y1n
y2n
...
ynn

,

onde Y = [yij]n×n
é a inversa de A. Resumindo, começa-se com A do lado esquerdo e I do
lado direito, ao passo que no ﬁnal do algoritmo deve-se ter I do lado e esquerdo e a matriz
obtida do lado direito é a inversa de A, quando esta existir. O próximo exemplo ilustra
como se aplica o algoritmo. Para tanto, utilizaremos expressões como L1 + 2L2 para
indicar que somamos as entradas da linha 1 com o dobro das entradas da linha 2 e, nesse
caso, substituímos L1 por L1 + 2L2. Expressões dessa forma serão chamadas, de agora
em diante, de combinações lineares. Estas serão estudadas com mais profundidade na
Seção 1.3.

Exemplo 1.13. Seja A a matriz do Exemplo 1.12. Vamos determinar A−1 seguindo o
processo descrito acima.

(cid:20)1 2 1 0
3 4 0 1

(cid:21)

L2−3L1
−−−−→

(cid:20)1
1 0
0 −2 −3 1

2

(cid:21)

(cid:20)1
1 0
0 −2 −3 1

2

(cid:21) L1+L2−−−−→

(cid:20)1
0 −2 1
0 −2 −3 1

(cid:21)

(cid:20)1
0 −2 1
0 −2 −3 1

(cid:21)

(cid:34)

1
−
L2
2
−−−→

31

1 0 −2
3

0 1

(cid:35)
1
2 − 1

2

Portanto, temos que

A−1 =

(cid:20)−2
3

(cid:21)
1
2 − 1

2

= −

(cid:21)
(cid:20) 4 −2
1
−3

1
2

Agora, da mesma forma, determinemos B−1.





−1 1

3 1 0 0
2 0 13 0 1 0
6 0 0 1

−1 2









L1+L2−−−−→
L2+2L1
−−−−→
L3−L1−−−−→

1 1 16
0 2 19
0 1

1 1 0
2 1 0
3 −1 0 1









1 1 16
0 2 19
0 1

1 1 0
2 1 0
3 −1 0 1









L1−L3−−−−→
L2−L3−−−−→
1
L3
3
−−→

1 0 13
0 1 16
0 1
3





2 1 −1
3 1 −1
1
3 0
3

1 − 1





1 0 13
0 1 16
0 1
3

1 − 1





2 1 −1
3 1 −1
1
3 0
3





2
13
1 0
3
0 1
16
3 − 4
0 0 − 13
3 − 1

1 −1
1 −1
2
3

3





1
L3−
L2
3
−−−−−→





1 0
13
2
3
16
0 1
3 − 1
3 − 4
0 0 − 13

1 −1
1 −1
2
3

3







1 0
0 1 16
1
0 0

0 −2
3
4
13


1
0
1 −1

13 − 2
1

13



L1+3L3
−−−−→



3
−
L3
13
−−−−→


L2−16L3
−−−−−→






1 0
0 1 16
1
0 0

0 −2
3
4
13


1
0
1 −1

13 − 2
1

13

1 0 0 −2
0 1 0 − 25
4
0 0 1
13

0
13 − 3

13
1

13 − 2

13


1

19

13

Logo, decorre que

B−1 =






−2
0
13 − 3
− 25
4
13

13
1

13 − 2

13


1

19
 =
13





1
13

−26
0
−25 −3
4

13
19
1 −2





Embora seja um processo prático para matrizes de pequena ordem, tal algoritmo
depende das combinações entre as linhas, o que pode se tornar uma tarefa bastante
dispendiosa, à medida em que o algoritmo for aplicado a matrizes de ordens maiores.
Quando estudarmos os determinantes no Capítulo 2, iremos conhecer uma outra maneira
de determinar a inversa de uma matriz.

Visto os exemplos de matrizes invertíveis e o algoritmo para determinar suas inversas,
devemos ter em mente o seguinte resultado, o qual possui grande utilidade na resolução
de problemas e demonstrações de outros teoremas relacionados a martizes. Tal resultado
diz que a inversa de uma matriz é única. Para melhor apresentar o resultado, enunciemos
o seguinte

Teorema 1.2. Seja A uma matriz quadrada de ordem n. Se A é invertível, então sua
inversa é única.

32

Demonstração. Seja A invertível e seja B sua inversa. Suponhamos que exista uma
matriz quadrada C, de mesma ordem que A, tal que AC = I = CA. Daí, temos que

B = BI = B(AC) = (BA)C = IC = C,

provando assim a unicidade de B.

Vejamos a seguir as principais propriedades das matrizes inversas. Para tanto, consi-

deremos duas matrizes não singulares A e B. Daí,

• (A−1)−1 = A

Tomemos X = A−1. Assim, temos que

XX−1 = I = A−1A ⇒ A−1X−1 = A−1A.

Em seguida, multiplicando (à esqueda) ambos os membros da última igualdade
acima, obtemos

AA−1X−1 = AA−1A ⇒ IX−1 = IA ⇒ X−1 = A,

isto é, A = X−1 = (A−1)−1, o que acaba por provar o resultado procurado.

• AB é não-singular.

Tomemos Y = B−1A−1. Como a multiplicação de matrizes é associativa, segue que

(AB)X = (AB)B−1A−1 = A(BB−1)A−1 = AIA−1 = AA−1 = I.

Segue, por deﬁnição, que X = (AB)−1 e, portanto, AB é não-singular.

• (AB)−1 = B−1A−1

Já provada na demonstração da propriedade acima.

• (A1A2 · · · Ak)−1 = A−1
k

· · · A−1

2 A−1
1

Basta aplicar a propriedade acima k − 1 vezes seguidas.

• (A−1)T = (AT )−1

Seja X = A−1. Assim

I = XX−1 ⇒ I = IT = (XX−1)T = (X−1)T XT = AT XT .

Temos então, por deﬁnição, que XT = (AT )−1. Por outro lado, como

X = A−1 ⇒ XT = (A−1)T ,

podemos concluir que

como queríamos demonstrar.

(A−1)T = XT = (AT )−1,

33

Tendo em vista as propriedades acima, é válida a dúvida sobre a veracidade ou in-
veracidade de (A + B)−1 = A−1 + B−1. Vejamos um contraexemplo para esse caso.
Sejam

A =

Temos que, por um lado

enquanto que

(cid:21)

(cid:20)1 2
3 4

(cid:21)

(cid:20)−1 5
0 4

.

B =

(cid:34)− 8
21
1
7

(cid:35)

1
3
0

(A + B)−1 =

A−1 + B−1 =





9
4

−3
2 − 1

4

3



 .

Os casos em que a igualdade é verdadeira existem, porém não os abordaremos aqui, a
ﬁm de evitar digressões. Uma abordagem de tais casos pode ser vista em [42].

Em relação à lei do cancelamento para a multiplicação, isto é, AB = AC ⇒ B = C,

esta também não é válida de modo geral. Por exemplo, se

A =

(cid:21)
(cid:20)1 0 0
0 1 0

B =





C =





1 2
3 4
6 8





1 2
3 4
5 6



 ,

segue que

porém B (cid:54)= C.

AB =

(cid:21)

(cid:20)1 2
3 4

= AC,

1.2 Matrizes de blocos

Esta seção será voltada ao estudo das submatrizes e matrizes de blocos, onde estas
não só facilitam o processo de multiplicar matrizes, bem como possuem aplicabilidade em
áreas da informática, como por exemplo a memória de dados.

Uma submatriz de uma matriz A é uma matriz obtida através da eliminação de

linhas e\ou colunas de A. Consideremos, a título de exemplo, a matriz



−1

3
4 10
0
1


6
7
19
3


5 −3 −13

2
7
9

A =





As matrizes

B =

(cid:21)

(cid:20)−1

7
0 −3

C =

(cid:21)

(cid:20)3 6
9 2

D = (cid:2)10 3 19(cid:3)

são exemplos de submatrizes de A, onde

34

• B é obtida eliminando-se: 2a e 4a linha, 2a e 4a coluna de A.

• C é obtida eliminando-se: 2a e 3a linha, 1a e 3a coluna de A.

• D é obtida eliminando-se as linhas 1, 3 e 4 e a coluna 1 de A.

Dada uma matriz A, dizemos que esta é uma matriz de blocos ou matriz partici-
onada quando é dividida em submatrizes por linhas horizontais e verticais , onde estas
ﬁcam entre as linhas e colunas de A. Temos a seguir alguns exemplos

(cid:20) 1 2 3 4
5 6 7 8

(cid:21)





−5 7 12

0 1
13 0

4
6 −5
7 −8











a b
f g
k
p q

c d e
j
h i
l m n o
t

s

r







Exemplo 1.14. Sejam A e B as matrizes de blocos a seguir:





A =

5 −1 2 3 4
3 1 0 6
8
1 −3 4 5 9



 =

(cid:20) A11 A12
A21 A22

(cid:21)

,

B =









3 −5
1
8
7
−5
0
2
4
−1









(cid:21)

=

(cid:20) B11
B21

Calculando AB, obtemos

AB =

(cid:20)A11 A12
A21 A22

(cid:21) (cid:20)B11
B21

(cid:21)

=

(cid:20)A11B11 + A12B21
A21B11 + A22B22

(cid:21)

onde

A11B11 =

A12B21 =

(cid:20)5 −1 2
3 1
8

(cid:20)3 −4
6
0

A21B11 = (cid:2)1 −3 4(cid:3)

A22B22 =

(cid:2)5 9(cid:3)

de modo que



(cid:21)



3 −5
1
8
−5
7
(cid:21) (cid:20) 2

(cid:21)

0
−1 4





3 −5
1
8
−5
7
(cid:20) 2 0
−1 4

(cid:21)


 =

(cid:21)
(cid:20)−3 −12
43 −30

=

(cid:21)
(cid:20) 10 −16
24
−6


 = (cid:2)−41 20(cid:3)

=

(cid:2)1 36(cid:3)

A11B11 + A12B21 =

(cid:21)
(cid:20) 7 −28
37 −6

,

A21B11 + A22B22 = (cid:2)−40 56(cid:3) .

Portanto

com

AB =





7 −28
37 −6
56

−40



 =

(cid:20) X1
X2

(cid:21)

,

35

X1 =

(cid:21)
(cid:20) 7 −28
37 −6

,

X2 = (cid:2)−40 56(cid:3)

A partir da deﬁnição de matrizes de blocos e do exemplo acima, podemos observar que
uma matriz A = [aij]m×n

pode ser escrita da forma

A = (cid:2)col1(A) col2(A)

. . . coln(A)(cid:3)

ou

A =








lin1(A)
lin2(A)
...
linm(A)








,

onde lini(A) representa a i-ésima linha de A e col1(A) a j-ésima coluna de A. Dessa
forma, dadas duas matrizes X = [xij]m×n

e Y = [yjk]n×p

e fazendo

X = (cid:2)col1(X) col2(X)

. . . coln(X)(cid:3)








Y =








lin1(Y)
lin2(Y)
...
linm(Y)

,

podemos escrever, de modo geral

XY = (cid:2)col1(X)lin1(Y) + col2(X)lin2(Y) + . . . + coln(X)linn(Y)(cid:3)

Exemplo 1.15. Quando uma sonda espacial é lançada, talvez sejam necessárias algu-
mas correções para colocá-la em uma trajetória precisa. A Rádio Telemetria4 fornece
uma sequência de matrizes X1, . . . , Xk, a qual informa, em momentos diferentes, uma
comparação entre a posição atual da sonda e a posição planejada. Tomando Ak como
sendo a matriz de blocos (cid:2) X1
é calculada à
medida que os dados do radar são analisados, de modo que

(cid:3), segue que a matriz Gk = AkAT

. . . Xk

k

Gk = (cid:2)X1

. . . Xk

(cid:3)


 = (cid:2)X2


1 + . . . + X2
k

(cid:3)






X1
...
Xk

Se uma nova matriz Xk+1 aparecer na sequência, então uma outra matriz Gk+1 =

4A Telemetria é uma tecnologia que permite medir e monitorar dados de maneira remota. Ela
normalmente se refere à direção a qual uma determinada informação está direcionada, ou seja, a direção
do sensor até o sistema de interrogação ou sistema de registro de dados. A Telemetria pode ser deﬁnida
como uma subclasse das Telecomunicações, uma maneira mais complexa de trocar informações como
internet, chamadas telefônicas ou transmissão de vídeo [36].

36

k+1

deve ser calculada. Como as matrizes Xi são alcançadas em alta velocidade, a
Ak+1AT
carga computacional para realizar o cálculo de uma matriz Gi+1 pode ser severo. Porém,
a multiplicação de matrizes em blocos reduz extremamente uma tal carga, visto que

Gk+1 = (cid:2)X1

. . . Xk Xk+1

(cid:3)















X1
...
Xk
Xk+1

= (cid:2)X2

1 + . . . + X2

k + X2

k+1

(cid:3)

ou seja, ao invés da matriz Gk+1 ser calculada do início, o computador apenas adiciona
uma nova coluna à matriz Gk, a qual é preenchida pela matriz X2

.

k+1

As matrizes em blocos possuem grande aplicabilidade computacional. De acordo com
GOLUB [27], algoritmos expressos em blocos são especiﬁcamente ricos em multiplicação
de matrizes, operação esta preferível em muitos ambientes de computação de alta perfor-
mance. Um dos conceitos ligados ao desempenho de um computador, mais precisamente
à memória deste, é a inversão de matrizes, pois este, segundo COSME [14] é um processo
que pode exceder a capacidade de memória do computador, quando trata-se de uma
matriz de ordem extremamente alta, como ocorre em problemas de regressão5.

Vejamos a seguir como a maneira de determinar a inversa de uma matriz de blocos
2 × 2 pode não ser uma tarefa tão simples. Consideremos assim, duas matrizes de blocos
A e B, ambas de ordem n

A =

(cid:21)

(cid:20)A11 A12
A21 A22

B =

(cid:21)

(cid:20)B11 B12
B21 B22

,

com A11 e A22 quadradas e não-singulares de ordem k e n − k, respectivamente. Aﬁm
de que seja B a inversa de A, devemos calcular AB, obtendo

(cid:20)A11 A12
A21 A22

(cid:21) (cid:20)B11 B12
B21 B22

(cid:21)

=

(cid:20) Ik×k
O(n−k)×k

Ok×(n−k)
I(n−k)×(n−k)

(cid:21)

.

A igualdade acima nos fornece as quatro equações a seguir:

A11B11 + A12B21 = Ik×k
A11B12 + A12B22 = Ok×(n−k)
A21B11 + A22B21 = O(n−k)×k
A21B12 + A22B22 = I(n−k)×(n−k).

Multiplicando 1.2 por A−1
11

à esquerda, obtemos

A−1

11 A11B12 + A−1
B12 + A−1

11 Ok×(n−k)

11 A12B22 = A−1
11 A12B22 = Ok×(n−k)
B12 = −A−1

11 A12B22

(1.1)
(1.2)
(1.3)
(1.4)

(1.5)

5Problemas de regressão são estudados pela Análise de Regressão, uma das técnicas estatísticas mais
amplamente usadas para investigar e modelar problemas que envolvem várias variáveis, onde uma delas
é a variável de interesse e as demais são variáveis preditoras [47].

37

Analogamente, multiplicando 1.3 por A−1
22

à esquerda, obtemos

B21 = −A−1

22 A21B11.

Agora, substituindo 1.6 em 1.1,

A11B11 − A12A−1
(A11 − A12A−1

22 A21B11 = Ik×k
22 A21)B11 = Ik×k

B11 = (A11 − A12A−1

22 A21)−1.

Da mesma forma, substituindo 1.5 em 1.4, obtemos

B22 = (A22 − A21A−1

11 A12)−1.

(1.6)

(1.7)

(1.8)

Antes de darmos prosseguimento, observemos que B depende da invertibilidade das ma-
trizes

K = A22 − A21A−1

11 A12,

L = A11 − A12A−1

22 A21,

as quais são denominadas complementos de Schur 6 de A11 e A22, respectivamente.
Suponhamos então que K e L são não-singulares. Daí, substituindo 1.8 em 1.5 e 1.7 em
1.6, obtemos

B12 = −A−1
B21 = −A−1

11 A12(A22 − A21A−1
22 A21(A11 − A12A−1

11 A12)−1
22 A21)−1

,

o que nos leva a

(cid:34)

B1 =

(A11 − A12A−1
22 A21(A11 − A12A−1

22 A21)−1

22 A21)−1

−A−1

−A−1

11 A12(A22 − A21A−1
(A22 − A21A−1

11 A12)−1

11 A12)−1

(cid:35)

.

Em termos dos complementos de Schur

B1 =

(cid:34)

L−1
22 A21L−1

−A−1

(cid:35)

−A−1

11 A12K−1
K−1.

Prosseguindo de maneira análoga para a igualdade abaixo

(cid:20)B11 B12
B21 B22

(cid:21) (cid:20)A11 A12
A21 A22

(cid:21)

=

(cid:20) Ik×k
O(n−k)×k

Ok×(n−k)
I(n−k)×(n−k)

(cid:21)

.

chegamos a

B2 =

(cid:34)

L−1
−KA21A−1
11

−L−1A12A−1
22
K−1

(cid:35)

.

Resta agora veriﬁcar se B1 = B2, para ﬁnalmente comprovar que B = A−1. Ora, tanto
a diagonal principal de B1 como a de B2 são iguais. Basta então veriﬁcar se

−A−1

11 A12K−1 = −L−1A12A−1
22

e − A−1

22 A21L−1 = −KA21A−1
11

6Este nome é uma homenagem ao matemático alemão Issai Schur (1875-1941), o qual foi o primeiro
a estudar matrizes deste tipo [46]. Suas descobertas matemáticas, particularmente em Álgebra, e sua
história de vida, que inclui o terrível tratamento sofrido por ele na Alemanha nazista entre 1933 e 1939,
são bem conhecidas [39].

38

Façamos o primeiro caso, sendo o segundo inteiramente análogo. Notemos que, por um
lado

(−A−1

11 A12K−1)−1 = −KA−1

12 A11
(A22 − A21A−1

=
= −A22A−1
= −A22A−1

12 A11 + A21A−1
12 A11 + A21,

11 A12)A−1

12 A11
11 A12A−1

12 A11

enquanto que

(−L−1A12A−1

22 )−1 = −A22A−1
= −A22A−1
= −A22A−1
= −A22A−1

12 L1
12 (A11 − A12A−1
12 A11 + A22A−1
12 A11 + A21.

22 A21)
12 A12A−1

22 A21

Com isso, temos que

(−A−1

11 A12K−1)−1 = −A22A−1

12 A11 + A21 = (−L−1A12A−1

22 )−1,

o que equivale a

Da mesma forma,

−A−1

11 A12K−1 = −L−1A12A−1
22 .

−A−1

22 A21L−1 = −KA21A−1
11 .

Logo, podemos concluir que B1 = B2 e, portanto, B = A−1.

Podemos assim enunciar o seguinte teorema.

Teorema 1.3. Seja A uma matriz quadrada de ordem n escrita como matriz de blocos
da forma

A =

(cid:21)

(cid:20)A11 A12
A21 A22

.

com A11 e A22 não-singulares. Sejam ainda

K = A22 − A21A−1

11 A12,

L = A11 − A12A−1

22 A21,

os complementos de Schur de A11 e A22, respectivamente. Se K e L forem invertíveis,
então A também o é, onde

A−1 =

(cid:34)

L−1
22 A21L−1

−A−1

−A−1

11 A12K−1
K−1

(cid:35)

(cid:34)

=

L−1
−KA21A−1
11

−L−1A12A−1
22
K−1

(cid:35)

.

Exemplo 1.16. Consideremos a matriz

Temos que

A =







1 2 1 0
3 4 2 1
1 4 4 3
5 1 2 1







.

39

A11 =

(cid:21)

(cid:20) 1 2
3 4

⇒ A−1

11 =

A22 =

(cid:21)

(cid:20) 4 3
2 1

⇒ A−1

22 =

(cid:34)

(cid:34) −2
3

1
2 − 1
2
3
− 1
2
2
1 −2

(cid:35)

(cid:35)

.

Daí, vem

K =

(cid:21)

(cid:20)4 3
2 1

(cid:20)1 4
5 1

−

L =

(cid:21)

(cid:20)1 2
3 4

(cid:20)1 0
2 1

−

(cid:21) (cid:34)

de modo que

(cid:21) (cid:34)−2
3

(cid:35) (cid:20)1 0
1
2 − 1
2 1
3
− 1
2
2
1 −2

2
(cid:35) (cid:20)1 4
5 1

(cid:21)

=

(cid:21)

=

2

(cid:35)
(cid:34) 2
4
2 − 7
3
(cid:35)
−6 5
2
3
−2

(cid:34)

⇒ K−1 =

(cid:21)

(cid:20)7
8
3 −4

1
26

⇒ L−1 =

(cid:21)
(cid:20)−6
5
−4 12

1
26

−A−1

11 A12K−1 = −

1
26

(cid:34)−2

(cid:35) (cid:20)1 0
1
2 − 1
2 1

3

(cid:21) (cid:20)7

8
3 −4

(cid:21)

= −

(cid:21)
(cid:20)3 −4
6
2

1
26

−A−1

22 A21L−1 = −

(cid:34)

1
26

3
− 1
2
2
1 −2

(cid:21) (cid:20)−6

(cid:21)
5
−4 12

= −

1
26

(cid:20)−40

29
46 −21

(cid:21)

.

2
(cid:35) (cid:20)1 4
5 1

Logo, temos que

A−1 = −







1
26

6 −5
4 −12

3 −4
2
6
29 −7 −8
4

46 −21 −3

−40







.

Se tivéssemos calculado a inversa de A diretamente, por exemplo, utilizando o algoritmo
descrito na seção anterior, teríamos encontrado

A−1 = −







1
26

6 −5
4 −12

3 −4
2
6
29 −7 −8
4







46 −21 −3

−40

A seção a seguir mostra uma aplicação interessante das matrizes, na qual poderemos

observar algumas das propriedades estudadas até aqui, utilizadas de maneira prática.

1.3 Matrizes elementares

As matrizes elementares estão diretamente ligadas ao algoritmo descrito na seção 1.2
utilizado para a inversão de matrizes. Além disso, permitem que algumas demonstrações
sejam realizadas sem a necessidade de envolver complicações notacionais e conceitos mais
avançados, como por exemplo a indução ﬁnita.

Do ponto de vista computacional, segundo POOLE [53], o uso de matriz elementares
não é uma boa ideia quando se trata de realizar operações elementares entre as linhas,
sendo preferível fazer isso de maneira direta, porém, elas são valiosas tanto na inversão

40

de matrizes, bem como para a resolução de sistemas lineares.

Deﬁnição 1.8. Uma matriz elementar E é uma matriz obtida através de uma das três
operações elementares entre linhas de uma matriz identidade:

• (Tipo 1) Troca da linha i pela linha j e vice-versa:

Li ←→ Lj.

• (Tipo 2) Multiplicação da linha i por uma constante real c (cid:54)= 0:

Li ←→ cLi.

• (Tipo 3) Substituição da linha i pela soma\subtração desta com um múltiplo da

linha j (combinação linear):

Li ←→ Li ± cLj.

Temos a seguir um exemplo de matriz Tipo 1.

EL1↔L2 =





0 1 0
1 0 0
0 0 1



 ,

a qual é obtida através da troca de posição entre a primeira e a segunda linha da matriz
, veremos que a mesma operação
I3×3. Se multiplicarmos uma matriz 3 × 3 por EL1↔L2
será realizada na referida matriz. Para comprovar isso, consideremos a matriz abaixo:

A =





c
a b
d e f
g h i





e então calculemos EL1↔L2A, de modo que

EL1↔L2A


0 1 0
1 0 0
0 0 1









a b
c
d e f
g h i









0 · a + 1 · d + 0 · g 0 · b + 1 · e + 0 · h 0 · c + 1 · f + 0 · i
1 · a + 0 · d + 0 · g 1 · b + 0 · e + 0 · h 1 · c + 0 · f + 0 · i
0 · a + 0 · d + 1 · g 0 · b + 0 · e + 1 · h 0 · c + 0 · f + 1 · i









d e f
c
a b
g h i





=

=

=

Como podemos observar, a primeira e a segunda coluna de A trocaram de posição. Essa
propriedade é importante, pois ela garante que a troca de linhas de uma matriz não a faz
deixar de ser matriz, apenas gera uma nova matriz.

41

As matrizes do Tipo 2 realizam a mesma operação pela qual são formadas, a uma

matriz que ela multiplica, como podemos ver a seguir com as matrizes









1 0 0
0 5 0
0 0 1

EL2↔5L2 =

por A

Multiplicando EL2↔5L2
EL2↔5L2A


1 0 0
0 5 0
0 0 1

a b
c
d e f
g h i

=













=

=





1 · a + 0 · d + 0 · g 1 · b + 0 · e + 0 · h 1 · c + 0 · f + 0 · i
0 · a + 5 · d + 0 · g 0 · b + 5 · e + 0 · h 0 · c + 5 · f + 0 · i
0 · a + 0 · d + 1 · g 0 · b + 0 · e + 1 · h 0 · c + 0 · f + 1 · i









a
c
b
5d 5e 5f
i
h
g



 .

Podemos então observar que a segunda linha de A foi multiplicada por 5, assim como em
EL2↔5L2

.

Consideremos agora a matriz de Tipo 3 a seguir

EL3↔L3−4L1 =

Multiplicando-a por A, obtemos





1 0 0
0 1 0
−4 0 1



 .

EL3↔L3−4L1A



1 0 0
0 1 0
−4 0 1







a b
c
d e f
g h i









1 · c + 0 · f + 0 · i
1 · b + 0 · e + 0 · h
1 · a + 0 · d + 0 · g
0 · c + 5 · f + 0 · i
0 · b + 5 · e + 0 · h
0 · a + 5 · d + 0 · g
−4 · a + 0 · d + 1 · g −4 · b + 0 · e + 1 · h −4 · c + 0 · f + 1 · i









a
d

b
e

g − 4a h − 4b





c
f
i − 4c

=

=

=

Mais uma vez, podemos observar que a operação pela qual EL3↔L3−4L1
realizada em A após a multiplicação.

foi obtida, é

Para generalizarmos a ideia, utilizaremos as matrizes de blocos, pois estas evitarão
uma sobrecarga notacional e, além disso, fará com que as ideias ﬁquem melhor organiza-

42

das. Vamos então escrever a matriz

In×n =








1 0 · · ·
0 1 · · ·
...
...
...
0 0 · · ·


0
0


...


1

sob a forma

I =








I11
I21
...
In1

e consideraremos uma matriz X qualquer n × p

X =








x11 x12
x21 x22
...
...
xn1 xn2















· · · x1n
· · · x2n
...
...
· · · xnp

a qual será escrita da forma

X = (cid:2)X11 X12

. . . X1p

(cid:3) .

Agora observemos que cada submatriz Ii1 é uma matriz linha 1 × n, enquanto que cada
submatriz X1k é uma matriz coluna n × 1, de modo que o produto Ii1X1k nos fornece,
pela deﬁnição de multiplicação de matrizes, uma matriz 1 × 1, a qual é [aik]. Em termos

Dessa forma, se trocarmos duas linhas de I (a primeira linha com alguma linha s de I,
por simplicidade), obteremos a matriz elementar abaixo

Ii1X1k = [xik] .

EL1↔Ls =

Daí, fazendo EL1↔LsA, obtemos























Is1
I21
...
I11
...
In1

EL1↔LsX =












Is1X11
I21X11
...
I11X11
...
In1X11

Is1X12
I21X12
...
I11X12
...
In1X12

· · ·
· · ·
...
· · ·
...
· · ·












Is1X1p
I21X1p
...
I11X1p
...
In1X1p

=












xs1 xs2
x21 x22
...
...
x11 x12
...
...
xn1 xn2












· · · xsp
· · · x2p
...
...
· · · x1p
...
...
· · · xnp

.

43

Logo, podemos comprovar que a mesma operação de EL1↔Ls
processo é análogo para quaisquer outra matriz de tipo 1.

foi realizada em A. O

Consideremos agora uma matriz de tipo 2 de ordem n e uma constante real c:

ELs↔cLs =

e em seguida calculemos ELs↔cLsX, obtendo












,












I11
I21
...
cIs1
...
In1







ELs↔cLsX =

x1p
x1p
...
cxsp
...
cxnp
o que mostra que a linha s de X foi multiplicada por c, assim como em ELi↔cLi

I11X1p
I21X1p
...
cIs1X1p
...
In1X1p

I11X11
I21X11
...
cIs1X11
...
In1X11

I11X12
I21X12
...
cIs1X12
...
In1X12

x11
x11
...
cxs1
...
cxn1

x12
x12
...
cxs2
...
cxn2

· · ·
· · ·
...
· · ·
...
· · ·

· · ·
· · ·
...
· · ·
...
· · ·




























=

,












.

Finalmente, seja ELs↔Ls±cLt

de blocos, tal que

uma matriz de tipo 3 e de ordem n, escrita como matriz

ELs↔Ls±cLt =

Daí, fazendo ELs↔Ls±cLtX, obtemos












I11
I21
...
Is1 ± cIt1
...
In1












.

ELs↔Ls±cLtX

I11X11
I21X11
...
(Is1 ± cIt1)X11
...
In1X11










I11X12
I21X12
...
(Is1 ± cIt1)X12
...
In1X12

· · ·
· · ·
...
· · ·
...
· · ·

I11X1p
I21X1p
...
(Is1 ± cIt1)X1p
...
In1X1p























x11
x21
...

x12
x22
...

xs1 ± cxt1 xs2 ± cxt2

...
xn1

...
xn2

x1p
x2p
...

· · ·
· · ·
...
· · · xsp ± cxtp
...
· · ·

...
xnp












.

=

=

Obtivemos acima, como resultado, uma matriz cuja linha s foi substituída pela soma\subtração

44

desta com um múltiplo de uma linha t qualquer de X. Portanto, podemos enunciar, tendo
em vista a discussão acima, o seguinte teorema:

Teorema 1.4. Sejam ELi↔Lj , ELi↔cLi e ELi↔Li±cLj matrizes elementares de ordem n e
X uma matriz de ordem n × p. Tem-se que

• ELi↔Lj X = XLi↔Lj
• ELi↔cLiX = XLi↔cLi
• ELi↔Li±cLj X = XLi↔Li±cLj .
Após a enunciação do teorema acima, vamos agora buscar, como prometido, o resul-
tado que valida o algoritmo supracitado para encontrar a inversa de uma matriz invertível.
Para tanto, necessitaremos dos lemas a seguir:

Lema 1.1. Toda matriz elementar é invertível e sua inversa é uma matriz elementar de
igual tipo.

Demonstração. Com efeito, consideremos inicialmente uma matriz elementar de tipo 1
escrita como matriz coluna de blocos, onde, por simplicidade, trocamos a linha s pela
primeira linha de I, sendo o processo análogo para quaisquer duas linhas.

EL1↔Ls =












.












Is1
I21
...
I11
...
In1

Em seguida escrevemos EL1↔Ls
primeira coluna de posições:

como matriz linha de blocos, trocando a coluna s e a

EL1↔Ls = (cid:2)I1s

I12

· · ·

I11

· · ·

(cid:3) .

I1n

Vamos mostrar que EL1↔Ls
revertermos uma troca de linhas, basta aplicar a troca novamente. Assim

é sua própria inversa, o que em palavras signiﬁca que para

EL1↔LsEL1↔Ls =












Is1I1s
I21I1s
...
I11I1s
...
In1I1s

Is1I12
I21I12
...
I11I12
...
In1I12

· · ·
· · ·
. . .
· · ·
...
· · ·

Is1I11
I21I11
...
I11I11
...
In1I11

· · ·
· · ·
...
· · ·
. . .
· · ·












Is1I1n
I21I1n
...
I11I1n
...
In1I1n

=

=












iss
i2s
...
i1s
...
ins

is2
i22
...
i12
...
in2

· · ·
· · ·
. . .
· · ·
...
· · ·

is1
i21
...
i11
...
in1

· · ·
· · ·
...
· · ·
. . .
· · ·

=












isn
i2n
...
i1n
...
inn

45












1 0 · · ·
0 1 · · ·
...
...
. . .
0 0 · · ·
...
...
...
0 0 · · ·

0 · · ·
0 · · ·
...
...
1 · · ·
...
. . .
0 · · ·


0
0

...




0

...


1

onde a última igualdade decorre do fato de que ijk = 1 quando j = k e ijk = 0 quando
j (cid:54)= k.

, onde c é uma constante real não-
Vamos agora tomar uma matriz de tipo 2 ELs↔cLs
, a qual mostraremos ser a sua
nula e consideraremos a matriz elementar de tipo 2 ELs↔ 1
inversa. Prosseguindo de maneira análoga ao primeiro caso provado acima, escreveremos
como matriz linha de blocos e em seguida
ELs↔ 1
desenvolvemos o produto abaixo

como matriz coluna de blocos, ELs↔cLs

c Ls

c Ls

c LsELs↔cLs
ELs↔ 1


I11
I21
...
1
c Is1
...
In1





















(cid:2)I11

I12

· · ·

cI1s

· · ·

(cid:3)

I1n













I11I11
I21I11
...
1
c Is1I11
...
In1I11

I11I12
I21I12
...
1
c Is1I12
...
In1I12

· · ·
· · ·
. . .
· · ·
...
· · ·

I11cI1s
I21cI1s
...
1
c Is1cI1s
...
In1cI1s

· · ·
· · ·
...
· · ·
. . .
· · ·

=

=













I11I1n
I21I1n
...
1
c Is1I1n
...
In1I1n






=











· · ·
· · ·
. . .
· · ·
...
· · ·

i11
i21
...
1
c is1
...
in1

i12
i22
...
1
c is2
...
in2

c · i1s
c · i2s
...
1
c · c · iss
...
c · ins

· · ·
· · ·
...
· · ·
. . .
· · ·
. Em palavras,
Temos portanto, provado que ELs↔ 1
podemos dizer que para reverter a multiplicação de uma linha por uma constante não-
nula, basta multiplicarmos esta linha pelo inverso de uma tal constante.

i1n
i2n
...
1
c isn
...
inn
é inversa à esquerda de ELs↔cLs

1 0 · · ·
0 1 · · ·
...
...
. . .
0 0 · · ·
...
...
...
0 0 · · ·

0 · · ·
0 · · ·
...
...
1 · · ·
...
. . .
0 · · ·


0
0

...




0

...


1




















c Ls

=

.

Para ﬁnalizar, provemos que a inversa de uma matriz elementar de tipo 3 ELs↔Ls±cL1
(onde a linha Li foi escolhida apenas por simplicidade) é uma matriz também elementar
de mesmo tipo, a saber ELs↔Ls∓cL1
. Prosseguindo de maneira análoga aos dois casos
provados anteriormente, temos que

46

ELs↔Ls∓cLiELs↔Ls±cL1



I11
I21
...
Is1 ∓ cI11
...
In1










(cid:2)I11 ± cI1s

I12

· · ·

I1s

· · ·

(cid:3)

I1n
































i11 ± i1s
i21 ± i2s
...
is1 ± ciss ∓ ci11 − c2i1s
...
in1 ± cins

i12
i22
...
is2 ∓ ci12
...
in2

· · ·
· · ·
. . .
· · ·
...
· · ·

i1s
i2s
...
iss ∓ ci1s
...
ins

· · ·
· · ·
...
· · ·
. . .
· · ·












i1n
i2n
...
isn ∓ ci1n
...
inn

0
1
...

1 ± 0
0 ± 0
...

· · ·
· · ·
. . .
0 ± c · 1 ∓ c · 1 − c2 · 0 0 ∓ c · 0 · · ·
...
· · ·

...
0 ± c · 0

...
0

0
0
...

· · ·
· · ·
...
1 ∓ c · 0 · · ·
. . .
· · ·

...
0












0
0
...
0 ∓ c · 0
...
1












1 0 · · ·
0 1 · · ·
...
...
. . .
0 0 · · ·
...
...
...
0 0 · · ·

0 · · ·
0 · · ·
...
...
1 · · ·
...
. . .
0 · · ·


0
0

...




0

...


1

.

=

=

=

=

Logo, podemos concluir que toda matriz elementar de tipo 1, 2 ou 3 possui inversa, a
qual é também elementar e de mesmo tipo da matriz a qual ela inverte.

Exemplo 1.17. As inversas das matrizes elementares abaixo

EL2↔L2−3L1 =

(cid:21)

(cid:20) 1 0
−3 1

,

EL1↔L1+L2 =

(cid:21)

(cid:20)1 1
0 1

,

EL2↔− 1

2 L2 =

(cid:21)
(cid:20)1
0
0 − 1
2

são

EL2↔L2+3L1 =

(cid:21)

(cid:20)1 0
3 1

,

EL1↔L1−L2 =

(cid:21)
(cid:20)1 −1
1
0

,

EL2↔−2L2 =

(cid:21)

(cid:20)1
0
0 −2

,

respectivamente.

O Lema 1.1 acima, juntamente com o Teorema 1.4, sustentam a validade do algoritmo
para inverter matrizes, pois cada operação realizada nas linhas de A é ao mesmo tempo
realizada nas linhas de I, porém de maneira reversa, levando A para I e I para A−1.
Formalizaremos tal discussão quando provarmos o Teorema Fundamental das Matrizes
Elementares.

Como já sabemos que a inversa de uma matriz, quando existe, é única, precisaremos

apenas do próximo lema para provarmos o supracitado teorema.

47

Lema 1.2. Se A é uma matriz quadrada invertível de ordem n e B, X são matrizes
n × 1, tais que AX = B, então a matriz X é única, a qual é dada por

X = A−1B

Demonstração. Provemos primeiro a existência de X, isto é, que existe uma matriz X
que cumpre AX = B. Para tanto, basta substituir X = A−1B na referida equação, de
modo que

AX = A(A−1B) = (AA−1)B = IB = B.
Segue então que tal matriz existe. Resta agora provarmos sua unicidade. Suponhamos
assim que exista uma matriz Y, tal que AY = B. Daí, multiplicando esta última
igualdade pela inversa de A em ambos os membros, decorre

A−1(AY) = A−1B ⇒ (A−1A)Y = A−1B ⇒ Y = A−1B = X,

provando assim a unicidade de X.

Deﬁnição 1.9. Dadas duas matrizes A e B, diz-se que A é equivalente por linhas
à matriz B se esta última pode ser obtida pela multiplicação de sucessivas matrizes ele-
mentares por A, ou seja,

B = E1 · · · EkA.
A deﬁnição acima nos permite ir além, pois se uma matriz B é equivalente por linhas a
uma matriz A, então podemos dizer que A é equivalente por linhas a B, pois toda matriz
elementar é invertível e sua inversa é uma matriz elementar de mesmo tipo. Dizemos
então, de maneira mais simples, que A e B são equivalente por linhas. Por exemplo, as
matrizes





4 −26 20
29 17

3
3
são equivalentes por linhas, pois

−8
12

A =



e

B =





0 −1
2
4

5
7 −3
1
1











4 −26 20
29 17
 =
3
3

−8
12





0 1 0
1 0 0
0 0 1









1 0 0
0 1 0
0 0 3







1 −4 0
1 0
0

0 1
0







0 −1
2

4

5
7 −3
1
1



 ,

isto é, A = EL1↔L2EL3↔3L3EL1↔L1−4L2B.

Podemos agora enunciar o Teorema Fundamental das Matrizes Elementares, o qual,
de acordo com POOLE [53], está conectado a boa parte dos assuntos da Álgebra Linear,
seja no desenvolvimento de suas caracterizações ou aplicações.

Teorema 1.5. Seja A = [aij] uma matriz n×n. As seguintes aﬁrmações são equivalentes:

a) A é invertível.

b) AX = B ⇒ X = A−1B, onde X = [xi1]n×1 e B = [bi1]n×1.

c) AX = 0 ⇒ X = 0, onde 0 =



0
...




0

n×1

48

d) A pode ser reduzida a I por meio de operações elementares.

e) A pode ser escrita como produto de matrizes elementares.

Demonstração. Provaremos as implicações

a) ⇒ b) ⇒ c) ⇒ d) ⇒ e) ⇒ a)

a ﬁm de garantir a equivalência entre cada aﬁrmação.

a) ⇒ b): Verdadeira, de acordo com o Lema 1.2.
b) ⇒ c): Seja AX = 0. Temos, pelo Lema 1.2, que X é única e é tal que

c) ⇒ d): Se AX = 0, então podemos escrever tal igualdade sob a forma

X = A−10 = 0.

a11x11 + a12x21 + · · · + a1nxn1 = 0
a21x11 + a22x21 + · · · + a2nxn1 = 0
...
an1x11 + an2x21 + · · · + annxn1 = 0

Porém, como a única solução é a trivial, segue que o algoritmo de inversão de matrizes
pode ser aplicado até obter-se

1 · x11 + 0 · x21 + · · · + 0 · xn1 = 0
0 · x11 + 1 · x21 + · · · + 0 · xn1 = 0
...
0 · x11 + 0 · x21 + · · · + 1 · xn1 = 0

o que signiﬁca que A pode ser reduzida à matriz identidade por meio de matrizes elemen-
tares. De fato, se A não pudesse ser reduzida à matriz identidade, então só seria possível
multiplicá-la por uma certa quantidade k de matrizes elementares, até obtermos

E1E2 · · · EkA = B.

Como B (cid:54)= I, então, no pior dos casos, B é tal que bii = 1, porém existe pelo menos uma
entrada bij, com i (cid:54)= j, tal que bij (cid:54)= 0. Suponhamos, sem perda de generalidade, que seja
b12 (cid:54)= 0. Temos assim que

x11 + b12x21 = 0
É claro que X = 0 é uma solução, porém, se for x21 (cid:54)= 0, então x11 = −b12x21 e a matriz

X =










−b12x21
x2
0
...
0










é também uma solução (não-trivial) da equação dada, o que contradiz a hipótese, isto é,
de que a única solução é a trivial. Logo, segue que B = I e assim,

E1E2 · · · EkA = I.

49

Logo, A pode ser reduzida à matriz identidade por meio de matrizes elementares.
d) ⇒ e): Se A pode ser reduzida a uma matriz identidade, então existe uma sequência
de matrizes elementares E1, E2, . . . Ek, tais que

Ek · · · E2E1A = I.

Com sorte, o Lema 1.1 nos garante que cada uma dessas matrizes elementares é invertível e
suas inversas também são matrizes elementares, de modo que podemos multiplicar ambos
os membros da igualdade acima por E−1

, e assim

1 E−1
2

· · · E−1
k

E−1

1 E−1
2

⇒

· · · E−1

k Ek · · · E2E1A = E−1
A = E−1

1 E−1
2
1 E−1
2

· · · E−1
k I
· · · E−1
k

Portanto, A pode ser escrita como produto de matrizes elementares.

e) ⇒ a): Sabemos que o produtos de matrizes inversas é igual à inversa do produto de
tais matrizes, em ordem contrária. Logo, temos que

A = E−1

1 E−1
2

· · · E−1

k = (Ek · · · E2E1)−1,

ou seja, A é invertível.

Um exemplo da redução citada no item d) do teorema acima pode ser vista a seguir

(cid:21)

(cid:20)1 2
3 4

=

(cid:20)1 0
3 1

(cid:21) (cid:20)1 −1
1
0

(cid:21) (cid:20)1

(cid:21)

.

0
0 −2

Provado o TFME, podemos aﬁrmar que o algoritmo descrito no Capítulo 1, o qual
recebe o nome de método de Gauss-Jordan 7 8 pode ser aplicado a qualquer matriz
A invertível. De modo geral, dada uma matriz A, podemos escrever a sua matriz
escalonada, isto é, uma matriz B equivalente por linhas a A dada por

a qual atende aos seguintes critérios:

B = E1 · · · EkA,

a) Se B possui uma ou mais linhas nulas, estas devem ser suas últimas linhas no sentido
de cima para baixo.

b) Em cada linha não-nula, a primeira entrada não-nula está imediatamente à esquerda
de qualquer uma das outras entradas não-nulas que estejam abaixo desta.

Podemos então concluir que, se uma matriz é invertível, então ela é escalonável e pode
ser reduzida a uma matriz identidade. Caso contrário, sua forma escalonada possui no
mínimo uma linha nula, a qual é a última das linhas de A.

7Carl Friederich Gauss (1777-1855), foi um brilhante matemático alemão que ﬁcou conhecido como
“Príncipe da Matemática” que desenvolveu trabalhos nas mais diversas áreas da matemática, tais como
Álgebra, Teoria dos números, Probabilidade, Estatística, Geometria não-euclidianas, Geometria Diferen-
cial entre muitas outras [56].

8Wilhelm Jordan (1842-1899) foi um professor alemão que contribuiu para a resolução de sistemas

lineares com um método sistemático de substituição reversa [53].

50

Exemplo 1.18. A matriz escalonada das matrizes





A =



 ,

1 3 5
4 2 3
7 1 6

são





D =

5

3

1
0 −10 −17
0

0

5



 ,

B =

E =







0 1 3 2
0 5 1 4
2 7 4 5
5 2 1 3







,

C =













1 2
3 4
5 6
7 8



4
2 7
0 5
1


0 0 14

5

0

0 0








5
4
6
5
93
14

e F =


1
2
0 −2


0
0

0
0







,

respectivamente.

O processo utilizado para determinar a matriz escalonada de uma matriz dada é o

seguinte:

(1) Se a11 (cid:54)= 0, então multiplica-se cada linha (a partir da segunda) por −ai1/a11, da

primeira coluna e abaixo de a11 sejam todos nulos.

(2) Se a11 = 0, então deve-se buscar uma linha cuja entrada ai1 seja não-nula, com

i ≥ 2, e aplicar uma troca de linhas.

(3) Se porém, todas as entradas da primeira coluna forem nulas, então passa-se à coluna
mais próxima onde exista pelo menos uma entrada (cid:54)= 0. Se tal entrada for a primeira
da coluna, opera-se como no passo 1. Do contrário, segue-se o passo 2.

(4) Quando todas as entradas abaixo daquela citada no passo 3 forem iguais a 0, passa-

se então para a próxima linha e aplica-se os passo anteriores.

Tal processo recebe o nome de eliminação Gaussiana, o qual possui grande uti-
lidade na determinação das soluções (quando existem) de um sistema linear. Podemos
então notar que o método de Gauss-Jordan é a aplicação da eliminação Gaussiana suces-
sivas vezes, de modo que somente as entradas da diagonal principal sejam não-nulas.

Exemplo 1.18. Vamos aplicar o processo de eliminação Gaussiana à matriz B do exem-
plo anterior, a ﬁm de demonstrar sua aplicabilidade na prática.

L1↔L3−−−−→

B =







0 1 3 2
0 5 1 4
2 7 4 5
5 2 1 3













2 7 4 5
0 5 1 4
0 1 3 2
5 2 1 3








2
7
0
5


0
0


2 −9 − 19
0 − 31

4
1
14
5


5
4


6

5


2

1
5 L2
L3−
−−−−−→
5
2 L1
L4−
−−−−−→








2 7
4
0 5
1
14
0 0
5
0 0 − 59
10






 L4+

59
28 L3
−−−−−−→

5
4
6
5
41
10








2 7
4
1
0 5
0 0 14
5
0

0 0








5
4
6
5
93
14

= E

31
L4+
10 L2
−−−−−−→

Finalizamos esta seção com um teorema que nos será útil na seção seguinte, o qual
aﬁrma que se uma matriz n × n é singular, então o seu produto por qualquer outra matriz

51

de mesma ordem é também singular.

Teorema 1.6. Seja A uma matriz singular de ordem n. Tem-se que AB é também
singular, qualquer que seja a matriz B de mesma ordem.

Demonstração. Suponhamos que AB seja invertível e consideremos uma matriz X, tal
que BX = 0. Multiplicando esta última igualdade à esquerda por A, obtemos ABX =
A0, o que nos leva a

ABX = 0,
o que por sua vez, sendo AB singular, implica, de acordo com o TFME, que X = 0 é a
única matriz que satisfaz a equação acima. Consequentemente, esta mesma matriz é a
única que satisfaz BX = 0 e, novamente pelo TFME, decorre que B é invertível. Assim,
temos que

A = AI = A(BB−1) = (AB)B−1.
Portanto, como o produto de matrizes invertíveis é também invertível, decorre que A é
não-singular, provando assim o resultado buscado.

1.4 Algumas aplicações matriciais

1.4.1 Matrizes de Leslie

As matrizes de Leslie9, de acordo com MURRAY [49] são um dos meios de estudo
populacional, pois estas podem abranger diferentes faixas etárias, tais como jovem e
adulto, bem como quantiﬁcar mudanças de uma faixa para outra. Tais matrizes compõem
o modelo de Leslie10, onde este têm como principal objetivo descrever o crescimento de
uma população de fêmeas, seja de animais ou humanos, uma vez que estas podem dar
origem a novos indivíduos. O modelo é estruturado da seguinte maneira:

1) Deﬁne-se a idade máxima da população como sendo Y anos.

2) A população é dividida em n faixas etárias.

3) Cada faixa etária está associada a um intervalo de tempo conforme o esquema

abaixo

Faixa etária

Intervalo de idade

1
2
3
...
n

[t0, t1)
[t1, t2)
[t2, t3)
...
[tn−1, tn]

9Patrick Holt Leslie (1900-1974) foi um dos primeiros cientistas a estudar a dinâmica de crescimento

populacional utilizando matrizes [1].

10De acordo com ANTON [3], este é um dos modelos de crescimento populacional mais comumente

usado, o qual foi desenvolvido na década de 1940.

52

onde tk = kY /n representa o tempo em que termina a faixa etária k e inicia-se a
faixa etária k + 1.

4) Deﬁne-se o número de fêmeas em cada uma das n faixas etárias no instante t = 0,
com t representando o tempo, de modo que tais quantidades são organizadas na
matriz a seguir

F11 =










,






f11
f21
...
fn1

onde fi1 é o número de fêmeas na faixa etária i. Tal matriz é denominada matriz
de distribuição etária inicial .

5) Dependendo da espécie a ser considerada, pode ser que esta possa gerar novos
indivíduos em diferentes faixas etárias, de modo que é necessário deﬁnir qual é
a taxa de natalidade mi ≥ 0 de fêmeas reproduzidas na faixa etária i, i =
1, 2, . . . , n. É necessário que pelo menos um dos mi seja não nulo, a ﬁm de garantir
que haja reprodução da espécie e, nesse caso, dizemos que i é uma faixa etária
fértil .

6) Deﬁne-se a taxa de sobrevivência 0 < sj ≤ 1 de fêmeas durante a transição da

faixa etária j para a faixa etària j + 1, onde j = 1, 2, . . . , n − 1.

7) No intervalo [t0, tn], todas as fêmeas em faixa etária fértil geram, de acordo com
suas respectivas médias de reprodução, um número f12 de fêmeas na faixa etária 1,
ou seja

f12 = m1f11 + m2f21 + · · · + mnfn1.
Ao término desse intervalo, sobreviverão fêmeas de acordo com a taxa de sobrevi-
vência de cada faixa etária., ou seja, de f11, restarão f22 = s1f11, de f21, restarão
f32 = s2f21 e assim sucessivamente até fn2 = sn−1f(n−1)1, de modo que tais valores
podem ser organizados na matriz

F12 =










f12
f22
f32
...
fn2










=










m1f11 + m2f21 + · · · + mnfn1
s1f11
s2f21
...
sn−1f(n−1)1










.

Escrevendo F12 como produto de matrizes, temos que






f12
f22
f32
...
fn2








=















m1 m2
0
s1
s2
0
...
...
0
0

· · · mn−1 mn
0
0
· · ·
0
0
· · ·
...
...
. . .
0
sn−1
· · ·



















,










f11
f21
f31
...
fn1

F12 =

isto é,

F12 = ŁF11,

53

onde

Ł =










m1 m2
0
s1
s2
0
...
...
0
0

· · · mn−1 mn
0
0
· · ·
0
0
· · ·
...
...
. . .
0
sn−1
· · ·










é a matriz de Leslie. Com um raciocínio análogo ao descrito acima, temos que

F13 =
F14 =
...

ŁF12
ŁF13

= Ł2F11
= ŁŁF11
= ŁŁ2F11 = Ł3F11

.

F1n = ŁF1(n−1) = ŁŁn−1F11 = ŁnF11

Portanto, a matriz de blocos

F = (cid:2)F11 F12

· · · F1n

(cid:3)

fornece o número de fêmeas nos n primeiros períodos Y .

Vejamos a seguir um exemplo, a ﬁm de melhor compreender o modelo acima apresen-

tado.

Exemplo 1.20. Consideremos uma colônia de guaxinins, na qual a população fêmea
vive no máximo 5 anos e está dividida em 5 faixas etárias, as quais mudam a cada ano,
conforme o quadro a seguir:

Faixa etária (em anos)

[0, 1)
[1, 2)
[2, 3)
[3, 4)
[4, 5]

fi1 mi
0
40
5
30
3
15
2
10
1
5

si
0,6
0,4
0,2
0,1
0

onde i = 1, . . . , 5. Temos assim que

F11 =

















40
30
15
10
5

Ł =









0
0,6
0
0
0

5
0
0,4
0
0

3
0
0
0,2
0



1
2
0
0


0
0


0
0

0,1 0

.

Dessa forma, após um ano, a população de fêmeas será dada pela matriz

F12 = ŁF11 =









0
0,6
0
0
0



1
2
0
0


0
0


0
0

0,1 0

5
0
0,4
0
0

3
0
0
0,2
0

54

















40
30
15
10
5

=

















220
24
12
3
1

.

Ao longo de 5 anos, temos que









F12 =









220
24
12
3
1

, F13 =

















163
132
9,6
2,4
0,3

, F14 =

















693,9
97,8
52,8
1,9
0,2

, F15 =

















651,48
416,34
39,12
10,56
0,19

,





2220,37
390,89
166,54
7,82
1,06
Observando o gráﬁco da Figura 1.4 abaixo apenas para os 10 primeiros anos, é possível

F16 =













.

perceber como o crescimento populacional se dá de forma exponencial

Figura 1.4: Crescimento da população de guaxinins ao longo de 10 anos.

Fonte: O autor

Agora observemos as taxas de crescimento populacional anual ao longo de 50 anos,

em ordem cronológica

160%,

52,45%, 119,11%,
73,5%,
98,96%,
89,39%,
79,85%,
83,21%,
86,82%,
85,52%,
84,16%,
85,16%,
84,65%,
84,79.%

18,2%, 175,52%, 32,01%, 149,32%,
59,75%, 110,2%,
95,44%,
76,2%,
81,06%, 88,29%,
86,35%, 83,69%,
84,33%, 85,37%,
84,7%,
85,10%,

65,51%, 103,72%,
78,27%,
92,82%,
87,45%,
81,98%,
83,93%,
86%,
85,25%,
84,47%,
84,75%,
85,04%,

43,3%, 131,57%,
70,01%,
90,86%,
82,68%,
85,73%,
84,57%,
85%,

É possível perceber que, nos anos iniciais, há uma grande divergência de aumento
populacional anual entre um ano e outro, porém, à medida que os anos avançam os
valores começam a ﬁcar mais próximos, de modo que o crescimento populacional anual

55

médio é aproximadamente 84,98%. O gráﬁco abaixo nos fornece uma melhor visão deste
fato

Figura 1.5: Estabilização do crescimento populacional ao longo de 50 anos.

Fonte: O autor

Observemos ainda que, nesse exemplo, o número médio de ﬁlhas que uma única gua-

xinim fêmea é obtido da seguinte forma:

• A fêmea nasce e, durante seu primeiro ano de vida não é capaz de reproduzir, pois

m1 = 0.

• Se ela sobreviver, reproduzirá m2s1 = 5·0,6 = 3 fêmeas, pois há 60% de chance dela
sobreviver. Até aqui temos que o valor esperado de fêmas reproduzidas durante seu
tempo de vida (até o dado momento) é 0 + 3 = m1 + m2s1.

• No terceiro ano de vida, caso sobreviva, produzirá 0,4 · 0,6 · 3 = 0,72 fêmeas, pois há
0,4 · 0,6 = 0,24 de probabilidade que ela sobreviva até o terceiro ano. Dessa forma,
temos que, até agora, ela produziu, em média, m1 + m2s1 + m3s1s2.

• Seguindo o raciocínio descrito acima, temos que, atingindo o quarto ano de vida,
essa fêmea produzirá 0,2·0,4·0,6·2 = 0,096 fêmeas e no quinto ano, 0,1·0,2·0,4·0,6·1 =
0,0048, totalizando assim

0 + 3 + 0,72 + 0,096 + 0,0048 = 3,8202

fêmeas reproduzidas, em média, ao longo de toda a vida do animal. Pondo os dados
em termos de mi e sj, com i = 1, . . . , 5 e j = 1, . . . , 4, obtemos

m1 + m2s1 + m3s1s2 + m4s1s2s3 + m5s1s2s3s4.

56

Podemos assim deﬁnir a média de ﬁlhas fêmeas de uma única fêmea ao longo de n faixas
etárias como sendo

m1 + m2s1 + m3s1s2 + . . . + mns1 · · · sn−1.

A expressão acima é denominada taxa líquida de reprodução da população fêmea, a
qual será representada por TL.

Observemos agora que as matrizes de Leslie podem apresentar comportamento perió-
dico, desde que exista uma relação entre a taxa de reprodução e as taxas de sobrevivência.
Para iniciar, consideremosa matriz de Leslie

e notemos que

Para a matriz

temos que

Ł3 =

Ł =

(cid:21)
(cid:20) 0 m
0
s1

Ł2 =

(cid:20)ms1

(cid:21)

0

0 ms1

= ms1

(cid:21)

(cid:20)1 0
0 1

= ms1I2.

Ł =





0
s1
0





0 m
0
0
0
s2

0
ms1s2
0

0
0
ms1s2



 = ms1s2





ms1s2
0
0







1 0 0
0 1 0
0 0 1

 = ms1s2I3.

Seguindo essa sequência, obtemos, para algum n ∈ N

Łn = ms1 · · · sn−1In.

Daí, para que seja Ł = In, devemos ter

m =

1
s1 · · · sn−1

.

Nesse caso, seja qual for nossa matriz inicial F11, teremos

F11 = F1(k+1) = F1(2k+1) = F1(3k+1) = . . .

signiﬁcando assim que o número de fêmeas de uma população que constituem uma matriz
de Leslie sob as condições acima, oscila durante k unidades de tempo e em seguida retorna
ao valor inicial, onde estas oscilações, de acordo com ANTON [3], recebem o nome de
ondas populacionais. Quando uma população volta a atingir o mesmo valor inicial de
modo periódico, diz-se que ela possui crescimento populacional nulo. Como nesse
caso temos m1 = · · · = mn−1 = 0 e mn = m, decorre que

TL = ms1 · · · sn−1 = 1,

ou seja, o crescimento populacional é nulo se, e somente se, TL = 1.

Exemplo 1.21. A semelparidade é, de acordo com HILL [30], a condição em que indiví-
duos são ﬁsiologicamente programados para se reproduzirem uma única vez em toda sua

57

vida. Esse é o caso do Rei Salmão (Oncorhynchus tschawytscha) que, segundo QUINN
[55], é o maior dentre as espécies de salmão do Oeano Pacíﬁco e possui uma taxa de
reprodução de aproximadamente 5400 ovos. Já o seu ciclo de vida pode atingir mais
de 4 anos, aponta Lewis [41]. Dividindo o tempo de vida dessa espécie em 4 faixas
etárias [0, 1) , [1, 2) , [2, 3) e [3, 4] onde a passagem de uma para outra se dá a cada ano,
consideremos as seguintes matrizes de Leslie S e de distribuição etária inicial F11

S =









0
1
8
0

0

0
0
1
25
0









5400
0

0

0

0
0

0
1
27

e

F11 =

















40

30

20

10

.

tais que

F12 =







54000
5
1,2
0,74







, F13 =













4000
6750
0,2
0,04

F14 =













240
500
270
0,007

, F15 =

















40

30

20

10

, · · ·

Supondo, apenas para uma análise gráﬁca, que essa população mantivesse o mesmo ritmo
reprodutivo durante 20 anos, o gráﬁco obtido seria o que se pode observar na Figura 1.6
abaixo

Figura 1.6: Periodicidade do crescimento populacional do Rei Salmão.

Fonte: O autor

mostrando assim que a população fêmea oscila durante 4 anos e retorna ao valor inicial.

58

1.4.2 Criptograﬁa

A Criptograﬁa, conforme [45], é o estudo de técnicas matemáticas relacionadas aos
aspectos de segurança da informação tais como, conﬁdencialidade, integridade de dados,
autenticação de entidades e autenticação de origem de dados [45]. De acordo com ISMAIL
[32], a cifra de Hill 11 é um algoritmo que consiste em tomar m letras e substituí-las
por m letras de texto cifrado. Vamos aqui tomar os números correspondentes às posições
das letras no alfabeto para substituí-las como podemos ver abaixo

2

J K L M
A B C D E F G H I
1
13
10
9
7
5
N O P Q R S T U V W X Y Z
26
20
14

25

21

22

11

16

19

24

23

12

15

17

18

6

8

3

4

O objetivo de fazer isso é criptografar uma mensagem por meio de uma matriz de ci-
fragem, a qual deve ser invertível, a ﬁm de que seja possível decodiﬁcar a mensagem
utilizando uma matriz de decifragem, isto é, a matriz inversa da matriz de cifragem.
Dependendo das palavras existentes na mesagem, pode ser necessário utilizar mais núme-
ros para representar, por exemplo, letras acentuadas, vírgulas, exclamações, etc. Vamos
codiﬁcar a mensagem “SÓ SEI QUE NADA SEI” utilizando a matriz de cifragem





MC =

−1 1
−1 4

3
1
1 1 −4



 .

Vamos dividir a frase em blocos de 3. Os espaços entre palavras serão representado por
0 e a letra Ó será representada por 27. Dessa forma, nosso quadro de codiﬁcação será

A B C D E F G H I
9
5
1

J K L M
0
13
10
N O P Q R S T U V W X Y Z Ó
27
14

16

23

21

18

12

15

24

20

19

17

22

11

26

25

2

8

4

3

7

6

Daí, escrevendo a frase dada conforme o esquema acima, obtemos

S Ó
27
19

0

S E I
9
5
19

Q U E
5
21
17

N
14

0

0

A D A
1
4
1

S E I
9
5
19

0

0

0

Organizando os blocos em matrizes 3 × 1, tem-se








 , P3 =



 , P4 =



P1 =

P5 =



 , P2 =

19
27
0


1
4
 , P6 =

1

19
5
9

0
19
5









 , P7 =





0
17
21


9
0
 .

0







5
0
14

11Lester Sanders Hill (1890-1961) foi um matemático americano que ﬁcou conhecido por seus trabalhos
em criptograﬁa e criptoanálise [64]. Hill publicou, em 1929, a cifra que leva seu nome em seu livro
intitulado “Cryptography in an algebric alphabet” [31].

59

Daí, multiplicando cada uma destas por MC, obtém-se matrizes Qi = MCPi, com i =
1, . . . , 7, tais que

 , Q3 =



 , Q4 =







37
9

−51

Q1 =

Q5 =






8
89
 , Q2 =
46






6
16
 , Q6 =
1







13
10
−12



34
81
−1







80
89
−67



−9
−9
 .
9

 , Q7 =



Agora note que algumas entradas das matrizes imediatamente acima são menores que 0
ou maiores que 28. Logo, para obtermos seus correspondentes no quadro de codiﬁcação,
devemos formar novas matrizes, onde as entradas destas serão os restos das divisões das
entradas anteriores por 28. Por exemplo,


 corresponde a





8
89
46


 , que corresponde a





8
5
18



 ,





H
E
R

pois

8 = 0 · 28 + 8 e H está na posição 8
89 = 3 · 28 + 5 e E está na posição 5
46 = 1 · 28 + 18 e R está na posição 18

Seguindo a ideia acima, obtemos as seguintes correspondências

Q1 =

 −→ R1 =



 −→

 −→ R2 =



 −→







8
89
46

13
10
−12

80
89
−67



















Q2 =

Q3 =

Q4 =

Q5 =

34
81
−1













Q6 =

Q7 =









8
5
18

13
10
16





 −→

24
5
17


9
9
 −→

5







6
16
1

 −→

 −→ R3 =



37
9
 −→ R4 =
−51


1
4
 −→ R5 =

1

 −→ R6 =



 −→

−9
−9
 −→ R7 =
9

 −→





6
25
27

19
19
9







60









H
E
R









M
J
P









X
E
Q









I
I
E









F
P
A









F
Y
Ó









S
S
I

Portanto, a mensagem codiﬁcada adquire a forma

“HERMJPXEQIIEFPAFYÓSSI”

Logo, para decifrar a mensagem, utiliza-se a matriz de decifragem MD = M−1
C

, onde

MD =





17 −7 11
2
3 −1
3
5 −2



 .

Daí, dividindo a mensagem codiﬁcada em blocos de 3, o que acaba por fornecer as matrizes
Ri, basta fazer as multiplicações MDRi, o que fornecerá matrizes Si, as quais serão
correspondentes à matrizes Pi. Por exemplo, observe que

S1 = MDR1 =

S2 = MDR2 =

S3 = MDR3 =

S4 = MDR4 =

299
55
84

327
61
93

560
101
161

























S5 = MDR5 =







224
47
61

 −→ P1 =



 −→

 −→ P2 =



 −→

 −→ P3 =



 −→

 −→ P4 =

145
28
42


1
4
 −→ P5 =

1







 −→

5
0
14


1
4
 −→

1





19
27
0

19
5
9

0
17
21













0
19
5

S6 = MDR6 =

 −→ P5 =



 −→





S
Ó













S
E
I



Q

U



E







N









A
D
A






S
E

S7 = MDR7 =







289
56
84

 −→ P5 =






9
0
 −→
0



I





o que acaba por fornecer a mensagem original.

Observe que podemos diﬁcultar o processo de cifragem com auxílio de funções, por
exemplo, por meio de combinação dos processos. Consideremos, como ilustração, uma
função aﬁm f : R → R, f (x) = ax + b, a, b ∈ R com a (cid:54)= 0. Dessa forma, para cada
valor atribuído a cada um dos caracteres utilizados no alfabeto do qual se quer enviar a
mensagem, toma-se sua imagem pela função escolhida, a ﬁm de determinar o caractere
cifrado correspondente. Por exemplo, se considerarmos a tabela do exemplo anterior e a
função m : R → R, m(x) = 3x + 5, temos que a letra “S” (com valor 19) será representada
por “F” (com valor 6), uma vez que

m(19) = 3 · 19 + 5 = 62 = 2 · 28 + 6.

61

Com posse de tal ideia, seja α o número de caracteres a serem utilizados. Daí, temos

que o remetente cifrará a mensagem por meio de

f (x) = ax + b = Q · α + r,

onde Q é o quociente da divisão de f (x) por α (cid:54)= 0 e r é o resto. Note que, sendo x ≤ α,
decorre que Q ≤ a. Por conseguinte, o destinatário receberá a função g, inversa de f ,
dada por

g(x) =

x − b
a

,

para decodiﬁcar a mensagem cifrada. Porém ele não receberá as imagens de m corres-
pondentes a cada caractere, mas sim os restos. Assim, nem todos os valores recebidos
poderão ser calculados por meio de ng de maneira direta. Como ilustração disso, note
que a inversa da função m é n : R → R, com

n(x) =

x − 5
3

.

Calculando n(6), obtemos 1
como resposta, valor este que não está relacionado a nenhum
3
dos caracteres do esquema utilizada. Para contornar o problema, devemos tomar a ima-
gem de 6 + 28k, pois foram utilizados 28 caracteres e k representa o número de “voltas”
dadas no alfabeto. Assim

n(6 + 28k) =

6 + 28k − 5
3

=

28k + 1
3

.

Notemos então que o único resultado inteiro de n(6+28k) se dá quando k = 2, resultando
em 19 (compreendido entre 0 e 28).

Generalizando a ideia acima, devemos tomar a imagem de r + α · Q por g, de modo

a obtermos

g(Q · α + r) =

Q · α + r − b
a

Vamos agora cifrar a mensagem “SÓ SEI QUE NADA SEI” utilizando a função m,

dada por m(x) = 3x + 5.

Letra

S
Ó

E
I
Q
U
N
A
D

x m(x) Q · α + r
19
27
0
5
9
17
21
14
1
4

2 · 28 + 6
3 · 28 + 2
0 · 28 + 5
0 · 28 + 20
1 · 28 + 4
2 · 28 + 0
2 · 28 + 12
1 · 28 + 19
0 · 28 + 8
0 · 28 + 17

62
86
5
20
32
56
68
47
8
17

Letra criptografada

F
B
E
T
D

L
S
H
Q

Logo a mensagem criptografada é

“FBEFTDE LTESHQHEFTD”.

62

Para descriptografá-la, utilizaremos a função n, com

n(x) =

x − 5
3

.

Já sabemos que “F” corresponde a “S” pelo que foi visto acima. A letra “B”, por sua vez,
possui valor 2. Assim

n(28Q + 2) =

28Q + 2 − 5
3

=

28Q
3

− 1.

Segue então que n(28Q + 2) = −1 para Q = 0, porém −1 não corresponde a nenhum
caractere da tabela acima. Por outro lado n(28Q + 2) = 27 para Q = 3, fornecendo,
portanto, a letra “Ó” como correspondente de “B”.

Prosseguindo de maneira análoga para os demais caracteres, segue que

• “E” tem valor 5:

n(28Q + 5) = 0,
n(28Q + 5) = 28, para Q = 3 ⇒ “E” −→ “Ó”

para Q = 0 ⇒ “E” −→ Espaço

• “T” tem valor 20:

para Q = 0 ⇒ “T” −→ “E”
n(28Q + 20) = 5,
n(28Q + 20) = 33, para Q = 3 ⇒ Sem caractere correspondente

• “D” tem valor 4:

n(28Q + 4) = 9, para Q = 1 ⇒ “D” −→ “I”

• “Espaço” tem valor 0:

n(28Q + 0) = 17, para Q = 2 ⇒ “Espaço” −→ “Q”

• “L” tem valor 12:

n(28Q + 12) = 21, para Q = 2 ⇒ “L” −→ “U”

• “S” tem valor 19:

n(28Q + 19) = 14, para Q = 1 ⇒ “S” −→ “N”

• “H” tem valor 8:

para Q = 0 ⇒ “H” −→ “A”
n(28Q + 8) = 1,
n(28Q + 8) = 29, para Q = 3 ⇒ Sem caractere correspondente

• “Q” tem valor 17:

n(28Q + 17) = 4, para Q = 0 ⇒ “Q” −→ “D”
n(28Q + 8) = 32, para Q = 3 ⇒ Sem caractere correspondente

63

Daí, fazendo as respectivas correspondências na ordem dos caracteres da mensagem crip-
tografada, percebe-se que a letra “E” de fato equivale a um espaço entre palavras e não à
letra “O”. Portanto, podemos observar o resultado no quadro abaixo

F B E F T D E

L T E S H Q H E F T D

S Ó

S E I

Q U E

N A D A

S E I

Note ainda que, nos casos sem caractere correspondente, basta aplicar a divisão do
resultado por α novamente, e o resto irá fornecer o caractere descriptografado. Por
exemplo, a letra “T” forneceu 33 como resultado para Q = 3, mas, se dividirmos 33 por
28, obtemos resto 5, o qual corresponde à letra “E”.

Vejamos um caso adicional, agora utilizando funções quadráticas. Sendo assim, tome-
mos a função f : R → R, f (x) = ax2 + bx + c, com a, b e c sendo reais e a (cid:54)= 0. Tomando
y = f (x) temos que f −1 é dada por

f −1(x) =

−b ± (cid:112)b2 − 4(c − y)
2a

.

Utilizando o mesmo raciocínio do exemplo anterior, escreveremos

Segue daí que

y = f (x) = Qα + r.

f −1(x) =

−b ± (cid:112)b2 − 4c + 4Qα + 4r
2a

.

Criptografaremos a mensagem “SÓ SEI QUE NADA SEI” por intermédio de f (x) =
x2 − 5x + 6, obtemos o quadro a seguir

Letra

S
Ó

E
I
Q
U
N
A
D

x
22
27
0
5
9
17
21
14
1
4

f (x) Q · α + r
272
600
6
6
42
210
342
132
2
2

9 · 28 + 20
21 · 28 + 12
0 · 28 + 6
0 · 28 + 6
1 · 28 + 14
7 · 28 + 14
12 · 28 + 6
4 · 28 + 20
0 · 28 + 2
0 · 28 + 2

Letra criptografada

T
L
F
F
N
N
F
T
B
B

a qual nos fornece a mensagem cifrada

“TLFTFNFNFFFTBBBFTFN”

Agora, sabendo que o vértice da parábola que representa f é (cid:0) 5

(cid:1), decorre que

(cid:18)

D(f ) =

−∞,

(cid:21)

5
2

∪

(cid:20)5
2

(cid:19)

, +∞

e

64

4

4, − 1
(cid:20)

Im(f ) =

−

(cid:19)

, +∞

1
4

4, +∞(cid:1). Daí, podemos observar que os valores atri-
Assim, temos que o D(f −1) = (cid:2)− 1
buídos aos caracteres (de 0 a 28) pertencem a D(f −1) e, além disso, a parábola possui a
propriedade de simetria, o que evita o risco de, durante o processo de decifragem, obter-se
um valor sem correspondentes. Notemos então que

f −1(x) =

√

5 ±

1 + 112Q + 4r

2

Para a decifragem, tomaremos

f −1(x) =

√

5 +

1 + 112Q + 4r

2

.

,

a ﬁm de evitar valores negativos, aos quais deve-se somar 28 para obter caracteres cor-
respondentes. Logo, tomando g(x) = f −1(x), temos que

• “T” tem valor 20:

• “L” tem valor 12:

• “F” tem valor 6:

para Q = 0 ⇒ “T” −→ “G”
g(20) = 7,
g(20) = 14, para Q = 4 ⇒ “T” −→ “N”
g(20) = 19, para Q = 9 ⇒ “T” −→ “S”
g(20) = 26, para Q = 19 ⇒ “T” −→ “Z”

para Q = 0 ⇒ “L” −→ “F”
g(12) = 6,
g(12) = 27, para Q = 21 ⇒ “L” −→ “Ó”

g(6) = 5,
g(6) = 12,
g(6) = 21,

para Q = 0 ⇒ “F” −→ “E”
para Q = 3 ⇒ “F” −→ “L”
para Q = 12 ⇒ “F” −→ “U”

g(6) = 28 = 1 · 28 + 0, para Q = 23 ⇒ “T” −→ Espaço

• “N” tem valor 14:

• “B” tem valor 2:

para Q = 1 ⇒ “N” −→ “I”
g(14) = 9,
g(14) = 16, para Q = 6 ⇒ “N” −→ “P”
g(14) = 17, para Q = 7 ⇒ “N” −→ “Q”
g(14) = 24, para Q = 16 ⇒ “N” −→ “X”

g(2) = 4,
g(14) = 8,
g(14) = 25,

para Q = 0 ⇒ “B” −→ “D”
para Q = 1 ⇒ “B” −→ “H”
para Q = 18 ⇒ “B” −→ “Y”
g(14) = 29 = 1 · 28 + 1, para Q = 25 ⇒ “B” −→ “A”

Obtemos então a seguinte tabela

a qual permite visualizar a mensagem descriptografada.

65

2 Determinantes e Sistemas Lineares

2.1 Determinantes

Neste capítulo iremos conhecer a deﬁnição de determinante de uma matriz, bem como
suas principais propriedades. Embora, de acordo com LARSON [38], a teroria dos de-
terminantes emergiu a partir de padrões encontrados em soluções de sistemas lineares,
seguiremos aqui a ordem contrária, somente por uma questão de organização. Quanto às
aplicações dos determinantes, estas serão melhor estudadas ao longo dos próximos capítu-
los, pois eles contém informações importantes para o entendimento de tais aplicações, as
quais podem ser, segundo KAZUNGA [33], cálculo de área de polígonos e volume de po-
liedros, determinar a solução de sistemas lineares com n equações e n incógnitas, inversão
de matrizes e determinar se um conjunto de vetores é ou não linearmente dependente.

O determinante de uma matriz é um número real. Somente as matrizes quadradas
possuem determinante, de modo que este é indeﬁnido para matrizes que possuem mais
linhas que colunas ou vice-versa. Para melhor organizar as ideias, apresentamos a deﬁni-
ção a seguir.

Deﬁnição 2.1. Seja A uma matriz quadrada de ordem n, com n ≤ 2. Se A = [a11],
então o determinante de A é igual a a11 e escrevemos

Se for

então

det(A) = |a11| = a11.

A =

(cid:21)

(cid:20)a11 a12
a21 a22

,

det(A) =

(cid:12)
(cid:12)
(cid:12)
(cid:12)

a11 a12
a21 a22

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= a11a22 − a12a21.

É importante lembrar que, no caso n = 1, não se deve confundir as barras | |, com a
notação de valor absoluto de um número, de modo que, por exemplo, a matriz X = [−7]
é tal que det(X) = −7 e não 7, onde este é o valor absoluto de −7. Vejamos mais alguns
exemplos de matrizes e seus respectivos determinantes:

• A = [−15] ⇒ det(A) = | − 15| = −15

• B =

(cid:21)

(cid:20)4 7
1 3

⇒ det(B) =

(cid:12)
(cid:12)
(cid:12)
(cid:12)

4 7
1 3

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= 4 · 3 − 7 · 1 = 5

• C =

(cid:21)
(cid:20)sen(x) cos(x)
sen(x) cos(x)

⇒ det(C) =

(cid:12)
(cid:12)
(cid:12)
(cid:12)

sen(x) cos(x)
sen(x)
cos(x)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

66

= sen2(x) − cos2(x).

Para as matrizes de ordem n ≥ 3, vamos precisar das noções de menor complementar

e cofator, onde estas são fornecidas nas deﬁnições abaixo.

Deﬁnição 2.2. Seja A = [aij]n×numa matriz, e seja Aij a submatriz obtida a partir
da supressão da linha e da coluna que contém o elemento aij. O determinante de Aij é
chamado menor complementar de aij. O cofator Aij de aij será deﬁnido por

Aij = (−1)i+j det(Aij).

Exemplo 2.1. Dada a matriz

A =

temos que





7
−2 3
1
4
9
−5 8 −1



 ,

•

•

•

...

•

det(A11) =

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
4
9
(cid:12)
(cid:12)
8 −1
(cid:12)

= −41 é o menor complementar de a11.

A11 = (−1)1+1 det(A11) = −41 é o cofator de a11.

det(A12) =

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1

(cid:12)
4
(cid:12)
(cid:12)
−5 −1
(cid:12)

= 19 é o menor complementar de a12.

A12 = (−1)1+2 det(A12) = −19 é o cofator de a12.

det(A13) =

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1 9
−5 8

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= 53 é o menor complementar de a13.

A13 = (−1)1+3 det(A12) = 53 é o cofator de a13.

det(A33) =

(cid:12)
(cid:12)
(cid:12)
(cid:12)

−2 3
1 9

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= −21 é o menor complementar de a33.

A33 = (−1)3+3 det(A33) = −21 é o cofator de a33.

Podemos agora fornecer a deﬁnição para determinantes de matrizes quadradas com or-
dem maior ou igual a 3.

Deﬁnição 2.3. O determinante de uma matriz A = [aij]n×n é um número real asso-
ciado à esta, o qual é deﬁnido indutivamente como

det(A) =






a11

, n = 1

ai1A11 + ai2A12 + · · · + ainA1n

, n > 1

A deﬁnição acima é de grande praticidade, uma vez que ela deﬁne o determinante de
uma dada matriz como sendo a soma dos produtos entre entre o menor complementar e
o cofator de cada entrada da primeira linha.

67

Exemplo 2.2. Seja A a matriz do exemplo anterior. Segue, pela Deﬁnição 2.3, que

det(A) = a11A11 + a12A12 + a13A13

= −2 · (−41) + 3 · (−19) + 7 · 53
= 396.

Ainda a respeito da deﬁnição, perceba que o sinal de cada cofator depende somente
da soma dos índices, de modo que quando a soma é par, o sinal é positivo e quando é
ímpar, negativo. Com isso, podemos escrever o determinante de uma matriz A como
somas e subtrações alternadas, ou seja,

det(A) = ai1 det(Ai1) − ai2 det(Ai2) + ai3 det(Ai3) − · · · + (−1)i+jain det(Ain).

Exemplo 2.3. A troca de linhas de uma matriz identidade fornece uma matriz elementar
de tipo I. Note que, seu determinante é igual a −1, independente da ordem e de quais
linhas foram trocadas de posição. A demonstração formal desse fato pode ser encontrada
em [53] por meio de indução ou utilizando análise combinatória em [9]. Observemos
apenas alguns exemplos para convenvermo-nos sobre tal fato:

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

0 1
1 0

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= 0 · 0 − 1 · 1 = −1.

(cid:12)
0 0 1
(cid:12)
(cid:12)
0 1 0
(cid:12)
(cid:12)
1 0 0
(cid:12)
(cid:12)
0 1 0
(cid:12)
(cid:12)
1 0 0
(cid:12)
(cid:12)
0 0 1
(cid:12)

= 0 ·

= 0 ·

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1 0
0 0

0 0
0 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

− 0 ·

− 1 ·

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

0 0
1 0

1 0
0 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ 1 ·

+ 0 ·

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

0 1
1 0

1 0
0 0

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= −1.

= −1.

(cid:12)
0 0 1 0
(cid:12)
(cid:12)
0 1 0 0
(cid:12)
(cid:12)
1 0 0 0
(cid:12)
(cid:12)
0 0 0 1
(cid:12)

= 0 ·

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
1 0 0
(cid:12)
(cid:12)
0 0 0
(cid:12)
(cid:12)
0 0 1
(cid:12)

− 0 ·

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
0 0 0
(cid:12)
(cid:12)
1 0 0
(cid:12)
(cid:12)
0 0 1
(cid:12)

+ 1 ·

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
0 1 0
(cid:12)
(cid:12)
1 0 0
(cid:12)
(cid:12)
0 0 1
(cid:12)

− 0 ·

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
0 1 0
(cid:12)
(cid:12)
1 0 0
(cid:12)
(cid:12)
0 0 0
(cid:12)

= −1

Quanto ao determinante de uma matriz elementar de tipo II ELi↔cLi

é igual a c. A
prova disto será dada na segunda propriedade da lista de propriedades da subseção 2.1.1.
Já as matrizes elementares de tipo III possuem determinante igual a 1. A prova deste fato
também pode ser encontrada nas referências logo acima citadas. Vejamos aqui apenas
um exemplo para o caso 2 × 2, considerando para tanto, a matriz a seguir EL2↔cL1+L2
, c
constante, de modo que

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1

0

k · 1 + 0 k · 0 + 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
1 0
(cid:12)
(cid:12)
k 1
(cid:12)

= 1 · 1 − 0 · k = 1.

Dessa forma, podemos então enunciar o seguinte teorema

Teorema 2.1. Sejam EI, EII e EIII matrizes elementares de tipos I, II e III, respecti-
vamente. Tem-se que

(cid:46) det(EI) = −1.

(cid:46) det(EII) = c, c (cid:54)= 0.

68

(cid:46) det(EIII) = 1.

O teorema acima, juntamente com o Teorema 1.4, nos permite enunciar o seguinte

resultado

Teorema 2.2 Sejam E uma matriz elementar qualquer e A uma matriz n × n. Então

det(EA) = det(E) det(A)

Provemos a validade do teorema para o caso 2 × 2, sendo análoga a demonstração para
casos de ordens superiores. Tomando as matrizes

EI =

(cid:21)

(cid:20)0 1
1 0

, EII =

(cid:21)
(cid:20)k 0
0 1

, EIII =

(cid:21)

(cid:20)1 k
0 1

e A =

(cid:21)

(cid:20)a b
c d

,

temos que

det(EIA) =

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
c d
(cid:12)
(cid:12)
a b
(cid:12)

= bc − ad

det(EI) det(A) = −1 · (ad − bc) = bc − ad

det(EIIA) =

(cid:12)
(cid:12)
(cid:12)
(cid:12)
det(EII) det(A) = k(ad − bc)

ak bk
d
c

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= akd − bkc = k(ad − bc)

det(EIIIA) =

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
a + kc b + kd
(cid:12)
(cid:12)
(cid:12)

d

c

= ad + cdk − bc − ckd = ad − bc

•

•

•

det(EIII) det(A) = 1 · (ad − bc) = ad − bc

Como consequência da aplicação sucessivas vezes do Teorema 2.1, temos o

Corolário 2.1. Dadas matrizes elementares E1, . . . , Ek e uma matriz A, tem-se que

det(E1 · · · EkA) = det(E1) · · · det(Ek) det(A).

O resultado carregado pelo Teorema 2.1, de maneira simples, nos diz que

(cid:63) Se duas linhas de uma matriz são trocadas de posição, então seu determinante é

multiplicado por −1.

(cid:63) Se uma das linhas de uma matriz é multiplicada por uma constante k ∈ R, então

seu determinante é multiplicado por k.

(cid:63) Se uma combinação linear for realizada entre as linhas de uma matriz, então seu

determinante não se altera.

A seguir, provaremos um dos principais resultados deste capítulo, o qual é, segundo
Poole [30], conhecido como caracterização de invertibilidade em termos de de-
terminantes

. Teorema 2.3. Uma matriz n × n é invertível se, e somente se, det(A) (cid:54)= 0.

69

Demonstração. Se A é invertível, então segue pelo TFME que

daí, pelo Corolário 2.1, temos que

E1 · · · EnA = I.

det(E1) · · · det(Ek) det(A) = 1.

Como det(Ei) (cid:54)= 0 com 0, i = 1, . . . , k, de acordo com o Teorema 2.1, a igualdade acima
implica que det(A) (cid:54)= 0. Reciprocamente, seja det(A) (cid:54)= 0. Então, escalonando A, temos

det(E1) · · · det(Ek) det(A) = det(B),

onde B é a matriz escalonada de A. Novamente, pelo Teorema 2.1, temos que det(Ei) (cid:54)= 0
para todo i ,e como det(A) (cid:54)= 0, decorre que det(B) (cid:54)= 0. Dessa forma, temos que B não
pode possuir linhas nulas, conforme a propriedade 2 da subseção 2.1.1 e portanto B = I
(se não fosse, então A não seria invertível, de acordo com o TFME) pela deﬁnição de
matriz escalonada. Logo,

ou seja, A é invertível.

A = (E1 · · · Ek)−1,

2.1.1 Propriedades dos determinantes

Esta subseção é voltada à listagem e discussão das principais propriedades dos deter-
minantes, as quais são de grande utilidade para diversos cálculos e demonstrações, como
veremos ao longo do presente trabalho. Inciamos assim com o fato de que

1) O determinante de uma matriz e o determinante de sua transposta são iguais.

det(A) = det(AT )

Demonstração: Façamos os casos em que A tem ordem 1, 2, e 3, respectivamente, sendo
análogos os demais casos. Quando A tem ordem 1, temos o caso trivial, de modo que

A = [a11] ⇒ AT = [a11] ⇒ det(A) = a11 = det(AT ).

Notemos que se A possui ordem 2, então

A =

AT =

(cid:21)

(cid:21)

(cid:20) a11 a12
a21 a22
(cid:20) a11 a21
a12 a22

⇒ det(A) = a11a22 − a12a21

⇒ det(AT ) = a11a22 − a12a21.

Logo, det(A) = a11a22 − a12a21 = det(A)T . Podemos assim escrever

det(A) = a11 det(A11) − a12 det(A12)
21)

11) − a21 det(AT

= a11 det(AT
= det(AT )

70

Para compreender o que foi dito acima, basta observar que quando transpomos uma
matriz, são também transpostas as suas submatrizes. Façamos isso observando o caso em
que A possui ordem 3. Sendo B = AT , temos que

A =





a11 a12 a13
a21 a22 a23
a31 a32 a33

A11 =

Daí, podemos observar que
(cid:20)a22 a23
a32 a33
(cid:20)a21 a23
a31 a33
(cid:20)a21 a22
a31 a32

A12 =

A13 =





(cid:21)

(cid:21)

(cid:21)

B =





b11
b21
b31

b12
b22
b32



 =

b13
b23
b33





a11 a21 a31
a12 a22 a32
a13 a23 a33



 .

B11 =

B12 =

B13 =

(cid:20)b22
b32
(cid:20)b21
b31
(cid:20)b21
b31

(cid:21)

(cid:21)

(cid:21)

b23
b33

b23
b33

b22
b32

=

=

=

(cid:21)

(cid:21)

(cid:21)

(cid:20)a22 a32
a23 a33
(cid:20)a12 a32
a13 a33
(cid:20)a12 a22
a13 a23

= AT
11

= AT
21

= AT
31

Agora, comparando os determinantes a seguir

a11 det(A11) = a11a22a33 − a11a23a32
b11 det(AT
11) = a11a22a33 − a11a32a23

−a12 det(A12) = −a12a21a33 + a12a23a31
−b12 det(AT
21) = −a21a12a33 + a21a32a13

a13 det(A13) = a13a21a32 − a13a22a31
b13 det(AT
31) = a31a12a23 − a31a22a13

a11 det(A11) = a11 det(AT

11)

podemos observar que

i)

ii)

iii)

−a12 det(A12) + a13 det(A13)

= −a12(a21a33 − a23a31) + a13(a21a32 − a22a31)

−b12 det(AT

21) + b13 det(AT

31)

= −a21a12a33 + a21a32a13 + a31a12a23 − a31a22a13
= −a21a12a33 + a31a12a23 + a21a32a13 − a31a22a13
= −a12(a21a33 − a23a31) + a13(a21a32 − a22a31)

Segue então, a partir de i), ii) e iii) que

det(A) = a11 det(A11) − a12 det(A12) + a13 det(A13)
= b11 det(AT
21) + b13 det(AT
31)
= b11 det(B11) − b12 det(B12) + b13 det(B13)
= det(B)
= det(AT ).

11) − b12 det(AT

71

Logo, pelo que foi discutido acima, podemos generalizar a ideia para uma matriz quadrada
de ordem n, de tal modo que

det(A) = a11 det(A11) − a12 det(A12) + · · · + (−1)1+na1n det(A1n)

= a11 det(AT

11) − a21 det(AT

21) + · · · + (−1)1+nan1 det(AT

n1)

= det(AT ).

2) Se A tiver uma linha ou uma coluna multiplicada por uma constante c, então seu
determinante é igual a c · det(A).

Demonstração. Se for B a matriz obtida por meio da multiplicação da linha i de A por
c, então o determinante dessa nova matriz é, por deﬁnição,

det(B) = cai1Ai1 + . . . + cainAin = c(ai1Ai1 + . . . + ainAin) = c det(A).

Caso seja C a matriz obtida por meio da multiplicação da coluna j de A por c, então,
pela propriedade anterior, temos que det(C) = det(CT ) e pela primeira parte provada
logo acima, decorre que det(CT ) = c det(A). Como consequência, se A possuir uma
linha ou coluna nula, então seu determinante é igual a 0. Além disso, considerando C
com c = 1, a propriedade anterior garante que o determinante de uma matriz pode ser
calculado pela expansão dos cofatores de qualquer uma de suas colunas.

3) (Teorema de Cauchy-Binet)12 13 O determinante do produto de duas matrizes A
e B é igual ao produto dos determinantes destas duas matrizes.

det(AB) = det(A) det(B)
Demonstração. Se A é não invertível, então de acordo com o Teorema 2.6, decorre que
AB também não é invertível. Segue daí que, conforme o Teorema 2.9, det(AB) = 0 =
det(A) det(B). Por outro lado, se for A não-singular, então podemos escrever, de acordo
com o TFME

A = E1 · · · Ek,
onde E1, . . . , Ek são matrizes elementares. Daí, AB = E1 · · · EkB, que pelo Corolário 2.1
nos fornece

det(AB) = det(E1) · · · det(Ek) det(B)

= det(E1 · · · Ek) det(B)
= det(A) det(B)

4) Se uma matriz quadrada possui duas linhas (ou colunas) iguais, então seu determinante
é igual a 0.

12Jacques Philippe Marie Binet (1776-1856) foi um matemático francês que desenvolveu trabalhos em
Mecânica, Matemática e Astronomia. Em 1812 ele descobriu a fórmula para calcular o determinante da
multiplicação de matrizes, feito este que gloriﬁcou seu nome mais que qualquer outro de seus trabalhos
[59].

13Augustin-Louis Cauchy (1789-1857) foi um matemático francês que obteve resultados importantes
em diversas áreas, tais como Aritmética, Álgebra, Geometria, Estatística, Mecânica, Análises Real e
Complexa e Física Matemática. Publicou diversos resultados sobre determinantes, incluindo a fórmula
do produto de determinates de duas matrizes, a qual ele publicou em 1812 com alto rigor matemático
[9].

72

Demonstração. Suponhamos que as linhas Li e Lj de uma matriz quadrada A sejam
iguais. Consideremos então uma matriz elementar ELi↔Li−Lj

e uma matriz B tal que

B = ELi↔Li−Lj A.

Assim, temos que B possui ma linha nula, de modo que det(B) = 0, pela propriedade 2.
Daí,

0 = det(B) = det(ELi↔Li−Lj A) = det(ELi↔Li−Lj ) det(A).
Como det(ELi↔Li−Lj ) = 1, devemos ter det(A) = 0. A demonstração para o caso de
colunas iguais é análogo.

5) Se A é invertível, então

det(A−1) =

1
det(A)

.

Demonstração. De fato, desde que A−1A = I, segue, pelo Teorema de Cauchy-Binet, que

det(A−1A) = det(I) = 1 ⇒ det(A−1 =

1
det(A)

.

6) O determinante de uma matriz diagonal é igual ao produto dos elementos de sua
diagonal principal.

Demonstração. Consideremos uma matriz diagonal D de ordem n, cujas entradas da
diagonal principal sejam dii e seja D11 o menor complementar de d11. Como d1j = 0 com
j ≥ 2, decorre que

det(D) = d11 det(D11).
A matriz D11 é também diagonal. Então sendo D22 o cofator de d22 e d2j (cid:54)= 0, j ≥ 3,
decorre que det(D11) = d22 det(D22) e assim

det(D) = d11d22 det(D22).

Prosseguindo com o argumento acima sucessivas vezes, obtemos

det(D) = d11d22 · · · d(n−1)(n−1) det(Dnn),

porém, Dnn = dnn, o que implica det(Dnn) = dnn e portanto

det(D) = d11d22 · · · dnn.

7) O determinante de uma matriz de Vandermonde n × n é dado por

det(Vn×n) = (xn − x1)(xn − x2) · · · (xj − xi)(xj − xi−1) · · · (x2 − x1),

onde 1 ≤ i < n ≤ j.

Demonstração. Vamos fazer a demonstração dos casos iniciais, sendo as demonstrações
dos demais casos inteiramente análoga.

73

• n = 2: É imediato, visto que

det(V2×2) =

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1 x1
1 x2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= x2 − x1

• n = 3: Como já sabemos, o determinante não se altera por transposição ou combi-

nações lineares. Logo, temos que

det(V3×3) =

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1 x1 x2
1
1 x2 x2
2
1 x3 x2
3

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
1
1
x1 x2 x3
2 x2
1 x2
x2
3

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Daí, multiplicando a segunda linha por x1 e subtraindo da terceira linha, segue que

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
1
1
x1 x2 x3
2 x2
1 x2
x2
3

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
x1
0 x2

1
x2

2 − x1x2 x2

1
x3
3 − x1x3

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Agora, multiplicando a primeira linha por x1 e subtraindo da segunda linha, vem
que

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
x1
0 x2

1
x2

2 − x1x2 x2

1
x3
3 − x1x3

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
0
0 x2

1
x2 − x1
2 − x1x2 x2

1
x3 − x1
3 − x1x3

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

que por sua vez fornece

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Note agora que

1
0
0 x2

1
x2 − x1
2 − x1x2 x2

1
x3 − x1
3 − x1x3

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
0
1 x2 − x1 x2
1 x3 − x1 x2

0
2 − x1x2
3 − x1x3

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

0
1
1 x2 − x1 x2
1 x3 − x1 x2

0
2 − x1x2
3 − x1x3

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= 1 ·

(cid:12)
(cid:12)
(cid:12)
(cid:12)

x2 − x1 x2
x3 − x1 x2

2 − x1x2
3 − x1x3

(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

onde a matriz do lado direito da igualdade acima possui a segunda linha como sendo
múltipla de x2 − x1 e a terceira linha múltipla de x3 − x1, o que implica, de acordo
com o Teorema 2.2, que

(cid:12)
(cid:12)
(cid:12)
(cid:12)

x2 − x1 x2
x3 − x1 x2

2 − x1x2
3 − x1x3

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= (x3 − x1)(x2 − x1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1 x2
1 x3

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= (x3 − x1)(x3 − x2)(x2 − x1).

Observe que os passos acima podem ser resumidos como

det(V3×3) = det(VT

3×3) = det(EL3↔L3−x1L2VT

3×3)

= det(EL2↔L2−x1L1EL3↔L3−x1L2VT
3×3)
= det(EL2↔L2−x1L1EL3↔L3−x1L2V3×3)

onde k1 = x2 − x1, k2 = x3 − x1 e (cid:98)V2×2 =

= k1k2 det( (cid:98)V2×2),
(cid:20)1 x2
1 x3

(cid:21)

.

74

• n = 4: Seguindo a ideia do caso anterior, temos

com k1 = x2 − x1, k2 = x3 − x1, k3 = x4 − x1 e

det(V4×4) = k1k2k3 det( (cid:98)V3×3),

(cid:98)V3×3 =





1 x2 x2
2
1 x3 x2
3
1 x4 x2
4





Logo, aplicando a ideia descrita acima sucessivas vezes, obtém-se o resultado procurado.

2.1.2 Matriz Adjunta e a Regra de Cramer

Seja ˚A a matriz formada por todos os cofatores de cada uma das entradas de uma
matriz An×n. Diz-se que ˚AT é a matriz adjunta de A. Denotaremos ˚AT apenas por
A. Por exemplo, os cofatores da matriz

A =





3 0
5
1 7 −2
1
6 3





são

A11 = +

A21 = +

A31 = +

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
7 −2
(cid:12)
(cid:12)
1
3
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

0 5
3 1

= 13,

A12 = −

= −13, A13 = +

= −15,

A22 = −

= −27,

A23 = +

(cid:12)
5
0
(cid:12)
(cid:12)
7 −2
(cid:12)

= −35, A32 = −

(cid:12)
5
3
(cid:12)
(cid:12)
1 −2
(cid:12)

= 11,

A33 = +

(cid:12)
1 −2
(cid:12)
(cid:12)
1
6
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

3 5
6 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1 7
6 3

3 0
6 3

3 0
1 7

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= −39,

= 9

= 21.

Portanto



A =



13 −15 −35
11
21

−13 −27
11
−39



 .

Podemos assim deﬁnir, de modo geral, que a matriz adjunta de uma matriz quadrada
n × n é dada por

A =








A11 A21
A12 A22
...
...
A1n A2n



.






· · · An1
· · · An2
...
...
· · · Ann

Deﬁnido o conceito de matriz adjunta, devemos entender como substituir uma deter-

minada coluna de uma matriz. Para entender o processo, vejamos o exemplo a seguir.

Exemplo 2.4. Consideremos as matrizes

A =





a b
c
d e f
g h i





e

K =

75



 .





k1
k2
k3

Vamos substituir a segunda coluna de A por K. Seja então A12 a segunda coluna de A
e I21 a segunda linha da matriz I3×3. Daí,

A + (K − A12)I21 =

=

=





a b
c
d e f
g h i





 +







k1
k2
k3





 −



b
e
h













0 k1 − b 0
0 k2 − e 0
0 k3 − h 0









a b
c
d e f
g h i



 +





a k1
c
d k2 f
i
g k3





(cid:2)0 1 0(cid:3)

.

Como a coluna substituída foi a segunda, podemos representar a nova matriz da seguinte
forma

A2(K) =





a k1
c
d k2 f
i
g k3



 .

Generalizando a ideia do exemplo acima, se quisermos substituir a coluna j de uma
, devemos proceder da seguinte

por uma matriz coluna K = [ki]m×1

matriz A = [aij]m×n
forma. Primeiro, escrevemos A sob a forma
A = (cid:2)A11

· · · A1n

(cid:3)

e em seguida calculamos

A + (K − A1j)Ij1
= (cid:2)A11 · · · A1j · · · A1n
= (cid:2)A11

(cid:3) − (cid:2)O11 · · · O1(j−1) K − A1j O1(j+1) · · · O1n
(cid:3)

(cid:3)
,

· · · A1(j−1) K A1(j+1)

· · · A1n

onde A1j é a coluna j de A e Ij1 é a linha j da matriz identidade In×n. Escrevemos então

Aj(K) = (cid:2)A11

· · · A1(j−1) K A1(j+1)

· · · A1n

(cid:3) .

Partiremos agora para a demonstração da regra de Cramer 14, a qual estabelece
uma ligação entre os determinantes à solução de sistemas lineares, bem como a matriz
inversa de tais sistemas.

Teorema 2.3. (Regra de Cramer) Sejam A uma matriz invertível n × n, B = [bi1]n×1
e X = [xi1]m×1 tais que AX = B. Então

xi1 =

det(Ai(B))
det(A)

14Gabriel Cramer nasceu em Geneva (1704) e morreu em Bagnols (1752). O trabalho pelo qual ele
é mais conhecido é o seu tratado sobre curvas algébricas (Introduction à l’Analyse des Lignes Courbes
Algebriques), publicado em 1750 [6]. Embora alguns autores atribuam a descoberta da regra de Cramer a
Colin Maclaurin (1698-1754), onde este, segundo TWEEDIE [61], foi um dos mais distintos do brilhante
círculo de matemáticos escoceses que marcaram o curso do século XVIII, é assegurado por KOSINSKI
[34] que tal regra foi obtida devido a Cramer.

76

Demonstração. Inicialmente, iremos escrever A como uma matriz de blocos 1 × 1, ou
seja, [A]. Em seguida, escrevendo a matriz identidade In×n da forma
I = (cid:2)I11

In1

(cid:3) ,

· · ·

· · ·

I1i

segue que

Daí, temos que

porém,

AIi(X) = [A] (cid:2)I11
= (cid:2)AI11
= (cid:2)A11
= Ai(B).

· · · X · · ·

(cid:3)

In1

· · · AX · · · AI1n

(cid:3)

· · · B · · · A1n

(cid:3)

det(A) det(Ii(X)) = det(Ai(B)),

1 0 · · · x11
0 1 · · · x21
...
...
...
0 0 · · · xi1
...
...
...
0 0 · · · xn1
de acordo com a propriedade 2 da subseção anterior. Assim, como A é invertível, temos
que det(A) (cid:54)= 0 e portanto

· · ·
· · ·
...
· · ·
...
· · ·

det(Ii(X)) =

= xi1

...

...

(cid:12)
0
(cid:12)
(cid:12)
0
(cid:12)
...
(cid:12)
(cid:12)
(cid:12)
(cid:12)
0
(cid:12)
...
(cid:12)
(cid:12)
(cid:12)
1
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

det(A)xi1 = det(Ai(B)) ⇒ xi1 =

det(Ai(B))
det(A)

,

ﬁnalizando a demonstração.

Corolário 2.2. Seja A uma matriz n × n invertível. Então

A−1 =

1
det(A)

A.

Demonstração. Seja X1j a j−ésima coluna de A−1. Segue então que

Pela regra de Cramer, temos que

AX1j = I1j.

xij =

det(Ai(I1j))
det(A)

.

Resta agora observar que Ai(I1j) é a matriz obtida de A, substituindo sua coluna i pela
coluna j da matriz identidade, de modo que

det(Ai(I1j)) = 0 · A1i + 0 · A2i + · · · + 1 · Aji + · · · 0 · A(n−1)i + 0 · Ani = Aji,

de modo que

xij =

1
det(A)

Aji,

77

ou seja,

A−1 = X =

1
det(A)

A.

Embora, de acordo com POOLE [53], a utilização da regra de Cramer para fornecer a
inversa de uma matriz de ordem grande (e por sua vez as soluções de um sistema linear)
é cumputacionalmente ineﬁciente, sua aplicação na obtenção da inversa de matrizes 2 × 2
se mostra eﬁcaz, pois, dada uma matriz invertível

temos que

Y =

(cid:21)

(cid:20)a b
c d

,

Y−1 =

1
det(A)

(cid:21)

(cid:20) d −c
a
−b

2.2 Sistemas Lineares

Deﬁnição 2.4. Uma equação sob a forma

a1x1 + a2x2 + · · · anxn = b

é chamada de equação linear, onde x1, · · · , xn são as variáveis, a1, . . . , an são os
coeﬁcientes e b é o termo constante.

Dessa forma, temos que, equações como

• −2x1 + 3x2 = 6

• 10p − 31q + 9r = 0

• − 2

3n = 5

são exemplos de equações lineares, enquanto que

√

•

5p − 2q = 7

2 ) − 1 = 4x2

• sen( x
√
3

• 2
π

x − 2y + cos(z) = 1

são exemplos de equações não-lineares.

Uma solução de uma equação linear a1x1 +a2x2 +· · · anxn = b é uma lista de números

(s1, . . . , sn), tal que x1 = s1, . . . , xn = sn, ou seja,

a1s1 + a2s2 + · · · ansn = b.

Por exemplo, uma solução para a equação −2x1 + 3x2 = 6 é (0, 2), uma vez que −2 · 0 +
3 · 2 = 0 + 6 = 6. Já, quando temos duas ou mais equações relacionadas pelas mesmas
variáveis, então temos o que se chama de sistema de equações lineares, onde, neste

78

caso, a solução deve satisfazer cada uma das equações do sistema. Vejamos abaixo um
sistema formado por duas equações lineares

−2x1 + x2 = 1
x1 + 3x2 = 10

Notemos que (1, 3) satisfaz ambas as equações, pois

−2 · 1 + 1 · 3 = −2 + 3 = 1
1 + 3 = 10

1 · 1 + 3 · 3 =

Agora observemos que tal sistema pode ser escrito em termos matriciais, obtendo o

seguinte aspecto

Uma vez que

(cid:12)
(cid:12)
(cid:12)
(cid:12)

−2 1
3
1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:20)−2 1
1 3

(cid:21) (cid:20)x1
x2

(cid:21)

(cid:21)

(cid:20) 1
10

=

= −7 (cid:54)= 0, decorre pela regra de Cramer que

x1 =

x2 =

(cid:12)
(cid:12)
1
1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
10 3
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
−2 1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1 3
(cid:12)
(cid:12)

(cid:12)
−2
(cid:12)
(cid:12)
(cid:12)
1
(cid:12)
(cid:12)

1
10
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
−2 1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1 3
(cid:12)

=

−7
−7

= 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

−21
−7

= 3

Dessa forma, de modo geral, dado um sistema linear

a11x1 + a12x2 + · · · + a1nxn = b1
a21x1 + a22x2 + · · · + a2nxn = b2
...
an1x1 + an2x2 + · · · + a1nxn = bn

,

podemos escrevê-lo sob a forma matricial a seguir








a11 a12
a21 a22
...
...
an1 an2








· · ·
a1n
· · ·
a2n
...
...
· · · ann








x1
x2
...
xn








=








,








b1
b2
...
bn

ou simplesmente

AX = B,
onde A = [aij]m×n
. Dessa
(chamada matriz do sistema), X = [xi1]i×1
forma, desde que a matriz A seja invertível, temos, de acordo com o TFME, que a solução
do sistema acima é única e é dada por X = A−1B, podendo assim ser solucionado utili-
zando a regra de Cramer. Porém, como adverte LIMA [43], não é necessário determinar a
inversa de A para resolver um sistema linear, uma vez que para isso é necessário resolver

e B = [bi1]i×1

79

n sistemas lineares. Nesta mesma linha de pensamento temos POOLE [53], o qual aﬁrma
que a eliminação gaussiana se sobrepõe até à forma mais eﬁcaz de computar soluções de
sistemas lineares pela regra de Cramer, uma vez que para tal, é necessário calcular uma
grande quantidade de determinantes.

Para resolver sistemas lineares por meio da eliminação gaussiana, basta tomarmos a

matriz aumentada do sistema, ou seja, uma matriz de blocos da forma








a11 a12
a21 a22
...
...
an1 an2

a1n
· · ·
a2n
· · ·
...
...
· · · ann








b1
b2
...
bn

e em seguida aplicar operações elementares às linhas da matriz acima, de modo que a
matriz do sistema, isto é, o bloco esquerdo, ﬁque sob a forma escalonada. Isso nos permite
determinar a solução ou as soluções do sistema, quando esta(s) existir(em).

Exemplo 2.5. (Ajuste de Curvas) Os sistemas lineares podem ser utilizados para
determinar um polinômio conhecendo alguns pontos pelos quais o seu gráﬁco passa, ou
determinar um polinômio que mais se aproxima de um gráﬁco de dados. Façamos isso
para um polinômio de 2o grau p(x) = ax2 + bx + c, onde a, b, c ∈ R e a (cid:54)= 0, cujo gráﬁco
passa pelos pontos (1, 15), (2, 19) e (3, 33). Notemos que

15 = p(1) = a · 12 + b · 1 + c = a + b + c
19 = p(2) = a · 22 + b · 2 + c = 4a + 2b + c
33 = p(3) = a · 32 + b · 3 + c = 9a + 3b + c

Obtemos assim o seguinte sistema linear

a + b + c = 15
4a + 2b + c = 19
9a + 3b + c = 33

cuja matriz aumentada é





1 1 1 15
4 2 1 19
9 3 1 33





Daí, aplicando a eliminação gaussiana, temos





1 1 1 15
4 2 1 19
9 3 1 33





1


1
15
1
0 −2 −3 −41

0 −6 −8 −102





L2−4L1
−−−−→
L3−9L1
−−−−→

L3−3L2
−−−−→

1


1
15
1
0 −2 −3 −41

21
1
0

0





Como a matriz do sistema já está escalonada, ou seja, sob a forma


1
1
1
0 −2 −3

1
0
0





podemos reescrever o sistema inical sob a forma

a + b + c = 15
− 2b − 3c = −41
c = 21

80

Agora basta fazer a substituição regressiva, ou seja, substituir as variáveis determina-
das nas equações que ainda possuem variáveis indeterminadas, de baixo para cima. Pois,
susbstituindo c = 21 em −2b − 3c = −41, obtemos

e substituindo c = 21, b = 11 em a + b + c = 15, decorre que

−2b − 63 = −41 ⇒ b = −11

Logo, temos que o polinômio p(x) é dado por

a − 11 + 21 = 15 ⇒ a = 5.

p(x) = 5x2 − 11x + 21.

Exemplo 2.6 Em relação ao exemplo anterior, podemos observar que a situação nele
descrita poderia ser representada como produto de matrizes, isto é,





1 1 12
1 2 22
1 3 32



 =









a
b
c





15
19
33



 ,

onde a matriz









1 1 12
1 2 22
1 3 32
é claramente de Vandermonde. Tal fato não é mera coincidência. A aproximação de uma
curva por meio de polinômios é realizada utilizando matrizes de vandermonde, cujas linhas
são progressões geométricas de bases x0, x1, . . . , xn, com xi (cid:54)= xj para todo i (cid:54)= j. Tal
matriz multiplica uma matriz coluna de coeﬁcientes a0, . . . , an, o que acaba por resultar
em uma matriz coluna com entradas y0, y1, . . . , yn. Em termos












1 x0 x2
0
1 x1 x2
1
...
...
...
1 xn x2
n






· · · xn
0
· · · xn
1
...
· · ·
· · · xn
n











=











a0
a1
...
an

y0
y1
...
yn






Daí, de acordo com a propriedade 7 da subseção 2.1.1, segue que o determinante da
matriz de Vandermonde é sempre diferente de zero, desde que xi (cid:54)= xj para todo i (cid:54)= j e,
ortanto, tal sistema sempre admite soluções via Regra de Cramer.

O exemplo acima nos fornece um resultado últil, pois desde que os elementos da
matriz de vandermonde sejam distintos, é possível resolver o sistema. Porém, como já
apontado anterimente, resolver um sistema até mesmo por meio da regra de Cramer
pode ser um trabalho laborioso dependendo da ordem da matriz de seu sistema. Para
contornar tal problema, apresentamos aqui o processo de interpolação polinomial de
Lagrange 15. Para compreendê-lo, utilizaremos um caso particular para, em seguida,
fornecer sua generalização.

15Joseph Louis Lagrange (1736-1813), nascido em Turin, Itália, é considerado um dos dois maiores

matemáticos do século XVIII - o outro é Euler [46].

81

Consideremos os pontos (x0, y0), (x1, y1), (x2, y2). Vamos obter um polinômio P (x) de
grau 2, cujo gráﬁco passe por tais pontos. Inicialmente precisaremos construir polinômios
P0(x), P1(x) e P2(x) de grau 2, tais que

Pi(xj) =

(cid:26)1, se i = j
0, se i (cid:54)= j

Em seguida, tomamos os produtos da forma yiPi, com i = 0, 1, 2. Daí, o polinômio P (x)
buscado será dado por

P (x) = y0P0 + y1P1 + y2P2.
Essa construção de P (x) permite que a curva descrita por tal polinômio passe pelos
pontos fornecidos inicialmente. Por exemplo, P (x0) = y0P0(x0) + y1P1(x0) + y2P2(x0) =
y0 · 1 + y1 · 0 + y2 · 0 = y0. Por construção, temos que cada polinômio Pi(x) possui duas
raízes. Podemos então escrever

P0(x) = r0(x − x1)(x − x2)

P1(x) = r1(x − x0)(x − x2)

P2(x) = r2(x − x0)(x − x1)

onde ri ∈ R, com i = 0, 1, 2. Agora observemos que

1 = P0(x0) = r0(x0 − x1)(x0 − x2) ⇒ r0 =

1 = P1(x1) = r1(x1 − x0)(x1 − x2) ⇒ r1 =

1 = P2(x2) = r2(x2 − x0)(x2 − x1) ⇒ r2 =

Logo, podemos escrever P (x) sob a forma

1
(x0 − x1)(x0 − x2)

1
(x1 − x0)(x1 − x2)

1
(x2 − x0)(x2 − x1)

P (x) = y0

(x − x1)(x − x2)
(x0 − x1)(x0 − x2)

+ y1

(x − x0)(x − x2)
(x1 − x0)(x1 − x2)

+ y2

(x − x0)(x − x1)
(x2 − x0)(x2 − x1)

Note que, aplicando a expressão acima aos pontos do Exemplo 2.7, obtemos

15

(x − 2)(x − 3)
(1 − 2)(1 − 3)

=

15x2 − 75x + 19
2

19

(x − 1)(x − 3)
(2 − 1)(2 − 3)

= −19x2 + 76x − 57

33

(x − 1)(x − 2)
(3 − 1)(3 − 2)

=

33x2 − 99x + 66
2

e portanto, somando os resultados obtidos, segue que

P (x) = 5x2 − 11x + 21,

como esperado.

82

Assim, generalizando a ideia descrita acima, temos que, dados os pontos (x0, y0), . . . , (xn, yn),

existe um polinômio de grau n

P (x) = y0P0 + · · · + ynPn

onde

Pi =

(x − x0)(x − x1) · · · (x − xi−1)(x − xi+1) · · · (x − xn)
(xi − x0)(xi − x1) · · · (xi − xi−1)(xi − xi+1) · · · (xi − xn)

para todo i = 0, 1, . . . , n. Tal polinômio é único. De fato, se não fosse, então existiria um
outro polinômio Q(x) que interpola os mesmos pontos. Daí, teríamos que P (x) − Q(x)
seria um polinômio de grau n que P (xi) = 0 para i = 0, 1, . . . , n, o que é absurdo,
uma vez que um polinômio de grau n possui no máximo n raízes reais. Logo, segue que
P (x) = Q(x), provando assim a unicidade.

Realizada a discussão acima, vejamos mais um exemplo de situação que pode ser

resolvida por meio de sistemas lineares.

Exemplo 2.7. Uma exposição cultural é realizada em um espaço formado por 4 salas
conforme a Figura 2.1 abaixo:

Figura 2.1: Planta do local da exposição cultural.

Fonte: O autor

O número de visitantes em um determinado dia é igual a 100, onde estes estão distri-
buídos de modo que há xi visitantes na sala #i, com i = 1, 2, 3, 4. A cada dez minutos
os participantes se redistribuem e, além disso, nenhum participante visita mais que um
quarto nesse intervalo de tempo. Terminados os dez minutos, 40% dos visitantes per-
manecem na sala em que estavam, enquanto que os demais se distribuem de maneira
uniforme para os quartos diretamente acessíveis ao que eles inicialmente ocupava, como
por exemplo, de #2, metade vai para #3 e a outra metade para #4. Supondo que ao
ﬁnal de dez minutos permaneceram 12, 25, 26 e 37 visitantes nos quarto #1, #2, #3 e
#4, respectivamente, podemos montar o sistema linear que nos fornece quantas pessoas
haviam inicialmente em cada sala. Seguindo o diagrama da Figura 2.2 abaixo,

83

Figura 2.2: Diagrama de troca de salas.

Fonte: O autor

podemos montar o sistema linear que descreve a situação, o qual é dado por

0x2 +

0.4x1 +

0x3 + 0.2x4 = 12
0x1 + 0.4x2 + 0.3x3 + 0.2x4 = 25
0x1 + 0.3x2 + 0.4x3 + 0.2x4 = 26
0.6x1 + 0.3x2 + 0.3x3 + 0.4x4 = 37

Escalonando tal sistema, obtemos

4
10 x1 + 0x2 + 0x3 +
10 x2 + 3
10 x3 +
7
40 x3 +

4

2
10 x4 = 120
2
10 x4 = 250
29
1
20 x4 =
4
70 x4 = − 20
− 5

7

de modo que, fazendo a substituição regressiva, obtemos como solução

x1 = 10,

x2 = 20,

x3 = 30,

x4 = 40.

Quando a matriz aumentada de um sistema é escalonada, os pivôs de cada linha não
nula são os coeﬁcientes das variáveis líderes, enquanto que os demais são os coeﬁcientes
das variáveis livres. O número de soluções de um sistema depende de vários ftores.
Por exemplo, os sistemas homogêneos, isto é, os sistemas da forma AX = O possuem
apenas a solução trivial X = O quando A é quadrada e invertível, fato comprovado pelo
TFME.

Note que o número de variáveis livres de um sistema homogêneo está ligado ao número
de linhas não nulas de sua forma escalonada. Com efeito, seja A(cid:48) a matriz escalonada
do sistema AX = O. Temos que A(cid:48) possui um número r de entradas não nulas. Se
r = 0, então o sistema possui inﬁnitas soluções possíveis, uma vez que inﬁnitas matrizes
B satisfazem a equação OB = O. Se r > 0, então A(cid:48) possui r variáveis líderes e n − r
variáveis livres. Dessa forma, podemos concluir que, se um sistem homogêneo possui mais
variáveis que equações, então o seu número de soluções é inﬁnito. Mais formalmente, se
AX = O possui m equações e n variáveis, com m < n, então o dado sistema possui

84

inﬁnitas soluções, pois sendo m ≥ r, decorre que n > m ≥ r, o que por sua vez implica
n − r > 0, o que caracteriza a inﬁnitude de soluções do referido sistema. Com isso,
podemos enunciar os dois teoremas que seguem.

Teorema 2.4. Se um sistema linear homogêneo AX = O possui n variáveis e sua matriz
A(cid:48) possui r linhas não nulas, então o seu número de variáveis livres é igual a n − r.

Teorema 2.5. Se um sistema linear homogêneo possui mais variáveis do que equações,
então o mesmo possui inﬁnitas soluções.

Perceba que, quando m ≥ n, então o sistema tanto pode apresentar inﬁnitas soluções,

como também ua única solução. para analisarmos isso, consideremos o sistema

com a, b (cid:54)= 0. Escalonando sua matriz aumentada obtemos

ax + by = 0
x + y = 0

(cid:21)
(cid:20)a b 0
0 0 0

ou seja, ax + by = 0, onde y é a única variável livre, o que nos permite escrever

x = −

b
a

y

Dessa forma, podemos escrever y como um parâmetro t real, o qual pode assumir inﬁ-
nitos valores, e portanto

também assume inﬁnitos valores e consequentemente sistema possui inﬁnitas soluções.
Já se fosse a = 0 e b (cid:54)= 0, por exemplo, então teríamos

x = −

b
a

t

by = 0
x + y = 0

ou seja, um sistema homogêneo com duas equações e duas variáveis, porém by = 0 ⇒
y = 0 ⇒ x = 0, de modo que o sistema possui apenas a solução trivial.
Já para o caso m > n, consideremos, por exemplo, os sistemas

2x = 0
3x = 0

e

3x + 3y = 0
2x + 2y = 0
x + y = 0

onde ambos possuem, respectivamente, a solução trivial e inﬁnitas soluções.

A seguir, provaremos que, independente de qual seja o sistema linear, só existem três

alternativas para a sua quantidade de soluções: inﬁnitas, uma ou nenhua.

Teorema 2.6. Dado um sistema linear, temos que só existem três possibilidades no que
se refere ao seu número de soluções, as quais são

• O sistema possui ua única solução.

• O sistema possui inﬁnitas soluções.

85

• O sistema não possui soluções.

Demonstração. Consideremos um sistema linear AX = B. Se tal sistema possui uma
única solução ou não possui nenhuma solução, nada há para ser provado. Porém, é
necessário provar que, se um sistema admite duas soluções diferentes, então na verdade
ele possui inﬁnitas soluções. Com efeito, suponhamos que as matrizes X1 e X2 são duas
soluções do dado sistema, com X1 (cid:54)= X2. Temos então que AX1 = B = AX2 e assim

AX1 − AX2 = O ⇒ A(X1 − X2) = O

No próximo capítulo vamos obter uma interpretação geométrica para o Teorema 2.6.

86

3 Transformações Lineares e Matrizes

3.1 O plano R2 e o espaço tridimensional R3

O plano é formado por duas retas perpendiculares x e y, tais que x ∩ y = {0}, ou
seja, intersectam-se em um único ponto, ao qual é atribuído o nome de origem do plano.
A reta x, também chamada de eixo das abcissas ﬁca na posição horizontal, enquanto
que a reta y, também chamada de eixo das ordenadas, ﬁca este na posição vertical.
Esse sistema de eixos recebe o nome de plano cartesiano, também representado por R2
(lê-se “r dois”). Cada ponto do plano cartesiano é localizado por duas cordenadas x e y,
sendo que x é a abcissa e y é a ordenada. Dessa forma, um ponto P qualquer do plano
pode ser representado por um par (x, y). A origem é representada pelo ponto O = (0, 0).

Figura 3.1

Fonte: O autor

O plano é dividido em quatro quadrantes, de modo que um ponto P = (x, y) pertence

a um desses quadrantes de acordo com as seguintes condições:

• P = (x, y) pertence ao 1o quadrante se x ≥ 0 e y ≥ 0.

• P = (x, y) pertence ao 2o quadrante se x ≤ 0 e y ≥ 0.

• P = (x, y) pertence ao 3o quadrante se x ≤ 0 e y ≤ 0.

• P = (x, y) pertence ao 4o quadrante se x ≥ 0 e y ≤ 0.

A imagem abaixo ilustra um caso em que Pi = (xi, yi) pertence ao quadrante i, com
i = 1, 2, 3, 4.

87

Figura 3.2

Fonte: O autor

Tendo deﬁnido o plano cartesiano, podemos agora deﬁnir o conceito de vetor . Um
vetor no plano R2 é uma matriz que indica o deslocamento de um ponto do plano para
outro. Tendo em vista tal deﬁnição, um vetor que, por exemplo, resulta do deslocamento
de um ponto A para um ponto B é geralmente representado como −→
AB, conforme podemos
observar na Figura 3.3.

A representação de um vetor em termos matemáticos é apresentada de diferentes
maneiras em diversos livros, seja como uma matriz linha ou como matriz coluna, indicada
por uma letra minúscula e em negrito. Utilizaremos aqui a notação v = (x, y) para nos
referirmos a um vetor de coordenadas x e y. Em alguns momentos, escreveremos v em
sua forma de matriz

v =

(cid:21)

(cid:20)x
y

.

Ao longo deste capítulo ﬁcará mais claro o porquê de adotar tais notações. O importante
é manter a ordem das coordenadas, a ﬁm de evitar ambiguidades, pois (x, y) (cid:54)= (y, x),
como se pode observar na ﬁgura 3.4.

Figura 3.3

Figura 3.4

Fonte: O autor

88

As operações de soma, subtração e multipicação por constante real (também há a
multiplicação de vetor por vetor, mas falaremos da mesma mais a diante) são válidas,
uma vez que os vetores de R2 são matrizes. Assim, para determinar as coordenadas
de um dado vetor no plano, devemos subtrair as coordenadas do ponto de partida das
coordenadas do ponto de chegada do vetor, de modo que, se um vetor parte da origem,
suas coordenadas são exatamente aquelas do ponto de chegada, ou seja, um vetor −→
OA,
com A = (x, y) é tal que −→
OA = (x, y) − (0, 0) = (x − 0, y − 0) = (x, y). Já no caso de dois
pontos arbitrários A = (x, y) e B = (z, w), temos que

−→
AB = (z, w) − (x, y) = (z − x, w − y).

Observe que as coordenadas z − x e w − y indicam que o vetor −→
AB é obtido pelo desloca-
mento horizontal do ponto A por z−x unidades de medida, e em seguida um deslocamento
vertical de w − y unidades de medida, tal como apresenta a imagem a seguir:

Figura 3.5

Fonte: O autor

A observação acima nos permite, por exemplo, observar graﬁcamente a soma de dois
vetores u e v. Consideremos então pontos A = (x1, y1), B = (x2, y2) e C = (x3, y3) tais
−−→
BC conforme a ﬁgura abaixo
que u =

−−→
BC e w =

−→
AB, v =

89

Figura 3.6

Fonte: O autor

daí, é fácil ver que u + v = w, pois

u + v = (x2 − x1, y2 − y1) + (x3 − x2, y3 − y2) = (x3 − x1, y3 − y1) = w.

(cid:21)

É claro que, se escritos em forma de matrizes, o resultado não é diferente, como podemos
ver a seguir

u + v =

(cid:20)x2 − x1
y2 − y1

(cid:20)x3 − x2
y3 − y2
Quanto à multiplicação de um vetor v por um escalar (número real), temos que esta
também é válida, uma vez que matrizes podem ser multiplicadas por números reais. A
−→
AB, onde A = (x1, y1) e B = (x2, y2).
título de exemplo, vamos considerar um vetor v =
Pelo que já foi visto anteriormente, podemos escrever v = (x2 − x1, y2 − y1). Tomando
v1 = x2 − x1 e v2 = y2 − y1, segue que

(cid:20)x3 − x1
y3 − y1

= w.

+

=

(cid:21)

(cid:21)

v =

(cid:21)

(cid:20)v1
v2

.

Se quiséssemos multiplicar v =
−→
AB, como é possível notar na imagem que segue
4

AB por, digamos, 4, obteríamos um novo vetor −→
−→

AC =

90

Figura 3.7

Fonte: O autor

Assim, dada uma constante real c arbitrária, temos que

cv = c

(cid:21)

(cid:20)v1
v2

=

(cid:21)

(cid:20)cv1
cv2

Com as ferramentas deﬁnidas acima, podemos enunciar um teorema sobre as proprie-
dades dos vetores, onde estas são garantidas pelas propriedades já provadas no Capítulo
1 a respeito das matrizes, uma vez que os vetores no plano R2 são também matrizes.
Desta forma, podemos enunciar o seguinte teorema

Teorema 3.1. Sejam u, v e w vetores em R2 e c, d ∈ R. Então

(cid:46) u + v = v + u

(cid:46) (u + v) + w = u + (v + w)

(cid:46) u + 0 = u

(cid:46) u + (−u) = 0

(cid:46) c(u + v) = cu + cv

(cid:46) (c + d)u = cu + du

(cid:46) c(du) = (cd)u

(cid:46) 1 · u = u

Exemplo 3.1. A ﬁgura abaixo mostra um paralelogramo ABCD, cujos lados são for-
mados pelos vetores u = (u1, u2) = (x2 − x1, y3 − y1) e v = (v1, v2) = (x3 − x1, y2 − y1).

91

Figura 3.8

Fonte: O autor

Aﬁrmamos que

(cid:18)(cid:20)u1 v1
u2 v2
De fato, para determinaros a área de ABCD basta calcular a área do retângulo AECF
e em seguida subtrair as áreas 1○, 2○, . . . , 6○. Portanto, notemos que

= |u1v2 − u2v1|

ÁreaABCD =

(cid:21)(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

det

(cid:12)
(cid:12)
(cid:12)
(cid:12)

ÁreaAECF = (u1 + v1)(u2 + v2) = u1u2 + u1v2 + u2v1 + v1v2

e que

o que nos leva a

Área 1○ = u1v2 = Área 2○
v1v2
= Área 4○
Área 3○ =
2
u1u2
= Área 6○
Área 5○ =
2

ÁreaABCD = ÁreaAECF − Área 1○ − Área 2○ − . . . − Área 6○

= ÁreaAECF − 2Área 1○ − 2Área 3○ − Área 5○
= u1u2 + u1v2 + u2v1 + v1v2 − 2u1v2 − v1v2 − u1u2

= u1v2 − u2v1
(cid:18)(cid:20)u1 v1
u2 v2

= det

(cid:21)(cid:19)

Como AECF está no primeiro quadrante, temo que u1v2 − u2v1 > 0, porém em outros
quadrantes o resultado poderia ser negativo, mas, uma vez que a área negativa de ua ﬁgura
plana não está deﬁnida, consideramos o valor absoluto de u1v2 − u2v1 > 0 e portanto
(cid:18)(cid:20)u1 v1
u2 v2

= |u1v2 − u2v1|

ÁreaABCD =

(cid:21)(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

det

(cid:12)
(cid:12)
(cid:12)
(cid:12)

92

Uma consequência disso é que, a área de um triângulo formado por dois vetores é igual
à metade do módulo do determinante acima.

O produto interno entre dois vetores u e v é representado por (cid:104)u, v(cid:105), sendo igual à
soma dos produtos entre as entradas correspondentes desses vetores. Ou seja, dados dois
vetores










u1
u2
...
un

u =






e

v =















v1
v2
...
vn

temos

(cid:104)u, v(cid:105) = uT v = u1v1 + u2v2 + · · · + unvn.
Note que (cid:104)u, v(cid:105) = uT v = vT u = (cid:104)v, u(cid:105), ou seja, o produto interno é comutativo. A
distributividade da multiplicação em relação à soa e a multiplicação por escalar também
estão deﬁnidos para produto interno, pois tais propriedades são válidas para matrizes.
Agora notemos que

1 + u2
o que nos permite concluir que (cid:104)u, u(cid:105)≥0, onde a igualdade ocorre se, e somente se, u = 0.
O conceito de produto interno nos permite calcular o comprimento de um vetor u =

2 + · · · + u2
n,

(cid:104)u, u(cid:105) = u2

(u1, u2), sendo denotado por (cid:107)u(cid:107). Observando a ﬁgura abaixo,

Figura 3.9

Fonte: O autor

podemos observar um triângulo retângulo, o qual, pelo teorema de Pitágoras, nos permite
deduzir que

(cid:107)u(cid:107) =

(cid:113)

u2
1 + u2

2 = (cid:112)(cid:104)u, u(cid:105)

Exemplo 3.2. Seja h a altura do paralelogramo ABCD do exemplo 3.1 e B (cid:98)AD = θ na
ﬁgura abaixo

93

Figura 3.10

Fonte: O autor

sen(θ) =

h
(cid:107)u(cid:107)

Temos que

e assim

ÁreaABCD = (cid:107)v(cid:107) · h = (cid:107)u(cid:107)(cid:107)v(cid:107)sen(θ)

Exemplo 3.3. o produto interno de dois vetores u = (u1, u2) e v = (v1, v2) pode ser
calculado multiplicando seus comprimentos pelo cosseno do ângulo θ entre eles. Para
tanto, basta considerar o vetor u − v.

Figura 3.11

Fonte: O autor

Daí, notemos que, pela lei dos cossenos

(cid:107)u − v(cid:107)2 = (cid:107)u(cid:107)2 + (cid:107)v(cid:107)2 − 2(cid:107)u(cid:107)(cid:107)v(cid:107) cos(θ).

94

Por outro lado, a deﬁnição de produto interno nos fornece

(cid:107)u − v(cid:107)2 = (cid:104)u − v, u − v(cid:105)

= (cid:104)u, u(cid:105) − (cid:104)u, v(cid:105) − (cid:104)v, u(cid:105) + (cid:104)v, v(cid:105)
= (cid:107)u(cid:107)2 + (cid:107)v(cid:107)2 − 2(cid:104)u, v(cid:105)

,

o que implica

e portanto

(cid:107)u(cid:107)2 + (cid:107)v(cid:107)2 − 2(cid:107)u(cid:107)(cid:107)v(cid:107) cos(θ) = (cid:107)u(cid:107)2 + (cid:107)v(cid:107)2 − 2(cid:104)u, v(cid:105)

(cid:104)u, v(cid:105) = (cid:107)u(cid:107)(cid:107)v(cid:107) cos(θ)

Deriva, da expressão acima, um resultado bastante importante, chamado desigual-
dade de Cauchy-Schwarz . Para tanto, consideremos a função cosseno f : R → R, tal
que f (x) = cos(x), para todo x ∈ R. Como é sabido da trigonometria, a função cosseno
é periódica, de modo que oscila de modo uniforme no intervalo [0, 1], isto é,

−1 ≤ cos(x) ≤ 1.

Como (cid:107)u(cid:107)(cid:107)v(cid:107) ≥ 0, podemos multiplicar a desigualdade acima por tal valor, sem mudar
os sinais, de modo que

−(cid:107)u(cid:107)(cid:107)v(cid:107) ≤ (cid:107)u(cid:107)(cid:107)v(cid:107) cos(x) ≤ (cid:107)u(cid:107)(cid:107)v(cid:107),

ou seja,

provando assim a desigualdade de Cauchy-Schwarz

|(cid:104)u, v(cid:105)| = |(cid:107)u(cid:107)(cid:107)v(cid:107) cos(x)| ≤ (cid:107)u(cid:107)(cid:107)v(cid:107),

|(cid:104)u, v(cid:105)| ≤ (cid:107)u(cid:107)(cid:107)v(cid:107).

Apresentamos agora o conceito de ortogonalidade entre vetores. Consideremos então
dois vetores u e v. Dizemos que u é ortogonal a v, o que representaremos por u ⊥ v,
quando

(cid:104)u, v(cid:105) = 0.
Dessa forma, dizemos que um conjunto R = {v1, · · · , vn} é ortogonal se, e somente se,

Conhecendo tal deﬁnição, provemos o teorema a seguir.

(cid:104)vi, vj(cid:105) = 0, ∀ i (cid:54)= j.

Teorema 3.2. Seja R = {v1, · · · , vn} um conjunto ortogonal de vetores não todos nulos.
Então tais vetores são linearmente indepedentes, isto é, nenhum deles pode ser escrito
como combinação linear dos demais.

Demonstração. Suponhamos que um dos vetores vi, i = 1, · · · , n, de R seja uma combi-
nação linear dos demais. Assim

vi = c1v1 + · · · + cnvn.

95

Daí, tomando o produto interndo da igualdade acima por vi, em ambos os membros,
obtemos

(cid:104)vi, vi(cid:105) = (cid:104)vi, c1v1 + · · · + cnvn(cid:105)

= (cid:104)vi, c1v1(cid:105) + · · · + (cid:104)vi, cnvn(cid:105)
= c1(cid:104)vi, v1(cid:105) + · · · + cn(cid:104)vi, vn(cid:105)
= 0.

Como (cid:107)vi(cid:107)2 = (cid:104)v1, v1(cid:105), segue que (cid:107)vi(cid:107)2 = 0 ⇐⇒ vi = 0. Porém, vi foi tomado de
maneira arbitrária, o que nos leva a concluir que vi = 0, para todo i = 1, · · · , n, o que é
absurdo, uma vez que nem todos os vetores de R são nulos.

Agora, levando a ideia de independência linear para as colunas de uma matriz qua-
drada A = [aij]n×n, podemos dizer que Ax = 0 apenas quando x = 0. Para entendermos
um tal fato, consideremos um conjunto {a1, · · · , an} de vetores linearmente independentes
e uma combinação linear

x1a1 + · · · + xnan = 0,
onde nem todos os escalares xi são todos nulos. Digamos, por simplicidade, que seja
x1 (cid:54)= 0. Assim, poderíamos escrever

a1 = −

(cid:18)x2
x1

a2 + · · · +

(cid:19)

an

,

xn
x1

isto é, a1 seria uma combinação linear dos demais vetores, o que acarretaria em um
absurdo, uma vez que tais vetores são linearmente independentes. Bilateralmente, se tais
vetores não fossem linearmente independentes, então pelo menos um deles poderia ser
escrito como combinação linear dos outros, digamos

o que por sua vez nos daria

a1 = k2a2 + · · · + knan,

−1a1 + k1a2 + · · · + knan = 0,

isto é, uma combinação linear dos vetores a1, · · · , an, onde pelo menos o primeiro escalar
é diferente de 0, neste caso, igual a −1. Podemos assim aﬁrmar que uma combinação
linear de vetores linearmente independentes é igual ao vetor nulo se, e somente se, todos
os escalares são iguais a 0.

Dessa forma, se Ax = 0, onde as colunas de A, que representaremos por vetores
a1, · · · , an são linearmente independentes e x = [x1j], j = 1, · · · , n. Assim, podemos
escrever esta última igualdade sob a forma

x1a1 + · · · + xnan = 0,

(3.1)

o que, de acordo com o que foi provado acima, equivale a xi = 0, ou seja, x = 0. Sendo
assim, a recíproca é imediata, isto é, se Ax = 0 apenas quando 0, então a combinação
linear (3.1) ocorre apenas se xi = 0 para todo i e, portanto, os vetores a1, · · · , an são
linearmente independetes.

Logo, pelo que foi provado acima juntamente com a equivalência dos itens a) e c) do

TFME, podemos concluir que

Uma matriz quadrada A é invertível se, e somente se, suas colunas são linearmente
independetes.

96

Vamos agora passar ao estudo do espaço tridimensional, ou mais simplesmente, R3,
para o qual todas as deﬁnições e propriedades sobre o R2 são também válidas. A diferença
aqui é que, ao invés de duas coordenadas tanto para pontos, bem como para vetores, estes
apresentam três coordenadas, tal como podemos visualizar na imagem a seguir o vetor
−→
OP , onde O = (0, 0, 0) e P = (x1, y1, z1).

Figura 3.12

Fonte: O autor

A ﬁm de melhor observar o vetor −→
paralelepípedo, conforme o esboço abaixo

OP , podemos imaginá-lo como a diagonal de um

Figura 3.13

Fonte: O autor

As retas perpendiculares aos pontos A1, B1 e C1 exatamente nos planos xy, yz e
xz, respectivamente, intersecta-se no ponto P. Em relação ao espaço tridimensional,

97

passaremos a nos referir ao mesmo utilizando os temors “espaço xyz”, “espaço R3” ou
simplesmente R3.

O espaço R3 permite deﬁnir o conceito de produto vetorial , onde este é um vetor
perpendicular a outros dois vetores u e v, ambos de mesma origem. O produto vetorial
neste caso é representado por u × v, tal como mostra a ﬁgura abaixo

Figura 3.14

Fonte: O autor

Para determinar as coordenadas do produto vetorial entre dois vetores u e v, devemos
tomar um vetor w, tal que w ⊥ u (lê-se w perpendicular a u) e w ⊥ v. Vale lembrar
que quando dois vetores são perpendiculares, então seu produto interno é igual a 0, como
podemos constatar de acordo com a fórmula obtida no Exemplo 3.3, uma vez que

(cid:104)u, w(cid:105) = (cid:107)u(cid:107)(cid:107)w(cid:107) cos(90o) = (cid:107)u(cid:107)(cid:107)w(cid:107) · 0 = 0,

sendo análogo o processo para (cid:104)v, w(cid:105). Assim, sejam u = (ux, uy, uz), v = (vx, vy, vz) e
w = (wx, wy, wz). Daí, temos que

0 = (cid:104)u, w(cid:105) = uxwx + uywy + uzwz
0 = (cid:104)v, w(cid:105) = vxwx + vywy + vzwz
Como o sistema acima é homogêneo e possui ais variáveis que equações, decorre, pelo
Teorema 2.9 que o meso possui inﬁnitas soluções. Suponhamos que uxvy − uyvx (cid:54)= 0 e
vamos então escrevê-lo sob a forma

uxwx + uywy = −uzwz
vxwx + vywy = −vzwz

Pela regra de Cramer, obtemos

wx =

wy =

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
−uzwz uy
(cid:12)
(cid:12)
(cid:12)
−vzwz
vy
(cid:12)
(cid:12)
(cid:12)
(cid:12)
ux uy
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
vy
vx
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
ux −uzwz
(cid:12)
(cid:12)
(cid:12)
vx −vzwz
(cid:12)
(cid:12)
(cid:12)
(cid:12)
ux uy
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
vy
vx
(cid:12)
(cid:12)
(cid:12)
(cid:12)

98

=

uyvz−uzvy
uxvy−uyvx

wz

=

uzvx−uxvz
uxvy−uyvx

wz

Daí, como wz pode assumir qualquer valor real, tomemos wz = uxvy − uyvx e assim

w = (wx, wy, wz) = (uyvz − uzvy, uzvx − uxvz, uxvy − uyvx).

Se fosse uxvy − uyvx = 0, então passamos a uxvz − uzvx e, se este for diferente de zero,
então basta seguir de modo análogo o processo descrito acima e tomar wy = uxvz − uzvx.
Porém, se for também uxvz − uzvx = 0, então passamos a uyvz − uzvy e repetimos o
processo. Dessa forma, podemos enunciar a seguinte deﬁnição

Deﬁnição 3.1. O produto vetorial u × v de dois vetores u, v ∈ R3 é tal que

u × v = (uyvz − uzvy, uzvx − uxvz, uxvy − uyvx).

Uma mnemônica bastante utilizada na literatura utilizando determinantes, permite
calcular o produto vetorial de dois vetores de fora mais prática. O processo se baseia em
calcular o “determinante” de uma matriz 3 × 3 formada pelas coordenadas dos vetores
u = (ux, uy, uz) e v = (vx, vy, vz), juntamente com três vetores unitários i = (1, 0, 0), j =
(0, 1, 0), k = (0, 0, 1), da seguinte forma

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

i
k
j
ux uy uz
vz
vy
vx

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)

uy uz
vz
vy

(cid:12)
(cid:12)
(cid:12)
(cid:12)

i −

(cid:12)
(cid:12)
(cid:12)
(cid:12)

ux uz
vz
vx

(cid:12)
(cid:12)
(cid:12)
(cid:12)

j +

(cid:12)
(cid:12)
(cid:12)
(cid:12)

ux uy
vy
vx

(cid:12)
(cid:12)
(cid:12)
(cid:12)

k.

O uso das aspas se deve ao fato de que o processo acima é utilizado de modo didático,
não sendo um determinante de fato, uma vez que este é deﬁnido como um número real,
enquanto que o produto vetorial entre dois vetores é também um vetor.

Outra forma de representar o produto vetorial será mostrada a seguir, a qual possui
uma interpretação geométrica a respeito dos determinantes, porém, ao invés de área, esta
refere-se ao volume. para tanto, inicialmente devemos calcular

(cid:107)u × v(cid:107)2 = (uyvz − uzvy)2 + (uzvx − uxvz)2 + (uxvy − uyvx)2.

A ﬁm de evitar operações com carga notacional, tomemos

a = uyvz
c = uzvx
e = uxvy

b = uzvy
d = uxvz
f = uyvx

g = uxvx
h = uyvy
i = uzvz

,

de modo que

(cid:107)u × v(cid:107)2 = (a − b)2 + (c − d)2 + (e − f )2

= a2 + b2 + c2 + d2 + e2 + f 2 − 2(ab + cd + ef ).

Por outro lado, temos que

(cid:107)u(cid:107)2(cid:107)v(cid:107)2 = (u2

x + u2

x + v2
= a2 + b2 + c2 + d2 + e2 + f 2 + g2 + h2 + i2

y + v2
z )

y + u2

z)(v2

e

(cid:104)u, v(cid:105)2 = (uxv2

x + uyvy + uzvz)2
= g2 + h2 + i2 + 2(ab + cd + ef ),

99

de modo que

(cid:107)u × v(cid:107)2 = (cid:107)u(cid:107)2(cid:107)v(cid:107)2 − (cid:104)u, v(cid:105)2.
Como (cid:104)u, v(cid:105) = (cid:107)u(cid:107)(cid:107)v(cid:107) cos(θ), onde θ é o ângulo formado por u e v, decorre da igualdade
acima que

(cid:107)u × v(cid:107)2 = (cid:107)u(cid:107)2(cid:107)v(cid:107)2 − (cid:107)u(cid:107)2(cid:107)v(cid:107)2 cos(θ)2

= (cid:107)u(cid:107)2(cid:107)v(cid:107)2(1 − cos(θ)2)
= (cid:107)u(cid:107)2(cid:107)v(cid:107)2sen(θ)2.

Daí, extraindo a raiz quadrada em ambos os membros da igualdade (cid:107)u × v(cid:107)2 = (cid:107)u(cid:107)2(cid:107)v(cid:107)2sen(θ)2,
obtemos

(cid:107)u × v(cid:107) = (cid:107)u(cid:107)(cid:107)v(cid:107)sen(θ).

Comparando o resultado acima com aquele obtido no Exemplo 3.2, podemos concluir
que o comprimento do produto vetorial de dois vetores u e v é igual à área de um
paralelogramo, cujos lados opostos são os vetores u e v. Além disso, podemos concluir
que, a área de um triângulo formado por u e v, tal como no Exemplo 3.3, é igual à metade
de (cid:107)u × v(cid:107).

Vamos ﬁnalizar nossa discussão a respeito das propriedades do R3 com o exemplo a

seguir, o qual mostra a interpretação geométrica de terminantes de matrizes 3 × 3.

Exemplo 3.4. Sejam

u =







 ,

v =





v1
v2
v3



 ,

e

w =

u1
u2
u3









w1
w2
w3

vetores arbitrários de R3 e chamemos de V a matriz





u1 v1 w1
u2 v2 w2
u3 v3 w3



 .

Daí, como

segue que

v × w =





v2w3 − v3w2
v3w1 − v1w3
v1w2 − v2w1



 ,

(cid:104)u, v × w(cid:105) = u1(v2w3 − v3w2) + u2(v3w1 − v1w3) + u3(v1w2 − v2w1)

= u1(v2w3 − v3w2) − u2(v1w3 − v3w1) + u3(v1w2 − v2w1)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

v2
v3
w2 w3

v1
v3
w1 w3

v1
v2
w1 w2

− u2

+ u3

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= u1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
= det(VT )

= det(V).

Agora consideremos o paralelpípedo formado pelos vetores u, v e w na ﬁgura a seguir

100

Figura 3.15

Fonte: O autor

Como já é sabido, a área do paralelogramo sombreado (base do paralelepípedo) é igual
a (cid:107)u × v(cid:107), enquanto que h = (cid:107)u(cid:107) cos(φ), onde φ é o ângulo entre u × v e u. Portanto,
temos que o volume Volp do paralelepípedo é dado por

Volp = (cid:107)u × v(cid:107) · (cid:107)u(cid:107) cos(φ) = (cid:104)u, v × w(cid:105)

Logo, podemos concluir que

Volp = (cid:104)u, v × w(cid:105) = det(V),

ou seja, o determinante de uma matriz 3 × 3 formada por três vetores do R3, todos de
mesma origem, é igual ao volume de um paralelepípedo, cujas arestas são formadas por
tais vetores.

Como prometido, vamos apresentar aqui uma interpretação geométrica do Teorema

2.10. Consideremos para tanto um sistema linear

(cid:26)ax + by = j
cx + dy = k

Sabemos que o mesmo só possui solução se suas equações forem linearmente independen-
tes, pois só assim a mesma será invertível. Nesse caso, o sistema possui ua única solução.
Geometricamente, signiﬁca dizer que as retas que representam cada equação não possuem
mesma inclinação, portanto são concorrentes em um único ponto (x0, y0), com

x0 =

dj − bk
ad − bc

y0 =

ak − cj
ad − bc

101

Figura 3.16

Fonte: O autor

No caso em que uma das linhas da matriz do sistema é múltipla da outra, ou seja,
são linearmente dependentes, o sistema possui inﬁnitas soluções, pois é possível partir de
uma e chegar a outra por meio de combinações elementares. Assim, o sistema reduz-se a
uma só equação. Por exemplo, dados α, β ∈ R não nulos, se tivermos o sistema

(cid:26)αax + αby = αj
βax + βby = βj

temos que a matriz escalonada do sistema é
(cid:20)a b
j
0 0 0

(cid:21)

ou seja, reduz-se a uma só equação, a qual possui inﬁnitas soluções (uma reta possui
inﬁnitos pontos).

Figura 3.17

Fonte: O autor

Existem ainda os casos em que se tem um sistema impossível, o qual não possui
soluções. Sabemos que duas retas só podem se intersectar em único ponto (solução

102

única) ou em todos os pontos, quando são coincidentes (inﬁnitas soluções). Logo, no
caso de sistemas impossíveis, temos que suas equações representam retas paralelas, não
possuindo assim pontos em comum e, consequentemente, nenhuma solução (x0, y0) para
o sistema. Tais sistemas possuem a forma

(cid:26)ax + by = j
ax + by = k

com j (cid:54)= k.

Figura 3.18

Fonte: O autor

Com raciocínio análogo podemos pensar a respeito de sistemas lineares com três in-

cógnitas. Considerando um sistema

(cid:26)a1x + b1y + c1z = d1
a2x + b2y + c2z = d2

onde a1x + b1y + c1z = d1 é a equação do plano Γ1 e a2x + b2y + c2z = d2 é a equação do
plano Γ2, temos duas possibilidades:

a) o sistema é possível e determinado, tendo inﬁnitas soluções, ou seja, os planos se
intersectam em uma reta como mostra a ﬁgura a seguir

103

Figura 3.19

Fonte: O autor

b) o sistema é impossível, ou seja, os planos são paralelos, não possuindo pontos em
comum.

Figura 3.20

Fonte: O autor

3.2 Transformações Lineares

3.2.1 Conceito e exemplos

Deﬁnição 3.2. Uma transformação linear é uma aplicação do tipo T : X → Y ,
tal que para quaisquer vetores u, v ∈ X e qualquer escalar c, são válidas as seguintes
propriedades

(cid:46) T (cv) = cT (v)

(cid:46) T (u + v) = T (u) + T (v)

A deﬁnição acima nos permite concluir um fato importante sobre as transformações

lineares, que é a preservação da origem, isto é, T (0) = 0, isto porque

T (0) = T (0 + 0) = T (0) + T (0) ⇒ T (0) = T (0) − T (0) = 0.

104

A propriedade acima é bastante útil para identiﬁcar quando uma aplicação é uma trans-
formação linear. Porém, vale ressaltar que Tv(0) = 0 não implica a linearidade de uma
aplicação, pois, tomando, por exemplo, a aplicação T : R2 → R2 com

temos que

porém

(cid:21)(cid:19)

(cid:18)(cid:20)x
y

T

(cid:21)

(cid:20) xy
x + y

,

=

T (0) = T

(cid:21)(cid:19)

(cid:18)(cid:20)0
0

(cid:21)

(cid:20) 0 · 0
0 + 0

=

=

(cid:21)
(cid:20)0
0

= 0,

(cid:21)

(cid:18)(cid:20)x
y

T

(cid:20)w
z

+

(cid:21)(cid:19)

= T

(cid:21)(cid:19)

(cid:18)(cid:20)x + w
y + z

=

=

=

(cid:54)=

(cid:21)

(cid:20) (x + w)(y + z)
(x + z) + (y + w)
(cid:20)xy + xz + wy + wz
(x + y) + (w + z)

(cid:21)

(cid:20)xy + xz
x + y
(cid:21)

(cid:20) xy
x + y
(cid:18)(cid:20)x
y

(cid:21)(cid:19)

+

(cid:21)

+

(cid:21)

(cid:20)wz + wy
w + z
(cid:21)

(cid:20) wz
w + z
(cid:18)(cid:20)w
z

+ T

(cid:21)(cid:19)

,

= T

contrariando assim a propriedade de soma da Deﬁnição 3.2, e portanto T não é linear.

Exemplo 3.5. Uma translação no plano é uma aplicação Tv : R2 → R2, tal que

Tv(P) = P + v,
o que signiﬁca que Tv desloca qualquer ponto P do plano na direção do vetor v, como
podemos observar na imagem 3.16, onde os vértices (e consequentemente todos os outros
pontos) do triângulo ABC, com A = (x1, y1), B = (x2, y2) e C = (x3, y3), são transladados
na direção do vetor v = (α, β), de modo a obtermos o triângulo A(cid:48)B(cid:48)C (cid:48), com A(cid:48) =
(x1 + α, y1 + β), B(cid:48) = (x2 + α, y2 + β) e C (cid:48) = (x3 + α, y3 + β).

Figura 3.21

Fonte: O autor

105

Note agora que se v não for o vetor nulo, então

Tv(0) = Tv(0, 0) = (0, 0) + v = (0, 0) + (α, β) = (0 + α, 0 + β) = (α, β) (cid:54)= 0,

o que contraria a propriedade de preservação da origem, nos permitindo concluir que
Tv é não-linear. Todavia, as translações possuem a propriedade de preservar distâncias.
Provemos isso utilizando os lados AB e A(cid:48)B(cid:48), sendo análoga a demonstração para os outrs
lados. Primeiro, observe que

d(A, B) = (cid:112)(x2 − x1)2 + (y2 − y1)2

ao passo que

d(A(cid:48), B(cid:48)) = (cid:112)(x2 + α − x1 − α)2 + (y2 + β − y1 − β)2

= (cid:112)(x2 − x1)2 + (y2 − y1)2
= d(A, B).

Portanto, AB = A(cid:48)B(cid:48), AC = A(cid:48)C(cid:48) e BC = B(cid:48)C(cid:48), ou seja, os triângulos ABC e A(cid:48)B(cid:48)C(cid:48) são
congruentes, pelo caso de congruência LLL. Analogamente, podemos observar que um
paralelepípedo Π pode ser transladado no R3 por uma translação Tv : R3 → R3, de modo
que Tv(Π) é um paralelepípedo com as mesmas medidas (e consequentemente o mesmo
volume) de Π.

Figura 3.22

Fonte: O autor

Um exemplo importante de transformação linear é a rotação. Uma rotação no R2
é uma aplicação Rθ : R2 → R2, a qual possui a propriedade de rotacionar um vetor u
qualquer do plano por um ângulo θ, tal como podemos observar na ﬁgura a seguir

106

Figura 3.23

Fonte: O autor

Observe que o ângulo α formado por u e o eixo x nos permite concluir que

cos(α) =

sen(α) =

⇒ ux = (cid:107)u(cid:107) cos(α)

ux
(cid:107)u(cid:107)
uy
(cid:107)u(cid:107) ⇒ uy = (cid:107)u(cid:107)sen(α),

ao passo que

cos(α + θ) =

sen(α + θ) =

urx
(cid:107)Rθ(u)(cid:107)
ury

⇒ urx = (cid:107)Rθ(u)(cid:107) cos(α + θ)

(cid:107)Rθ(u)(cid:107) ⇒ ury = (cid:107)Rθ(u)(cid:107)sen(α + θ).

Sendo (cid:107)u(cid:107) e (cid:107)Rθ(u)(cid:107) raios de um arco de circunferência de centro na origem, segue que
(cid:107)Rθ(u)(cid:107) = (cid:107)u(cid:107). Daí, segue que

e

urx = (cid:107)Rθ(u)(cid:107) cos(α + θ)

= (cid:107)u(cid:107) cos(α + θ)

= (cid:107)u(cid:107) cos(α) cos(θ) − (cid:107)u(cid:107)sen(α)sen(θ)

= ux cos(θ) − uysen(θ).

ury = (cid:107)Rθ(u)(cid:107)sen(α + θ)

= (cid:107)u(cid:107)sen(α + θ)

= (cid:107)u(cid:107)sen(α) cos(θ) + (cid:107)u(cid:107)sen(θ) cos(α)

= uxsen(θ) + uy cos(θ).

Utilizando a multiplicação de matrizes, podemos escrever os dois resultados obtidos acima
da seguinte maneira

Rθ(u) =

(cid:21)

(cid:20)urx
ury

=

(cid:20)cos(θ) −sen(θ)
cos(θ)
sen(θ)

(cid:21) (cid:20)ux
uy

(cid:21)

,

107

onde a matriz

(cid:20)cos(θ) −sen(θ)
cos(θ)
sen(θ)

(cid:21)

é denominada matriz da transformação Rθ. Tomando-a por R, podemos simples-
mente escrever

Rθ(u) = Ru.
Vamos agora provar que Rθ é linear. Consideremos para tanto dois vetores u = (u1, u2),
v = (v1, v2) de R2 e uma constante real arbitrária c. Daí,

Rθ(u + v) = R(u + v) = Ru + Rv = Rθ(u) + Rθ(v)

e

Rθ(cv) = R(cv) = cRv = cRθ(v),

o que prova a linearidade de Rθ.

visualmente na ﬁgura abaixo

Por ser menos intuitivo, o fato de que Rθ(u + v) = Rθ(u) + Rθ(v) pode ser observado

Figura 3.24

Fonte: Adaptado de LIMA [43], p.42

Note que os vetores −→AC e −→AF estão sobre as diagonais dos paralelogramos ABCD e AEFG,
respectivamente, o que justiﬁca os ângulos B(cid:98)AC e E(cid:98)AG serem bisectados. Assim, temos
que

2α + γ = θ = 2β + γ ⇒ α = β.
Decorre daí que C(cid:98)AF = α + β + γ = θ, ou seja, rotacionar o vetor u + v por um ângulo
θ é o mesmo que a somar as rotações de u e v por este mesmo ângulo.

Vale ressaltar que a rotação é invertível, uma vez que det(R) = cos2(θ) + sen2(θ) =
1 (cid:54)= 0. Portanto, para reverter uma rotação de ângulo θ, basta realizar a rotação em
sentido contrário R−θ, sendo sua matriz de transformação

R−1 =

(cid:21)
(cid:20)cos(−θ) −sen(−θ)
cos(−θ)
sen(−θ)

=

(cid:20) cos(θ)
sen(θ)
−sen(θ) cos(θ)

(cid:21)

= RT ,

108

o que acaba por mostrar que a inversa da matriz de uma rotação é igual à transposta
desta mesma matriz.

Assim como podem ser realizadas rotações no plano, também podem ser realizadas no
espaço. A diferença neste caso é que, a rotação deve ser em torno de um ou mais eixos.
Por exemplo, a matriz de rotação de ângulo ψ (no sentido anti-horário) em torno do eixo
z é a seguinte

Rz,ψ =





cos(ψ) −sen(ψ) 0
0
sen(ψ)
1
0

cos(ψ)
0



 ,

pois uma rotação de ângulo ψ de um vetor v = (vx, vy, vz) em torno do eixo z equivale
a rotacionar as coordenadas vx, vy por um ângulo ψ, deixando a coordenada vz intacta,
o que justiﬁca o fato da terceira linha e terceira coluna de Rz,ψ serem, respectivamente,
iguais à terceira linha e à terceira coluna da matriz I3×3.

De maneira análoga, as matrizes de rotação (no sentido anti-horário) de ângulo θ e φ,

em torno dos eixos x e y, respectivamente, são





Rx,θ =

0

0

1
0 cos(θ) −sen(θ)
cos(θ)
0 sen(θ)





e

Ry,φ =





cos(φ) 0 −sen(φ)
1
sen(φ) 0

0
cos(φ)

0



 .

Dessa forma, a combinação, por exemplo, z −→ x −→ y das rotações acima aplicadas a
um vetor arbitrário do R3 é dada por uma matriz R, tal que

R = Rz,ψRx,θRy,φ,

ou seja,

R =





c(φ)c(ψ) + s(φ)s(ψ)s(θ) −c(θ)s(ψ) −c(ψ)s(φ) + c(φ)s(ψ)s(θ)
c(ψ)c(θ) −s(φ)s(ψ) − c(φ)c(ψ)s(θ)
c(φ)s(ψ) − c(ψ)s(φ)s(θ)
c(θ)s(φ)

c(φ)s(θ)

s(θ)



 ,

onde s() = sen() e c() = cos().

Exemplo 3.6. A projeção ortogonal P : R2 → R2 de um vetor v = (vx, vy) sobre uma
reta y = ax (vide Figura 3.20. abaixo) também é uma transformação linear. Mas antes
de provarmos sua linearidade, vamos obter sua matriz de transformação.

109

Figura 3.25
]

Fonte: Adaptado de LIMA [43], p.44

Notemos que P (v) = (v(cid:48)

x, av(cid:48)

x). Daí, pelo teorema de Pitágoras, temos que

d(v, 0)2 = d(v, P (v))2 + d(P (v), 0)2,

o que equivale a

x + v2
v2

y = {(v(cid:48)
= 2(v(cid:48)

x − vx)2 + (av(cid:48)
x)2 + v2

x + 2a2(v(cid:48)

x − vy)2} + {(v(cid:48)
y − 2v(cid:48)

x)2 + v2

x)2 + a2(v(cid:48)
xvx − 2av(cid:48)

x)2}
.
xvy

Decorre, a partir daí, que

2v(cid:48)

xvx + 2av(cid:48)

xvy = 2(v(cid:48)
x(vx + avy) = (1 + a2)(v(cid:48)

x)2 + 2a2(v(cid:48)

x)2

x)2

⇐⇒ v(cid:48)

.

Portanto

e

⇐⇒

v(cid:48)
x =

vx+avy
1+a2

v(cid:48)
x =

(cid:18) 1

(cid:19)

1 + a2

vx +

(cid:18) a

(cid:19)

1 + a2

v(cid:48)
y =

(cid:18) a

(cid:19)

1 + a2

vx +

(cid:18) a2

(cid:19)

1 + a2

vy

vy.

Logo, temos que a matriz da projeção P é

P =





1
1+a2
a
1+a2





a
1+a2
a2
1+a2

de modo que, utilizando as propriedades de multiplicação de matrizes, podemos mostrar
, o que nos permite escrever
facilmente a linearidade de P . Vamos então tomar α =

1
1+a2

P = α

(cid:20)1
a
a a2

(cid:21)

.

110

Assim, considerando dois vetores u = (u1, u2), v = (v1, v2) arbitrários de R2 e uma
constante real k, temos que

P (u + kv) = P

(cid:21)(cid:19)

(cid:18)(cid:20)u1 + kv1
u2 + kv2

= α

= α

= α

= α

(cid:21)

(cid:21)

(cid:21) (cid:20)u1 + kv1
(cid:20)1
a
a a2
u2 + kv2
(cid:20) u1 + au2 + kv1 + akv2
au1 + a2u2 + akv1 + a2kv2
(cid:18)(cid:20) u1 + au2
(cid:20) v1 + av2
(cid:21)
av1 + a2v2
au1 + a2u2
(cid:21)
(cid:21) (cid:20)v1
(cid:21)
(cid:21) (cid:20)u1
v2
u2

(cid:20)1
a
a a2

+ kα

+ k

(cid:21)(cid:19)
,

(cid:20)1
a
a a2
(cid:21)
(cid:20)u1
u2

= P

+ kP

(cid:21)

(cid:20)v1
v2

o que acaba por provar a linearidade de P .

= P (u) + kP (v)

Exemplo 3.7. Uma reﬂexão S : R2 → R2 em torno de uma reta que passa pela origem
é uma transformação linear que leva um vetor v = (v1, v2) em um vetor S(v), de modo
que P (v) = P (S(v)), como podemos observar na Figura 3.21. Dessa forma, temos que o
vetor P (v) coincide com a metade da diagonal do paralelogramo formado pela soma dos
vetores v e S(v), o que justiﬁca o fato de v + S(v) = 2P (v).

Figura 3.26

Fonte: Adaptado de LIMA [43], p.45

Daí, temos que

Assim, temos que a matriz da transformação S é dada por

S(v) = 2P (v) − v = 2Pv − Iv = (2P − I)v.

S = 2P − I = 2α

(cid:21)

(cid:20)1
a
a a2

(cid:21)

(cid:20)1 0
0 1

−

(cid:20)2α − 1
2αa

=

(cid:21)
2αa
2αa2 − 1

111

Portanto, as coordenadas do vetor S(v) = (v(cid:48)

1, v(cid:48)

2) são dadas por

v(cid:48)
1 = 2α(v1 + av2) − v1
v(cid:48)
2 = 2αa(v1 + av2) − v2

Quanto à linearidade de S, esta pode ser facilmente veriﬁcada, seguindo passos análogos
aqueles mostrados no exemplo anterior.

Os casos em que a reﬂexão ocorre em torno de uma reta y = ax + b, com b (cid:54)= 0 podem
ser mais facilmente resolvidos com o auxílio das translações. Por exemplo, se quiséssemos
reﬂetir o ponto (4, 3) em torno da reta y = 2x + 5 da ﬁgura abaixo

Figura 3.27
Fonte: O autor

bastaria seguir os seguintes passos

• Aplicar a translação T : R2 → R2, T (x, y) = (x, y) + (0, −5) à reta y = 2x + 5 e ao
ponto P = (−4, 7), de modo a obtermos, respectivamente, a reta yT = 2x e o ponto
PT = (−4, 2)

112

Figura 3.28

Fonte: O autor

• Aplicar a reﬂexão S : R2 → R2 do ponto PT em torno da reta y = 2x, de modo a

obter o ponto PST = (p1, p2). Sabendo que α = 1

1+22 = 1

5

, segue que

p1 = 2 · 1
p2 = 2 · 1

5 (−4 + 2 · 2) − (−4) = 4
5 (−4 + 2 · 2) − 2 = −2

ou seja, PST = (4, −2).

Figura 3.29

Fonte: O autor

113

• Aplicar a inversa de T , isto é, T −1 : R2 → R2, T −1(x, y) = (x, y) + (0, 5), tanto à
reta y = 2x, como aos pontos PT e PST , de modo a obtermos novamente a reta
y = 2x + 5, o ponto P e a reﬂexão deste em torno desta última, ou seja, o ponto
PT ST = (p(cid:48)

2), onde

1, p(cid:48)

PT ST = T −1(PST ) = T −1(4, −2) = (4, −2) + (0, 5) = (4, 3),

o que ﬁnaliza o processo de reﬂexão, podendo assi ser observado na imagem abaixo

Figura 3.30

Fonte: O autor

A subseção a seguir está diretamente voltada à aplicação das translações, rotações e
reﬂexões às cônicas, um dos principais foco deste trabalho.

3.2.2 Translação, rotação e reﬂexão de cônicas

Será admitido, a partir desta subseção, que o leitor conhece as cônicas, de modo que
não será apresentada aqui a forma como a equação de cada uma delas é obtida, pois
acabaríamos digredindo do real foco, que são as aplicações das matrizes em diferentes
áreas do conhecimento, tanto dentro como fora da Matemática.

Exemplo 3.8. Considere uma circunferência C, de centro P = (a, b) e raio r. Vamos
transladar C por um vetor v = (α, β) de modo a obtermos a circunferência CT , de
mesmo raio, porém com centro PT = (c, d). Seja então Tv : R2 → R2 uma translação,
com Tv(x, y) = (x + α, y + β), para todo x, y ∈ R2. Agora notemos que, desenvolvendo a
equação de C, obtemos

(x − a)2 + (y − b)2 = r2

⇒ x2 + y2 − 2ax − 2by = r2 − (a2 + b2)

114

Em seguida, sabendo que as coordenadas de cada ponto de CT serão dadas por (x(cid:48), y(cid:48)),
temos que

x(cid:48) = x + α ⇐⇒ x = x(cid:48) − α
y(cid:48) = y + β ⇐⇒ y = y(cid:48) − β

Daí, substituindo x por x(cid:48) − α e y por y(cid:48) − β na expansão da equação de C, obtemos

(x(cid:48))2 + (y(cid:48))2 − 2(α + a)x(cid:48) − 2(β + b)y(cid:48) = k,

onde k = r2 − (a2 + b2) − 2(αa + βb). Por outro lado, a equação de CT nos fornece

(x(cid:48))2 + (y(cid:48))2 − 2c(x(cid:48)) − 2d(y(cid:48)) = r2 − (c2 + d2)

Assim, para que (3.1) e (3.2) coincidam, é necessário que

(3.1)

(3.2)

α + a = c ⇐⇒ α = c − a
β + b = d ⇐⇒ β = d − b

o que nos permite concluir que v = (c − a, d − b) e, portanto, Tv é dada por

Tv(x, y) = (x + c − a, y + d − b)

Figura 3.31

Fonte: O autor

O exemplo acima nos mostra um resultado que já é esperado das translações, o qual
nos diz, de modo mais simples, que para transladar uma circunferência por um vetor v,
basta aplicarmos tal translação a cada um dos pontos da circunferência.

Vejamos mais um exemplo, apenas para reforçar essa ideia.

Exemplo 3.9. Consideremos a função f : R → R, com f (x) = ax2 + bx + c, a (cid:54)= 0.
Sabemos que seu gráﬁco G = {(x, y) ∈ R2 ; y = ax2 + bx + c} é uma parábola. Vamos
supor, sem perda de generalidade, que a > 0 e ∆ < 0. Aplicando a translação Tv : R2 ⇒
R2, v = (α, β), aos pontos P = (x, y) ∈ G

115

Figura 3.32

Fonte: O autor

temos que as novas coordenadas da parábola serão dadas por pontos PT = (x(cid:48), y(cid:48)) =
(x + α, y + β), que equivale a

x = x(cid:48) − α
y = y(cid:48) − β

e substituindo as expressões acima em y = ax2 + bx + c, obtemos

y(cid:48) − β = a(x(cid:48) − α)2 + b(x(cid:48) − α) + c

⇐⇒

y(cid:48) = a(x(cid:48))2 + (−2αa + b)x(cid:48) + (aα2 − bα + c + β)

= a(cid:48)(x(cid:48))2 + b(cid:48)x(cid:48) + c(cid:48),

onde

a(cid:48) = a
b(cid:48) = −2αa + b
c(cid:48) = aα2 − bα + c + β.
Algo que podemos observar é que, os valores de c e β podem interferir no número de
raízes reais da equação a(cid:48)(x(cid:48))2 + b(cid:48)x(cid:48) + c(cid:48) = 0, uma vez que o discriminante ∆(cid:48) da parábola
G(cid:48) = {(x(cid:48), y(cid:48)) ∈ R2 ; y(cid:48) = a(cid:48)(x(cid:48))2 + b(cid:48)x(cid:48) + c(cid:48)} é

∆(cid:48) = (b(cid:48))2 − 4a(cid:48)c(cid:48)

= (−2αa + b)2 − 4a(aα2 − bα + c + β)
= b2 − 4a(c + β).

e sendo o número de raízes reais um ou dois apenas quando ∆(cid:48) ≥ 0, decorre que

b2 − 4a(c + β) ≥ 0 ⇐⇒ b ≥ ±2(cid:112)c + β,

isto é, só existem raízes reais quando c ≥ −β. Nos casos em que b = 0, para que seja
∆(cid:48) ≥ 0, é necessário que a < 0 e c + β ≥ 0 ou a > 0 e c + β ≤ 0.

É importante lembrar que uma cônica pode assumir qualquer posição no plano, de
modo que, por exemplo, uma parábola de vértice V pode não apresentar o seu eixo de
simetria paralelo aos eixos x ou y, mas concorrente a ambos, como mostra a ﬁgura a
seguir

116

Figura 3.33

Fonte: O autor

Vejamos que sempre é possível obter uma parábola como a que aparece na ﬁgura
acima, partindo de uma parábola na posição padrão, isto é, com eixo de simetria paralelo
a um dos eixos x ou y, por meio da composição duas translações e uma rotação. Devemos
antes lembrar que, uma parábola rotacionada não apresenta uma lei de formação padrão,
isto é, a forma ax2 + bx + c. A forma geral de uma função quadrática possui dua variáveis
e é dada por ϕ : R2 → R2, tal que

ϕ(x, y) = Ax2 + 2Bxy + Cy2 + Dx + Ey + F,

onde A, B, C, D, E e F são constantes reais. De modo mais geral, uma função como
a citada acima é chamada forma quadrática, porém não trataremos aqui os detalhes
sobre tais formas. Para uma abordagem minuciosa, consultar LIMA [44].

Vamos tomar, sem perda de generalidade, uma parábola G = {(x, y) ; y = ax2 + bx +
c, a (cid:54)= 0} que possua eixo de simetria paralelo ao eixo y e seja a > 0. Procederemos
de forma similar ao que foi realizado no exemplo de reﬂexão realizado na seção anterior.
(cid:1) coincida
Primeiro, transladamos a parábola de modo que seu vértice (h, k) = (cid:0)− b
com a origem (Figura 3.27). Para tanto, basta fazer α = −h e β = −k no resultado
obtido no Exemplo 3.9, de modo a obtermos a parábola GT , tal que

2a, − ∆

4a

y(cid:48) = a(x(cid:48))2 + (2ah + b)x(cid:48) + (ah2 + bh + c − k)

(3.3)

117

Figura 3.34

Fonte: O autor

A expressão acima se torna mais simples, uma vez que 2ah + b = 0 e ah2 + bh + c − k = 0,
reduzindo-se então a

y(cid:48) = a(x(cid:48))2
Agora, aplicamos uma rotação de ângulo θ no sentido horário, obtendo assim a parábola
GRT , a qual pode ser visualizada na ﬁgura a seguir

Figura 3.35

Fonte: O autor

onde suas coordenadas são dadas por

x(cid:48)(cid:48) =
x(cid:48)cos(θ) + y(cid:48)sen(θ)
y(cid:48)(cid:48) = −x(cid:48)sen(θ) + y(cid:48)cos(θ)

Uma vez que

(cid:12)
(cid:12)
(cid:12)
(cid:12)

cos(θ)
sen(θ)
−sen(θ) cos(θ)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= 1 (cid:54)= 0, temos que o sistema acima possui solução única,

118

a qual, pela Regra de Cramer, é

x(cid:48) =

y(cid:48) =

(cid:12)
(cid:12)
x(cid:48)(cid:48)
sen(θ)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
y(cid:48)(cid:48)
cos(θ)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
sen(θ)
cos(θ)
(cid:12)
(cid:12)
(cid:12)
−sen(θ) cos(θ)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
x(cid:48)(cid:48)
cos(θ)
(cid:12)
(cid:12)
(cid:12)
−sen(θ) y(cid:48)(cid:48)
(cid:12)
(cid:12)
(cid:12)
cos(θ)
sen(θ)
(cid:12)
(cid:12)
(cid:12)
−sen(θ) cos(θ)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= x(cid:48)(cid:48) cos(θ) − y(cid:48)(cid:48)sen(θ)

= x(cid:48)(cid:48)sen(θ) + y(cid:48)(cid:48) cos(θ)

Substituindo os valores acima em (3.3), obtemos uma expressão da forma

A(x(cid:48)(cid:48))2 + 2Bx(cid:48)(cid:48)y(cid:48)(cid:48) + C(y(cid:48)(cid:48))2 + Dx(cid:48)(cid:48) + Ey(cid:48)(cid:48) + F = 0,

(3.4)

onde

A = a cos2(θ)
B = −asen(θ) cos(θ)
C = asen2(θ)

D = − cos(θ)
E = −sen(θ)
F = 0

OBS.: Note que se, ao invés de termos aplicado a translação sobre o vértice da parábola,
mas sobre um ponto (p, q) qualquer da mesma, os coeﬁcientes A, B e C seriam os mesmos,
ao passo que

D = (2ap + b − 1) cos(θ)
E = −(2ap + b + 1)sen(θ)
F = ap2 + bp + c − q

Por ﬁm, aplicamos a translação T−v, com v = (h, k), o que nos fornece a parábola GT RT
(Figura 3.29), cujos pontos possuem coordenadas

(x, y) = (x(cid:48)(cid:48) + h, y(cid:48)(cid:48) + k)

Figura 3.36

Fonte: O autor

119

Daí, substituindo x(cid:48)(cid:48) = x − h e y(cid:48)(cid:48) = x − k em (3.4) e reagrupando os termos semelhantes,
obtemos

onde

Ax2 + 2Bx y + Cy2 + Dx + Ey + F = 0,

D = −2Ah − 2Bk + D
E = −2Bh − 2Ck + E
F = Ah2 + 2Bhk + Ck2 − Dh − Ek + F

Apesar de termos utilizado uma parábola como exemplo, os resultados obtidos acima

são análogos para uma forma quádrica qualquer ϕ : R2 → R2, com

ϕ(x, y) = Ax2 + 2Bxy + Cy2 + Dx + Ey + F.

Desse modo, se realizarmos translações a esta, os seus coeﬁcientes A, B e C não se
alteram, o que nos permite dizer que estes são invariantes por translações, enquanto que
os demais coeﬁcientes se tornam

D(cid:48) = −2Ah − 2Bk + D
E(cid:48) = −2Bh − 2Ck + E
F (cid:48) = Ah2 + 2Bhk + Ck2 − Dh − Ek + F

Exemplo 3.10. Vamos aplicar à parábola G = {(x, y) ; y = x2 − 5x + 6} uma rotação de
45o no sentido horário em torno de seu vértice (h, k) utilizando os passos acima. Sabemos
que

(h, k) =

−

(cid:18)

b
2a

, −

∆
4a

(cid:19)

=

(cid:18) 5
2

, −

1
4

(cid:19)

.

Transladando G para origem pelo vetor v = (cid:0)− 5

2, 1

4

(cid:1), obtemos GT , onde

Aplicando a rotação de 45o à GT , obtemos a parábola GRT , cujos coeﬁcientes são

y(cid:48) = (x(cid:48))2.

A = 1 ·

(cid:17)2

(cid:16) 1√
2

=

1
2

B = −1 ·

C = 1 ·

·

1√
2
(cid:17)2

(cid:16) 1√
2

1√
2

=

1
2

=

1
2

√

2
2
√

2
2

1√
2

1√
2

= −

= −

D = −

E = −

F = 0,

ou seja, sua equação é dada por

1
2

x2 − xy +

1
2

y2 −

√
2
2

x −

√

2
2

y = 0.

Transladando GRT por v = (cid:0) 5
translações são

2, − ∆

4a

(cid:1), obtemos GT RT , cujos coeﬁcientes invariantes por

− 2

(cid:16)

−

1
2

(cid:17) (cid:16)

−

(cid:17)

1
4

−

D(cid:48) = −2 ·

1
2

E(cid:48) = −2

(cid:16)

−

5
2

(cid:17)

·

1
2

·

5
2

− 2 ·

1
2
√

F (cid:48) =

25
8

+

5
8

+

1
32

+

5

2

4

√

2
2
√

2
2

√

−11−2

2

4
√

11−2
4

2

=

=

√

121+36

32

2

,

(cid:17)

−

1
4
√

(cid:16)

−

−

2
8

+ 0 =

120

o que ﬁnalmente nos fornece sua equação

1
2

x2 − xy +

1
2

y2 −

√

11+2
4

2

x +

√

11−2
4

2

y +

√

121+36

32

2

= 0

Figura 3.37

Fonte: O autor

Feita a discussão acima, uma pergunta pode ocorrer: “o que acontece com as raízes
de nova parábola?” Bem, as raízes de G são 2 e 3. Ao aplicarmos a translação sobre o
vértice de G, a parábola GT obtida possui apenas uma raíz, a qual é 0. Os pontos (2, 0)
e (3, 0) se tornam (− 1
4 ), deixando de assumir o papel de raízes, tornando-se
apenas soluções da equação y = x2. Quando a rotação é realizada, podemos observar que
a parábola GRT obtida novamente intercepta o eixo x em dois pontos, os quais fornecem
as raízes de tal parábola. Uma delas obviamente é x = 0, mas vamos determiná-las de
maneira formal. Para tanto, vamos multiplicar a equação abaixo por 2 em ambos os
membros

4 ) e ( 1

2 , 1

2 , 1

x2 − xy +

y2 −

x −

y = 0.

√
2
2

√

2
2

1
2

e em seguida reescrevê-la em termos de x, de forma a obtermos
(cid:16)

√

x2 +

−2y −

x + y2 −

2y = 0.

√
(cid:17)
2

Agora notemos que, resolvendo a equação quadrática acima, obtemos

2
2
e fazendo y = 0, segue que x = 0 ou x =

x = y +

√

±

(cid:113)
√
8

1
2

2y + 2

2, que são as raízes de GRT .

A ﬁm de determinar as raízes de GT RT , basta realizar o mesmo processo realizado

acima, ou seja, reescrevemos a equação de GT RT em termos de x, ou seja
√
(cid:33)(cid:35)

√

(cid:32)

(cid:34)

√
2

11 + 2
2

x + y2 +

11 − 2
2

2

y +

121 + 36

2

= 0.

16

x2 +

−2y −

Resolvendo-a, obtemos

1
2

√

x = y +

√

2

11 + 2
4

(cid:113)
√
8

±

1
2

2y + 2

√

2 + 2.

121

Tomando y = 0 e

√

ou

2 ∼= 1,414, obtemos
√

x =

11 + 2
4

2

(cid:113)
√
2

+

1
2

2 + 2 ∼= 4,56

x =

11 + 2
4

√
2

(cid:113)
√
2

−

1
2

2 + 2 ∼= 2,36,

onde tais valores são as respectivas raízes de GT RT .

Vejamos a seguir que, assim como é possível rotacionar e transladar cônicas, também

é possível reﬂexioná-las em torno de uma reta qualquer.

Exemplo 3.11. Vamos reﬂexionar a parábola G = {(x, y) ; y = x2 − 4x + 5} em torno da
reta r de equação y = −2x + 1. Para tanto, devemos determinar o foco e a reta diretriz
de G. Ora, completando quadrados em x2 − 4x + 5 = y, obtemos

(x − 2)2 = 4 ·

1
4

(y − 1) ,

o que nos fornece o foco F =

(cid:17)

(cid:16)

2, 5
4

e a reta diretriz yd = 3
4

de G. Daí,

• Transladando r, F e yd = 3
4

pelo vetor v = (0, −1), obtemos

y(cid:48) = −2x,

F(cid:48) =

(cid:18)

2,

(cid:19)

1
4

e

y(cid:48)
d = −

1
4

.

• Reﬂexionando F(cid:48) em torno de y(cid:48), obtemos

F(cid:48)(cid:48) =

(cid:18)

−

7
5

, −

(cid:19)

.

29
20

Agora, reﬂexionando y(cid:48)
obtemos pontos (x(cid:48)(cid:48)
d, t(cid:48)(cid:48)

d = − 1
4
d), tais que

, isto é, pontos da forma

(cid:16)

d, − 1
x(cid:48)
4

(cid:17)

, em torno de y(cid:48),

d = 2 · 1
x(cid:48)(cid:48)
5 ·

(cid:16)

x(cid:48)
d + (−2) ·

(cid:17)(cid:17)

(cid:16)

− 1
4

− x(cid:48)

d = −

3
5

x(cid:48)
d +

1
5

⇒ x(cid:48)

d = −

5
3

x(cid:48)(cid:48)
d +

1
3

d = 2 · 1
y(cid:48)(cid:48)

5 · (−2) ·

(cid:18)

−

5
3

x(cid:48)(cid:48)
d +

1
3

+ (−2) ·

(cid:17)(cid:19)

(cid:18)

−

−

(cid:16)

− 1
4

(cid:19)

1
4

=

4
3

x(cid:48)(cid:48)
d −

5
12

• Agora, transladando F(cid:48)(cid:48) e y(cid:48)(cid:48)

d = 4

d − 5
3 x(cid:48)(cid:48)
12
(cid:19)

F(cid:48)(cid:48)(cid:48) =

(cid:18)

−

7
5

, −

9
20

pelo vetor w = (0, 1), temos

e

y(cid:48)(cid:48)(cid:48)
d =

4
3

x(cid:48)(cid:48)(cid:48)
d +

7
12

.

Obtivemos com os passos acima, tanto o foco F(cid:48)(cid:48)(cid:48), que chamaremos simplesmete de FX,
da parábola GX, ou seja, a reﬂexão de G em torno de r. Por
como a reta diretriz y(cid:48)(cid:48)(cid:48)
d
uma questão de simplicidade, vamos escrever a reta diretriz de GX apenas como y =
3 x + 7
4
, a qual chamaremos de ydX. Logo, para determinar a equação de GX, basta

12

122

utilizarmos a deﬁnição de parábola, ou seja, encontrarmos o conjunto de pontos (x, y)
que são equidistantes de F(cid:48)(cid:48)(cid:48) e y(cid:48)(cid:48)(cid:48)
d

. Para tanto, notemos que

d1 = d((x, y), F(cid:48)(cid:48)(cid:48)) =

(cid:114)(cid:16)

x + 7
5

(cid:17)2

(cid:16)

+

y + 9
20

(cid:17)2

(cid:113)

=

x2 + y2 + 14

5 x + 9

10 y + 173

80

e

d2 = d((x, y), r) =

|16x − 12y + 7|
162 + 122

√

Daí, como devemos ter d1 = d2, elevando esta última ao quadrado e reordenando os
termos, obtemos

9
25

x2 +

24
25

xy +

16
25

y2 +

56
25

x +

33
25

y +

51
25

= 0,

que é exatamente a equação de GX.

Figura 3.38

Fonte: O autor

3.2.3

Identiﬁcando formas quadráticas

Anteriormente, vimos como realizar uma translação de vetor v e rotação de ângulo
θ a uma forma quadrática (a qual, em seus casos mais interessantes, assumem a forma
de ua cônica). Porém, como fazer o processo reverso? isto é, como partir de uma forma
quadrática qualquer e descobrir se ela é uma elipse, uma parábola ou uma hipérbole?
Bem, sabemos que uma forma quadrática pode ser facilmente identiﬁcada, desde que
esteja sob a forma

ϕ(x, y) = M x2 + N y2

123

onde M, N são números reais não ambos nulos. Logo, dependendo dos valores de M e
N , ϕ e tomando ϕ(x, y) = 0 (pois este é o caso que nos interessa) pode assumir a forma
de uma cônica (parábola, elipse ou hipérbole). Neste caso, dada uma forma quadrática

ϕ(x, y) = Ax2 + Bxy + Cy2 + Dx + Ey + F

temos de encontrar uma forma de colocá-la em sua forma padrão, isto é, centrada na
origem, e então aplicar as transformações lineares convenientes (translação e rotação) a
ﬁm de identiﬁcar sua posição original.

Como já discutido na subseção anterior, se transladarmos uma forma quadrática como
a que temos acima por um vetor v = (α, β), temos que os coeﬁcientes A, B e C não se
alteram, enquanto que

D(cid:48) = −2Aα − 2Bβ + D
E(cid:48) = −2Bα − 2Cβ + E
F (cid:48) = Aα2 + 2Bαβ + Cβ2 − Dα − Eβ + F

Note que devemos nos preocupar em eliminar os termos D(cid:48) e E(cid:48), pois estes são termos
(também chamados de termos lineares) diretamente ligados a x e y (ou seja, podem
alterar o tipo de forma quadrática) respectivamente, enquanto que F inﬂui apenas na
posição que a forma quadrática vai assumir. Neste caso, ϕ se reduzirá a

ϕ(x, y) = Ax2 + Bxy + Cy2 + F (cid:48)

Note que ainda temos o termo B, que também deverá ser eliminado, mas já sabemos que
este surge da rotação, então vamos atentar inicialmente para D(cid:48) = E(cid:48) = 0. Ora, se isso
ocorre, então obtemos os seguinte sistema

cuja matriz é




Aα + Bβ =



Bα + Cβ =

D
2
E
2

M =

(cid:21)

(cid:20)A B
B C

,

a qual será chamada de matriz de ϕ. Logo, desde que AC − B2 (cid:54)= 0, o sistema acima
possui uma única solução, a qual é, pela regra de Cramer

α =

CD−BE
2(AC−B2)

β =

AE−BD
2(AC−B2)

Exemplo 3.12 Considere a forma quádrica ϕ(x, y) = 7x2 − 6xy + 7y2 + 2x + 10y − 4.
Vamos eliminar os termos lineares da mesma. Para tanto, temos que a matriz de ϕ é

M =

(cid:21)
(cid:20) 7 −3
7
−3

,

onde det(M) = 40 (cid:54)= 0. Daí, temos que

α =

β =

2·7−(−3)·10
2·40

7·10−(−3)·2
2·40

= 11
20
= 19
20

124

Portanto, o vetor que elimina os termos 2x e 10y é v = ( 11
F (cid:48) = − 93
10

, de modo que a nova forma quadrática possui a forma

20 , 19

20 ). Calculando F (cid:48), obtemos

ϕ(x, y) = 7x2 − 6xy + 7y2 − 93
10 .

O exemplo acima nos mostra, na prática, como eliminar os termos lineares. Agora
precisamos encontrar uma forma de eliminar o termo que contém xy. Como já sabemos,
este surge após uma rotação. Então, sem perda de generalidade, tomemos uma forma
quádrica com D = E = F = 0, ou seja, sob a forma

Aplicando uma rotação de ângulo θ, podemos fazer a substituição

ϕ(x, y) = Ax2 + Bxy + Cy2.

x (cid:55)−→ ax − by
y (cid:55)−→ bx + ay

onde a = cos(θ), b = sen(θ), tal que

ϕ(x, y) = ϕ(ax − by, bx + ay) = ϕ(x, y) = A(cid:48)x2 + B(cid:48)xy + C (cid:48)y2,

com

A(cid:48) = Aa2 + 2Bab + Cb2
B(cid:48) = −Aab + B(a2 − b2) + Cab
C (cid:48) = Ab2 − 2Bab + Ca2

Note que B(cid:48) pode ser reescrito da forma

B(cid:48) = a(Ba + Cb) − b(Aa + Bb) =

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
a Aa + Bb
(cid:12)
(cid:12)
b Ba + Cb
(cid:12)

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Para que seja B(cid:48) = 0, então a matriz

(cid:20)

a

(cid:21)

b

Aa + Bb Ba + Cb

a

(cid:12)
(cid:12)
(cid:12)
Aa + Bb Ba + Cb
(cid:12)

b

.

deve possuir uma linha (ou coluna) nula ou uma linha (ou coluna) seja múltipla da outra.
Se for o primeiro caso, então, temos as seguintes possibilidades

• (a, b) = (0, 0): absurdo, uma vez que (a, b) é um vetor unitário.

• (Aa + Bb, Ba + Cb) = (0, 0): Nesse caso, teríamos obviamente B(cid:48) = 0, porém

A(cid:48) = a(Aa + Bb) + b(Ba + Cb) = 0.

Além disso, escrevendo C (cid:48) da seguinte forma

C (cid:48) = b(Ab − Ba) − a(Bb − Ca)

e notando que Ba = −Cb e Bb = −Aa, segue que

C (cid:48) = b(Ab + Cb) + a(Aa + Ca) = (A + C)(a2 + b2) = A + C,

reduzindo ϕ a

ϕ(x, y) = (A + C)y2.
Daí, teríamos ϕ(x, y) = 0 apenas se A + C = 0, e neste caso ϕ seria reduzida a um
ponto, o que é absurdo. Se A + C (cid:54)= 0, então y = 0 e ϕ novamente se reduz a um
ponto.

125

• (a, Aa + Bb) = (0, 0): Aqui temos a = 0, o que implica b = 1 (pois (a, b) é unitário),

de modo que

Assim, a matriz em estudo se condensa em

Aa + Bb = 0 ⇒ Bb = 0 ⇒ B = 0.

(cid:21)

(cid:20)0 1
0 C

onde a segunda linha é múltipla da primeira, bastando multiplicá-la por C.

• (b, Ba + Cb) = (0, 0): análogo ao item anterior.

Logo, de acordo com o que foi discutido acima, podemos constatar que o vetor (Aa +

Bb, Ba + Cb) é múltiplo do vetor (a, b), ou seja, existe uma constante λ, tal que

Aa + Bb = λa

e

Ba + Cb = λb

fornecendo-nos assim o seguinte sistema

(cid:26)(A − λ)a + Bb = 0
Ba + (C − λ)b = 0

onde (a, b) deve ser uma solução não-trivial para que se tenha B(cid:48) = 0, portanto, o
determinante (A − λ)(C − λ) − B2 da matriz do sistema acima deve ser igual a zero, ou
seja,

λ2 − (A + C)λ + AC − B2 = 0.
A equação acima é chamada, devido à sua importância, de equação característica da
matriz de ϕ. Note que esta sempre possui raízes reais λ1 e λ2, pois seu discriminante é
não negativo, como podemos ver abaixo

∆ = (A + C)2 − 4 · 1 · (AC − B2)

= A2 + 2AC + C 2 − 4AC + 4B2
= A2 − 2AC + C 2 + 4B2
= (A − C)2 + 4B2
≥ 0.

Além disso, como tr(M) = A + C e det(M) = AC − B2, podemos escrever a equação
característica simplesmente como

λ2 − tr(M)λ + det(M) = 0

Dito isto, resta agora vermos como determinar o vetor (a, b), que por sua vez fornece
o ângulo de rotação necessário para eliminar o termo B(cid:48). Para tanto, uma vez que o
sistema acima possui inﬁnitas soluções, podemos tomar, por exemplo, x = 1, o que
implica y = (λ − A)/B. Daí, como o vetor (a, b) é unitário, basta então tomarmos

a =

x
(cid:107)(x, y)(cid:107)

=

x
(cid:112)x2 + y2

e

b =

y
(cid:107)(x, y)(cid:107)

=

y
(cid:112)x2 + y2

.

Note que (a, b) continua sendo solução do sistema com λ = λ1, pois

Aa + Bb =

Ax + By
(cid:107)(x, y)(cid:107)

=

λ1x
(cid:107)(x, y)(cid:107)

= λ1

x
(cid:107)(x, y)(cid:107)

= λ1a

126

Ba + Cb =

Bx + Cy
(cid:107)(x, y)(cid:107)

=

λ1y
(cid:107)(x, y)(cid:107)

= λ1

y
(cid:107)(x, y)(cid:107)

= λ1b,

que podemos escrever de forma resumida como

(cid:21)

(cid:20)A B
B C

(cid:21) (cid:20)a
b

= λ1

(cid:21)

(cid:20)a
b

,

ou seja, o vetor (a, b) está relacionado à rais λ1. Agora, notemos que o vetor (−b, a),
perpendicular a (a, b) está associado à raiz λ2. De fato, como

λ1 + λ2 = −

−(A + C)
1

= A + C ⇒ λ1 = A + C − λ2,

decorre que

ou seja,

Ba + Cb = λ1b = (A + C − λ2)b ⇒ A(−b) + Ba = λ2(−b)

Aa + Bb = λ1a = (A + C − λ2)a ⇒ B(−b) + Ca = λ2a,

(cid:20)A B
B C

(cid:21) (cid:20)−b
a

(cid:21)

= λ2

(cid:21)

(cid:20)−b
a

.

Uma vez encontrado o vetor (a, b) (e consequentemente (−b, a)), o termo B(cid:48) é elimi-

nado, de modo a termos

ϕ(x, y) = A(cid:48)x2 + C (cid:48)y2,

porém, sendo

decorre

A(cid:48) = a(Aa + Bb) + b(Ba + Cb) = λ1a2 + λ1b2 = λ1
C (cid:48) = b(Ab − Ba) − a(Bb − Ca) = λ2b2 + λ2a2 = λ2

ϕ(x, y) = λ1x2 + λ2y2.
Podemos então concluir que, ao eliminar o termo B(cid:48), os coeﬁcientes A(cid:48) e C (cid:48) não precisam
mais ser calculados, pois estes são iguais a λ1 e λ2, respectivamente.

Exemplo 3.13. Vamos aplicar o processo à forma quadrática

ϕ(x, y) = 7x2 − 6xy + 7y2 − 93
10 ,

obtida no Exemplo 3.12. Temos que sua equação característica é dada por

λ2 − 14λ + 40 = 0,

cujas raízes são λ1 = 4 e λ2 = 10. Estas informações já são suﬁcientes para identiﬁcar a
forma quadrática em questão, pois após a rotação, obtemos

ϕ(x, y) = 4x2 + 10y2 −

93
10

.

Assim, quando ϕ(x, y) = 0, temos

4x2 + 10y2 =

93
10

127

que pode ser reescrita como

x2
(cid:16)√
93
40

(cid:17)2 +

(cid:16)√

y2
93
100

(cid:17)2 = 1,

o que nos leva a concluir que ϕ(x, y) = 0 é uma elipse centrada na origem.

Daí, para determinar o ângulo de rotação, devemos calcular as coordenadas dos vetores

(a, b) e (−b, a). Já sabemos que (a, b) (associado a λ1) deve ser solução do sistema

(cid:26) (7 − λ1)x − 3y = 0
−3x + (7 − λ1)y = 0

Uma vez que det(M) (cid:54)= 0, temos que o mesmo possui inﬁnitas soluções, de modo que
tomando x = 1, decorre y = (7 − 4)/3 = 1, o que nos leva a

cos(θ) = a =

√

1
12 + 12

sen(θ) = b =

√

1
12 + 12

=

=

√

2
2
√

2
2

e assim

(cid:32)

(−b, a) =

−

√
2
2

,

√

2
2

(cid:33)

.

Logo, a matriz de rotação é dada por






√

2
2
√
2
2

−






√

2
2
√
2
2

ou seja, uma rotação de θ = 45o no sentido horário elimina o termo −6xy.

Temos, de acordo com os exemplos 3.11 e 3.12 que ϕ(x, y) = 0, ϕ(x, y) = 0 e ϕ(x, y) =
0 são elipses em diferentes posições no plano xy, as quais chamaremos, por simplicidade,
de E1, E2 e E3, respectivamente. Se quisermos obter informações a cerca de E1, como por
exemplo, as coordenadas de seus focos, deveremos primeiro encontrar os focos de E3, em
seguida os focos de E2 e por último, os de E1, ou seja, de modo resumido, basta reverter
o processo (Figura 3.31). Como sabemos, a distância focal de E3 é dada por
(cid:114)93
40

93
100

√
3

c =

20

62

−

=

,

logo, seus focos são dados por

(cid:32)

F 1 =

−

√
3

62

20

(cid:33)

, 0

e

(cid:32)

√
3

62

20

(cid:33)

, 0

.

F 2 =

Para determinar os focos de E2, basta aplicar a rotação de 45o no sentido anti-horário a
F 1 e F 2, ou seja,

F 1 =






√

2
2
√
2
2

−

√

2
2
√
2
2

√



3

31

20
√

3

31

20










√

3

62








20
0

 =




128

e

F 2 =






√

2
2
√
2
2










−

−

√

2
2
√
2
2

√

3



62

 =

20
0

−






√

3



31

20
√

3

31




Finalmente, aplicando a translação pelo vetor v = (− 11
F2 abaixo

(cid:32)

F1 =

√
3

√
3

31 − 11
20

,

31 − 19
20

(cid:33)

(cid:32)

e F2 =

−3

−
20
20 , − 19
20 ) a F 1 e F 2, obtemos F1 e
√

√

(cid:33)

−3

31 − 11
20

,

31 − 19
20

que são os focos de E1.

Figura 3.39

Fonte: O autor

A identiﬁcação de uma forma quadrática ϕ, como aponta LIMA [44], pode ser reali-
zada por mera inspeção dos coeﬁcientes A, B e C, mais especiﬁcamente pelo determinante
de sua matriz, já que este está diretamente ligado ao processo de eliminação dos termos
lineares e do termo que contém xy e, além disso, AC − B2 = λ1 · λ2. Tal observação
nos permite analisar o caso em que det(M) = 0, pois sendo este o caso, então teríamos
λ1 · λ2 = 0, logo, uma das raízes da equação característica é igual a 0. Porém, ambas não
poderiam ser iguais a 0, uma vez que isso levaria a ϕ(x, y) = 0 para todo x, y ∈ R, isto
é, após realizar a rotação e a translação da cônica, esta simplesmente sumiria, o que não
faz sentido. Dessa forma, supondo λ1 (cid:54)= 0, teríamos
ϕ(x, y) = λ1x2.

Daí, observando o signiﬁcado de ϕ(x, y) = c (cid:54)= 0, temos que

x = ±

(cid:114) c
λ1

o que nos fornece um par de retas paralelas, desde que c e λ1 possuam o mesmo sinal.

129

4 Diagonalização de matrizes

4.1 O conceito

Deﬁnição 4.1 Uma matriz A = [aij]n×n é dita diagonalizável quando existe uma matriz
invertível X = [xij]n×n e uma matriz diagonal D = [dij]n×n, tal que

ou, equivalentemente

A = XDX−1,

D = X−1AX.

Neste último caso, dizemos que X diagonaliza A.

Consideremos, por exemplo, a matriz

A =

(cid:21)

(cid:20)1 2
4 3

Temos que a matriz X que a diagonaliza A é

e portanto

(cid:21)

(cid:20)−1 1
2
1

A =

(cid:20)−1 1
1 2

(cid:21) (cid:20)−1 0
0 5

(cid:21) (cid:34)− 2

3
1
3

(cid:35)

.

1
3
1
3

O processo aplicado para determinar a matriz X que diagonaliza A surge da amplia-
(cid:21)
,

ção/contração de um vetor. Para entendê-lo, vamos considerar uma matriz A =

(cid:20)a b
c d

um vetor coluna X1 =

(cid:21)

(cid:20)x
y

e um número real λ, tal que AX1 = λX1, ou seja,

(cid:21)

(cid:20)a b
c d

(cid:21) (cid:20)x
y

= λ

(cid:21)

(cid:20)x
y

,

onde a equação acima nos fornece o sistema homogêneo

(cid:26)(a − λ)x + by = 0
cx + (d − λ)y = 0

com o qual já somos familiarizados e sabemos que seu determinante deve ser igual a zero
para que possamos ter soluções não-triviais, de modo que

λ2 − tr(A)λ + det(A) = 0

130

é a equação característica de A. Neste caso, não poderemos assegurar que sempre
haverão raízes de tal equação como no caso das formas quadráticas, uma vez que ∆ =
(a − d)2 + 4bc, pois b e c não necessariamente precisam ser iguais, muito menos ter o
mesmo sinal.

Vejamos como aplicar o processo de diagonalização á matriz

A =

(cid:21)

(cid:20)1 2
4 3

Primeiro calculamos as raízes de sua equação característica

que são λ1 = −1 e λ2 = 5. Daí, tomando λ1 = −1 no sistema

λ2 − 4λ − 5 = 0

obtemos

(cid:26)(1 − λ)x + 2y = 0
4x + (3 − λ)y = 0

(I)

(cid:26)2x + 2y = 0
4x + 4y = 0

cuja solução é x = −y, de modo que tomando y = 1, o vetor

x1 =

(cid:21)
(cid:20)−1
1

está associado a λ1 = −1. Já se tomarmos λ = λ2 nesse mesmo sistema, obtemos

(II)

(cid:26)−4x + 2y = 0
4x − 2y = 0

o que nos leva a y = 2x. Daí, tomando x = 1, seque que o vetor

x2 =

(cid:21)
(cid:20)1
2

está associado a λ2 = 5. Temos assim que a matrix X que diagonaliza A é

cuja inversa é dada por

e assim

onde

X =

(cid:21)

(cid:20)−1 1
1 2

,

X−1 =


− 2
3

1
3





1
3
1
3

A = XDX−1,

D =

(cid:21)

(cid:20)−1 0
0 5

.

131

Note que se tivéssemos tomado λ1 = 5 e λ2 = −1, então seriam

X =

(cid:21)

(cid:20)1
1
2 −1

,

D =

(cid:21)

(cid:20)5
0
0 −1

,





X−1 =

1
3
2

1
3





3 − 1

3

mas ainda assim

XDX−1 = A.

Podemos assim perceber que a ordem que tomamos as raízes da equação característica
não importa e, além disso, ﬁca claro que a matriz que diagonaliza uma determinada matriz
não é única. Se tivéssemos tomado x = 1 ao invés de y = 1 no sistema (I) e y = 1 ao
invés de x = 1 no sistema (II), a matriz X seria

(cid:34)

X =

(cid:35)

−1 − 1
2
1

1

Assim, não precisamos nos preocupar com a ordem das raízes e nem mesmo, a qual das
variáveis se atribui valor. Ainda assim, a Deﬁnição 4.1 abre portas apara a seguinte
pergunta: “Uma matriz precisa ser invertível para ser diagonalizável?” Não. Tomemos
por exemplo a matriz singular

A =

(cid:21)

(cid:20)1 0
0 0

.

Sua forma diagonalizada é

enquanto que a matriz B =

(cid:20)1 0
0 1
(cid:21)

(cid:20)0 1
0 0

(cid:21) (cid:20)1 0
0 0

(cid:21) (cid:20)1 0
0 1

(cid:21)

,

não é diagonalizável, pois o sistema

(cid:26)−λx + y = 0
−λy = 0

possui equação característica λ2 = 0, o que implica λ1 = λ2 = 0. Daí, teríamos D como
sendo uma matriz nula e assim, se existisse uma matriz invertível X que diagonalizasse
B, então seria

B = XDX−1 = O,

o que é absurdo, uma vez que B é não-nula.

Outra questão importante acerca da diagonalização de uma matriz é: “Quais as van-
tagens de diagonalizar uma matriz?” A resposta para essa pergunta é: Depende do
problema que se trata, como por exemplo, de acordo com LEON [40], problemas que
envolvem a aplicação de uma transformação linear sucessivas vezes. Mas antes de apre-
sentarmos um tal problema, vamos conhecer uma propriedade interessante (que também
é uma vantagem) da diagonalização, a qual é, dada uma matriz A diagonalizável, então

A = XDX−1 ⇒ An = XDnX−1,

para todo n ∈ N. De fato, sabemos que para n = 1 o caso é verdadeiro por deﬁnição.
Suponhamos então que seja válido para algum k ∈ N, isto é,

Ak = XDkX−1.

132

Agora notemos que

Ak+1 = AAk = (XDX−1)(XDkX−1) = XDDkX−1 = XDk+1X−1,

logo, podemos concluir que tal propriedade vale para qualquer que seja n ∈ N.

Observemos agora um exemplo em que as duas vantagens acima podem ser utilizadas

para analisar uma determinada matriz.

Exemplo 4.1. Uma determinada cidade possui 78768 habitantes, onde 21535 destes
praticam exercícios, enquanto que 57233 são sedentários. A cada ano, 35% dos praticantes
de atividades físicas passam a ser sedentários, ao passo que 25% da população sedentária
começa a praticar exercícios. Vamos representar por A a matriz de alternância entre os
dois estilos de vida citados acima, onde

A =

(cid:21)
(cid:20)0.65 0.25
0.35 0.75

.

(cid:21)
(cid:20)21535
57233

Seja v0 =
pessoas estarão em um desses dois estilos de vida após um ano, basta fazer

o vetor inicial de estilo de vida dessa cidade. Daí, para saber quantas

No ano seguinte, temos

v1 = Av0 =

(cid:21)
(cid:20)28306
50462

.

v2 = Av1 = A2v0

∼=

(cid:21)
(cid:20)31014
47754

.

Podemos assim perceber que após n anos, tem-se vn = Anv0. Porém, se utilizarmos um
software matemático, podemos observar que o vetor de estilos de vida para n ≥ 14 se
estabiliza, ou seja,

vn =

(cid:20)32820
(cid:21)
45948

,

∀ n ≥ 14.

Tal processo pode ser realizado sem a necessidade de um software matemático, utilizando
para tanto apenas a diagonalização de A. Ora, aplicando o processo de diagonalização a
tal matriz, obtemos

A =

(cid:34) 1
1
−1 7
5

o que por sua vez, implica

(cid:35) (cid:34) 2

(cid:35) 


5 0
0 1

7

12 − 5

5
12



 ,

12
5
12

An =

(cid:34) 1
1
−1 7
5

(cid:17)n

(cid:35) (cid:34)(cid:16) 2
5
0

(cid:35) 


0

1n

7

12 − 5

5
12



 .

12
5
12

Como 2
5
valor de
n ≥ 14), podemos escrever

está entre 0 e 1, sabemos que quanto maior for n, mais próximo de zero será o
(cid:16) 2
, enquanto que 1n = 1 para todo n ∈ N. Logo, para n grande (nesse caso,
5

(cid:17)n

An =

(cid:34) 1
1
−1 7
5

(cid:21)

(cid:35) (cid:20)0 0
0 1





7

12 − 5

12
5
12

5
12

133



 =

(cid:21)

(cid:20)5 5
7 7

1
12

e assim

vn = Anv0 =

(cid:20)5 5
7 7

(cid:21)
(cid:21) (cid:20)21535
57233

1
12

(cid:21)
(cid:20)32820
45948

,

=

conﬁrmando assim a matriz encontrada com software matemático. Dessa forma, podemos
concluir que, mbora a taxa de pessoas que se tornam sedentárias seja maior que a taxa
de pessoas que passam a praticar exercícios físicos, o número de sedentários diminui e o
de praticantes de atividades físicas aumenta durante um determinado período, até então
se estabilizar.

O exemplo acima permite-nos perceber que a diagonalização de uma matriz pode fa-
cilitar problemas que, em outras circunstâncias, só poderiam ser resolvidos com auxílio
de um software matemático. Obviamente, não está sendo aqui colocada de lado a im-
portância do uso de tais ferramentas, uma vez que, de acordo com, AYUB [5], o uso de
softwares dinâmicos pode ser benéﬁco tanto para o ensino como para a aprendizagem de
matemática. Além disso, segundo GLADCHEFF [26], o computador pode ser um grande
aliado do desenvolvimento cognitivo dos alunos, permitindo que este possa aprender com
seus erros de acordo com o seu ritmo de aprendizagem. Porém, a utilização de recursos
tecnológicos nem sempre é possível, pois, como aponta PACHECO [52], o emprego de
tais ferramentas depende da disponibilidade das mesmas no ambiente escolar, bem como
do preparo didático dos professores para utilizá-las.

Com relação à utilização de ferramentas tecnológicas em sala de aula por meio dos
professores, é algo que depende não somente de seu preparo, mas também de sua li-
nha pedagógica de pensamento, como por exemplo,de acordo com o estudo realizado
por NIEDERHAUSER [50], professores construtivistas permitem que os alunos explorem
ferramentas manipulativas aplicando suas próprias estratégias, enquanto que professores
behavioristas permitem o uso de tais ferramentas, porém utilizando passos especíﬁcos.

Pedagogias baseadas em orientações teóricas behaviorista e
construtivista representam visões dramaticamente diferen-
tes de ensino e aprendizagem e dão origem a concepções
fundamentalmente diferentes do uso de computadores na
instrução como uma máquina de ensino didático ou como
ferramentas construtivas de pensamento e reﬂexão. (NIE-
DERHAUSER [50], 2001).

Estando ciente das observações realizadas logo acima, vamos aplicar o processo de
diagonalização a uma matriz 3 × 3. Sabemos que o processo é o mesmo e que, no caso
das matrizes de ordem 2 o processo de determinação da equação característica pode ser
simpliﬁcado, uma vez que tal equação pode ser facilmente obtida por meio do traço e do
determinante da matriz, o que acaba por fornecer uma equação quadrática, cujas raízes
são obtidas pelo então chamado método de Bháskara. Antes de iniciarmos, precisaremos
de três deﬁnições que serão úteis para que no caso das matrizes de ordem 3, o processo seja
também facilitado. Vale ressaltar que tais deﬁnições não foram encontradas na literatura,
porém estas serão baseadas em deﬁnições já existentes na matemática, o que por sua vez
mantém a validade das mesmas.

134

Deﬁnição 4.2. Seja A = [aij] uma matriz quadrada de ordem n. Diz-se que a matriz



aii

ai(i+1)

a(i+1)i a(i+1)(i+1)

A∗

i =






...
ani

...
an(i+1)

ain

· · ·
· · · a(i+1)n
. . .
· · ·

...
ann








,

com i = 1, 2, . . . , n − 1 é o i-ésimo menor complementar diagonal da matriz A.

Exemplo 4.2. Os menores complementares diagonais da matriz

A =







7 4 2
3
0 1 3
−1
9
5 4 5
1 −6 7 2







são

A∗

1 =






 , A∗

2 =

1 3
0
5
4 5
−6 7 2

(cid:21)

(cid:20)4 5
7 2

,

e A∗

3 = (cid:2)2(cid:3)

Deﬁnição 4.3. Seja A = [aij] uma matriz quadrada de ordem n. A expressão

aijaji + ai(j+1)a(j+1)i + · · · + a(n−1)nan(n−1),

com i (cid:54)= j para todo i, j = 1, 2, . . . n,é chamada sutura de A e é representada por st(A).
O termo “sutura” foi escolhido devido à forma com que é realizada a multiplicação,
onde os saltos sobre a diagonal a principal são semelhantes ao processo de costura, como
podemos observar na imagem abaixo

Figura 4.1

Fonte: O autor

Por exemplo, a sutura da matriz A dada no Exemplo 4.2 é

st(A) = 7 · (−1) + 4 · 9 + 2 · 1 + 1 · 5 + 3 · (−6) + 5 · 7 = 53.

A deﬁnição a seguir depende das duas últimas e será utilizada para simpliﬁcar a

determinação da equação característica para matrizes quadradas de ordem 3.

135

Deﬁnição 4.4. Seja A uma matriz quadrada de ordem n. A grade de A, representada
por gr(A), é deﬁnida como sendo

gr(A) = a11 · tr(A∗

1) + · · · + a(n−1)(n−1) · tr(A∗

n−1) − st(A).

Exemplo 4.3. Consideremos a matriz

C =





11
−4 −2
10

5 −8
4
5 −7



 .

Temos que sua sutura é st(C) = −80, e assim

gr(C) = 11 · (−9) + (−2) · (−7) − (−80) = −5.

Fornecidas as três deﬁnições acima, tomemos agora uma matriz

A =





a b
c
d e f
g h i





e um vetor

x =



 .





x
y
z

Daí, a equação Av = λv, onde λ ∈ R, nos fornece o sistema



(a − λ)x + by + cz = 0
dx + (e − λ)y + f z = 0
gx + hy + (i − λ)z = 0



onde o determinante de sua matriz é dado por

(a − λ)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

e − λ
h

f
i − λ

(cid:12)
(cid:12)
(cid:12)
(cid:12)

− b

(cid:12)
(cid:12)
(cid:12)
(cid:12)

f

(cid:12)
d
(cid:12)
(cid:12)
g i − λ
(cid:12)

+ c

(cid:12)
(cid:12)
(cid:12)
(cid:12)

d e − λ
g

h

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= −λ3 + (a + e + i)λ2 − (ae + ai + ei − bd − cg − f h)λ + det(A)
= −λ3 + tr(A)λ2 − gr(A) + det(A).

Logo, a equação característica de uma matriz 3 × 3 é

−λ3 + tr(A)λ2 − gr(A)λ + det(A) = 0.

Exemplo 4.4. Vamos diagonalizar a matriz do Exemplo 4.3. Como já sabemos, gr(C) =
−5 e, além disso, podemos obter facilmente tr(C) = 2 e det(C) = −6. Daí, decorre que
a equação característica de C é

cujas raízes são λ1 = 1, λ2 = −2 e λ3 = 3. Dessa forma, para λ = λ1, obtemos o sistema

−λ3 + 2λ2 + 5λ − 6 = 0,






10x + 5y − 8z = 0
−4x − 3y + 4z = 0
10x + 5y − 8z = 0

,

136

cuja solução é x = 2

5 z, y = 4

5 z, z. Logo, tomando z = 5, obtemos


2
4
 .

5

x1 =

Para λ = λ2, o sistema obtido é






13x + 5y − 8z = 0
−4x + 0y + 4z = 0
10x + 5y − 5z = 0

,

que ao ser solucionado, nos fornece x = z, y = −z, z. Assim, tomando z = 1, segue que

Finalmente, para λ = λ3, temos

x2 =






1
−1
 .
1






8x + 5y − 8z = 0
−4x − 5y + 4z = 0
10x + 5y − 10z = 0

,

cuja solução é x = z, y = 0, z. Daí, para z = 1, decorre que

x3 =



1
0


1

e portanto

A =


2
1 1
4 −1 0

1 1
5






1
0 0
0 −2 0

0 3
0





0


− 1
3

− 4

3 −1

3

1
3
4
3
1 −2







.

Para o caso n = 4, veremos que a grade da matriz também aparece no equação carac-
terística (sendo o coeﬁciente do termo de grau 2), porém novos termos aparecem também.
Dessa forma, não se deve esperar um fórmula para determinar qualquer polinômio carac-
terístico, a menos da expansão do determinantes por meio dos cofatores, o que pode ser
laborioso, para matrizes de ordem superior.

Consideremos então

A =







b
c d
a
f g h
e
j k l
i
m n o p







e

x =













x1
x2
x3
x4

.

Desenvolvendo det(A − Ix), teremos

• O coeﬁciente do termo de grau 4 é 1.

• O coeﬁciente do termo de grau 3 é igual a −tr(A).

137

• O coeﬁciente do termo de grau 2 é gr(A).

• O coeﬁciente do termo de grau 1 é

−(det(A11) + det(A22) + det(A33) + det(A44)).

• O coeﬁciente do termo de grau 0 é det(A).

Podemos assim observar que, a partir de n = 4, aparece um novo termo, cujo cálculo pode
ser ainda mais longo que o de gr(A), a menos que a diagonal principal de A contenha
alguma(s) entrada(s) nula(s). Todavia, um padrão padrão polinomial pode ser observado
na determinação da equação característica de uma matriz A de ordem n, o qual é

(−1)n(λn − tr(A)λn−1 + gr(A)λn−2 + · · · + (−1)n det(A)) = 0.

Dessa forma, podemos extrair uma propriedade bastante interessante acerca das raízes
de tal equação, uma vez que, pelas relações de Viète, a soma e o produto das raízes do
polinômio

p(λ) = (−1)n(λn − tr(A)λn−1 + gr(A)λn−2 + · · · + (−1)n det(A))

são, respectivamente tr(A) e det(A). Isto porque,

e

λ1 + · · · + λn = (−1)n

tr(A)
(−1)n = tr(A).

λ1 · · · λn =

(−1)n det(A)
(−1)n

= det(A).

Observe, por exemplo, que na matriz C do Exemplo 4.4

λ1 + λ2 + λ3 = 1 + (−2) + 3 = 2 = tr(A)
λ1 · λ2 · λ3 = 1 · (−2) · 3 = −6 = det(A)

Um comentário adicional, o qual será importante na compreensão do Teorema Es-
pectral (Apêndice) é que, as raízes do polinômio característico de uma matriz triangular
estão dispostos em sua diagonal prinicipal. De fato, seja T = [tij]n×n uma matriz tri-
angular. Como sabemos, o determinate de uma matriz triangular é igual ao produto
das entradas de sua diagonal princiapal. Como T é triangular, então a matriz T − λI é
também triangular, onde seu determinantes coincide com o polinômio característico de
T, isto é,

0 = det(T − λI) = (t11 − λ1) · · · (tnn − λn),

o que nos fornece λ1 = t11, . . . , λn = tnn.

Tendo em vista tudo o que foi apresentado nesta seção, uma pergunta pode ser le-
“ Existe alguma condição necessária e suﬁciente para que uma matriz seja

vantada:
diagonalizável?” Felizmente, a resposta é sim e a prova disso se encontra no

Teorema 4.1. Seja A uma matriz quadrada de ordem n. Então A é diagonalizável
se, e somente se, os vetores associados às raízes do polinômio característico de A forem
linearmente independentes.

138

Demonstração. Suponhamos que A seja diagonalizável. Existem então uma matriz X
invertível e uma matriz diagonal D, tais que AX = XD. Esta última igualdade pode ser
escrita como

(cid:3) ,

(cid:2)Ax1

· · · Axn

· · · λnxn

(cid:3) = (cid:2)λ1x1
onde x1, . . . , xn são os vetores colunas de X associados às raízes λ1, . . . , λn nesta mesma
ordem. Como X é invertível, segue do Teorema 3.3 que os vetores coluna x1, . . . , xn
são linearmente independetes. Reciprocamente, se os vetores associados às raízes do
polinômio característico de A são linearmente independetes, então as igualdades Ax1 =
λ1x1, . . . , Axn = λnxn implicam AX = XD. Pela independência linear dos vetores
coluna de X, decorre pelo Teorema 3.3 que X é invertível, o que nos permite escrever
A = XDX−1, o que signiﬁca que A é diagonalizável.

4.2 A diagonalização de matrizes na rotação de cônicas

Antes de iniciarmos aplicarmos o processo de diagonalização para realizar translações e/ou
rotações de cônicas, necessitaremos do conceito de matriz ortogonal, o qual é deﬁnido a
seguir.

Deﬁnição 4.5. Uma matriz A quadrada de ordem n é dita ortogonal quando

o que equivale a

AT = A−1,

AT A = AAT = I.

Em outras palavras, a deﬁnição acima nos diz que uma matriz é ortogonal se, somente

se, esta for igual a sua inversa. A matriz

é um exemplo de matriz ortogonal, pois

5

5

5 − 2

5





√

√

2

5

√



5



√

5
5

5

5 − 2

5

√



5











√

√

2

5

5

√

5
5

√

5
5
√
− 2

5

√

2



5

 =

5
√

5
5

5

(cid:21)

(cid:20)1 0
0 1

.

Deﬁnição 4.6. Diz-se que uma matriz quadrada A é ortogonalmente diagonalizável
se existe uma matriz ortogonal P e uma matriz diagonal D, tal que

A = PDPT .

Nesse caso, diz-se que P diagonaliza ortogonalmente a matriz A. A deﬁnição acima
é de grande utilidade para o processo de rotação, uma vez que, permite que provemos
um resultado que associa matrizes ortogonais e matrizes simétricas, como por exemplo a

139

matriz M de uma forma quadrática ϕ. Antes disso, observemos abaixo um exemplo de
matriz ortogonalmente diagonalizável

(cid:21)

(cid:20)−1 2
2 2

=





√

√

2

5

5

5

5 − 2

5

√



5



√

5
5

(cid:21)

(cid:20)3
0
0 −2





√

5
5
√
− 2

5

√

2



5

 .

5
√

5
5

5

Teorema 4.2. Uma matriz A é simétrica e diagonalizável se, e somente se, é ortogo-
nalmente diagonalizável.

Demonstração. Se A é diagonalizável, então, temos que, existe uma matriz X e uma
matriz diagonal D, tal que

Agora, notemos que, sendo A simétrica, tem-se

A = XDX−1.

A = AT = (XDX−1)T = (X−1)T DXT .

Porém, como sabemos, a forma de diagonalizar a matriz A não é única, de modo que

A = (X−1)T DXT

é também uma forma diagonalizada de A. Daí, A será orotogonalmente diagonalizada
se, e somente se, (X−1)T = X. Multiplicando ambos os membros desta última igualdade
por X−1, obtemos

X−1(X−1)T = X−1X

⇔ X−1(XT )−1 = I
⇔ (XT X)−1 = I
XT X = I,
⇔

ou seja, X é ortogonal e assim A = XDXT . Reciprocamente, se A é diagonalmente
ortogonalizável, então, existe P e D diagonal, tais que A = XDXT e assim

AT = (XDXT )T = (XT )T DXT = XDXT = A,

isto é, A é simétrica.

Tendo conhecimento das deﬁnições e resultados acima, consideremos agora uma função

quadrática ϕ : R2 → R

ϕ(x, y) = Ax2 + 2Bxy + Cy2 + Dx + Ey + F.

Note que a mesma pode ser escrita sob a forma matricial

ϕ(x, y) = (cid:2)x y(cid:3)

(cid:21)

(cid:20)A B
B C

(cid:21) (cid:20)x
y

+ (cid:2)D E(cid:3)

(cid:21)

(cid:20)x
y

+ [F ].

Daí, fazendo x =

(cid:20)x
y

(cid:21)
, N = (cid:2)D E(cid:3) e P = [F ], a igualdade acima se resume a

ϕ(x, y) = xT Mx + N x + P.

(4.1)

140

Agora vamos supor que M é diagonalizável. Sendo esta simétrica, segue do Teorema
4.1 que a mesma é também ortogonalmente diagonalizável. Dessa forma, por deﬁnição,
existe P ortogonal e D diagonal, tais que

M = PDPT ,

de modo que

xT Mx = xT PDPT x = (PT x)T D(PT x).
Tomando y = PT x, podemos multiplicar (à esquerda) ambos os membros desta última
por PT e, sendo P ortogonal, temos que PT = P−1 e assim obtemos x = Py. Em seguida,
antes de reescrevermos (4.1), observemos que

y = PT x =

(cid:20)p11 p21
p12 p22

(cid:21) (cid:20)x
y

(cid:21)

(cid:21)

(cid:20)x
y

,

=

onde x = p11x + p21y e y = p12x + p22y.

xT Mx = yT Dy = [λ1x2 + λ2y2],
com λ1 e λ2 sendo as respectivas entradas não-nulas de D, ou seja, as raízes da
equação característica de M.

•

•

•

onde

N x = N Py = [D(cid:48)x + E(cid:48)y],

D(cid:48) = Dp11 + Ep21
E(cid:48) = Dp12 + Ep22

.

Agora, utilizaremos um abuso de notação, omitindo os colchetes e escrevendo F ao

invés de P = [F ], de modo que a forma quádrica em (4.1) passa a ser

ϕ(x, y) = λ1x2 + λ2y2 + D(cid:48)x + E(cid:48)y + F.

(4.2)

Observe que o termo contendo xy desapareceu, o que nos fornece a forma quádrica ro-
tacionada, contendo seus eixos de simetria paralelos aos eixos x e y, respectivamente.
Resta então determinar o vetor de translação, o qual será determinado pelo método de
completar quadrados, onde este, sendo aplicado à igualdade acima, nos fornece

ϕ(x, y) = λ1

(cid:18)

x +

D(cid:48)
2λ1

(cid:19)2

(cid:18)

+ λ2

y +

(cid:19)2

E(cid:48)
2λ1

− F (cid:48).

(4.3)

(D(cid:48))2
4λ1

(E(cid:48))2
4λ2

+

onde F (cid:48) = −F +
. Note que os termos lineares também “desaparecem”
após a completação de quadrados. Dessa forma, a mudança de coordenadas realizada nos
três pontos acima elimina o termo que contém xy, mas os termos lineares permanecem,
porém de maneira implícita. Daí, para ϕ(x, y) = 0, decorre que

(cid:16)

(cid:17)2
(cid:17)2 +

x + D(cid:48)
2λ1
(cid:16)(cid:113) F (cid:48)
λ1

(cid:17)2
(cid:17)2 = 1,

(cid:16)
y + E(cid:48)
2λ2
(cid:16)(cid:113) F (cid:48)
λ2

(4.4)

141

onde o vetor de translação é

(cid:18) D(cid:48)
2λ1
Dessa forma, para eliminar os termos lineares, basta aplicarmos uma translação de vetor
−v a ϕ, obtendo assim ϕ, cuja equação quando ϕ(x, y) = 0 é dade por

E(cid:48)
2λ2

v =

(cid:19)

,

.

x2
(cid:16)(cid:113) F (cid:48)
λ1

(cid:17)2 +

y2
(cid:16)(cid:113) F (cid:48)
λ2

(cid:17)2 = 1.

(4.5)

Exemplo 4.5. Vamos identiﬁcar a forma quádrica ϕ : R2 → R, com ϕ(x, y) = 7x2 −
6xy + 7y2 + 2x + 10y − 4, quando ϕ(x, y) = 0. Incialmente, vamos diagonalizar a matrix
M de ϕ. Já sabemos que λ1 = 4 e λ2 = 10. Daí, para λ = λ1, obtemos o sistema

(cid:26) 3x − 3y = 0
−3x + 3y = 0

,

cuja solução é x = y e, tomando y = 1, obtemos x = 1, de modo que o vetor
associado a λ1. Já para λ = λ2, o sistema obtido é
(cid:26)−3x − 3y = 0
−3x − 3y = 0

,

(cid:21)
(cid:20)1
1

está

com solução x = −y. Assim, pondo y = 1, segue que x = −1 e assim
a λ2. Obtemos assim a matriz

X =

(cid:21)
(cid:20)1 −1
1
1

.

(cid:21)
(cid:20)−1
1

está associado

Temos assim que

M =

(cid:20)1 −1
1
1

(cid:21) (cid:20)4

(cid:21)
0
0 10



1
2

− 1
2



 .

1
2
1
2

Porém, notemos que X−1 (cid:54)= XT , ou seja, X não é ortogonal. Para resolver isso, basta
normalizarmos os vetores x1 e x2, isto é, tomarmos os vetores

u =

1
(cid:107)x1(cid:107)

x1 =









√

2
2√
2
2
√

−









2
2√
2
2

v =

1
(cid:107)x2(cid:107)

x2 =

Obtemos assim uma matriz ortogonal

P =





√
2
2 −
√
2
2

√

2
2
√
2
2



 ,

142

tal que

M =





√
2
2 −
√
2
2

√

2
2
√
2
2





(cid:21)
(cid:20)4
0
0 10





−

√

2
2
√
2
2

√

2
2
√
2
2


 = PDPT .

Assim, decorre que

xT Mx = 4x2 + 10y2.

Agora, resta determinar D(cid:48), E(cid:48) e F (cid:48), o que já é conhecido como fazer. Logo

F (cid:48) = −(−4) +

o que ﬁnalmente nos permite escrever

D(cid:48) = 2 ·

E(cid:48) = 2 ·

√
2
2 + 10 ·
√
(cid:19)
(cid:18)
2
2

−

2

√

√
2
2 = 6
√
2
2 = 4
√

+ 10 ·

√
2

√

(6

2)2

4·4

+

=

93
10

,

(4

2)2

4·10

√

(cid:17)2

2

(cid:16)
x+ 3
(cid:16)√

(cid:17)2

2
(cid:17)2 +

(cid:16)
y+
(cid:16)√

√

4

93
40

(cid:17)2 = 1,

5

93
100

onde a equação acima caracteriza uma elipse de centro

(cid:18)

− 3

√
2
4 , −

(cid:19)

√

2
5

, a qual chamare-

mos de E. Para escrevê-la de uma maneira mais simples, basta aplicarmos uma translação
a E pelo vetor

, de modo a obtermos a elipse E, com equação

(cid:18)

(cid:19)

√

3

√
2
4 ,

2
5

x2
(cid:16)√
93
40

(cid:17)2 +

(cid:16)√

y2
93
100

(cid:17)2 = 1.

Logo, podemos concluir que a equação 7x2 − 6xy + 7y2 + 2x + 10y − 4 = 0 representa
uma elipse, a qual já havíamos obtido no Exemplo 3.12 e a chamamos de E.

Quanto aos focos de E, estes podem ser encontrados de duas maneiras:

• A primeira é partindo dos focos F 1 e F 2 de E, os quais já conhecemos do Exemplo

3.12. Aplicamos então uma translação de vetor v =

(cid:18)

− 3

√
2
4 , −

√

(cid:19)

2
5

, e assim

√

√

−3(5

62)

62)











2+
20
√

2
5

2+
20
√

2
5

−

−

√

√

3(−5

F 1 = F 1 + v =

F 2 = F 2 + v =











143

Em seguida, aplicamos uma rotação de 45o no sentido anti-horário a F 1 e F 2, de
modo que

F1 =





√
2
2 −
√
2
2

√

2
2
√
2
2










−3(5

√

√

2+
20
√

2
5

−



62)




 =




√

√

−3

−3

31−11
20

31−19
20






F2 =





√
2
2 −
√
2
2

√

2
2
√
2
2










3(−5

√

√

2+
20
√

2
5

−



62)

√



3


 =




√

3






31−11
20

31−19
20

resultado este que já havia sido encontrado no Exemplo 3.12. Se considerarmos
√

31 ∼= 5, 56776, obtemos

F1 = (−1,38516, −1,78516)

F2 = (0,285165, −0,114835)

• A segunda maneira é, partindo do centro de E, o qual já sabemos que é

2
5
Como é sabido, se uma elipse possui centro (h, k), então seus focos são (h ± c, k),
onde c =
62

√
a2 − b2, a > b. Como c = 3

, decorre que

− 3

√

20

(cid:18)

√

√
2
4 , −

(cid:19)

.

(cid:18)

(cid:18)

F 1 =

F 2 =

√

√

− 3

2

4 − 3

− 3

2

4 + 3

√
62
20 , −
√
62
20 , −

√

2
5
√

2
5

(cid:19)

(cid:19)

=

=

√

√

(cid:18) −3(5

√

√

(cid:18) 3(−5

2+
20

2+
20

62)

, −

62)

, −

(cid:19)

(cid:19),

√

2
5
√

2
5

Daí, aplicando uma rotação de 45o no sentido anti-horário, obtemos F1 e F2, como
já foi visto acima.

Vale ressaltar que a elipse E obtida no Exemplo 4.5 não é aquela obtida no Exemplo
3.12, uma vez que, neste último, a translação é realizada primeiro e em seguida é realizada
a rotação. Já no Exemplo 4.5, a ordem é contrária, ou seja, primeiro é realizada a rotação
e, por ﬁm, a translação (Figura 4.2). Tais exemplos mostram por si que, independente da
ordem que sejam aplicadas as transformações lineares (rotação e translação), o resultado
obtido é o mesmo.

144

Figura 4.2

Fonte: O autor

Antes de ﬁnalizarmos esta seção, é necessário lembrar que, quando um dos autovalores
é igual a 0, não se pode utilizar a forma (4.4), muito menos (4.5). Em casos assim, como
adverte CAMPOLINO [12], é possível que a forma quadrática em questão seja uma
parábola. Em casos assim, utilizaremos (4.2), a qual se resume, tomando λ2 = 0 (por
exemplo), a

ϕ(x, y) = λ1x2 + D(cid:48)x + E(cid:48)y + F.
Para entender o que fazer em situações como esta, vamos utilizar a técnica de diagonali-
zação para identiﬁcar a forma quadrática ϕ, tal que

ϕ(x, y) =

1
2

x2 − xy +

1
2

y2 −

√

11+2
4

2

x +

√

11−2
4

2

y +

√

121+36

32

2

,

a qual já sabemos, pelo Exemplo 3.10, que quando ϕ(x, y) = 0 temos a parábola GT RT .
Mas utilizaremos a diagonalização orotogonal para veriﬁcar tal fato. Temos então que a
matriz de ϕ é

cuja forma ortogonalmente diagonalizada é

M =



−



1
2
1
2

−





1
2
1
2

√

√



−

2
2
√
2
2
com λ1 = 1 e λ2 = 0. Daí, vem que

2
2
2
2

M =



√





(cid:21)

(cid:20)1 0
0 0



−



√

√

2
2
2
2


 = PDPT ,

√

2
2
√
2
2

e, além disso

xT Mx = [x2]

√

D(cid:48) = 11
4

2

e

145

E(cid:48) = −1,

o que por sua vez implica

N x =

Temos assim

(cid:104) 11

√
4 x − y

2

(cid:105)

.

ϕ(x, y) = x2 + 11

2

4 x − y + 121+36

32

√

2

.

√

√

y = x2 + 11

2

4 x + 121+36

32

√

2

,

Como podemos observar, não é possível completar quadrados para y, uma vez que a forma
quadrática acima não possui termo com y2. Porém , notemos que ϕ(x, y) = 0 equivale a

equação esta que, como já se sabe, é de segundo grau e representa uma parábola (a qual
chamaremos de P ), assim como já era de se esperar, visto que a expressão de ϕ já havia
sido obtida no Exemplo 3.10. Completando quadrados e reescrevendo a equação acima
em sua forma padrão, obtemos

(cid:18)

(cid:18)

x −

− 11
8

√

(cid:19)(cid:19)2

2

= 4 ·

(cid:18)

y − 9

√

(cid:19)

2

8

,

1
4

o que nos permite concluir que P é uma parábola cujo vértice é

(cid:18)

√

2

, 9

√

2

(cid:19)

8

.

− 11
8

Daí, fazendo uma translação de vetor v =

(cid:18)

√

11
8

2

y = x2,

, − 9

√

8

2

(cid:19)

, obtemos ϕ, tal que

ou seja, uma parábola com vértice na origem, que será chamada de P . Temos então
comprovado que ϕ(x, y) = 0 e GT RT são ambas a mesma parábola.

Figura 4.3

Fonte: O autor

146

Note que, se quisermos determinar o foco de GT RT com as informações obtidas nesse
exemplo, precisaremos seguir mais alguns passos. Com efeito, como visto no Exemplo
3.10, sabemos que ϕ é resultado de translações e rotações sobre a parábola de equação
y = x2 − 5x + 6. Então, cabe a pergunta: “É possível reverter o processo?”. A resposta é
“sim”. Para tanto, observemos que a diretriz de P é

√

y = 9

2−2
8

,

enquanto que a reta diretriz de P é y = 1
4
vértice tenha ordenada − 1
4
P à origem. Ora, a distância de P à origem é

. Precisamos então determinar a parábola, cujo

e cuja distância à origem seja igual à distância do vértice de

(cid:115)(cid:18)

√

(cid:18)

(cid:19)2

2

+

0 − 9

0 + 11
8

√

8

(cid:19)2

2

=

√

101
4

.

Devemos então determinar o ponto
logo acima. Assim, devemos ter

(cid:17)

(cid:16)

h, − 1
4

que possui distância igual ao valor obtido

(cid:114)

(0 − h)2 +

√

(cid:17)2

(cid:16)

0 + 1
4

=

101

4 ⇒ h = 5
2 .

Portanto, a parábola procurada deve possui vértice

(cid:17)

(cid:16) 5
2 , − 1

4

, ou seja, com equação

que pode ser reescrita como

(cid:17)2

(cid:16)

x − 5
2

= 4 · 1
4

(cid:16)

y + 1
4

(cid:17)

,

y = x2 − 5x + 6,

que é, de fato, a equação da parábola G do Exemplo 3.10.

Figura 4.4

Fonte: O autor

147

Tais informações nos permitem agora determinar:

• O foco de GT RT : Note que o foco de P é

sentido horário, obtemos

(cid:17)

(cid:16)

0, 1
4

. Daí, aplicando rotação de 45o no





P (cid:48) =

√

√
2
2 −
√
√
2
2

2
2
2
2

(cid:35)





(cid:34) 0
− 1
4





=

√

2
8
√

2
8





Em seguida, aplica-se uma translação de vetor u =
o foco da parábola GRT do Exemplo 3.10) de modo a obtermos o foco P (cid:48)(cid:48) de P

a P (cid:48) (note que este é

4

(cid:17)

(cid:16) 5
2 , − 1





P (cid:48)(cid:48) =



 .

√

√

2+20
8

2−2
8

• A reta diretriz de GT RT : Sabemos que P (cid:48) pertence à reta y = x. Logo, seu simétrico
√
2
, está sob a reta diretriz de GRT . Logo,
em relação à esta reta, isto é,
8
para determinar a equação de tal reta, basta aplicarmos uma translação de vetor
(cid:16)

√
2
8 , −

à reta y = −x (perpendicular a y = x), de modo a obtermos

−

−

(cid:17)

(cid:16)

(cid:17)

√

√
2
8 , −

2
8

y(cid:48) +

√
2
8 = −x(cid:48) −
y(cid:48) = −x(cid:48) −

⇔

√

2
8
√

2
4

.

Por ﬁm, aplicamos uma translação de vetor
que por sua vez resulta em

(cid:17)

(cid:16) 5
2 , − 1

4

à reta obtida logo acima, o

y(cid:48)(cid:48) = −x(cid:48)(cid:48) + 18−2
8

√

2

,

a qual é a reta diretriz de GT RT .

Figura 4.5

Fonte: O autor

148

Observe que poderíamos ter determinado o vértice de GT RT apenas multiplicando a

matriz P pelo vetor

(cid:18)

√

− 11
8

2

, 9

8

√

(cid:19)

2

, ou seja,



−



√

√

2
2
2
2





√

2
2
√
2
2

√


− 11
8
√

2

9

8



2



 =



−



 .

5
2
1
4

Finalizamos assim esta seção, tendo como objetivo seguinte, a apresentação de uma
aplicação de matrizes, onde estas, assumem a característica de um número, embora não
deixem de ser matrizes, pois como já vimos, um número real e uma matriz não são a
mesma coisa.

4.3 Exponencial de Matriz

Antes de abordamos o foco principal desta seção, faz-se necessário deﬁnir a noção de

exponencial escalar de um número real a, isto é, um número da forma

ea = 1 + a +

1
2!

a2 +

1
3!

a3 +

1
4!

a4 + · · · ,

o qual representa uma potência do famoso número de Euler16. Tal deﬁnição é importante,
visto que a soma acima é inﬁnita, logo, ao realizarmos uma soma inﬁnita de determina-
das matrizes, poderemos obter determinar o resultado da soma inﬁnita das respectivas
entradas de cada uma delas. Dito isto, podemos então seguir para a deﬁnição chave desta
seção.

Deﬁnição 4.7. Seja A uma matriz quadrada. A exponencial da matriz A é dada por

eA = I + A +

1
2!

A2 +

1
3!

A3 +

1
4!

A4 + · · ·

Embora a matriz A apareça como um expoente, o resultado de eA não é um número,
como podemos notar na deﬁnição acima, uma vez que a soma de matrizes é uma matriz.
Para melhor compreender, vamos calcular eD, onde D é uma matriz diagonal de ordem

16Leonhard Euler, um dos maiores e mais prolíﬁcos matemáticos que o mundo já produziu, nasceu em
Basel (Suíça), em 15 de abril de 1707 e morreu em São Ptesburgo (Rússia), em 18 de novembro de 1783
[20].

149

n.

eD = I + D +


=

1 0 · · ·
0 1 · · ·
...
...
. . .
0 0 · · ·






1
2! D2 + · · ·


0
d1
0
0


...
...


1
0






+

0
d2
...
0








0
· · ·
0
· · ·
...
. . .
· · · dn

+

1
2!








d2
1
0
...
0

0
d2
2
...
0








0
· · ·
0
· · ·
...
. . .
· · · d2
n

+ · · ·



1 + d1 +

=







1 + · · ·

1
2! d2
0
...
0

1 + d2 +

2 + · · ·

0
1
2! d2
...
0

· · ·

· · ·
. . .

· · ·








ed1
0
...
0

=

0
ed2
...
0

· · ·
· · ·
. . .
· · ·








0
0
...
edn









0

0
...
1
2! d2

n + · · ·

1 + dn +

Vale ressaltar que a exponencial de uma matriz não apresenta todas as propriedades
da exponencial de um escalar, como por exemplo, não é verdade que eAeB = eA+B
para quaisquer matrizes A e B. Neste caso, é necessário que AB = BA, porém não
provaremos tal fato, a ﬁ de não fugir do escopo do presente trabalho. Porém, será
provado adiante um teorema de grande importância para o cálculo de exponenciais de
matrizes diagonalizáveis.

Deﬁnição 4.8. Duas matrizes A e B são ditas conjugadas ou semelhantes se existe
uma matriz Q invertível, tal que

o que é equivalente a

AQ = QB,

A = QBQ−1.

A deﬁnição acima é uma generalização da Deﬁnição 4.1, uma vez que a matriz B não

necessariamente precisa ser diagonal. Temos, por exemplo,

(cid:20)3 2
5 4

(cid:21)
(cid:21) (cid:20) 1 −1
2
−1

=

(cid:21)

(cid:20)1 1
1 3

(cid:20) 1 −1
2
−1

(cid:21) (cid:20)3 5
2 4

(cid:21)

=

As matrizes conjugadas apresentam a seguinte propriedade: Sejam A e B duas matrizes
cunjugadas por uma matriz Q. Então é verdade que

AnQ = QBn,

para todo n ∈ N. De fato, observemos que, dado um número natural qualquer, temos
que

Repetindo novamente o argumento acima, obtemos

AnQ = An−1AQ = An−1QB.

An−1QB = An−2AQB = An−2QBB = An−2QB2.

150

Continuando a aplicar o argumento acima sucessivas vezes, decorre que

Tal propriedade nos permite enunciar o seguinte teorema:

AnQ = QBn.

Teorema 4.3. Se A e B são matrizes conjugadas por uma matriz invertível Q, então
as matrizes eAQ e QeB são também conjugadas. De forma resumida,

AQ = QB ⇒ eAQ = QeB.

Demonstração. Temos que

eAQ =

(cid:16)

I + A +

Q

(cid:17)

1
2! A2 + · · ·
1
2! A2Q + · · ·
1
2! QB2 + · · ·
(cid:17)
1
2! B + · · ·

= Q + AQ +

= Q + QB +
(cid:16)

= Q

I + B +

= QeB,

onde a terceira igualdade acima ocorre devido ao Teorema 4.2.

Note que o Teorema acima é equivalente a

Dessa forma, se A é diagonalizável com AX = XD, decorre que

AQ = QB ⇒ eA = QeBQ−1.

Exemplo 4.6. Sabemos do Capítulo 4 que

eA = XeDX−1.

A =

(cid:21)

(cid:20)1 2
4 3

=

(cid:20) 1 1
−1 2

(cid:21) (cid:20)−1 0
0 5

(cid:21)





Assim, tem-se que

eA = XeDX−1

1
3
2

1
3



 .

3 − 1

3

1
3







(cid:21)



1
3
2

0
e5

(cid:20) 1 1
−1 2

(cid:21) (cid:20)e−1
0

3 − 1
(cid:21)
(cid:20) 99, 065 −49, 348
197, 762 −99, 065

3

=

∼=

Agora que já sabemos como resolver exponenciais de matrizes diagonalizáveis, vamos
aplicar tal conhecimento na resolução de problemas de valor inicial , isto é, problemas
que envolvem uma matriz Y(cid:48)(t) da forma

Y(cid:48)(t) = AY(t),

151

(4.6)

onde Y(t) = etAY(0) = etAY0 é uma matriz que depende de t ∈ R, Y(0) = Y0 é um
vetor coluna chamado condição inicial e A é uma matriz diagonalizável. Como A é
diagonalizável, então existe uma matriz X invertível e uma matriz diagonal D, tais que
A = XDX−1. Assim, podemos escrever Y(t) da forma

Y(t) = XetDX−1Y0.
Agora notemos que X−1Y0 é uma matriz coluna, logo podemos escrevê-la como um vetor
c, isto é,

X−1Y0 = c =





 .





c1
...
cn

Daí, escrevendo X como uma matriz de blocos (cid:2)X1 · · · Xn

(cid:3), segue que

Y(t) = (cid:2)X1 · · · Xn

(cid:3) etD


 = (cid:2)c1eλ1tX1 + · · · + cneλntXn


(cid:3) .






c1
...
cn

Com absuso de notação, podemos excrever simplesmente

Y(t) = c1eλ1tX1 + · · · + cneλntXn.

Agora vamos utilizar um exemplo para entender de fato o que as matrizes Y(cid:48)(t) e

Y(t) representam.

Exemplo 4.7. Dois tanques A e B possuem a mesma capacidade de 240 L. No interior
do Tanque A foram dissolvidos 80g de sal, formando assim uma salmoura, a qual circula
pelos canos que conectam os tanques A e B conforme a ﬁgura a seguir

Figura 4.6

Fonte: O autor

Observe que há um ﬂuxo de 15 L/min de água pura entrando no Tanque A, enquanto
há um ﬂuxo de saída igual a 15 L/min de salmoura saindo do Tanque B. Além disso,
ambos os tanques recebem 16 L de mistura por minuto, o que nos leva a concluir que o
volume ocupado por ambos continua sendo de 240 L. Agora, chamando de y1(t) e y2(t)
as respectivas quantidades de salmoura nos tanques A e B no instante t, temos que

152

• Taxa de entrada de salmoura no Tanque A

(4 L/min)

(cid:18) y2(t)
240

(cid:19)

g/L

=

y2(t)
60

g/min

• Taxa de saída de salmoura do Tanque A

(16 L/min)

(cid:18) y1(t)
240

(cid:19)

g/L

=

y2(t)
15

g/min

• Taxa de variação de quantidade de salmoura para o Tanque A

y(cid:48)
1(t) =

y2(t)
60

−

y1(t)
15

• Taxa de variação de quantidade de salmoura para o Tanque B

y(cid:48)
2(t) =

y1(t)
15

−

y2(t)
15

Obtemos assim

Y(cid:48)(t) =

(cid:21)

(cid:20)y(cid:48)
1(t)
y(cid:48)
2(t)

=

Agora, sabendo que

1
60


− 1
15

15 − 1
1

15





(cid:21)

(cid:20)y1(t)
y2(t)

= AY(t).

Y0 =

(cid:21)
(cid:20)80
0

,

e diagonalizando A, o que nos fornece

A =

(cid:20)1 −1
2
2

(cid:21) (cid:34)− 1
30
0

(cid:35) 
1
2

− 1
2

0
− 1
10



 ,

1
4
1
4

segue que

Y(t) = c1e− 1
30 t

(cid:21)
(cid:20)1
2

+ c2e− 1
10 t

(cid:21)
(cid:20)−1
2

=

(cid:20) c1e− 1
2c1e− 1

30 t − c2e− 1
10 t
30 t + 2c2e− 1
10 t

(cid:21)

.

Por ﬁm, para determinar os valores das constantes c1 e c2, basta notar que




(cid:21)

(cid:20)c1
c2

=

1
2

− 1
2

1
4
1
4

(cid:21)
(cid:20)80
0



(cid:21)
(cid:20) 40
−40

=

e assim

Y(t) =

(cid:20)40e− 1
80e− 1

30 t + 40e− 1
10 t
30 t − 80e− 1
10 t

(cid:21)

.

O exemplo acima nos mostra que a matriz Y(cid:48)(t) está relacionada com taxas de variação
de uma determinada quantidade de salmoura ao longo do tempo , ao passo que a matriz
Y(t) fornece as quantidades de salmoura de cada tanque no momento t. Vejamos abaixo

153

os gráﬁcos de das quantidades de salmoura nos dois tanques ao longo dos 100 primeiros
minutos.

Figura 4.7

Fonte: O autor

Note que em um dado momento t, as quantidades de salmoura em ambos os tanques é
exatamente igual. Para determinar um tal ponto, basta resolvermos a equação a seguir

40e− 1
⇔ 40e− t
⇔
⇔

30 t + 40e− 1
10 (e t

10 t = 80e− 1
15 + 1) = 80e− t
e t
15 = 3
t = 15 ln 3

30 t − 80e− 1
10 t
10 (e t
15 − 1)

Daí, considerando ln 3 =, 0986, decorre que no momento t ∼= 16, 48 min (aproxima-
damente 16 minutos e 29 segundos), as quantidades de salmoura nos Tanques A e B
coincidem.

Ainda com base no Exemplo 4.7, vamos entender o signiﬁcado das raízes λ1 = − 1
30
e λ2 = − 1
do polinômio característico da matriz A. Observemos, para tanto, que o
10
decrescimento percentual da quantidade de salmoura no tanque A após 1 minuto é dado
por

y1(1) − y1(0)
y1(0)

∼= −6, 397%

Note que este é também o decrescimento percentual médio no intervalo [0, 1]. Para
calcular o decrescimento percentual médio no intervalo [0, 2], basta fazermos

1
2

(cid:18) y1(1) − y1(0)
y1(0)

+

y1(2) − y1(1)
y1(1)

(cid:19)

∼= −6, 345%

154

Agora, utilizando a ideia acima e com auxílio de um software matemático, obtemos a
seguinte aproximação

1
1184

(cid:18)y1(1) − y1(0)
y1(0)

+ · · · +

y1(1184) − y1(1183)
y1(1183)

(cid:19)

∼= −3, 334%,

ou seja, o decrescimento percentual médio no intervalo [0, 1184] é aproximandamente
−3, 334%. Por outro lado, temos que

−

1
30

∼= −3, 333%,

o que nos leva a concluir que a raíz λ1 = − 1
nos fornece o decrescimento percentual médio
30
de y1(t) ao longo do tempo t, quando t assume valores altos. Quanto à quantidade y2(t),
, o
é de se esperar que sua taxa de decrescimento seja 10%, tendo em vista que λ2 = − 1
10
que não é verdade, como podemos constatar até mesmo por mera inspeção do gráﬁco, pois
y2(t) aumenta durante um certo período e em seguida passa a decrescer. Porém, uma
aproximação para tal média de decrescimento percentual é obtida de maneira análoga
para y1(t), contudo, a mesma somente é atingida para t > 5000. Por exemplo, para
t = 5500, obtemos

1
5500

(cid:18) y2(2) − y1(1)
y1(1)

+ · · · +

y1(5500) − y1(5499)
y1(5499)

(cid:19)

∼= 3, 222%.

Note que a diferença percentual entre 3, 333% e 3, 222% é aproximadamente igual a
3, 333%.

O Exemplo 4.7 enquadra-se como um problema de Análise de Redes 17, por se tratar

de uma situação que envolve “ﬂuxo de entrada igual ao ﬂuxo de saída”.

Uma última evidência quanto ao signiﬁcado das raízes do polinômio característico de
uma matriz está presente no exemplo 1.22, onde foi calculado o crescimento populacional
anual médio. Utilizando um software matemático, pode-se encontrar uma dessas raízes,
a saber λ = 1, 84886, a qual indica o crescimento anual médio de aproximadamente 84, 9,
o que está de acordo com a aproximação obtida no referido exemplo.

17A palavra “rede” pode ter vários signiﬁcados, uma vez que trata-se da relação entre dados. Dessa
forma, a análise de redes possui aplicações em várias áreas, tais como planejamento de projetos, sistemas
complexos, circuitos elétricos, redes sociais, sistemas de transporte, redes de comunicação, epidemiologia,
bioinformática, sistemas de hipertexto, análise de texto, bibliometria, teoria organizacional, pesquisa
genealógica e análise de eventos [11].

155

5 Sobre números complexos e quatérnios

5.1 Números complexos

O conjunto dos números complexos é deﬁnido como sendo

C = {a + bi ; i =

√

−1 e a, b ∈ R}.

Tais números demoraram bastante tempo para serem estudados, pois, segundo ANDRE-
ESCU [2], até o século XVIII, matemáticos evitavam equações quadráticas que não pos-
suíssem raíres reais, sendo x2 +1 = 0 a mais simples entre elas. A letra i acima, de acordo
com DUNHAM [29], foi utilizada pela primeira vez por Euler em seu livro Elements of
Algebra para representar

−1.

√

Um número complexo z = a + bi qualquer é dividido em duas partes, sendo uma delas
a parte real Re(z) = a e a outra, sua parte imaginária Im(z) = b. Dessa forma, dois
números complexos z1 = a + bi e z2 = c + di são iguais, se e somente se, a = Re(z1) =
Re(z2) = c e b = Im(z1) = Im(z2) = d.

As operações de soma e multiplicação de números complexos são bem deﬁnidas, uma
vez que, dados dois números complexos z1 = a + bi e z2 = c + di, tem-se que a soma e o
produto destes são ainda números complexos, como podemos observar logo abaixo

• z1 + z2 = a + bi + c + di = (a + c) + (b + d)i

• z1 · z2 = (a + bi)(c + di) = (ac − bd) + (ad + bc)i

Os números complexos apresentam as propriedades a seguir, as quais podem ser fa-

cilmente veriﬁcadas: Sejam z1, z2 e z3 números complexos, tem-se

(cid:63) Comutatividade: z1 + z2 = z2 + z1

(cid:63) Associatividade: z1 + (z2 + z3) = (z1 + z2) + z3

(cid:63) Distributividade: z1(z2 + z3) = z1z2 + z1z3

(cid:63) Elemento neutro (soma): z1 + 0 = z1

(cid:63) Elemento neutro (multiplicação): z1 · 1 = z1

(cid:63) Inverso aditivo: Existe um único elemento u = −z1, tal que z1 + u = u + z1 = 0,

para qualquer que seja z1 ∈ C.

(cid:63) Inverso multiplicativo: Existe um único elemento w = z−1

1 , tal que z1w = wz1 = 1,

para qualquer que seja z1 ∈ C.

156

Tendo em mente o conceito de números complexos, bem como suas principais propri-
edades, buscaremos agora uma maneira de relacionar números complexos com matrizes.
Para tanto, vamos inicialmente considerar a equação

X2 = −I,

(5.1)

onde X =

(cid:21)

(cid:20)m n
q
p

, com m, n, p, q ∈ R e I é a matriz identidade de ordem 2. Para

determinar as entradas da matrix X, notemos que (5.1) nos fornece as seguintes equações

m2 + np = −1

n(m + q) = 0

(5.2)

(5.3)

p(m + q) = 0
q2 + np = −1
Note que, se m + q (cid:54)= 0, então segue de (5.2) e (5.3) que n = 0 e p = 0. Daí, por (5.2)
decorre que m2 = −1, de modo que m não assumiria valores reais. Já, se m + q = 0,
então q = −m e assim, segue de (5.2) que

(5.5)

(5.4)

p = −

1 + m2
n

,

o que nos fornece

(cid:34) m
− 1+m2
n
As matrizes X que satisfazem (5.1), isto é, da forma (5.6), são involuções. Tomando
m = 0 e n = −1 em (5.6), obtemos uma matriz, a qual chamaremos de I, dada por

(5.6)

X =

−m

n

(cid:35)

.

I =

(cid:21)
(cid:20)0 −1
0
1

.

A matriz acima será de grande importância no estabelecimento da relação existente entre
os números coplexos, isto porque será através dela que faremos a correspondência de um
número complexo com uma matriz. Tomemos assim um número complexo z = a + bi e
uma matriz Z = aI + bI. Notemos que

Z = aI + bI =

(cid:21)

(cid:20)a −b
a
b

.

Como sabemos, dois números complexos são iguais se suas respectivas partes reais e
imaginárias forem iguais e, além disso, duas matrizes são iguais se suas respetivas entradas
forem iguais. Dessa forma, podemos estabelecer a seguinte sentença: Dado um número
complexo z = a + bi, existe uma única matriz

Z =

(cid:21)

(cid:20)a −b
a
b

,

associada a z.
matrizes Z = aI + bI, ou seja,

Tal associação nos permite escrever os elementos de C como sendo

(cid:26)

Z = aI + bI ; a, b ∈ R, I =

C =

(cid:21)

(cid:20)1 0
0 1

e

I =

(cid:21)(cid:27)

(cid:20)0 −1
0
1

157

Podemos assim aﬁrmar que tanto as operações como as propriedades enunciadas no
início desta seção podem ser provadas utilizando matrizes. Porém, como estas foram
provadas no Capítulo 1, podemos então considerá-las provadas. Vejamos, por exemplo,
que a multiplicação de números complexos do ponto de vista matricial se dá da mesma
maneira. Tomemos, para tanto, números complexos z1 = a + bi e z2 = c + di. Sabemos
que estes correspondem, respectivamente, a duas matrizes

Z1 =

(cid:21)

(cid:20)a −b
a
b

e

Z2 =

(cid:21)
(cid:20)c −d
c
d

.

Daí, temos que

Z1 · Z2 =

(cid:21)
(cid:20)ac − bd −(ad + bc)
ac − bd
ad + bc
(cid:21)

(cid:20)1 0
0 1

= (ac − bd)

+ (ad + bc)

(cid:21)
(cid:20)0 −1
0
1

= (ac − bd)I + (ad + bc)I.

Outra propriedade interessante conecta o módulo de um número complexo e o deter-
minante da matriz associada a este. O módulo de um número complexo z = a + bi é
deﬁnido como sendo
√

Agora notemos que, sendo Z =

a matriz associada a z, segue

a2 + b2.

|z| =
(cid:21)

(cid:20)a −b
a
b

isto é, o módulo de z pode ser obtido por meio do determinante de Z.

det(Z) = a2 + b2 = |z|2,

5.2 Quatérnios

O termo quatérnio foi utilizado pela primeira vez, de acordo com KRAMER [35],
por William Rowan Hamilton18. Em seu livro Elements of Quaternions [28], Hamilton
deﬁniu um quatérnio como sendo uma expressão quadrinomial da forma

onde w, x, y, z ∈ R e i, j, k tais que

q = w + ix + jy + kz,

e

i2 = j2 = k2 = ijk = −1

ij = k
ji = −k

jk = i
kj = −i

ki = j
ik = −j

18William Rowan Hamilton (1805-1865), indiscutivelmente o maior cientista da Irlanda, nasceu em
Dublin (Irlanda). Ele realizou descobertas importantes tanto em Óptica, como em Mecânica Analítica,
porém considerava a descoberta dos quatérnios o seu maior resultado [19]. Segundo VINCE [63], Hamilton
é reconhecido como criador da Álgebra de Quatérnios, que se tornou a primeira álgebra não-comutativa
a ser descoberta.

158

É importante lembrar que os quatérnios levaram algum tempo para assumirem a
forma acima. Como atesta VINCE [65], Hamilton inicialmente imaginou que assim como
os números complexos no plano possuem a forma a + bi, no espaço teriam a forma a +
bi + cj. porém , o problema encontrado por Hamilton foi que, ao tomar dois termos
q1 = a1 + b1i + c1j e q2 = a2 + b2i + c2j, o produto entre eles resultava em

q1q2 = (a1a2 − b1b2 − c1c2) + (a1b2 + b1a2)i + (a1c2 + c1a2)j + b1c2ij + c1b2ji,
signiﬁcando assim que a multiplicação não estava bem deﬁnida, devido à presença do
termo b1c2ij + c1b2ji. Ainda de acordo com VINCE [65], Hamilton levou cerca de dez
anos para resolver tal problema, chegando a conclusão de que seriam necessários três
termos imaginários i, j, k obedecendo às regras supracitadas.

Agora, com uma breve noção histórica dos quatérnios em mente, vamos estudar al-
gumas entre as várias propriedades que estes apresentam. Antes disso, uma observação.
Assim como na vasta literatura sobre o assunto, vamos escrever, apenas por uma questão
estética, um quatérnio sobre a forma

q = a + bi + cj + dk.

Já sabemos que C = {a + bi ; a, b ∈ R e

−1} representa o conjunto dos
números complexos. De forma parecida, podemos deﬁnir o conjunto dos quatérnio como
sendo

i =

√

H = {a + bi + cj + dk ; a, b, c, d ∈ R e i2 = j2 = k2 = ijk = −1}.
Tanto a soma como a multiplicação dos elementos de H, bem como as propriedades destas
operações (comutatividade, distributividade, elemento neutro, etc) estão bem deﬁnidas.
De fato, sejam q1 = a1 + b1i + c1j e q2 = a2 + b2i + c2j doi quatérnios. Então

• Soma: q1 + q2 = (a1 + a2) + (b1 + b2)i + (c1 + c2)j + (d1 + d2)k

• Produto: q1 · q2 = A + Bi + Cj + Dk

A = a1a2 − b1b2 − c1c2 − d1d2
B = a1b2 + b1a2 + c1d2 − d1c2
C = a1c2 − b1d2 + c1a2 + d1b2
D = a1d2 + b1c2 − c1b2 + d1a2

onde a expressões acima decorrem diretamente das propriedades de soma e multiplicação
de números reais.

Cabe agora veriﬁcarmos se é possível representar quatérnios via matrizes, assim como
ocorre com os números complexos. Felizmente, tal possibilidade existe. Para enxergá-la,
basta observarmos a expressão do produto de dois quatérnios. Note a seguir que esta
pode ser escrita, com um pequeno abuso de notação, como produto de matrizes

q1 · q2 =







A
B
C
D







(cid:2)1 i

j k(cid:3) =







a1 −b1 −c1 −d1
a1 −d1
b1
c1
a1 −b1
c1
d1
a1
b1
d1 −c1













a2
b2
c2
d2







(cid:2)1 i

j k(cid:3)

Daí, tomando q2 = 1 + 0 · i + 0 · j + 0 · k = 1, segue que




q1 =





a1 −b1 −c1 −d1
a1 −d1
b1
c1
a1 −b1
c1
d1
a1
b1
d1 −c1







1
0




0


0

(cid:2)1 i

j k(cid:3) .

159

Como q1 foi tomado arbitrariamente, podemos concluir que a todo quatérnio q = a + bi +
cj + dk corresponde uma matriz da forma







a −b −c −d
c
a −d
b
a −b
c
d
a
b
d −c







Perceba que não foi mencionada a unicidade, pois uma tal representação não é única
assim como para os números complexos. Vejamos que, por exemplo, podemos escrever q
sob a forma

q =







a
−d

d −b −c
c −b
a
a −d
b −c
a
d
b
c









1
0




0


0

(cid:2)1 −k i

j(cid:3) .

É possível representar um quatérnio q = a + bi + cj + dk por uma matriz 2 × 2.
Aﬁm de fazê-lo, notemos que um quatérnio pode ser escrito como um número complexo.
Bastando para isso observar que, sendo k = ij

q = a + bi + cj + dk = (a + bi) + (c + di)j = z + wj,

onde z = a + bi e w = c + di. Daí, tomando dois quatérnios

q1 = a1 + b1i + c1j + d1k = z1 + w1j
q2 = a2 + b2i + c2j + d2k = z2 + w2j

e calculando o produto entre os mesmos, obtemos

q1 · q2 = (z1z2 − w1w2) + (z1w2 + w1z2),

onde o resultado obtido deriva de uma reordenação dos termos (semelhantes a A, B, C, D)
obtidos inicialmente na multiplicação de dois quatérnios. Notemos então que tal expressão
pode ser relacionada ao produto de matrizes abaixo

(cid:20) z1 w1
z1
−w1

(cid:21) (cid:20) z2 w2
z2
−w2

(cid:21)

Daí, tomando q2 = 1, decorre que q1 está relacionado a matriz

(cid:21)

(cid:20) z1 w1
−w1
z1

Porém, como q1 foi tomado arbitrariamente, segue que todo quatérnio q = a+bi+cj+dk =
z + wj pode ser relacionado à uma matriz 2 × 2 da forma

(cid:21)

(cid:20) z w
−w z

=

(cid:21)

(cid:20) a + bi
c + di
−c + di a − bi
(cid:20)1 0
0 1

+ b

(cid:20)i
0
0 −i

(cid:21)

= a

(cid:21)

+ c

(cid:21)

(cid:20) 0
1
−1 0

+ d

(cid:21)

(cid:20)0 i
i 0

= aI + bI + cJ + dK,

160

onde

I =

(cid:21)

(cid:20)1 0
0 1

, I =

(cid:21)

(cid:20)i
0
0 −i

, J =

(cid:21)

(cid:20) 0
1
−1 0

, K =

(cid:21)

(cid:20)0 i
i 0

.

Além disso, é facilmente veriﬁcácel que I 2 = J 2 = K2 = −I e

IJ = K
J I = −K

J K = I
KJ = −I

KI = J
IK = −J

Exemplo 5.1. O quatérnio q = 1 + 2i + 3j + 4k pode ser representado tanto pela matriz



1 −2 −3 −4
3
1 −4
2


1 −2
3
4

1
2
4 −3







como pela matriz

(cid:21)
(cid:20) 1 + 2i 3 + 4i
−3 + 4i 1 − 2i

.

O módulo de um quatérnio q = a + bi + cj + dk é calculado de maneira semelhante

ao de um número complexo, sendo representado por

√

|q| =

a2 + b2 + c2 + d2.

Temos assim que, por exemplo, o módulo de q = 1 + 2i + 3j + 4k é dado por

√

|q| =

12 + 22 + 32 + 42 =

√

30.

Por outro lado, se calcularmos o determinante da matriz associada a q, segue que

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
1 −2 −3 −4
(cid:12)
(cid:12)
3
1 −4
2
(cid:12)
(cid:12)
1 −2
3
4
(cid:12)
(cid:12)
1
2
4 −3
(cid:12)

= 900 = (

√

30)4.

Podemos assim suspeitar que existe alguma relação entre o determinante da matriz asso-
ciada a um quatérnio e o seu módulo. De fato, dado um quatérnio q = a + bi + cj + dk,
decorre que

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
a −b −c −d
(cid:12)
(cid:12)
a −d
b
c
(cid:12)
(cid:12)
a −b
c
d
(cid:12)
(cid:12)
a
b
d −c
(cid:12)

= (a2 + b2 + c2 + d2)2 = (

√

a2 + b2 + c2 + d2)4,

ou seja, o determinante da matriz associada é igual à quarta potência do módulo de um
quatérnio.

Uma outra maneira de associar um quatérnio a uma matriz 2 × 2 é escrevendo sua

matriz 4 × 4 como uma matriz de blocos







a −b −c −d
c
a −d
b
a −b
c
d
d −c
a
b







161

Agora notemos que

Em seguida, tomando

(cid:21)

(cid:20)−c −d
c
−d

=

(cid:21) (cid:20)−1 0
(cid:20)c −d
0 1
c
d

(cid:21)

.

Z =

(cid:21)

(cid:20)a −b
a
b

,

W =

(cid:21)

(cid:20)c −d
c
d

,

L =

(cid:21)

(cid:20)−1 0
1
0

segue que podemos escrever a matriz associada a q simplesmente como

(cid:20) Z WL
Z
−WL

(cid:21)

,

onde as matrizes Z e W são associadas, respectivamente, aos números complexos

z = a + bi

e

w = c + di.

162

Considerações Finais

A realização do presente trabalho traz uma abordagem que, espera-se ser signiﬁcativa
tanto para o ensino, bem como para a prendizagem não só de Álgebra Linear, mas também
de outros ramos do conhecimento onde as matrizes e suas propriedades são utilizadas.

A abordagem das cônicas sob posições incomuns no plano, por exemplo, realizada
no Capítulo 3, gera a possibilidade de tratar de temas como transformações lineares no
plano para alunos de Ensino Médio, ainda que de maneira elementar. Este é um ponto de
partida para apresentar aos alunos uma das tantas aplicações das matrizes. Além disso,
quando e se possível, trabalhar de tal tema utilizando softwares matemáticos, pois estes
proporcionam a visualização geométrica das transformações lineares.

As diferentes maneiras de tratar de matrizes utilizando suas aplicações são diversas,
uma vez que são deixados vários exemplos de teor didático, tal como o Exemplo 1.15, o
qual mostra como criptografar uma mensagem utilizando matrizes, o que pode propor-
cionar uma aula diferenciada sem fugir do tema principal. Até mesmo os tópicos que
geralmente são vistos em graduações que possuem Álgebra Linear na matriz curricular,
tais como Exponencial de Matriz, podem ser trabalhados em sala de aula, claro, depen-
dendo do nível de aprendizado em que a turma se encontre, pois como é sabido, o ensino
de Matemática, de acordo com COIMBRA [13], apresenta os mesmos problemas e quei-
xas por parte dos alunos em relação ao aprendizado dos conteúdos matemáticos, desde
as séries mais elementares até o curso superior.

O que se espera deste trabalho é que, de alguma forma, ele possa contribuir posi-
tivamente para o processo de ensino-aprendizagem tanto para o Ensino Médio, como
também para o Ensino Superior. No que compete ao professor, é esperado que o material
aqui apresentado leve fornecça alguma nova alternativa para expor os conteúdos voltado
às matrizes, de uma maneira mais dinâmica, principalmente nos casos das turmas que
possuem mais diﬁculdades em compreender a teoria por trás das matrizes e suas propri-
edades. A busca de novas ideias é de grande importância, pois, como ressalta PRADO
[54] apud MORAES [48], o professor precisa aprender a construir e a comparar novas
estratégias de ações, novas teorias e novas formas de enfrentar um problema.

Espera-se, de um modo desaﬁador, que este material possa levantar ideias de traba-
lhos, agora direcionado ao ensino de matrizes no Ensino Fundamental, de modo que os
alunos possam estabelecer familiaridade com o conteúdo desde cedo, mesmo na eventu-
alidade de que sejam apenas as ideias básicas, como por exemplo, as quatro operações
entre matrizes. Portanto, espera-se que de algum modo, o trabalho aqui apresentado
possa contribuir de maneira auspiciosa para a Educação.

163

Referências

[1] ALLEN, Linda JS. An Introduction to Mathematical Biology. 2007.

ISBN, v. 10, p. 0-13.

[2] ANDREESCU, Titu et al. Complex Numbers from A to... Z. Boston:

Birkhäuser, 2006.

[3] ANTON, Howard; RORRES, Chris. Elementary linear algebra: applica-

tions version. John Wiley & Sons, 2013.

[4] APT, Krzysztof R. Edsger Wybe Dijkstra (1930–2002): A portrait of a genius.

Formal Aspects of Computing, p. 92-98, 2002.

[5] AYUB, A. et al. Perceived ease of use and usefulness of dynamic mathematical
software: Experiences of Malaysian secondary students. In: Proceeding of
the 7th WSEAS International Conference on Education and Educa-
tional Technology. Venice, Nov. 2008. p. 2008.

[6] BALL, Walter William Rouse. A short account of the history of mathe-

matics. Courier Corporation, 1960.

[7] BAPAT, Ravindra B. Graphs and matrices. London: Springer, 2010.

[8] BASHARIN, Gely P.; LANGVILLE, Amy N.; NAUMOV, Valeriy A. The life
and work of AA Markov. Linear algebra and its applications, v. 386, p.
3-26, 2004.

[9] BELHOSTE, Bruno. Augustin-Louis cauchy: a biography. Springer Sci-

ence & Business Media, 2012.

[10] BOURBAKI, Nicolas. Elements of the History of Mathematics. Springer Sci-

ence & Business Media, 1998.

[11] BRANDES, Ulrik. Network analysis: methodological foundations.

Springer Science & Business Media, 2005.

[12] CAMPOLINO, Marcio Lopes. Translação e rotação de cônicas em R2.
Dissertação (Mestrado em Matemática) - PROFMAT, Universidade de Brasília.
Brasília, 2014.

[13] COIMBRA, Jarbas L. Alguns aspectos problemáticos relacionados ao
ensino-aprendizagem da álgebra linear. Tese de Doutorado. Dissertação
de mestrado, Universidade Federal do Pará, Pará, 2008.

164

[14] COSME, Iria Caline Saraiva. Um método para o cálculo da in-
versa de matrizes em blocos com uso limitado de memória. 2018.
Tese(Doutorado em Engenharia Elétrica e de Computação) - Universidade Re-
gional do Rio Grande do Norte,2018.

[15] CUNHA, Simone Miguez; CARRILHO, Denise Madruga. O processo de adap-
tação ao ensino superior e o rendimento acadêmico. Psicologia escolar e
educacional, v. 9, n. 2, p. 215-224, 2005.

[16] DANTE, Luiz Roberto. Matemática: contexto e aplicações, vol. 3.

Ensino Médio. São Paulo: Ática, 2010.

[17] DENCKER, Ada de Freitas Maneti. A pesquisa ea interdisciplinaridade
no ensino superior: uma experiência no curso de turismo. Tese de
Doutorado. 2000.

[18] DIESTEL, Reinhard; SCHRIJVER, Alexander; SEYMOUR, Paul. Graph

theory. Oberwolfach Reports, v. 7, n. 1, p. 521-580, 2010.

[19] DIMITRIĆ, Radoslav M.; GOLDSMITH, Brendan. Sir William Rowan Hamil-
ton. Pokroky matematiky, fyziky a astronomie, v. 35, n. 5, p. 277-279,
1990.

[20] DUNHAM, William (Ed.). The genius of Euler: reﬂections on his life

and work. MAA, 2007.

[21] DUNHAM, William. Euler: The master of us all. American Mathematical

Soc., 2020.

[22] FAVARÃO, Neide Rodrigues Lago; ARAÚJO, Cíntia de Souza Alferes.

Im-
portância da interdisciplinaridade no ensino superior. Educere-Revista da
Educação da UNIPAR, v. 4, n. 2, 2004.

[23] FIORENTINI, Dario et al. Uma reﬂexão sobre o uso de materiais concretos e

jogos no Ensino da Matemática. Boletim da SBEM-SP, v. 4, n. 7, 1990.

[24] FRANA, Philip L.; MISA, Thomas J. An interview with Edsger W. Dijkstra.

Communications of the ACM, v. 53, n. 8, p. 41-47, 2010.

[25] GENTLE, James E. Matrix algebra. Springer texts in statistics, Springer,

New York, NY, doi, v. 10, p. 978-0, 2007.

[26] GLADCHEFF, Ana Paula; ZUFFI, Edna Maura; SILVA, DM da. Um instru-
mento para avaliação da qualidade de softwares educacionais de matemática
para o ensino fundamental. In: Anais do XXI Congresso da Sociedade
Brasileira de Computação. 2001.

[27] GOLUB, Gene H.; VAN LOAN, Charles F. Matrix computations. JHU

press, 2012.

[28] HAMILTON, William Rowan. Elements of quaternions. Longmans, Green,

& Company, 1866.

[29] HERMITE, Charles. Oeuvres de Charles Hermite. Gauthier-Villars, 1905.

165

[30] HILL, Richard W.; WYSE, Gordon A.; ANDERSON, Margareth. Animal

Physiology. 3rd ed. - Sinauer Associantes Inc, 2012.

[31] HILL, Lester S. Cryptography in an algebraic alphabet. The American

Mathematical Monthly, v. 36, n. 6, p. 306-312, 1929.

[32]

ISMAIL, I. A.; AMIN, Mohammed; DIAB, Hossam. How to repair the Hill
cipher. Journal of Zhejiang University-Science A, v. 7, n. 12, p. 2022-
2030, 2006.

[33] KAZUNGA, Cathrine; BANSILAL, Sarah. Misconceptions about determi-
nants. In: Challenges and strategies in teaching linear algebra. Sprin-
ger, Cham, 2018. p. 127-145.

[34] KOSINSKI, A. A. Cramer’s rule is due to Cramer. Mathematics Magazine,

v. 74, n. 4, p. 310-312, 2001.

[35] KRAMER, Jürg; VON PIPPICH, Anna-Maria. From Natural Numbers to

Quaternions. Springer, 2017.

[36] KREJCAR, Ondrej (Ed.). Modern telemetry. BoD–Books on Demand,

2011

[37] KREYSZIG, E. et al. Advanced Engineering Mathematics. Wiley, Hobo-

ken, NJ (2011).

[38] LARSON, Ron. Elementary linear algebra. 8 ed. Nelson Education, 2016.

[39] LEDERMANN, Walter; NEUMANN, Peter M. The life of Issai Schur th-
rough letters and other documents. PROGRESS IN MATHEMATICS-
BOSTON-, v. 210, p. xlv-xc, 2003.

[40] LEON, Steven J.; BICA, Ion; HOHN, Tiina. Linear algebra with applica-

tions. Upper Saddle River, NJ: Prentice Hall, 1998.

[41] LEWIS, Bert et al. Changes in size and age of Chinook salmon Oncorhynchus

tshawytscha returning to Alaska. PLoS One, v. 10, n. 6, 2015.

[42] LIM, Yongdo. The inverse mean problem of geometric mean and contrahar-
monic means. Linear algebra and its applications, v. 408, p. 221-229,
2005.

[43] LIMA, Elon Lages. Algebra linear. 8 ed - IMPA, Rio de Janeiro, 2012.

[44] LIMA, Elon Lages. Geometria Analítica e Álgebra Linear, 2a ed. IMPA,

Rio de janeiro, 2012.

[45] MENEZES, Alfred J. et al. Handbook of applied cryptography. CRC

press, 1996.

[46] MEYER, Carl D. Matrix analysis and applied linear algebra. Siam, 2000.

166

[47] MONTGOMERY, Douglas C.; PECK, Elizabeth A.; VINING, G. Geoﬀrey.
Introduction to linear regression analysis. John Wiley & Sons, 2012.

[48] MORAES, Maria Candida. O paradigma educacional emergente:

im-
plicações na formação do professor e nas práticas pedagógicas. Em
aberto, v. 16, n. 70, 2008.

[49] MURRAY, James D. Mathematical biology I: An introduction. Springer

Science & Business Media, 2007.

[50] NIEDERHAUSER, Dale S.; STODDART, Trish. Teachers’ instructional pers-
pectives and use of educational software. Teaching and teacher education,
v. 17, n. 1, p. 15-31, 2001.

[51] NÓBREGA, Luciano Xavier Gomes da. Princípio da Indução Matemática
no Ensino Médio. Dissertação de Mestrado. Universidade Federal do Rio
Grande do Norte, 2013.

[52] PACHECO, José Adson D.; BARROS, Janaina V. O uso de softwares educa-
tivos no ensino de matemática. Revista Diálogos, v. 8, p. 5-13, 2013.

[53] POOLE, David. Linear algebra: A modern introduction. Cengage Lear-

ning, 2014.

[54] PRADO, Maria Elisabette Brisola Brito. O uso do computador nos cursos de
formação de professor: um enfoque reﬂexivo da prática pedagógica. Revista
Brasileira de Informática na Educação, v. 3, n. 1, p. 63-64, 1998.

[55] QUINN, Thomas P. The Behavior and Ecology of Paciﬁc Salmon and

Trout. 2rd ed. - University of Washington Press, 2018.

[56] REICH, Karin. Carl Friederich Gauss (1777-1855). Conferències FME, p.

75, 1977.

[57] RICARDO, Henry. A modern introduction to linear algebra. CRC Press,

2009.

[58] SOARES, Ana Paula; ALMEIDA, Leandro S. Transição para a universidade:
Apresentação e validação do Questionário de Expectativas Académicas (QEA).
2001.

[59] STAKHOV, Alexey. The mathematics of harmony: From Euclid to
contemporary mathematics and computer science. World Scientiﬁc,
2009.

[60] TERRADAS, Rodrigo Donizete. A importância da interdisciplinaridade na
educação matemática. Revista da Faculdade de Educação, v. 14, n. 16,
p. 95-114, 2019.

[61] TWEEDIE, Charles. A study of the life and writings of Colin MacLaurin. The

mathematical gazette, v. 8, n. 119, p. 133-151, 1915.

167

[62] VASUDEV, C. Graph theory with applications. New Age International,

2006.

[63] VINCE, John. Quaternions for computer graphics. Springer Science &

Business Media, 2011.

[64] WILLIAM, Mary E.

“Hill, Lester Sanders”.

Complete Dictionary
of Scientiﬁc Biography.
2020
<https://www.encyclopedia.com>, acessado em 22 de setembro de 2020,
16h34min.

Encyclopedia.com.

12 Aug.

168

Apêndice

Embora muitos itens ainda possam ser provados, a ligação entre números complexos
e matrizes vai além de propriedades. Quando uma matriz apresenta números complexos
em suas entradas, esta pode acabar formando matrizes de grande interesse de estudo. Um
exemplo disso pode ser dado por meio do conceito de conjugado de um número complexo
z = a + bi, o qual é deﬁnido como

z = a − bi,

ou seja, o conjugado de um número complexo é obtido por meio da preservação de sua
parte real e da troca do sinal de sua parte imaginária. Vale ressaltar que nos casos em
que z = z, tem-se z = a ∈ R. Em outras palavras, se um número complexo é igual ao
seu conjugado, então ele é real.

Dessa maneira, podemos deﬁnir o conceito de conjugado de uma matriz M = A+iB,

onde A, B são matrizes com entradas reais, como sendo

Por exemplo, o conjugado da matriz

M = A − iB.

é dado por

M =

M =

(cid:20) 3 + i
1 − 2i 4 + 4i

−5

(cid:21)

(cid:20) 3 − i
1 + 2i 4 − 4i

−5

(cid:21)

.

Denotaremos por MH a transposta do conjugado de uma matriz M = A + iB, ou

seja,

MH = M
Nos casos em que ocorre M = MH, diz-se que M é uma matriz hermitiana 19. Obter
exemplos de matrizes hermitianas é uma tarefa simples. Para comprovar isso, considere-
mos uma matriz

.

T

M =

(cid:21)

(cid:20)m1 m2
m3 m4

com mi ∈ C, i ∈ {1, 2, 3, 4}, e suponhamos que a mesma é hermitiana. Desse modo,
devemos ter

(cid:21)

(cid:20)m1 m3
m2 m4

= MH = M =

(cid:21)

(cid:20)m1 m2
m3 m4

.

19Charles Hermite (1822-1901) foi um matemático francês que realizou grandes contribuições para a
matemática pura, em especial, Teoria dos Números e Álgebra. Em 1958 ele resolveu a equação de grau
5 por funções elípticas e, em 1873, ele provou que o número de Euler e é transcendental. O legado
de seu trabalho pode ser demonstrado no grande número de termos matemáticos que leval o adjetivo
“hermitiano(a)” [29].

169

Daí, podemos concluir que m1 e m4 devem ser números reais, ao passo que m2 e m3
devem ser um o conjugado do outro. Logo, com base em tais informações, podemos obter
facilmente uma matriz hermitiana, como por exemplo

(cid:20) 2
3 + i

(cid:21)

3 − i
5

.

Note ainda que, se as entradas de uma matriz hermitiana são todas reais, então tal
matriz é simétrica, isto é, MH = MT , o que por sua vez equivale dizer que se M é
simétrica, então é hermitiana. Já no caso em que M é hermitiana e triangular, entãoM
é uma matriz diagonal, visto que as entradas aij, com i (cid:54)= j deverão ter conjugado igual
a 0, o que força-as serem iguais a zero, restando somente os valores da diagonal principal.
Antes de prosseguirmos devemos aqui deﬁnir a norma de um vetor z = (z1, · · · , zn),

zi ∈ C, a qual é dada por

(cid:107)z(cid:107) = (cid:112)|z1|2 + · · · + |zn|2.

Temos, por exemplo, que

(cid:107)(7 − 2i, 7 + 2i)(cid:107) =

(cid:113)√

72 + 22 +

√

72 + 22 =

(cid:113)
√
2

53.

Note que, se

então

z =

(cid:21)
(cid:20)7 − 2i
7 + 2i

zH = (cid:2)7 + 2i 7 − 2i(cid:3)

e assim

(cid:13)
(cid:20)53
(cid:13)
(cid:13)
53
(cid:13)
Isso acontece devido ao fato de que, dado um número complexo z = a + bi, tem-se que

(cid:113)
√
2

(cid:13)zHz(cid:13)
(cid:13)

(cid:21)(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13) =

53.

=

|z|2 = zz,

o que nos permite escrever

(cid:107)z(cid:107) = (cid:112)(cid:107)zHz(cid:107).
Como (cid:107)z(cid:107) = (cid:112)(cid:104)z, z(cid:105), tal expressão nos unduz a deﬁnir o produto interno para vetores

complexos da forma

(cid:104)z, w(cid:105) = wHz.
De fato, dados três vetores u = (u1, . . . , un), w = (w1, . . . , wn), z = (z1, . . . , zn) e escalares
reais α, β, podemos comprovar isso vendo que tal produto possui

• Positividade: (cid:104)z, z(cid:105) ≥ 0 e (cid:104)z, z(cid:105) = 0 ⇐⇒ z = 0.

De fato,

(cid:104)z, z(cid:105) = (cid:107)z(cid:107)2 ≥ 0.
Além disso, se (cid:104)z, z(cid:105) = 0, então (cid:107)z(cid:107) = 0 ⇐⇒ z = 0.

170

• Simetria: (cid:104)z, w(cid:105) = (cid:104)w, z(cid:105)

Com efeito, temos por um lado que

(cid:104)z, w(cid:105) = wHz = z1w1 + · · · znwn,

enquanto que

(cid:104)z, w(cid:105) = zHw = z1w1 + · · · + znwn = z1w1 + · · · znwn = (cid:104)z, w(cid:105)

• Linearidade: (cid:104)αz + βw, u(cid:105) = α(cid:104)z, u(cid:105) + β(cid:104)w, u(cid:105)

Basta notar que

(cid:104)αz + βw, u(cid:105) = uH(αz + βw) = αuHz + βuHw = α(cid:104)z, u(cid:105) + β(cid:104)w, u(cid:105)

Podemos agora provar o seguinte resultado.

Teorema A.1. Os vetores associados às raízes do polinômio característico de uma matriz
hermitiana são todos reais. Além disso, se tais raízes são distintas, então os vetores
associados são ortogonais.

Demonstração. Dada uma matriz hermitiana A, seja λ a raiz do polinômio característico
associada ao vetor x, tomando k = (cid:104)Ax, x(cid:105), segue que

k = (cid:104)Ax, x(cid:105) = (cid:104)x, Ax(cid:105) = (Ax)Hx = xHAHx = xHAx = (cid:104)Ax, x(cid:105) = k,

Daí, temos que k é real. Agora note que k = xHAx = xHλx, pois Ax = λx, e portanto

k = λxHx = λ(cid:107)x(cid:107)2 ⇐⇒ λ =

k
(cid:107)x(cid:107)2 .

Logo, como k e (cid:107)x(cid:107)2 são ambos números reais, decorre que λ é real. Agora, consideremos
dois vetores x1 e x2 associados a duas raízes λ1 e λ2 do polinômio característico de A,
tais que λ1 (cid:54)= λ2. Assim, notemos que, por um lado

(cid:104)x2, Ax1(cid:105) = (cid:104)x2, λ1x1 = λ1(cid:104)x2, x1(cid:105),

ao passo que

Obtemos assim,

(cid:104)x2, Ax1(cid:105) = xH

1 Ax2 = xH

1 λ2x2 = λ2xH

1 x2 = λ2(cid:104)x2, x1(cid:105).

λ1(cid:104)x2, x1(cid:105) = λ2(cid:104)x2, x1(cid:105) ⇐⇒ (λ1 − λ2)(cid:104)x2, x1(cid:105) = 0.

Logo, como λ1 (cid:54)= λ2 ⇐⇒ λ1 − λ2 (cid:54)= 0, decorre que

ou seja, x1 e x2 são ortogonais.

(cid:104)x2, x1(cid:105) = 0,

171

Deﬁnição A.1. Dois vetores complexos u1 e u2 são ditos ortonormais quando (cid:107)u1(cid:107) =
(cid:107)u2(cid:107) = 1 e (cid:104)u1, u2(cid:105) = 0. Mais geralmente, um conjunto de vetores complexos {u1, . . . , un}
é dito ortonormal se (cid:107)u1(cid:107) = · · · = (cid:107)un(cid:107) = 1 e (cid:104)ui, uj(cid:105) = 0, para todo i (cid:54)= j, com
i, j = 1, 2, . . . , n. Dessa forma, uma matriz complexa U é dita unitária quando seus
vetores coluna formam um conjunto ortonormal. Além disso, U é dita ortogonal quando
UH = U−1.

Teorema A.2. Uma matrix complexa U é ortogonal se, e somente se, é unitária.

Demonstração. Com efeito, escrevendo U como uma matriz de blocos

(cid:2)u1

· · · un

(cid:3) ,

i uj. Sendo assim, se U for
decorre que as entradas da matriz UHU serão dadas por uH
i uj = 0 para i (cid:54)= j, de modo que UHU = I, o
i uj = 1 para i = j e uH
unitária, então uH
que equivale a UH = U−1 e portanto |U é ortogonal. Reciprocamente, se U é ortogonal,
então as entradas da matriz UHU, a saber, uH
i uj são iguais a 1 para i = j e iguais a
0 para i (cid:54)= j, ou seja, o conjunto das colunas de U é ortonormal e, por conseguinte, tal
matriz é unitária.

A Deﬁnição A.1 em conjunto com o Teorema A.1 garantem que se os vetores asso-
ciados às raízes do polinômio característico de uma matriz A são distintos, então existe
uma matriz unitária U que diagonaliza A. De fato, se tais raízes são distintas, então
os vetores associados são ortogonais e portanto, de acordo com o Teorema 3.2, estes são
linearmente independentes. Logo, segue pelo Torema 4.1 que A é diagonalizável e por-
tanto, ortogonalmente diagonalizável. Assim, existe uma matriz X que ortogonaliza A,
ou seja, A = XDXT . Daí, basta normalizarmos os vetores coluna xi de X, tais que

1
(cid:107)xi(cid:107)
obtendo assim uma matriz matriz U com colunas ui, as quais formam um conjunto
ortonormal, de modo que U é unitária e portanto

ui =

xi,

Exemplo A.1. Considere a matriz hermitiana

A = UDUH.

(cid:20) 2
3 + i

(cid:21)

3 − i
5

.

Temos que as raízes do polinômio característico dessa matriz são λ1 = 0, associada ao
vetor x1 = (−3 + i, 2) e λ2 = 7, associada ao vetor x2 = (3 − i, 5). Note que tais raízes são
reais e distintas, o que implica, de acordo com o Teorema 5.1, que x1 e x2 são ortogonais.
Normalizando tais vetores, obtemos vetores ortonormais u1 e u2, tais que

u1 =

1
(cid:107)x1(cid:107)

x1 =






u2 =

1
(cid:107)x2(cid:107)

x2 =






172

−3+i√
14
2√






14
3−i√
35
5√

35

.






Finalmente, os vetores acima compo em uma matriz unitária, a qual diagonaliza ortogo-
nalmente a matriz informada inicialmente, pois






−3+i√
14
2√

14






3−i√
35
5√

35

(cid:21)

(cid:20)0 0
0 7






−3+i√
14
3−i√
35




 =

2√

14

5√

35

(cid:20) 2
3 + i

(cid:21)

3 − i
5

.

Antes de provarmos o próximo resultado, devemos apresentar aqui uma ferramenta
de grande utilidade quando o assunto é vetores ortonormais. Para isso, vamos chamar
de projeção ortogonal do vetor w sobre o eixo que contém x o vetor (cid:104)x, w(cid:105)x.
(cid:107)x(cid:107)x estão alinhados e na mesma direção. Daí,
Observemos então que os vetores x e u = 1
escrevendo x = (cid:107)x(cid:107)u, segue que

(cid:104)x, w(cid:105)x = (cid:104)(cid:107)x(cid:107)u, w(cid:105)(cid:107)x(cid:107)u = (cid:107)x(cid:107)2(cid:104)u, w(cid:105)u ⇐⇒ (cid:104)u, w(cid:105)u =

(cid:104)x, w(cid:105)
(cid:104)x, x(cid:105)

x,

ou seja, podemos representar a projeção do vetor w sobre o eixo que contém x como
sendo

Podemos, a partir daí, obter um vetor v perpendicular a x, desde que

pr

u(v) =

(cid:104)x, w(cid:105)
(cid:104)x, x(cid:105)

x.

v = w −

(cid:104)x, w(cid:105)
(cid:104)x, x(cid:105)

x.

Com efeito,

(cid:28)

(cid:104)x, v(cid:105) =

x, w −

(cid:29)

(cid:104)x,w(cid:105)
(cid:104)x,x(cid:105)

x

= (cid:104)x, w(cid:105) −

(cid:104)x,w(cid:105)
(cid:104)x,x(cid:105)

(cid:104)x, x(cid:105) = 0,

o que comprova a ortogonalidade entre x e v. Vejamos a situação descrita acima na ﬁgura
a seguir

Figura A.1

Fonte: Adaptado de LIMA [43], p.123

173

Seguindo a ideia descrita acima, podemos obter, a partir de um conjunto de vetores

x1, . . . xn, um conjunto ortogonal v1, . . . , vn, bastando para isso, tomar

v1 = x1

v2 = x2 −

v3 = x3 −
...
vn = xn −

(cid:104)x2,v1(cid:105)
(cid:104)v1,v1(cid:105) v1
(cid:104)x3,v1(cid:105)
(cid:104)v1,v1(cid:105) v1 −

(cid:104)x3,v2(cid:105)
(cid:104)v2,v2(cid:105) v2

(cid:104)xn,v1(cid:105)
(cid:104)v1,v1(cid:105) v1 − · · · −

(cid:104)xn,vn−1(cid:105)
(cid:104)vn−1,vn−1(cid:105) vn−1

Em seguida, normalizando cada um dos vetores vi sob a forma yi = 1
vi, obtemos um
(cid:107)v(cid:107)i
conjunto y1, . . . , yn de vetores ortonormais. Tal procedimento é denominado processo
de ortonormalização de Gram-Schmidt.

Teorema A.3. (Teorema de Schur) Seja A uma matriz complexa de ordem n. Existe
uma matriz unitária U tal que UHAU é uma matriz triangular superior.

Demonstração. Para realizar a demonstração, vamos entender o processo utilizado na
mesma, de modo que este poderá ser repetido sucessivas vezes até chegar-se ao resultado
desejado.

• Para o caso n = 1 temos A = [a] e portanto a U = [1], de modo que [1]HA[1] = [a].

• Para n = 2: Sejam x1 e x2 os vetores associados à respectivas raízes λ1, λ2 do
polinômio característico da matriz A. Pelo processo de ortonormalização de Gram-
Schmidt podemos obter vetores ortonormais u1 e u2 (e assim (cid:104)u1, u2(cid:105) = 0, mas não
necessariamente (cid:104)u2, u1(cid:105) = 0) associados a λ1 e λ2, respectivamente. Daí, fazendo
U = (cid:2)u1 u2

(cid:3), segue que

UHAU =

=

=

=

=

(cid:3)

(cid:2)Au1 Au2

(cid:3)

(cid:35)

(cid:35)

A (cid:2)u1 u2

(cid:34)uH
1
uH
2
(cid:34)uH
1
uH
2
(cid:34)uH
1 Au1 uH
2 Au1 uH
uH
(cid:20)λ1uH
1 u1 λ2uH
λ2uH
2 u1 λ2uH
(cid:20)λ1 λ2uH
1 u2
λ2

(cid:21)

0

1 Au2
2 Au2
1 u2
2 u2

(cid:35)

(cid:21)

o que resulta numa matriz triangular superior. Note que a última igualdade acima
1 u1 = λ1(cid:107)u1(cid:107)2 = λ1,
ocorre pelo fato de que, sendo u1 e u2 ortonormais, tem-se λ1uH
λ2uH

2 u1 = λ1(cid:104)u1, u2(cid:105) = λ1 · 0 = 0.

2 u1 = λ1(cid:107)u2(cid:107)2 = λ2 e λ2uH

• Para n = 3: Analogamente ao caso anterior, dados os vetores x1, x2 e x3 associados
às respectivas raízes λ1, λ2, λ3 do polinômio característico de A, podemos utilizar

174

Gram-Schmidt para obter vetores ortonormais u1, u2, u3. Daí, considerando U =
(cid:2)u1 U1

(cid:3), onde U1 = (cid:2)u2 u3

(cid:3), temos que

UHAU = (cid:2)u1 U1
(cid:35)

(cid:3)H A (cid:2)u1 U1

(cid:3)

=

=

(cid:2)Au1 AU1

(cid:3)

(cid:34) uH
1
UH
1
(cid:34) uH
1 Au1 uH
1 Au1 UH
UH

(cid:35)

1 AU1
1 AU1

Agora observe que:

i) uH
ii) uH

1 Au1 = λ1
1 AU1 é uma matriz linha, a saber,

(cid:2)λ2uH

1 u2 λ3uH

1 u3

(cid:3)

iii) UH

1 Au1 é uma matriz coluna nula, pois

UH

1 Au1 =

(cid:21)

(cid:20)λ1(cid:104)u1, u2(cid:105)
λ1(cid:104)u1, u3(cid:105)

(cid:21)
(cid:20)0
0

=

iv) Pelo caso n = 2, temos

Logo,

UH

1 AU1 =

(cid:20)λ2 λ2uH
2 u3
λ3

0

(cid:21)

UHAU =





1 u2 λ3uH
λ1 λ2uH
1 u3
λ2uH
λ2
0
2 u3
λ3
0
0





obtendo assim umam atriz triangular superior.

• A ideia é a mesma para os demais casos, ou seja, dada uma matriz A de ordem
n, pode-se obter vetores ortonormais u1, . . . , un associados às respectivas raízes
λ1, . . . , λn do polinômio característico de A, tais que

UHAU =








λ1 ∗
0
...
0

· · ·

∗

UH

1 AU1








onde UH
também o é.

1 AU1 é uma matriz triangular superior de ordem n − 1 e, portanto, UHAU

Note que no teorema acima não foi mencionado que as raízes do polinômio caracte-
rístico de A devem ser distintos, o que nos fornece um resultado ainda mais forte que
aquele obtido anteriormente a ele. Consideremos, por exemplo, a matriz

A =

(cid:21)

(cid:20) 2 0
−3 2

175

Temos que as raízes do seu polinômio característico são λ1 = λ2 = 1. Daí, os vetores
associados a estas são da forma x =

. Tomemos então

(cid:21)

(cid:20)0
y

x1 =

(cid:21)
(cid:20)0
1

.

Para obtermos um vetor x2 =

(cid:21)

(cid:20)x
y

ortogonal a x1, devemos ter

0 = (cid:104)x1, x2(cid:105) = y.
(cid:21)
(cid:20)1
0

(cid:21)

(cid:20)x
0

Logo, x2 =
normalizá-los, de modo que a matriz unitária U é dada por

. Tomemos x2 =

. Como x1 e x2 são unitários, não é necessário

o que nos fornece a matriz

U =

(cid:21)

(cid:20)0 1
1 0

UHAU =

(cid:21)
(cid:20)2 −3
2
0

a qual é triangular superior. Note que, desde que

U =





0

1
(cid:107)x1(cid:107)





1
(cid:107)x2(cid:107)

0

tem-se que UHAU é triangular superior. Vejamos a seguir um exemplo em que obteremos
uma matriz unitária utilizando o processo de ortonormalização de Gram-Schimdt.

Exemplo A.2. Consideremos a matriz


A =



1
−2 0
1 1
0
0 0 −2



 .

Temos que as raízes de seu polinômio característico são λ1 = 1, λ2 = λ3 = −2. Para
λ = λ1, obtemos vetores associados da forma




0
y

0

 .

Já para λ = λ2, obtemos vetores associados da forma



 .





−3y
y
0

Tomando y = 1, obtemos

x1 =



0
1


0

x2 =

e

176



 .





−3
1
0

Precisaremos de um vetor x3 tal que o conjunto {x1, x2, x3} seja linearmente indepen-
dente. Ora, seja x3 = (x, y, z) tal que (cid:104)x2, x3(cid:105) = 0. Temos daí que

x3 =









x
3x
z

Se x = 1 e z = 0, obtemos x3 =



1
. Com sorte, temos que x1 e x3 são linearmente inde-
3

0

pendetes. Assim, como x1 e x2 também o são (pelo Teorema 4.1), segue que {x1, x2, x3}
é linearmente independente, o que nos permite aplicar Gram-Schimdt. Assim

v1 = x1 =



0
1


0

v2 = x2 −

(cid:104)x2,v1(cid:105)
(cid:104)v1,v1(cid:105) v1 =









−3
0
0

v3 = x3 −

(cid:104)x3,v1(cid:105)
(cid:104)v1,v1(cid:105) v1 −

(cid:104)x3,v2(cid:105)
(cid:104)v2,v2(cid:105) v2 =






0
0

0

Como v3 é nulo, logo não poderá ser coluna de uma matriz unitária. Vamos então

substituí-lo pelo vetor y =


, o qual é claramente linearmente independente em relação



0
0

z

a v1 e v2. Tomando z = 1, obtemos

y =



0
0
 .

1

Daí, como v1 e y são unitários, segue que

u1 = v1 =



0
1
 ,

0

u2 =

1
(cid:107)v2(cid:107)

v2 =









−1
0
0

e u3 = y =



0
0
 ,

1

fornecendo-nos assim

Finalmente

U =



0 −1 0
0 0
1

0 1
0



 .

UHAU =





−1 1
0
1
2
0
0 −2
0



 .

Vamos agora provar um dos teoremas mais belos deste trabalho, que é o

Teorema A.3. (Teorema Espectral) Se A é uma matriz hermitiana, então existe
uma matriz unitária U que diagonaliza A.

177

Demonstração. Sabemos, pelo Teorema de Schur que existe uma matriz unitária U que
diagonaliza A, tal que

UHAU = S,

onde S é uma matriz triangular superior. Além disso, note que

SH = (UHAU)H = UHAHU = UHAU = S,

ou seja, S é hermitiana. Além disso, como S é triangular superior, decorre que a mesma
é uma matriz diagonal.

O Teorema Espectral nos proporciona uma aﬁrmação muito especial:

Toda matriz real simétrica é diagonalizável.

Isto porque toda matriz simétrica cujas entradas são números reais, é também hermitiana,
logo pode ser diagonalizada.

178

