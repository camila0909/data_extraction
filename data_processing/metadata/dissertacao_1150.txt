UNIVERSIDADE DE BRASÃLIA
Instituto de CiÃªncias Exatas
Departamento de MatemÃ¡tica
Mestrado Proï¬ssional em MatemÃ¡tica em Rede Nacional

Matrizes de Markov:
o Teorema de Perron-Frobenius; PageRank
e outras aplicaÃ§Ãµes

por
LÃ¡zaro Sousa Pereira

BrasÃ­lia-DF
2019

UNIVERSIDADE DE BRASÃLIA
Instituto de CiÃªncias Exatas
Departamento de MatemÃ¡tica
Mestrado Proï¬ssional em MatemÃ¡tica em Rede Nacional

LÃ¡zaro Sousa Pereira

Matrizes de Markov: o Teorema de Perron-Frobenius;
PageRank e outras aplicaÃ§Ãµes

DissertaÃ§Ã£o de Mestrado apresentada ao De-
partamento de MatemÃ¡tica da Universidade
BrasÃ­lia como requisito parcial do Programa
de Mestrado Proï¬ssional em MatemÃ¡tica em
Rede Nacional - PROFMAT, para obtenÃ§Ã£o do
tÃ­tulo de Mestre em MatemÃ¡tica.

Orientador: Prof. Dr. Theo Allan Darn Zapata

BrasÃ­lia-DF
2019

Ficha catalogrÃ¡fica elaborada automaticamente, com os dados fornecidos pelo(a) autor(a)SSO725mSousa Pereira, LÃ¡zaro    Matrizes de Markov: o Teorema de Perron-Frobenius;PageRank e outras aplicaÃ§Ãµes. / LÃ¡zaro  Sousa Pereira;orientador Theo Allan Darn Zapata. -- BrasÃ­lia, 2019.   68 p.   DissertaÃ§Ã£o (Mestrado - Mestrado Profissional emMatemÃ¡tica) -- Universidade de BrasÃ­lia, 2019.   1. Matrizes de Markov. 2. Matrizes IrredutÃ­veis. 3.Teorema de Perron-Frobenius. 4. PageRank. 5. Modelo deDifusÃ£o de Ehrenfest. I. Allan Darn Zapata, Theo, orient.II. TÃ­tulo.Com amor, Ã  minha esposa e aos meus ï¬lhos; aos meus pais, irmÃ£o e familiares.
Com respeito, a cada colega professor e professora da educaÃ§Ã£o pÃºblica do nosso paÃ­s; em
especial, Ã  minha colega e irmÃ£ D. Marllene (ProfÂª Allene Martins Rezende).
Em memÃ³ria, aos meus avÃ³s: JoÃ£o e Regino; ao colega e amigo Prof. Romeu(Antonio Vidal).
AtÃ© algum dia!

Agradecimentos

As minhas experiÃªncias de vida revelam que devo ser grato atÃ© mesmo, por ter pelo
que agradecer. A realizaÃ§Ã£o desse curso e desse trabalho tornou-se possÃ­vel graÃ§as a
Deus, a todo plano espiritual e, a tolerÃ¢ncia e a cumplicidade direta ou indireta de
algumas pessoas. Registro entÃ£o, meus sinceros agradecimentos a todas, ainda que eu
nÃ£o as mencione nessas linhas:

A todos os professores do MAT/UnB que empreenderam esforÃ§os no PROFMAT.
Pelas aulas, pelo tempo dedicado a cada conversa e a cada atendimento alÃ©m das aulas.
Em especial aos membros da banca examinadora, o Prof. Dr. Ary Vasconcelos Medino
pela apreciaÃ§Ã£o crÃ­tica e valiosas sugestÃµes e o Prof. Dr. Nilton Moura Barroso Neto
pela leitura atenta e consideraÃ§Ãµes. Ao Prof. Dr. Mauro Ribeiro de Oliveira Junior pela
leitura crÃ­tica e sugestÃµes. Suas contribuiÃ§Ãµes foram inspiradoras e determinantes para
esta importante e conclusiva etapa de minha formaÃ§Ã£o.

Em particular, agradeÃ§o ao meu orientador Prof. Dr. Theo Allan Darn Zapata; ter
sido seu aluno e tÃª-lo como orientador foi uma experiÃªncia muito signiï¬cativa, pelo
que aprendi, pela consciÃªncia da dimensÃ£o do que preciso aprender, e pelo sentido e
valorizaÃ§Ã£o das buscas por entendimento, compreensÃ£o e aprendizagem signiï¬cativa.
Muito obrigado pela generosidade! O tenho tambÃ©m como inspiraÃ§Ã£o e referÃªncia a
partir de entÃ£o.

A todos os colegas e amigos que encontrei durante os dois anos dessa jornada. Nos-
sas discussÃµes virtuais e presenciais foram muito importantes. Em particular, deixo
meus agradecimentos aos colegas de grupo em vÃ¡rias aulas - Cesa Sabino, Marcelo
Carvalho, Edeilson Cavalcante e Roosevelt Bessoni - e em muitas horas de estudos, es-
pecialmente nos ï¬ns de semana e feriados - Rubens Cardoso, Ricardo Pinto e Michel.
Foi relevante compartilhar com vocÃªs essa experiÃªncia.

A agÃªncia de fomento Ã  pesquisa CAPES, pela bolsa concedida.
â™¡ Aos meus pais Antonia e Joel, pelo amor, pelo exemplo e pelas escolhas que ï¬-
zeram em minha educaÃ§Ã£o; ao meu irmÃ£o Leandro, pela torcida de sempre. Mesmo
distantes fÃ­sica e geograï¬camente, os tenho sempre aqui!

â™¡ Ã€ minha esposa e companheira EmÃ­lia Grazielle, pelo amor, pelo sacrifÃ­cio e pela
reiterada resiliÃªncia; aos meus ï¬lhos JoÃ£o Miguel e Bernardo pela paciÃªncia e compa-
nhia. Obrigado por jamais terem duvidado ou desacreditado, mesmo quando eu tive
dÃºvidas.

O correr da vida embrulha tudo, a vida Ã© assim: esquenta e esfria, aperta e daÃ­ afrouxa,
sossega e depois desinquieta. O que ela quer da gente Ã© coragem.
JoÃ£o GuimarÃ£es Rosa, Grande SertÃ£o: Veredas, (p.448), Nova Aguilar, 1Âª ed. 1994

Resumo

Nesta dissertaÃ§Ã£o de mestrado, discutiremos de forma bÃ¡sica a Teoria da Probabilidade
e o cÃ¡lculo de probabilidades alÃ©m de algumas de suas propriedades; mostraremos e
discutiremos as propriedades das Matrizes de Markov e apresentaremos uma demons-
traÃ§Ã£o do Teorema de Perron-Frobenius. AlÃ©m disso daremos exemplos com aplicaÃ§Ãµes
no contexto da Teoria da Probabilidade.

Palavras-chave: Probabilidade, matrizes de Markov, matrizes irredutÃ­veis, Perron-
Frobenius, PageRank, modelo de Ehrenfest.

Abstract

In this master dissertation, we shall basically discuss Probability Theory and the calcu-
lation of probabilities in addition to some of its properties; we will show and discuss
the properties of the Markov Matrices and present a demonstration of the Perron-
Frobenius Theorem. At the end, we shall show you four examples with applications in
the context of Probability Theory.

Key words: Probability, Markov matrices, irreducible matrices, Perron-Frobenius, Pa-
geRank, Ehrenfest model.

SumÃ¡rio

IntroduÃ§Ã£o .

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

. 17

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

PRELIMINARES .
.
.
NoÃ§Ãµes BÃ¡sicas de Probabilidade . . . . . . . . . . . . . . . . . . .
Probabilidade Condicional . . . . . . . . . . . . . . . . . . . . . . . .
IndependÃªncia de Eventos . . . . . . . . . . . . . . . . . . . . . . . .
VariÃ¡veis AleatÃ³rias e Processos EstocÃ¡sticos . . . . . . . . . . . .
Matrizes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
OperaÃ§Ãµes com Matrizes . . . . . . . . . . . . . . . . . . . . . . . . .
Autovalor e Autovetor
. . . . . . . . . . . . . . . . . . . . . . . . . .

. 21
21

24
27
28
29

32
35

.

.

.

.

.

.

.

.

.

.

.

.

.

MATRIZES DE MARKOV .
.
.
Deï¬niÃ§Ã£o e exemplos iniciais . . . . . . . . . . . . . . . . . . . . . .
Matrizes IrredutÃ­veis . . . . . . . . . . . . . . . . . . . . . . . . . . .
Exemplos de matrizes redutÃ­veis . . . . . . . . . . . . . . . . . . . .
Exemplos de matrizes irredutÃ­veis . . . . . . . . . . . . . . . . . . .
Perron-Frobenius . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

.

.

.

.

.

.

.

.

.

. 39
39
41

45
46
47

.

.

.

.

.

.

.

.

.

.

.

.

.

.
.
APLICAÃ‡Ã•ES .
XXIII OBM - OlimpÃ­ada Brasileira de MatemÃ¡tica . . . . . . . . . .
O Problema dos ğ‘› mentirosos . . . . . . . . . . . . . . . . . . . . . .
O PageRank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
O Modelo de DifusÃ£o de Ehrenfest . . . . . . . . . . . . . . . . . . .

. 53
53
56
58
61

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

REFERÃŠNCIAS .

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

. 67

1
1.1

1.1.1
1.1.2
1.1.3
1.2

1.2.1
1.3

2
2.1
2.2

2.2.1
2.2.2
2.3

3
3.1
3.2
3.3
3.4

IntroduÃ§Ã£o

Estudar matemÃ¡tica em nosso paÃ­s, ainda que nÃ£o seja o desejo inicial de muitos
dos estudantes de escolas pÃºblicas Brasil afora, vem sendo uma atividade cada vez
mais incentivada pelas universidades em seus cursos de graduaÃ§Ã£o e por instituiÃ§Ãµes
como o IMPA - Instituto Nacional de MatemÃ¡tica Pura e Aplicada. Como um profes-
sor de escola pÃºblica, a necessidade de atualizaÃ§Ã£o e aperfeiÃ§oamento Ã© companheira
constante. Ainda que nossos incentivos e motivaÃ§Ãµes sejam quase sempre pessoais, es-
tudar matemÃ¡tica, compreender sua dimensÃ£o e relevÃ¢ncia para tantos outros ramos
das ciÃªncias, nos habilita continuamente a trabalhar em nossas salas de aula de forma
mais segura e honesta. Na Ãºltima etapa da educaÃ§Ã£o bÃ¡sica - no ensino mÃ©dio - onde
todos os conceitos matemÃ¡ticos anteriormente estudados precisam ser consolidados,
ampliados e aprofundados, faz-se necessÃ¡rio uma abordagem contextualizada com a
realidade e para isso, Ã© natural trabalharmos com aplicaÃ§Ãµes e modelos que deem sen-
tido Ã  teoria. Esse Ã© um dos principais motivos dessa jornada: estudar e aprender.

Nesta dissertaÃ§Ã£o Ã© apresentada e discutida uma parte interessante, porÃ©m, pouco
estudada em cursos de Ãlgebra Linear e Probabilidade em graduaÃ§Ãµes de licenciatura
MatemÃ¡tica: as matrizes de Markov. Esse tipo de matriz dÃ¡ suporte para ramiï¬ca-
Ã§Ãµes de estudos diversos da matemÃ¡tica, como na teoria da probabilidade e na teoria
e representaÃ§Ã£o de grupos, nas teorias de automorï¬smos de grupos livres [BH12], no
estudo de sistemas dinÃ¢micos e ergÃ³dicos [PY98] e de categorias tensoriais [EGNO15].
Sua principal caracterÃ­stica Ã© o fato de ser quadrada e ter cada vetor-coluna com en-
tradas â‰¥ 0, e soma igual a 1. Esse tipo de vetor Ã© tambÃ©m chamado de vetor de pro-
babilidade, ou simplesmente, vetor estocÃ¡stico. Essas matrizes resultam dos estudos
do matemÃ¡tico russo Andrey Andreyevich Markov, que descreveu do ponto de vista
do produto matricial, o cÃ¡lculo de probabilidades de ocorrÃªncia de certos eventos que
dependem apenas do estado em que o fenÃ´meno se encontra para que seja calculada a
probabilidade de estar num estado seguinte. Essa caracterÃ­stica de nÃ£o estar associada
a resultados de uma memÃ³ria mais extensa de repetiÃ§Ãµes, Ã© o que deï¬ne uma Cadeia
de Markov.

Utilizaremos ao longo de todo o trabalho a denominaÃ§Ã£o matrizes de Markov para
nos referirmos Ã s tradicionalmente conhecidas, matrizes de transiÃ§Ã£o de probabilidades,
porÃ©m com uma interpretaÃ§Ã£o distinta da tradicional, visto que, consideraremos os
vetores-coluna como os vetores estocÃ¡sticos. Com alguns recursos de Ãlgebra Linear
referentes Ã  propriedades e operaÃ§Ãµes entre matrizes, obtemos o mesmo efeito do pro-
duto tradicionalmente conhecido.

Desde o inÃ­cio do programa de mestrado PROFMAT em 2011, foram publicadas
atÃ© o momento mais de 4600 dissertaÃ§Ãµes. A cadeia de Markov foi objeto de estudo

18

INTRODUÃ‡ÃƒO

e discussÃ£o em menos de duas dezenas de dissertaÃ§Ãµes, sendo 10 delas, com foco
numa discussÃ£o que compreendesse o contexto do ensino mÃ©dio. Sendo um incen-
tivo e um desaï¬o a professores e estudantes desta etapa pois, a maioria dos recursos
matemÃ¡ticos necessÃ¡rios encontram-se nas obras e nos cursos de graduaÃ§Ã£o e de pÃ³s-
graduaÃ§Ã£o. Destas dissertaÃ§Ãµes, destaco: Silva, C. E. V. da. AplicaÃ§Ãµes da Ãlgebra
Linear nas Cadeias de Markov, UFG - Universidade Federal de GoiÃ¡s, 2013, que ex-
plana e propÃµe exemplos explorando a resoluÃ§Ã£o de sistemas lineares; Oliveira, J. C.
F. de, NoÃ§Ãµes de grafos dirigidos, cadeias de Markov e as buscas do Google, Univer-
sidade Federal de Sergipe, 2014, que propÃ´s uma aplicaÃ§Ã£o utilizando um exemplo
com o PageRank com estudantes do ensino mÃ©dio; Ribeiro, T. S. G. Processos de Mar-
kov discretos: exemplos voltados para o ensino mÃ©dio, UNESP - Universidade Esta-
dual Paulista JÃºlio de Mesquita Filho, 2017, que discorre sobre matrizes regulares e
utiliza o Teorema de Perron-Frobenius numa versÃ£o que nÃ£o utiliza recursos conside-
rados avanÃ§ados de Ãlgebra Linear, demonstrando-o para matrizes de ordem 2. Em
todos esses trabalhos encontramos material pertinente acerca da cadeia de Markov e
aplicaÃ§Ãµes. Todo o acervo de dissertaÃ§Ãµes pode ser consultado livremente no ende-
reÃ§o http://www.profmat-sbm.org.br/dissertacoes/. Com o ï¬rme propÃ³sito de
apresentar aspectos teÃ³ricos e aplicaÃ§Ãµes das matrizes de Markov, o centro do nosso
trabalho estÃ¡ no Teorema de Perron-frobenius com Ãªnfase na aÃ§Ã£o das matrizes irredu-
tÃ­veis, e suas respectivas consequÃªncias. Para tanto, lanÃ§amos mÃ£o de recursos como
autovalores e autovetores em sua demonstraÃ§Ã£o e nas resoluÃ§Ãµes das aplicaÃ§Ãµes que
propomos.

No capÃ­tulo 1, apresentamos e discutimos de forma bÃ¡sica o cÃ¡lculo de probabili-
dade, da probabilidade condicional, alÃ©m de propriedades que tornam prÃ¡ticas as ite-
raÃ§Ãµes na resoluÃ§Ã£o de certos problemas, como o problema dos ğ‘› mentirosos [Fel67].
Apresentamos ainda deï¬niÃ§Ãµes Ãºteis no contexto das variÃ¡veis aleatÃ³rias e dos proces-
sos estocÃ¡sticos; apresentamos e discutimos tambÃ©m, tÃ³picos sobre matrizes, operaÃ§Ãµes
entre matrizes, autovalores e autovetores.

No capÃ­tulo 2 estÃ¡ o cerne da discussÃ£o acerca das matrizes de Markov. Nele apre-
sentamos uma matriz de Markov e alguns resultados relevantes como por exemplo,
a existÃªncia de um autovalor maximal igual a 1. Em seguida, deï¬nimos uma matriz
irredutÃ­vel, exibimos resultados e exemplos que a caracterizam e apresentamos uma
demonstraÃ§Ã£o do Teorema de Perron-Frobenius sugerida por Wielandt em sua obra
sobre matrizes irredutÃ­veis.

No capÃ­tulo 3, apresentamos e discutimos a resoluÃ§Ã£o de quatro problemas onde
aplicamos esse suporte teÃ³rico. Iniciamos com um problema que foi proposto na 23Âª
OlimpÃ­ada Brasileira de MatemÃ¡tica, onde deseja-se calcular a probabilidade de um
ratinho estar numa certa gaiola, sob certas condiÃ§Ãµes, apÃ³s 23 sinais sonoros. Em se-
guida, discutimos a resoluÃ§Ã£o do problema dos ğ‘› mentirosos, onde desejamos calcular

INTRODUÃ‡ÃƒO

19

a probabilidade de, num grupo com ğ‘› pessoas, uma determinada pessoa estar dizendo
a verdade diante de um conjunto de aï¬rmaÃ§Ãµes feitas pelas demais. Na ediÃ§Ã£o de ja-
neiro / fevereiro de 2000 da Computing in Science and Engineering, Jack Dongarra e
Francis Sullivan selecionaram 10 algoritmos com a maior inï¬‚uÃªncia em ciÃªncia, anÃ¡lise
numÃ©rica e engenharia no sÃ©culo XX. Em marÃ§o de 2016, Nick Higham (presidente do
SIAM, 2017-2018) apresentou uma lista ligeiramente revisada. Em nenhuma ordem
particular, a lista Ã©: (1) mÃ©todos Newton e quasi-Newton; (2) fatoraÃ§Ãµes matriciais (LU,
Cholesky, QR); (3) decomposiÃ§Ã£o do valor singular, algoritmos QR e QZ; (4) mÃ©to-
dos de Monte-Carlo; (5) Transformada rÃ¡pida de Fourier; (6) mÃ©todos do subespaÃ§o
de Krylov; (7) JPEG; (8) PageRank; (9) mÃ©todo simplex; e (10) ï¬ltro de Kalman. No
terceiro problema deste capÃ­tulo, calcularemos o PageRank para uma rede formada
por sÃ­tios da web. Desde 1998, quando da publicaÃ§Ã£o do resultado obtido por Brin e
Page, o PageRank Ã© assunto que vem sendo discutido e estudado em diversos institu-
tos de pesquisa mundo afora. Destacamos o Google Matrix: Fundamentals Applications
and Beyond, workshop organizado em outubro de 2018 pelo IHES - Institut des Hautes
Ã‰tudes Scientiï¬ques (Instituto de Altos Estudos CientÃ­ï¬cos), que Ã© uma das principais
referÃªncias - francesa e mundial - em estudos avanÃ§ados de matemÃ¡tica, fÃ­sica teÃ³rica e
ciÃªncias aï¬ns. Ao ï¬nal, apresentamos e discutimos o modelo de difusÃ£o de Ehrenfest,
onde deseja-se calcular a probabilidade de retorno de cada molÃ©cula para um recipi-
ente ocupado por elas inicialmente. Este modelo foi estudado pelos fÃ­sicos Tatyana
e Paul Ehrenfest no inÃ­cio do sÃ©culo XX, e nessa versÃ£o bÃ¡sica, descreve a origem da
irreversibilidade em sistemas fÃ­sicos.

1 Preliminares

1.1 NoÃ§Ãµes BÃ¡sicas de Probabilidade

Historicamente, a Teoria da Probabilidade surgiu a partir de problemas sobre a dis-
tribuiÃ§Ã£o das apostas em jogos de azar. Seu desenvolvimento subsequente trouxe nÃ£o
sÃ³ para a MatemÃ¡tica, mas, para a EstatÃ­stica, as ciÃªncias da natureza e tambÃ©m soci-
ais, uma poderosa ferramenta de investigaÃ§Ã£o e anÃ¡lise que subsidiaram importantes
resultados desde entÃ£o. Nesta seÃ§Ã£o, vamos deï¬nir e apresentar elementos bÃ¡sicos e
propriedades elementares dessa rica teoria. Ao leitor que desejar veriï¬car uma exposi-
Ã§Ã£o mais detalhada e com bastante exemplos ilustrativos, recomendamos a leitura dos
capÃ­tulos 2, 3 e 5 de [MCCF06].

Deï¬niÃ§Ã£o 1.1.1. Um experimento que nÃ£o apresenta exatamente o mesmo resultado, ainda
que repetido sob condiÃ§Ãµes ï¬xas, Ã© denominado experimento aleatÃ³rio. Do contrÃ¡rio, Ã©
denominado experimento determinÃ­stico.

Deï¬niÃ§Ã£o 1.1.2. Ao conjunto que coleciona todos os resultados possÃ­veis de um experimento
dÃ¡-se o nome de espaÃ§o amostral.

Deï¬niÃ§Ã£o 1.1.3. Denomina-se evento, todo resultado ou subconjunto de resultados de um
experimento; quando o subconjunto Ã© unitÃ¡rio, o evento Ã© chamado de evento simples.

AlÃ©m disso, um espaÃ§o amostral Ã© chamado de discreto se contiver um nÃºmero
ï¬nito de pontos ou se seus inï¬nitos pontos podem ser postos em correspondÃªncia biu-
nÃ­voca com o conjunto dos nÃºmeros naturais. Nesse contexto, vamos denotar o espaÃ§o
amostral por ğ’® e um evento qualquer por â„°. Para uma sequÃªncia de eventos â„°
3, ...,
de ğ’®, conforme seja ï¬nita ou inï¬nita, temos

2, â„°

1, â„°

ğ‘›â‹ƒï¸

â„°

ğ‘– = ğ’® ou

ğ‘–=1
ConvÃ©m ainda registrar que:

âˆ
â‹ƒï¸

ğ‘–=1

â„°

ğ‘– = ğ’®.

(i) a um ponto de ğ’® corresponde um, e tÃ£o somente um, resultado possÃ­vel; e

(ii) cada resultado distinto corresponde a pontos distintos em ğ’®.

Neste trabalho, consideraremos apenas espaÃ§os amostrais discretos.

Exemplo 1.1.1. Uma linha de produÃ§Ã£o de uma industria metalÃºrgica, produz peÃ§as para
automÃ³veis que sÃ£o sempre inspecionadas e classiï¬cadas como B (boa), ou D (defeituosa).
ApÃ³s uma inspeÃ§Ã£o, foram retiradas ao acaso, trÃªs dessas peÃ§as.

22

CAPÃTULO 1. PRELIMINARES

Note que o espaÃ§o amostral associado a esse experimento Ã© o conjunto

ğ’® = {ğµğµğµ, ğ·ğ·ğ·, ğ·ğµğµ, ğµğ·ğµ, ğµğµğ·, ğ·ğ·ğµ, ğ·ğµğ·, ğµğ·ğ·} .

O evento â„°

1 = {ğ·ğµğµ, ğµğ·ğµ, ğµğµğ·}, por exemplo, reÃºne as possÃ­veis amostras onde
exatamente uma peÃ§a Ã© classiï¬cada defeituosa; enquanto cada sequÃªncia, como por
exemplo, {ğµğ·ğµ}, Ã© um evento simples.

Quando nÃ£o mencionado de outra forma, tomaremos por referÃªncia um experi-
mento com um nÃºmero ï¬nito de elementos, e que cada um desses elementos tÃªm a
mesma chance de ocorrÃªncia, isto Ã©, sÃ£o equiprovÃ¡veis. Temos, dessa forma, elementos
suï¬cientes para uma deï¬niÃ§Ã£o de probabilidade que atende nossas pretensÃµes neste
trabalho. Para uma explanaÃ§Ã£o mais aprofundada ver [Jam15], e o clÃ¡ssico [Fel67].

Deï¬niÃ§Ã£o 1.1.4. Dado um espaÃ§o amostral discreto ğ’® de ğ‘ elementos, seja â„° um subcon-
junto de ğ’® composto por ğ‘› elementos. EntÃ£o, a probabilidade de â„°, que denota-se por ğ‘ƒ (â„°) Ã©
o nÃºmero real nÃ£o-negativo obtido a partir da razÃ£o:

ğ‘ƒ (â„°) =

ğ‘›
ğ‘

.

Tradicionalmente, interpretamos esse nÃºmero como a divisÃ£o do nÃºmero de casos
favorÃ¡veis Ã  ocorrÃªncia do evento â„° pelo nÃºmero total de casos possÃ­veis ğ‘ . Observamos
tambÃ©m que se trata de uma funÃ§Ã£o deï¬nida para uma classe dos eventos (subconjun-
tos) de ğ’®.

De forma axiomÃ¡tica, para uma classe ğ’ de eventos de ğ’®, a probabilidade satisfaz:

I. ğ‘ƒ (ğ´) â‰¥ 0 para todo ğ´ âˆˆ ğ’;

II. Se ğ´1, . . . , ğ´ğ‘š Ã© uma sequÃªncia de eventos disjuntos dois a dois, de ğ’ entÃ£o,
â›
ğ‘šâ‹ƒï¸
âœâœâœâœâœâ

ğ‘ƒ (ğ´ğ‘˜);

ğ‘šâˆ‘ï¸

ğ´ğ‘˜

âŸâŸâŸâŸâŸâ 

=

ğ‘ƒ

â

ğ‘˜=1

ğ‘˜=1

III. ğ‘ƒ (ğ’®) = 1.

A partir da deï¬niÃ§Ã£o e dos axiomas, veriï¬camos que a probabilidade possui as propri-
edades:

(ğ‘–) Para todo â„° âŠ‚ ğ’®, tem-se ğ‘ƒ (â„°) â‰¥ 0;

(ğ‘–ğ‘–) Sendo â„°

1 e â„°

2 eventos distintos de ğ’®, tais que, â„°

1

âˆ© â„°

2 = âˆ… entÃ£o:

ğ‘ƒ (â„°
1

âˆª â„°

2) = ğ‘ƒ (â„°

1) + ğ‘ƒ (â„°

2).

De modo geral, para â„°

1 e â„°

2 arbitrÃ¡rios, segue que,

ğ‘ƒ (â„°

1

âˆª â„°

2) â‰¤ ğ‘ƒ (â„°

1) + ğ‘ƒ (â„°

2);

CAPÃTULO 1. PRELIMINARES

23

(ğ‘–ğ‘–ğ‘–) Para â„°ğ‘ = ğ’® âˆ’ â„°, segue que ğ‘ƒ (â„°ğ‘) = 1 âˆ’ ğ‘ƒ (â„°).

De fato, para (ğ‘–) basta veriï¬carmos que como ğ‘ > 0 e ğ‘› â‰¥ 0 temos o suï¬ciente para
ğ‘ƒ (â„°) â‰¥ 0. Supondo que â„°
2 tÃªm, respectivamente, ğ‘›1 e ğ‘›2 eventos simples, e sabendo
que eles nÃ£o tÃªm eventos simples comuns, o nÃºmero de eventos simples de â„°
2 Ã©,
1
entÃ£o, igual a ğ‘›1 + ğ‘›2. Logo, pela deï¬niÃ§Ã£o de probabilidade,

1 e â„°

âˆª â„°

ğ‘ƒ (â„°

1

âˆª â„°

2) =

ğ‘›1 + ğ‘›2
ğ‘

=

ğ‘›1
ğ‘

+

ğ‘›2
ğ‘

= ğ‘ƒ (â„°

1) + ğ‘ƒ (â„°

2).

1 e â„°

Para â„°
evento simples comum. Denotemos esse evento comum por ğ‘›3. EntÃ£o â„°
ğ‘›1 + ğ‘›2

2 arbitrÃ¡rios, Ã© suï¬ciente considerarmos a possibilidade de terem algum
2 tem

âˆ’ ğ‘›3 eventos, e da deï¬niÃ§Ã£o de probabilidade, chegamos a

âˆª â„°

1

ğ‘ƒ (â„°

1

âˆª â„°

2) =

âˆ’ ğ‘›3

ğ‘›1 + ğ‘›2
ğ‘

=

ğ‘›1
ğ‘

+

ğ‘›2
ğ‘

âˆ’ ğ‘›3
ğ‘

e assim obtemos (ğ‘–ğ‘–).
Por ï¬m, como ğ‘ƒ (ğ’®) = 1, para (ğ‘–ğ‘–ğ‘–), temos

= ğ‘ƒ (â„°

1) + ğ‘ƒ (â„°

2) âˆ’ ğ‘ƒ (â„°

1

âˆ© â„°

2) â‰¤ ğ‘ƒ (â„°

1) + ğ‘ƒ (â„°

2),

1 = ğ‘ƒ (ğ’®) = ğ‘ƒ (â„° âˆª â„°ğ‘) = ğ‘ƒ (â„°) + ğ‘ƒ (â„°ğ‘),

pois, â„° e â„°ğ‘ sÃ£o disjuntos. Consequentemente,

ğ‘ƒ (â„°ğ‘) = 1 âˆ’ ğ‘ƒ (â„°).

Exemplo 1.1.2. Problema dos AniversÃ¡rios [Fel67, p. 33]: - Em um grupo formado por
ğ‘Ÿ pessoas, qual Ã© a probabilidade de pelo menos duas delas fazerem aniversÃ¡rio no mesmo
dia?

Esse problema tem intrigado e surpreendido estudantes, pois, uma primeira obser-
vaÃ§Ã£o natural a fazer Ã© que, em funÃ§Ã£o do valor de ğ‘Ÿ, a probabilidade pode ser muito
alta. Sabemos que nem todos os anos tÃªm a mesma duraÃ§Ã£o (anos bissextos tÃªm 366
dias). Com isso, consideraremos um ano com 365 dias. AlÃ©m disso, cada uma das
ğ‘Ÿ pessoas pode ter nascido em qualquer uma das 365 datas disponÃ­veis, isto Ã©, todos
os dias sÃ£o equiprovÃ¡veis, o que nos dÃ¡ um espaÃ§o amostral ğ’® com 365ğ‘Ÿ eventos sim-
ples (que sÃ£o sequÃªncias de tamanho ğ‘Ÿ formadas por datas). Assim, vamos supor que
ğ‘Ÿ < 365 uma vez que, se ğ‘Ÿ â‰¥ 365 a probabilidade desejada seria 1.

Seja o evento â„° = {ao menos 2 pessoas aniversariam no mesmo dia}; entÃ£o, seu com-
plementar Ã© â„°ğ‘ = {ninguÃ©m faz aniversÃ¡rio num mesmo dia}. Note que, para calcular o
nÃºmero de casos favorÃ¡veis Ã  â„°ğ‘ precisamos contar o nÃºmero de sequÃªncias distintas
com ğ‘Ÿ elementos (datas) tomados de um total de 365 datas disponÃ­veis, ou seja, basta
calcularmos um arranjo de 365 datas tomadas de ğ‘Ÿ em ğ‘Ÿ. Com isso, torna-se mais viÃ¡vel
calcularmos ğ‘ƒ (â„°ğ‘) e em seguida, obtemos ğ‘ƒ (â„°) = 1 âˆ’ ğ‘ƒ (â„°ğ‘). Assim,

24

CAPÃTULO 1. PRELIMINARES

ğ‘ƒ (â„°) = 1 âˆ’

]ï¸

[ï¸ 365!
(365âˆ’ğ‘Ÿ)!
365ğ‘Ÿ

[ï¸ƒ

= 1 âˆ’

365 Â· 364 Â· ... Â· (365 âˆ’ ğ‘Ÿ + 1)
365ğ‘Ÿ

]ï¸ƒ

= 1 âˆ’

(ï¸‚

[ï¸‚
1.

1 âˆ’ 1
365

)ï¸‚

Â· ... Â·

(ï¸‚

1 âˆ’ ğ‘Ÿ âˆ’ 1
365

)ï¸‚]ï¸‚

.

A seguinte tabela mostra que nÃ£o Ã© necessÃ¡rio um grupo muito grande de pessoas para
que tenhamos uma possibilidade real de ao menos duas, aniversariarem numa mesma
data.

NÃºmero de pessoas
10
20
22
23
40
60
80

ğ‘ƒ (â„°)
0,11695
0,41144
0,47569
0,50729
0,89123
0,99412
0,99991

Tabela 1 â€“ Probabilidades para um grupo com ğ‘Ÿ pessoas

Em particular, para ğ‘Ÿ = 23 pessoas, a chance de pelo menos duas terem aniversÃ¡rio

em comum excede 50%.

Isso mostra que nÃ£o Ã© necessÃ¡rio uma grupo muito grande de pessoas para que

tenhamos a possibilidade real de ao menos duas, aniversariarem numa mesma data.

1.1.1 Probabilidade Condicional

Os conceitos de probabilidade condicional e de independÃªncia de eventos sÃ£o fun-
damentais na Teoria da Probabilidade e lastreiam parte signiï¬cativa de seu desenvol-
vimento. Para tanto, consideremos um espaÃ§o amostral com resultados equiprovÃ¡veis.

Deï¬niÃ§Ã£o 1.1.5. Sejam ğ´ e ğµ dois eventos de um espaÃ§o amostral ğ’®. Se ğ‘ƒ (ğµ) > 0, a proba-
bilidade condicional de ğ´ dado que ocorreu ğµ, Ã© dada por:

ğ‘ƒ (ğ´|ğµ) =

ğ‘ƒ (ğ´ âˆ© ğµ)
ğ‘ƒ (ğµ)

;

(1.1)

caso ğ‘ƒ (ğµ) = 0, para alguns autores, Ã© conveniente deï¬nir ğ‘ƒ (ğ´|ğµ) = ğ‘ƒ (ğ´).

Comparando com a deï¬niÃ§Ã£o de probabilidade, notamos que, uma vez "certo que
ğµ ocorreu", esse evento passa a ï¬gurar como referÃªncia para um novo espaÃ§o amostral
â€². Como ilustraÃ§Ã£o para essa deï¬niÃ§Ã£o, vamos considerar a seguinte situaÃ§Ã£o espe-
ğ‘†
cial: um experimento consiste em lanÃ§ar um dado honesto duas vezes sobre uma mesa
perfeitamente plana, e observar o nÃºmero de pontos na face superior em cada um dos

CAPÃTULO 1. PRELIMINARES

25

lanÃ§amentos. Suponha que nÃ£o houve a observaÃ§Ã£o dos lanÃ§amentos, porÃ©m, foi infor-
mado que em cada um dos lanÃ§amentos, o nÃºmero de pontos observados Ã© menor do
que ou igual a trÃªs. Nessas condiÃ§Ãµes, qual Ã© a probabilidade de que a soma dos pontos
observados nos dois lanÃ§amentos seja igual a um nÃºmero primo? Para organizar as ideias,
denotemos por ğµ, o evento: em cada um dos lanÃ§amentos, o nÃºmero de pontos observa-
dos Ã© menor do que ou igual a trÃªs; e por ğ´ o evento: a soma dos pontos observados nos
dois lanÃ§amentos seja igual a um nÃºmero primo. Ora, claramente, estamos interessados
em saber qual Ã© a probabilidade de ocorrer o evento ğ´, dado como certo, que ocorreu
o evento ğµ. Para o espaÃ§o amostral associado a esse experimento, temos os seguintes
pares (ğ‘›1, ğ‘›2) das possibilidades para o 1Âº e 2Âº lanÃ§amentos, respectivamente:

(1, 1)
(2, 1)
(3, 1)
(4, 1)
(5, 1)
(6, 1)

(1, 2)
(2, 2)
(3, 2)
(4, 2)
(5, 2)
(6, 2)

(1, 3)
(2, 3)
(3, 3)
(4, 3)
(5, 3)
(6, 3)

(1, 4)
(2, 4)
(3, 4)
(4, 4)
(5, 4)
(6, 4)

(1, 5)
(2, 5)
(3, 5)
(4, 5)
(5, 5)
(6, 5)

(1, 6)
(2, 6)
(3, 6)
(4, 6)
(5, 6)
(6, 6)

Tabela 2 â€“ Resultados possÃ­veis para dois lanÃ§amentos

Consideremos ğµ = {ğ‘›ğ‘˜ Ã© inteiro positivo â‰¤ 3} com 1 â‰¤ ğ‘˜ â‰¤ 2 e, ğ´ = {ğ‘›1 + ğ‘›2 Ã© primo}

e a tabela acima, temos resumidamente:

ğµ = {(1, 1); (1, 2); (1, 3); (2, 1); (2, 2); (2, 3); (3, 1); (3, 2); (3, 3)}

e

ğ´ = {(1, 1); (1, 2); (1, 4); (1, 6); (2, 1); (2, 3); (2, 5); (3, 2); (3, 4); (4, 1); (4, 3); (5, 2); (5, 6); (6, 1); (6, 5)} .

Entretanto, os Ãºnicos pontos de ğ´ que interessam sÃ£o os que tÃªm a soma de suas co-
ordenadas como valores primos â‰¤ 5. Nessas condiÃ§Ãµes, aï¬rmar que o evento ğµ ocor-
reu signiï¬ca dizer que, agora, devemos considerar apenas os pontos do espaÃ§o amos-
tral que pertenÃ§am a ğµ. Assim, ï¬ca fÃ¡cil concluir que a probabilidade procurada Ã©
5
9
ğ´ âˆ© ğµ = {(1, 1); (1, 2); (2, 1); (2, 3); (3, 2)}, segue ğ‘ƒ (ğ´ âˆ© ğµ) =

; pois dos 9 pontos de ğµ, apenas 5 deles pertencem a ğ´. Em outros termos, como

, e entÃ£o,

e ğ‘ƒ (ğµ) =

5
36

9
36

ğ‘ƒ (ğ´|ğµ) =

)ï¸‚

)ï¸‚ =

5
9

.

(ï¸‚ 5
36
(ï¸‚ 9
36

AlÃ©m de servirem como base para modelagens diversas em situaÃ§Ãµes prÃ¡ticas, as
probabilidades condicionais ainda podem ser usadas para calcularmos probabilidades
em geral, embora nem sempre tenhamos facilidade em caracterizar eventos. Para os
casos eventualmente mais espinhosos, convÃ©m construirmos um condicionamento me-
nos complicado. Da fÃ³rmula (1.1), obtemos ğ‘ƒ (ğ´ âˆ© ğµ) = ğ‘ƒ (ğµ)ğ‘ƒ (ğ´|ğµ). Para um nÃºmero

26

CAPÃTULO 1. PRELIMINARES

ï¬nito ğ‘› de interseÃ§Ãµes, a ocorrÃªncia de um evento numa etapa ğ‘˜ < ğ‘› Ã© inï¬‚uenciada pe-
las ocorrÃªncias das ğ‘˜ âˆ’ 1 etapas anteriores. Essa generalizaÃ§Ã£o exprime a probabilidade
da interseÃ§Ã£o de ğ‘› eventos por meio das probabilidades condicionais sucessivas, como
veremos nos lemas a seguir.

Lema 1.1.1 (Regra do Produto de Probabilidades). Seja ğ‘› um inteiro â‰¥ 2 e sejam ğ´1,
ğ´2, ..., ğ´ğ‘› eventos do espaÃ§o amostral ğ’®, para o qual estÃ¡ deï¬nida a probabilidade ğ‘ƒ e com
ğ‘ƒ (

ğ‘–=1 ğ´ğ‘–) > 0, tem-se entÃ£o:

â‹‚ï¸€ğ‘›

ğ‘ƒ (ğ´1

âˆ© ğ´2

âˆ© Â· Â· Â· âˆ© ğ´ğ‘›) = ğ‘ƒ (ğ´1)ğ‘ƒ (ğ´2

|ğ´1)ğ‘ƒ (ğ´3

|ğ´1

âˆ© ğ´2) Â· Â· Â· ğ‘ƒ (ğ´ğ‘›

|ğ´1

âˆ© ğ´2

âˆ© Â· Â· Â· âˆ© ğ´ğ‘›âˆ’1). (1.2)

DemonstraÃ§Ã£o: Por induÃ§Ã£o sobre ğ‘›. Para ğ‘› = 2, segue da deï¬niÃ§Ã£o de probabilidade
condicional que,

ğ‘ƒ (ğ´1

âˆ© ğ´2) = ğ‘ƒ (ğ´1)ğ‘ƒ (ğ´2

|ğ´1),

pois ğ‘ƒ (ğ´1) > 0. Suponha que a igualdade (1.2) seja vÃ¡lida para ğ‘› = ğ‘˜. Temos,

ğ‘ƒ (ğ´1

âˆ© ğ´2

âˆ© Â· Â· Â· âˆ© ğ´ğ‘˜

âˆ© ğ´ğ‘˜+1) = ğ‘ƒ ((ğ´1
= ğ‘ƒ (ğ´1

âˆ© ğ´2
âˆ© ğ´2
= (ğ‘ƒ (ğ´1)ğ‘ƒ (ğ´2

âˆ© Â· Â· Â· âˆ© ğ´ğ‘˜) âˆ© ğ´ğ‘˜+1)
|ğ´1
âˆ© Â· Â· Â· âˆ© ğ´ğ‘˜)ğ‘ƒ (ğ´ğ‘˜+1
âˆ© ğ´2
|ğ´1
|ğ´1) Â· Â· Â· ğ‘ƒ (ğ´ğ‘˜

âˆ© Â· Â· Â· âˆ© ğ´ğ‘˜)
âˆ© ğ´2
âˆ© Â· Â· Â· âˆ© ğ´ğ‘˜âˆ’1))

ğ‘ƒ (ğ´ğ‘˜+1

|ğ´1

âˆ© ğ´2

âˆ© Â· Â· Â· âˆ© ğ´ğ‘˜).

Logo, pelo PrincÃ­pio da InduÃ§Ã£o MatemÃ¡tica, o lema estÃ¡ provado.

O prÃ³ximo lema nos dÃ¡ as ferramentas para calcular a probabilidade de um evento
ğ´ conhecidas as probabilidades dos eventos que compÃµem uma partiÃ§Ã£o de ğ’® e as
respectivas probabilidades condicionais de ğ´ com cada um deles.

Lema 1.1.2 (FÃ³rmula da Probabilidade Total). Seja ğ¶1, ğ¶2, ..., ğ¶ğ‘› uma partiÃ§Ã£o do es-
âˆ© ğ¶ğ‘— = âˆ… sempre que ğ‘– (cid:44) ğ‘—. Seja ğ´ um evento
paÃ§o amostral ğ’®, isto Ã©, tem-se
e ğ‘ƒ uma probabilidade > 0 deï¬nida nos eventos de ğ’®, entÃ£o:

ğ‘–=1 ğ¶ğ‘– = ğ’® e ğ¶ğ‘–

â‹ƒï¸€ğ‘›

ğ‘›âˆ‘ï¸

ğ‘ƒ (ğ´) =

ğ‘ƒ (ğ¶ğ‘–)ğ‘ƒ (ğ´|ğ¶ğ‘–).

ğ‘–=1
DemonstraÃ§Ã£o: Temos por hipÃ³tese, ğ’® =
segue que,

â‹ƒï¸€ğ‘›

ğ‘–=1 ğ¶ğ‘– e tambÃ©m ğ´ âˆˆ ğ’®. Como ğ´ âˆ© ğ’® = ğ´,

ğ´ = ğ´ âˆ© ğ’® = ğ´ âˆ©

â›
ğ‘›â‹ƒï¸
âœâœâœâœâœâ

â

âŸâŸâŸâŸâŸâ 

ğ¶ğ‘–

ğ‘›â‹ƒï¸

(ğ´ âˆ© ğ¶ğ‘–).

=

ğ‘–=1
Calculando a probabilidade de ğ´, utilizando o fato dos eventos serem disjuntos, o lema
anterior e a deï¬niÃ§Ã£o de probabilidade condicional, obtemos:
ğ‘ƒ (ğ´) = ğ‘ƒ (

ğ‘–=1 ğ‘ƒ (ğ¶ğ‘–)ğ‘ƒ (ğ´|ğ¶ğ‘–)

ğ‘–=1 ğ‘ƒ (ğ´ âˆ© ğ¶ğ‘–) =

ğ‘–=1(ğ´ âˆ© ğ¶ğ‘–)) =

â‹ƒï¸€ğ‘›

âˆ‘ï¸€ğ‘›

âˆ‘ï¸€ğ‘›

ğ‘–=1

Com o propÃ³sito de ilustrar esses dois Ãºltimos resultados, vamos discutir um exem-

plo.

CAPÃTULO 1. PRELIMINARES

27

Exemplo 1.1.3. Tem-se 33 bolas que se distinguem apenas pela cor, distribuÃ­das em cinco
urnas distintas. Dessas bolas, 13 sÃ£o brancas e 20 sÃ£o pretas. Dessas urnas, sabe-se que:
uma delas tem 5 bolas brancas e 10 pretas; duas delas tÃªm 2 bolas brancas e 3 pretas e, as
outras duas, tÃªm 2 bolas brancas e 2 pretas.

Uma dessas urnas Ã© aleatoriamente escolhida, e desta, uma bola Ã© aleatoriamente reti-

rada. Qual Ã© a probabilidade de que a bola retirada seja preta?

ResoluÃ§Ã£o: Vamos denotar por ğ’« , o evento: a bola retirada Ã© preta. Denotemos ainda
os eventos: uma delas tem 5 bolas brancas e 10 bolas pretas por ğ´1; duas delas tÃªm 2
bolas brancas e 3 bolas pretas por ğ´2; e, as outras duas, tÃªm 2 bolas brancas e 2 pretas
por ğ´3. Sabendo que uma bola somente pode ser retirada de uma urna de composiÃ§Ã£o
âˆ© ğ’« . Pela deï¬niÃ§Ã£o de probabilidade
âˆ© ğ’« + ğ´3
ğ´1 , ğ´2 ou ğ´3, temos que ğ’« = ğ´1
condicional e pela fÃ³rmula de probabilidade total,

âˆ© ğ’« + ğ´2

ğ‘ƒ (ğ’« ) = ğ‘ƒ (ğ´1)ğ‘ƒ (ğ’« |ğ´1) + ğ‘ƒ (ğ´2)ğ‘ƒ (ğ’« |ğ´2) + ğ‘ƒ (ğ´3)ğ‘ƒ (ğ’« |ğ´3).

Por outro lado, temos as probabilidades:
2
5

; ğ‘ƒ (ğ´2) =

; ğ‘ƒ (ğ´3) =

ğ‘ƒ (ğ´1) =

1
5

2
5

Com isso, concluÃ­mos que ğ‘ƒ (ğ’« ) =

; ğ‘ƒ (ğ’« |ğ´1) =
2
Â· 2
5
3

1
5

+

2
3
Â· 3
5

3
; ğ‘ƒ (ğ’« |ğ´2) =
5
43
Â· 1
.
75
2

2
5

=

+

e ğ‘ƒ (ğ’« |ğ´3) =

1
2

.

1.1.2 IndependÃªncia de Eventos

Apresentamos inicialmente uma noÃ§Ã£o intuitiva da independÃªncia de dois eventos,
pois, mostra que a probabilidade de um nÃ£o Ã© alterada pela informaÃ§Ã£o de que o outro
ocorreu.

Deï¬niÃ§Ã£o 1.1.6. Sejam ğ´ e ğµ dois eventos de um espaÃ§o amostral ğ’®, e suponha que ğ‘ƒ (ğµ) > 0.
O evento ğ´ Ã© dito independente do evento ğµ se: ğ‘ƒ (ğ´|ğµ) = ğ‘ƒ (ğ´).

Se o evento ğ´ Ã© independente do evento ğµ, entÃ£o,

ğ‘ƒ (ğ´ âˆ© ğµ) = ğ‘ƒ (ğµ)ğ‘ƒ (ğ´|ğµ) = ğ‘ƒ (ğ´)ğ‘ƒ (ğµ).

Ademais, parece natural que o evento ğµ seja independente do evento ğ´. De fato, pois,
para ğ‘ƒ (ğ´) > 0,

ğ‘ƒ (ğµ|ğ´) =

ğ‘ƒ (ğ´ âˆ© ğµ)
ğ‘ƒ (ğ´)

=

ğ‘ƒ (ğ´)ğ‘ƒ (ğµ)
ğ‘ƒ (ğ´)

= ğ‘ƒ (ğµ).

Com isso, podemos aï¬rmar que os eventos ğ´ e ğµ sÃ£o independentes se, e somente se,

ğ‘ƒ (ğ´ âˆ© ğµ) = ğ‘ƒ (ğ´)ğ‘ƒ (ğµ).

A partir dessas condiÃ§Ãµes, e da Ãºltima igualdade, podemos generalizar a deï¬niÃ§Ã£o de
independÃªncia de eventos para mais de dois eventos.

28

CAPÃTULO 1. PRELIMINARES

Deï¬niÃ§Ã£o 1.1.7. Sejam os eventos ğ´1, ğ´2, ..., ğ´ğ‘› de um espaÃ§o amostral ğ’®. Esses eventos sÃ£o
Â· Â· Â·âˆ©ğ´ğ‘–ğ‘˜ ) = ğ‘ƒ (ğ´ğ‘–1)ğ‘ƒ (ğ´ğ‘–2) Â· Â· Â· ğ‘ƒ (ğ´ğ‘–ğ‘˜ ) para todo ğ‘˜ = 2, 3, . . . , ğ‘›
ditos independentes se: ğ‘ƒ (ğ´ğ‘–1
e todo {ğ‘–1, ğ‘–2, . . . , ğ‘–ğ‘˜

âˆ©ğ´ğ‘–2
} âŠ‚ {1, 2, . . . , ğ‘›}.

Um problema clÃ¡ssico no estudo da Teoria das Probabilidades e cuja soluÃ§Ã£o Ã© atri-
buÃ­da a Pierre de Fermat no sÃ©culo XVII, Ã© o de observar que, para ensaios do tipo
sucesso-fracasso que sÃ£o realizados de forma independente, com probabilidade de su-
cesso ğ‘, qual seria a probabilidade de ocorrerem ğ‘ sucessos antes de ğ‘€ fracassos?

Na soluÃ§Ã£o Ã© inicialmente apontado que o evento: N sucessos antes de M fracassos Ã©
equivalente Ã : ocorrerem, ao menos, N sucessos nos primeiros M+N-1 ensaios. Para obter-
mos a probabilidade de ğ‘˜ sucessos em ğ‘ + ğ‘€ âˆ’ 1 ensaios, como a ordem de ocorrÃªncia
dos ensaios bem sucedidos nÃ£o importa, entÃ£o, pela independÃªncia entre os ensaios,
temos,

ğ‘ƒ ({ğ‘˜ sucessos em (ğ‘ + ğ‘€ âˆ’ 1) ensaios}) =

)ï¸ƒ

(ï¸ƒ
ğ‘ + ğ‘€ âˆ’ 1
ğ‘˜

Â· ğ‘ğ‘˜ Â· (1 âˆ’ ğ‘)(ğ‘ +ğ‘€âˆ’1)âˆ’ğ‘˜

e, entÃ£o,

ğ‘ƒ ({ğ‘ sucessos antes de ğ‘€ fracassos}) =

ğ‘ +ğ‘€âˆ’1âˆ‘ï¸

ğ‘˜=ğ‘

)ï¸ƒ
(ï¸ƒ
ğ‘ + ğ‘€ âˆ’ 1
ğ‘˜

Â· ğ‘ğ‘˜ Â· (1 âˆ’ ğ‘)(ğ‘ +ğ‘€âˆ’1)âˆ’ğ‘˜.

1.1.3 VariÃ¡veis AleatÃ³rias e Processos EstocÃ¡sticos

Nesta seÃ§Ã£o, apresentaremos deï¬niÃ§Ãµes de variÃ¡vel aleatÃ³ria e de processos estocÃ¡s-
ticos que serÃ£o Ãºteis na resoluÃ§Ã£o de aplicaÃ§Ãµes do capÃ­tulo 3. Havendo curiosidade e
interesse por parte do leitor em uma exposiÃ§Ã£o mais detalhada e aprofundada, vale a
leitura de [Jam15] e [Doo53].

Podemos deï¬nir, informalmente, uma variÃ¡vel aleatÃ³ria como uma funÃ§Ã£o capaz de
associar a cada ponto de um espaÃ§o amostral um nÃºmero real. Neste trabalho, estamos
interessados em um tipo especÃ­ï¬co que Ã© a variÃ¡vel aleatÃ³ria discreta.

Deï¬niÃ§Ã£o 1.1.8. Uma variÃ¡vel aleatÃ³ria ğ‘‹ em um espaÃ§o de probabilidade Ã© uma funÃ§Ã£o
real ğ‘‹ : ğ‘† â†’ R se o conjunto {ğ‘‹ â‰¤ ğ‘¥} = {ğ‘Ÿ âˆˆ ğ‘†, ğ‘¡ğ‘ğ‘™ğ‘ğ‘¢ğ‘’, ğ‘‹(ğ‘Ÿ) â‰¤ ğ‘¥} Ã© um evento aleatÃ³rio para
todo ğ‘¥ âˆˆ R.

Deï¬niÃ§Ã£o 1.1.9. Seja ğ‘‹ uma variÃ¡vel aleatÃ³ria. Se ğ‘‹ assume valores em um conjunto
enumerÃ¡vel diz-se que ğ‘‹ Ã© uma variÃ¡vel aleatÃ³ria discreta.

Em outros termos, se ğ‘‹ Ã© uma variÃ¡vel com valores ğ‘¥1, ğ‘¥2, . . ., entÃ£o, para ğ‘– = 1, 2, . . .

tem-se ğ‘(ğ‘¥ğ‘–) = ğ‘ƒ (ğ‘‹ = ğ‘¥ğ‘–).

Exemplo 1.1.4. Consideremos a situaÃ§Ã£o descrita no exemplo 1.1.1. Precisamos determinar
a probabilidade de, ao ï¬nal da terceira retirada ao acaso, exatamente duas dessas peÃ§as sejam

CAPÃTULO 1. PRELIMINARES

29

boas. Para isso, deï¬namos ğ‘‹ a variÃ¡vel aleatÃ³ria que nos dÃ¡ o nÃºmero de peÃ§as boas, e,
considere ainda que nessa linha de produÃ§Ã£o 4% das peÃ§as produzidas sÃ£o defeituosas e que
as retiradas sÃ£o independentes.

Estamos interessados em calcular ğ‘ƒ (ğ‘‹ = 2). Observando os pontos do espaÃ§o amos-
tral, temos favoravelmente ğµğµğ·, ğµğ·ğµ e ğ·ğµğµ. Assim e sendo Ã³bvio que 96% das peÃ§as
produzidas sÃ£o boas, segue que:

ğ‘ƒ (ğ‘‹ = 2) = ğ‘ƒ (ğµğµğ·) + ğ‘ƒ (ğµğ·ğµ) + ğ‘ƒ (ğ·ğµğµ) = 3(0, 96)2(0, 04) = 11, 06%.

Deï¬niÃ§Ã£o 1.1.10. Um processo estocÃ¡stico Ã© um modelo matemÃ¡tico utilizado no estudo de
fenÃ´menos aleatÃ³rios cujos resultados sÃ£o funÃ§Ãµes.

Em outros termos, consideremos um conjunto ğ¾ nÃ£o-vazio de reais â‰¥ 0. Fixando um
ğ‘˜, podemos pensar num processo estocÃ¡stico como uma famÃ­lia de variÃ¡veis aleatÃ³rias
ğ‘˜âˆˆğ¾ onde ğ‘‹ğ‘˜ : ğ‘† â†’ ğ¸ para cada ğ‘˜ âˆˆ ğ¾ e um conjunto enumerÃ¡vel ğ¸ que possui uma
}
{ğ‘‹ğ‘˜
sequÃªncia de resultados possÃ­veis ğ¸1, ğ¸2, . . .. Um processo estocÃ¡stico onde, dado um
nÃºmero ï¬nito de variÃ¡veis, o valor condicionalmente associado a uma delas depende
do valor imediatamente anterior Ã© chamado processo de Markov. Resumidamente,

ğ‘ƒ (ğ‘‹ğ‘˜ = ğ‘¥ğ‘˜

| ğ‘‹1 = ğ‘¥1, . . . , ğ‘‹ğ‘˜âˆ’2 = ğ‘¥ğ‘˜âˆ’2, ğ‘‹ğ‘˜âˆ’1 = ğ‘¥ğ‘˜âˆ’1) = ğ‘ƒ (ğ‘‹ğ‘˜ = ğ‘¥ğ‘˜

| ğ‘‹ğ‘˜âˆ’1 = ğ‘¥ğ‘˜âˆ’1) = ğ‘ğ‘˜âˆ’1ğ‘˜;

e um processo cuja probabilidade condicional Ã© satisfeita nesses termos Ã© dito ser uma
cadeia de Markov.

1.2 Matrizes

Nesta seÃ§Ã£o, deï¬niremos uma matriz e, resumidamente, um espaÃ§o vetorial. ApÃ³s,
apresentaremos alguns dos principais tipos de matrizes. Em seguida, apresentaremos
de forma sucinta conceitos considerados bÃ¡sicos, como as operaÃ§Ãµes entre matrizes,
determinantes, sistemas e transformaÃ§Ãµes lineares, autovalores e autovetores. Para
uma apreciaÃ§Ã£o mais aprofundada do que trataremos nesta seÃ§Ã£o, Ã© necessÃ¡rio algumas
leituras da jovem, porÃ©m, vasta literatura da Ãlgebra Linear. Em particular, de autores
com os quais servir-me, como [Ser02], [Bol78] e [Lim04].

Deï¬niÃ§Ã£o 1.2.1. Dados ğ‘š e ğ‘› naturais, deï¬nimos uma matriz real ğ´ de ordem ğ‘šâˆ’porâˆ’ğ‘›,
ou simplesmente, ğ´ğ‘šÃ—ğ‘› - como uma tabela formada por nÃºmeros reais ğ‘ğ‘–ğ‘— com 1 â‰¤ ğ‘– â‰¤ ğ‘š e
1 â‰¤ ğ‘— â‰¤ ğ‘›, dispostos em ğ‘š linhas e ğ‘› colunas. Os nÃºmeros reais ğ‘ğ‘–ğ‘— sÃ£o chamados de entradas
da matriz ğ´ e localizam-se na intersecÃ§Ã£o da linha ğ‘– com a coluna ğ‘—.

ğ´ =

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

ğ‘12
ğ‘11
ğ‘22
ğ‘21
...
...
ğ‘ğ‘š1 ğ‘ğ‘š2

Â· Â· Â·
ğ‘1ğ‘›
Â· Â· Â·
ğ‘2ğ‘›
...
. . .
Â· Â· Â· ğ‘ğ‘šğ‘›

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

.

30

CAPÃTULO 1. PRELIMINARES

Deste ponto em diante, cabe registrarmos para ï¬ns prÃ¡ticos que, sempre que nos
referirmos a uma matriz o faremos denominando-a por uma letra maiÃºscula do nosso
alfabeto e, quando necessÃ¡rio, indicaremos sua ordem; tambÃ©m a representaremos por
uma tabela entre parÃªnteses conforme ï¬zemos na deï¬niÃ§Ã£o acima. O sÃ­mbolo â„³(ğ‘š, ğ‘›)
denota o conjunto de todas as matrizes ğ‘šâˆ’porâˆ’ğ‘›. Em relaÃ§Ã£o Ã s suas entradas, para
uma referÃªncia escrita tambÃ©m de ordem prÃ¡tica, utilizaremos a notaÃ§Ã£o ğ‘ğ‘–ğ‘—, e tam-
bÃ©m, ğ´ğ‘–ğ‘— sempre que nÃ£o houver dÃºvidas ou ambiguidades. Uma matriz ğ‘šâˆ’porâˆ’ğ‘›
cujas entradas sÃ£o todas nulas, dÃ¡-se o nome de matriz nula; na literatura encontra-se
simplesmente expressa por 0ğ‘šÃ—ğ‘›.

Embora o termo vetor evoque intuitivamente aos estudantes, a noÃ§Ã£o de uma gran-
deza fÃ­sica com direÃ§Ã£o, sentido e magnitude, aqui entretanto, o tomaremos com uma
noÃ§Ã£o e um contexto mais amplos. Em outros termos, o tomaremos como um elemento
de um conjunto algebricamente estruturado, no qual estÃ£o bem deï¬nidas a adiÃ§Ã£o en-
tre esses elementos e a multiplicaÃ§Ã£o por um nÃºmero real. Estas operaÃ§Ãµes satisfazem
a comutatividade, a associatividade e a distributividade. Este conjunto, que Ã© o ter-
reno onde se desenvolve a Ãlgebra Linear Ã© denominado espaÃ§o vetorial. Como um
exemplo, para todo natural ğ‘›, temos o conjunto Rğ‘›, que representa o espaÃ§o vetorial
euclidiano n-dimensional. Os elementos de Rğ‘› sÃ£o as listas ordenadas de nÃºmeros
reais denominadas como âƒ—ğ‘¢ = (ğ‘¢1, ğ‘¢2, . . . , ğ‘¢ğ‘›). Para o espaÃ§o vetorial Râˆ, seus elementos
sÃ£o as listas inï¬nitas como âƒ—ğ‘ = (ğ‘1, ğ‘2, . . . , ğ‘ğ‘› . . .).

Um subconjunto ğ¹ de um espaÃ§o vetorial ğ¸, para o qual sejam deï¬nidas as mesmas
operaÃ§Ãµes, e satisfaÃ§a as mesmas condiÃ§Ãµes, Ã© um subespaÃ§o vetorial do espaÃ§o ğ¸.
Dizemos que ğ¹ Ã© um conjunto linearmente independente (abreviadamente, l.i.) quando
nenhum vetor âƒ—ğ‘£ âˆˆ ğ¹ Ã© escrito como combinaÃ§Ã£o linear de outros vetores de ğ¹, isto Ã©,
nÃ£o existem ğ›¼ğ‘– reais (cid:44) 0, para 1 â‰¤ ğ‘– â‰¤ ğ‘› tais que, âƒ—ğ‘£ = ğ›¼1 âƒ—ğ‘£1 + Â· Â· Â· + ğ›¼ğ‘› âƒ—ğ‘£ğ‘›, com âƒ—ğ‘£ğ‘–
âˆˆ ğ¹. Para
o caso em que ğ¹ = {ï¸€âƒ—ğ‘£}ï¸€, se âƒ—ğ‘£ (cid:44) âƒ—0 entÃ£o, por deï¬niÃ§Ã£o, ğ¹ Ã© l.i. Quando ğ¹ Ã© l.i., diz-se
tambÃ©m que seus elementos sÃ£o todos (cid:44) âƒ—0 e sÃ£o vetores linearmente independentes, pois,
naturalmente, o vetor nulo Ã© combinaÃ§Ã£o linear de quaisquer outros. Do contrÃ¡rio, um
conjunto ğ‘‹ âˆˆ ğ¸ diz-se linearmente dependente (abreviadamente l.d.) quando nÃ£o Ã© l.i.
Isto signiï¬ca que algum dos vetores âƒ—ğ‘¢ âˆˆ ğ‘‹ Ã© combinaÃ§Ã£o linear de outros elementos de
ğ‘‹, ou entÃ£o, que ğ‘‹ =

{ï¸
âƒ—0

}ï¸
.

Uma base de um espaÃ§o vetorial ğ¸ Ã© um conjunto â„¬ âŠ‚ ğ¸ linearmente independente
que gera ğ¸. Isto signiï¬ca que todo vetor âƒ—ğ‘£ âˆˆ ğ¸ Ã© expresso de modo Ãºnico como combi-
naÃ§Ã£o linear de elementos de â„¬. Os vetores ğ‘’1 = (1, 0, 0, . . . , 0) , ğ‘’2 = (0, 1, 0, . . . , 0) , Â· Â· Â· , ğ‘’ğ‘› =
(0, 0, . . . , 1) formam uma base de Rğ‘› que se chama base canÃ´nica. Se um espaÃ§o veto-
rial ğ¸ admite uma base com ğ‘˜ elementos, entÃ£o, todas as bases de ğ¸ tem exatamente ğ‘˜
elementos, e esse nÃºmero ğ‘˜ Ã© chamado de dimensÃ£o do espaÃ§o vetorial ğ¸.

No contexto das matrizes, por exemplo, uma matriz ğ´ de ordem ğ‘›âˆ’porâˆ’ğ‘š, pode-
Â· Â· Â· ğ‘ğ‘–ğ‘›), enquanto a ğ‘—-Ã©sima

mos representar a ğ‘–-Ã©sima linha pelo vetor-linha (ğ‘ğ‘–1 ğ‘ğ‘–2

CAPÃTULO 1. PRELIMINARES

31

coluna pelo vetor-coluna

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

ğ‘1ğ‘—
ğ‘2ğ‘—
...
ğ‘ğ‘šğ‘—

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

que sÃ£o, respectivamente, elementos dos espaÃ§os vetoriais Rğ‘› e Rğ‘š.

Utilizaremos a partir desse ponto, exceto quando mencionado de forma distinta, a

representaÃ§Ã£o dos vetores como

â›

â

âƒ—ğ‘£ =

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

ğ‘£1
ğ‘£2
...
ğ‘£ğ‘›
A transposta de uma matriz ğ´ âˆˆ â„³(ğ‘š, ğ‘›) Ã© a matriz ğ´ğ‘‡ cujas entradas sÃ£o ğ‘ğ‘—ğ‘–, isto
Ã©, as colunas de ğ´ sÃ£o as linhas de ğ´ğ‘‡ . Quando ğ‘š = ğ‘›, entÃ£o ğ´ Ã© dita ser uma matriz
quadrada (ou simplesmente, de ordem ğ‘›). Essa matriz, Ã© ainda chamada de simÃ©trica
se, e somente se ğ´ğ‘‡ = ğ´, isto Ã©, ğ‘ğ‘–ğ‘— = ğ‘ğ‘—ğ‘– para todo ğ‘– e todo ğ‘—. Ilustrando; a matriz

.

ğ´ =

â›

âœâœâœâœâœâœâœâœâœâ

1 âˆ’7 6
âˆ’7
0
1
5
0
6

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

Ã© uma matriz simÃ©trica.

As entradas ğ‘ğ‘–ğ‘– com 1 â‰¤ ğ‘– â‰¤ ğ‘›, de uma matriz quadrada ğ´ formam a sua diagonal
principal. Uma matriz diagonal de ordem ğ‘› Ã© uma matriz quadrada de ordem ğ‘› em
que todos os elementos que nÃ£o pertenÃ§am Ã  sua diagonal principal sÃ£o nulos, isto Ã©,
se ğ´ Ã© uma matriz diagonal, entÃ£o, ğ‘ğ‘–ğ‘— = 0 sempre que ğ‘– (cid:44) ğ‘—:

ğ´ =

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

ğ‘11
0
...
0

0
ğ‘22
...
0

Â· Â· Â·
0
Â· Â· Â·
0
...
. . .
Â· Â· Â· ğ‘ğ‘›ğ‘›

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

.

Uma matriz diagonal de ordem ğ‘› para a qual, ğ‘šğ‘–ğ‘— =

â§
âªâªâªâ¨
0, se ğ‘– (cid:44) ğ‘—
âªâªâªâ©
1 se ğ‘– = ğ‘—

Ã© chamada de

matriz identidade de ordem ğ‘› e denotada por ğ¼ğ‘›. Em outras palavras,

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

1 0 Â· Â· Â· 0
0 1 Â· Â· Â· 0
...
...
. . .
0 0 Â· Â· Â· 1

...

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

.

ğ¼ğ‘› =

32

CAPÃTULO 1. PRELIMINARES

A matriz identidade Ã© um caso especial de uma matriz de permutaÃ§Ã£o. Esta, Ã©
uma matriz quadrada com exatamente uma entrada diferente de zero em cada linha e
cada coluna, sendo essa entrada um 1. Trataremos de forma detalhada das matrizes de
permutaÃ§Ã£o na seÃ§Ã£o 2.2.

A uma matriz quadrada ğ´ tal que, ğ‘ğ‘–ğ‘— = 0 sempre que ğ‘– > ğ‘—, dÃ¡-se o nome de matriz

triangular superior de ordem ğ‘›:

ğ´ =

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

ğ‘11 ğ‘12
0
ğ‘22
...
...
0
0

Â· Â· Â· ğ‘1ğ‘›
Â· Â· Â· ğ‘2ğ‘›
...
. . .
Â· Â· Â· ğ‘ğ‘›ğ‘›

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

;

por outro lado, quando ğ‘ğ‘–ğ‘— = 0 sempre que ğ‘– < ğ‘—, Ã© uma matriz triangular inferior de
ordem ğ‘›:

ğ´ =

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

0
ğ‘11
ğ‘21 ğ‘22
...
...
ğ‘ğ‘›1 ğ‘ğ‘›2

Â· Â· Â·
0
Â· Â· Â·
0
...
. . .
Â· Â· Â· ğ‘ğ‘›ğ‘›

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

.

1.2.1 OperaÃ§Ãµes com Matrizes

Dizemos que duas matrizes ğ´ e ğµ de mesma ordem ğ‘šâˆ’porâˆ’ğ‘› sÃ£o iguais, e escreve-

mos ğ´ = ğµ, quando, para todo 1 â‰¤ ğ‘– â‰¤ ğ‘š e 1 â‰¤ ğ‘— â‰¤ ğ‘› tem-se ğ‘ğ‘–ğ‘— = ğ‘ğ‘–ğ‘—.

Se ğ´ e ğµ sÃ£o duas matrizes de mesma ordem ğ‘š Ã— ğ‘› e, ğ›¼ e ğ›½ sÃ£o constantes reais
quaisquer, entÃ£o, a soma de ğ´ e ğµ denotada por ğ´ + ğµ, Ã© uma matriz ğ¶ com ordem
ğ‘šâˆ’porâˆ’ğ‘› tal que, ğ‘ğ‘–ğ‘— = ğ‘ğ‘–ğ‘— + ğ‘ğ‘–ğ‘— para todo 1 â‰¤ ğ‘– â‰¤ ğ‘š, e todo 1 â‰¤ ğ‘— â‰¤ ğ‘›; a multiplicaÃ§Ã£o
por escalar ğ›¼ de uma matriz ğ´ Ã© uma matriz ğ›¼ğ´ = ğ›¼ğ‘ğ‘–ğ‘— pata todo ğ‘– e todo ğ‘—.

ğ›¼ğ´ = ğ›¼ Â·

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

ğ‘12
ğ‘11
ğ‘22
ğ‘21
...
...
ğ‘ğ‘š1 ğ‘ğ‘š2

Â· Â· Â·
ğ‘1ğ‘›
Â· Â· Â·
ğ‘2ğ‘›
...
. . .
Â· Â· Â· ğ‘ğ‘šğ‘›

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

=

ğ›¼ğ‘11 ğ›¼ğ‘12
ğ›¼ğ‘21 ğ›¼ğ‘22

...

...

ğ›¼ğ‘ğ‘š1 ğ›¼ğ‘ğ‘š2

Â· Â· Â· ğ›¼ğ‘1ğ‘›
Â· Â· Â· ğ›¼ğ‘2ğ‘›
. . .
Â· Â· Â· ğ›¼ğ‘ğ‘šğ‘›

...

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

.

A adiÃ§Ã£o entre matrizes tem propriedades semelhantes Ã  adiÃ§Ã£o entre nÃºmeros re-

ais e, Ã  adiÃ§Ã£o entre elementos de um espaÃ§o vetorial:

(i) ğ´ + ğµ = ğµ + ğ´ (Comutatividade);

(ii) (ğ´ + ğµ) + ğ¶ = ğ´ + (ğµ + ğ¶) (Associatividade);

(iii) ğ´ + (âˆ’ğ´) = 0 com 0 a matriz nula com mesma ordem de ğ´;

CAPÃTULO 1. PRELIMINARES

33

(iv) ğ´ + 0 = ğ´ (Elemento Neutro).

Veriï¬camos ainda que, para quaisquer ğ´ e ğµ em â„³(ğ‘š, ğ‘›), e ğ›¼, ğ›½ em R vale:

(i) ğ›¼(ğ´ + ğµ) = ğ›¼ğ´ + ğ›¼ğµ (Distributividade);

(ii) (ğ›¼ + ğ›½)ğ´ = ğ›¼ğ´ + ğ›½ğ´;

(iii) ğ›¼(ğ›½ğ´) = (ğ›¼ğ›½)ğ´ e 1ğ´ = ğ´.

O conceito de multiplicaÃ§Ã£o de matrizes, cuja ideia foi expandida, formalizada e
apresentada por Arthur Cayley em 1858, em seu trabalho intitulado A Memoir on the
Theory of Matrices, publicado na revista Philosophical Transactions of the Royal Society of
London; embora nÃ£o seja de trivial execuÃ§Ã£o, Ã© de amplo conhecimento por estudantes
do Ensino MÃ©dio. O produto matricial ğ´ğµ entre as matrizes ğ´ğ‘šÃ—ğ‘› e ğµğ‘›Ã—ğ‘ Ã© deï¬nido como
a matriz ğ·ğ‘šÃ—ğ‘, tal que, ğ· = ğ´ğµ, isto Ã©,

ğ‘‘ğ‘–ğ‘— =

âˆ‘ï¸

1â‰¤ğ‘˜â‰¤ğ‘›

ğ‘ğ‘–ğ‘˜ğ‘ğ‘˜ğ‘— = ğ‘ğ‘–1ğ‘1ğ‘— + Â· Â· Â· + ğ‘ğ‘–ğ‘›ğ‘ğ‘›ğ‘—.

Observando a deï¬niÃ§Ã£o assim apresentada, veriï¬camos que, para que seja possÃ­vel
o produto ğ´ğµ Ã© necessÃ¡rio que o nÃºmero de colunas de ğ´ seja igual ao nÃºmero de linhas
de ğµ. Uma outra forma de entender esse produto Ã© por meio de uma combinaÃ§Ã£o linear
entre os vetores-coluna de ğ´ e as entradas (ou escalares) de cada vetor-coluna de ğµ.
Resumidamente, consideremos a matriz ğ´ğ‘šÃ—ğ‘› e o vetor âƒ—ğ‘£ em Rğ‘›. EntÃ£o, da deï¬niÃ§Ã£o
acima temos:

ğ´âƒ—ğ‘£ =

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

ğ‘12
ğ‘11
ğ‘22
ğ‘21
...
...
ğ‘ğ‘š1 ğ‘ğ‘š2

Â· Â· Â·
ğ‘1ğ‘›
Â· Â· Â·
ğ‘2ğ‘›
...
. . .
Â· Â· Â· ğ‘ğ‘šğ‘›

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

Â·

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

ğ‘£1
ğ‘£2
...
ğ‘£ğ‘›

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

=

ğ‘11ğ‘£1 + ğ‘12ğ‘£2 + Â· Â· Â· + ğ‘1ğ‘›ğ‘£ğ‘›
ğ‘21ğ‘£1 + ğ‘22ğ‘£2 + Â· Â· Â· + ğ‘2ğ‘›ğ‘£ğ‘›
...
ğ‘ğ‘š1ğ‘£1 + ğ‘ğ‘š2ğ‘£2 + Â· Â· Â· + ğ‘ğ‘šğ‘›ğ‘£ğ‘›

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

.

Vale ressaltar ainda que, em geral, o produto matricial nÃ£o e comutativo, isto Ã©, ğ´ğµ (cid:44)
ğµğ´. Algumas razÃµes justiï¬cam essa particularidade: para ğ´ğ‘šÃ—ğ‘› e ğµğ‘›Ã—ğ‘, segue que ğ´ğµ
estÃ¡ bem deï¬nida e ğµğ´, por sua vez, nÃ£o se ğ‘š (cid:44) ğ‘ ; supondo ambas bem deï¬nidas e com
mesmo tamanho, podemos ter elementos de ğ´ğµ distintos dos elementos de ğµğ´, por
2 1
5 6

â
, temos que, ğ´ğµ =

contra-exemplo, se ğ´ =

6
5
19 20

0 1
2 3

enquanto

e ğµ =

âŸâŸâŸâŸâ 

âŸâŸâŸâŸâ 

âŸâŸâŸâŸâ 

âœâœâœâœâ

âœâœâœâœâ

âœâœâœâœâ

â

â

â›

â›

â›

ğµğ´ =

. Dentre as propriedades do produto matricial, destacamos:

â›

âœâœâœâœâ

2
5
12 23

â

âŸâŸâŸâŸâ 

(ğ‘–) ğ´(ğµğ¶) = (ğ´ğµ)ğ¶ Associatividade;

(ğ‘–ğ‘–) ğ´(ğµ + ğ¶) = ğ´ğµ + ğ´ğ¶ Distributividade Ã  esquerda;

34

CAPÃTULO 1. PRELIMINARES

(ğ‘–ğ‘–ğ‘–) (ğ´ + ğµ)ğ¶ = ğ´ğ¶ + ğµğ¶ Distributividade Ã  direita.

ConvÃ©m destacarmos que para uma matriz quadrada ğ´ de ordem ğ‘›âˆ’porâˆ’ğ‘›, deï¬ne-
se ğ´0 = ğ¼ğ‘›. Ademais, ğ´1 = ğ´; ğ´2 = ğ´ Â· ğ´; e de modo geral ğ´ğ‘˜ = ğ´ Â· Â· Â· ğ´ com ğ‘˜ fatores
iguais a ğ´.

Deï¬niÃ§Ã£o 1.2.2. Seja ğ´ uma matriz quadrada de ordem ğ‘›. Se existir uma matriz quadrada
ğµ com ordem ğ‘›, tal que, ğ´ğµ = ğµğ´ = ğ¼ğ‘›, entÃ£o, diremos que ğ´ Ã© invertÃ­vel e que ğµ Ã© sua in-
âˆ’1. Se nÃ£o existir uma matriz ğµ, entÃ£o a matriz ğ´ nÃ£o Ã© invertÃ­vel
versa denotada por ğµ = ğ´
(equivalentemente, Ã© singular).

Da deï¬niÃ§Ã£o acima, veriï¬camos que vale tambÃ©m a aï¬rmaÃ§Ã£o de que ğµ Ã© invertÃ­vel
e que ğ´ Ã© sua inversa. Veriï¬camos tambÃ©m que se ğµ e ğ¶ sÃ£o inversas de uma matriz
ğ´ entÃ£o ğµ = ğ¶. Com efeito, pois, dado que ğµ Ã© inversa de ğ´, temos ğµğ´ = ğ¼. Com isso,
multiplicando Ã  direita ambos os membros, pela matriz ğ¶ , temos (ğµğ´)ğ¶ = ğ¼ğ¶ = ğ¶.
Por outro lado, Ã© fato que (ğµğ´)ğ¶ = ğµ(ğ´ğ¶) = ğµğ¼ = ğµ dado que ğ¶ tambÃ©m Ã© inversa de
ğ´. EntÃ£o, concluÃ­mos que ğµ = ğ¶. HÃ¡ ainda um rico contexto onde as operaÃ§Ãµes entre
matrizes, em particular, a multiplicaÃ§Ã£o, tem papel de destaque. Trata-se das transfor-
maÃ§Ãµes lineares. Por ser distinto das ferramentas que precisaremos, nÃ£o trataremos
desse contexto.

Outro importante conceito acerca das matrizes quadradas e da aplicabilidade para
modelar e resolver problemas diversos, Ã© o de determinante. Inicialmente sendo utili-
zado na resoluÃ§Ã£o de sistemas de equaÃ§Ãµes lineares ou simplesmente, sistemas linea-
res com ğ‘› equaÃ§Ãµes e ğ‘› incÃ³gnitas, que encontramos na literatura inclusive do ensino
mÃ©dio.

Resumidamente apresentado de forma matricial como ğ´âƒ—ğ‘¥ = âƒ—ğ‘, Ã© expresso como a

igualdade a seguir:

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

ğ‘11
ğ‘12
ğ‘21
ğ‘22
...
...
ğ‘ğ‘š1 ğ‘ğ‘š2

Â· Â· Â·
ğ‘1ğ‘›
Â· Â· Â·
ğ‘2ğ‘›
...
. . .
Â· Â· Â· ğ‘ğ‘šğ‘›

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

Â·

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

ğ‘¥1
ğ‘¥2
...
ğ‘¥ğ‘›

=

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

ğ‘1
ğ‘2
...
ğ‘ğ‘›

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

.

Posteriormente, o determinante foi tambÃ©m identiï¬cado como a Ã¡rea de um paralelo-
gramo, como o volume de um paralelepÃ­pedo e, como uma funÃ§Ã£o multilinear alter-
nada que diferentemente das funÃ§Ãµes reais, associam nÃºmeros reais a variÃ¡veis que sÃ£o
matrizes quadradas. Denotaremos o determinante de uma matriz ğ´ por det ğ´. A fun-
Ã§Ã£o determinante tem propriedades Ãºteis que tornam prÃ¡ticas provas e demonstraÃ§Ãµes
de resultados importantes. Destacamos algumas:

Dada uma matriz quadrada ğ´ de ordem ğ‘›,

CAPÃTULO 1. PRELIMINARES

35

(ğ‘–) det ğ´ = 0 se, e somente se ğ´ Ã© singular. Outro aspecto dessa singularidade sÃ£o as
matrizes que possuem linhas(ou colunas) nulas, possuem duas linhas idÃªnticas,
ou possuem duas linhas proporcionais ou duas colunas proporcionais;

(ğ‘–ğ‘–) det ğ´ = det ğ´ğ‘‡ ;

(ğ‘–ğ‘–ğ‘–) det(ğ‘˜.ğ´) = ğ‘˜ğ‘›. det ğ´ para ğ‘˜ âˆˆ R;

(ğ‘–ğ‘£) det(ğ´.ğµ) = det ğ´. det ğµ se ğµ for quadrada com mesma ordem de ğ´;

Ademais, se det ğ´ (cid:44) 0, entÃ£o ğ´ Ã© invertÃ­vel, e segue diretamente que det ğ´

âˆ’1 =

1
det ğ´

.

1.3 Autovalor e Autovetor

Sejam ğ´ uma matriz quadrada de ordem ğ‘› e âƒ—ğ‘¢ um vetor nÃ£o-nulo em Rğ‘›. O vetor
âƒ—ğ‘¢ Ã© chamado de autovetor de ğ´ se ğ´âƒ—ğ‘¢ for um mÃºltiplo escalar de âƒ—ğ‘¢, isto Ã©, ğ´âƒ—ğ‘¢ = ğœ†âƒ—ğ‘¢
para algum escalar real (ou complexo) ğœ†, que por sua vez, Ã© denominado autovalor de
ğ´. Dizemos ainda, mais precisamente que âƒ—ğ‘¢ Ã© um autovetor associado ao autovalor ğœ†.

Em linhas gerais, quando âƒ—ğ‘¢ for um autovetor de ğ´, a multiplicaÃ§Ã£o por ğ´ preservarÃ¡
sua direÃ§Ã£o. Em virtude do sinal e da magnitude do autovalor ğœ† associado a âƒ—ğ‘¢, a
multiplicaÃ§Ã£o ğ´âƒ—ğ‘¢ = ğœ†âƒ—ğ‘¢ comprime ou expande o vetor âƒ—ğ‘¢, invertendo seu sentido no
caso em que ğœ† < 0. A equaÃ§Ã£o ğ´âƒ—ğ‘¢ = ğœ†âƒ—ğ‘¢ pode ser equivalentemente escrita como
âˆ’ ğ´)âƒ—ğ‘¢ = âƒ—0 . Dessa forma, para que ğœ† seja um autovalor de ğ´
ğ´âƒ—ğ‘¢ = ğœ†ğ¼ğ‘›âƒ—ğ‘¢, e daÃ­, (ğœ†ğ¼ğ‘›
esta equaÃ§Ã£o caracterÃ­stica deve possuir alguma soluÃ§Ã£o âƒ—ğ‘¢ nÃ£o-nula (nÃ£o-trivial), e
âˆ’ ğ´) = 0.
isso ocorre se, e somente se a matriz (ğœ†ğ¼ğ‘›
âˆ’ ğ´) encontramos um polinÃ´mio de grau ğ‘› chamado polinÃ´mio
Expandindo det(ğœ†ğ¼ğ‘›
caracterÃ­stico de ğ´ em ğœ† da forma, ğ‘ƒ (ğœ†) = ğœ†ğ‘› + ğ›¼1ğœ†ğ‘›âˆ’1 + Â· Â· Â· + ğ›¼ğ‘› onde o coeï¬ciente lÃ­der
Ã© 1.

âˆ’ ğ´) Ã© singular, ou seja, se det(ğœ†ğ¼ğ‘›

Pelo Teorema Fundamental de Ãlgebra, ele terÃ¡ exatamente ğ‘› raÃ­zes complexas
(contadas suas multiplicidades) que sÃ£o os autovalores de ğ´. A multiplicidade al-
gÃ©brica Ã© o nÃºmero de vezes que ğœ† zera o polinÃ´mio e, a multiplicidade geomÃ©trica
Ã© a dimensÃ£o do subespaÃ§o de autovetores associados a ğœ†. Como consequÃªncias das
deï¬niÃ§Ãµes de autovalores e de autovetores, e das caracterÃ­sticas acima, seguem as pro-
posiÃ§Ãµes [1.3.1] e [1.3.2].

ProposiÃ§Ã£o 1.3.1. Autovetores de uma matriz quadrada ğ´ de ordem ğ‘› associados a autova-
lores distintos sÃ£o linearmente independentes.

ProposiÃ§Ã£o 1.3.2. Se ğ´ for uma matriz quadrada de ordem ğ‘› triangular (superior, inferior
ou diagonal) entÃ£o seus autovalores sÃ£o as entradas de sua diagonal principal.

36

CAPÃTULO 1. PRELIMINARES

Por outro lado, observamos ainda que, sendo âƒ—ğ‘¢ e âƒ—ğ‘£ dois autovetores associados
ao mesmo autovalor ğœ†0, isto Ã©, ğ´âƒ—ğ‘¢ = ğœ†0âƒ—ğ‘¢ e ğ´âƒ—ğ‘£ = ğœ†0âƒ—ğ‘£, entÃ£o, para quaisquer ğ‘1 e ğ‘2
complexos (e obviamente, reais) um vetor âƒ—ğ‘¤ = ğ‘1âƒ—ğ‘¢ + ğ‘2âƒ—ğ‘£ Ã© tal que, ğ´ âƒ—ğ‘¤ = ğœ†0 âƒ—ğ‘¤.

Com efeito, pois,

ğ´ âƒ—ğ‘¤ = ğ´(ğ‘1âƒ—ğ‘¢ + ğ‘2âƒ—ğ‘£) = ğ‘1ğ´âƒ—ğ‘¢ + ğ‘2ğ´âƒ—ğ‘£ = ğ‘1ğœ†0âƒ—ğ‘¢ + ğ‘2ğœ†0âƒ—ğ‘£ = ğœ†0(ğ‘1âƒ—ğ‘¢ + ğ‘2âƒ—ğ‘£) = ğœ†0 âƒ—ğ‘¤.

Como os autovetores associados a um autovalor ğœ† de uma matriz ğ´ sÃ£o os vetores nÃ£o
âˆ’ ğ´)âƒ—ğ‘¢ = âƒ—0, podemos aï¬rmar que esses autovetores
nulos que satisfazem a equaÃ§Ã£o (ğœ†ğ¼ğ‘›
âˆ’ ğ´), ou seja, do conjunto que
sÃ£o os vetores nÃ£o nulos do espaÃ§o nulo da matriz (ğœ†ğ¼ğ‘›
âˆ’ğ´)âƒ—ğ‘¢ = âƒ—0. Esse conjunto Ã© denominado
coleciona as soluÃ§Ãµes nÃ£o-nulas do sistema (ğœ†ğ¼ğ‘›
auto-espaÃ§o de ğ´ associado ğœ†.

ProposiÃ§Ã£o 1.3.3. Se ğ‘˜ for inteiro positivo, ğœ† um autovalor de uma matriz quadrada ğ´ de
ordem ğ‘› e âƒ—ğ‘¢ um autovetor associado, entÃ£o ğœ†ğ‘˜ Ã© um autovalor de ğ´ğ‘˜ e âƒ—ğ‘¢ Ã© um autovetor
associado.

ProposiÃ§Ã£o 1.3.4. Se ğ´ Ã© uma matriz quadrada de ordem ğ‘›, entÃ£o sÃ£o equivalentes as se-
guintes aï¬rmaÃ§Ãµes:

(a) ğ‘‘ğ‘’ğ‘¡(ğ´) (cid:44) 0, isto Ã©, ğ´ Ã© invertÃ­vel(nÃ£o singular);

(b) Os vetores-coluna de ğ´ sÃ£o linearmente independentes e geram Rğ‘›;

(c) Os vetores-linha de ğ´ sÃ£o linearmente independentes e geram Rğ‘›;

(d) Os vetores-coluna(e tambÃ©m, linha) de ğ´ formam uma base de Rğ‘›.

Como consequÃªncia desses resultados e para tornar prÃ¡tica a resoluÃ§Ã£o de proble-
mas onde hÃ¡ necessidade de calcular potÃªncias de matrizes, a decomposiÃ§Ã£o de uma
âˆ’1 onde ğ‘† Ã© uma matriz invertÃ­vel e ğ· uma matriz diagonal,
matriz ğ´ na forma ğ´ = ğ‘†ğ·ğ‘†
Ã© uma chave importante. Essa decomposiÃ§Ã£o Ã© chamada de decomposiÃ§Ã£o espectral da
matriz ğ´ que por sua vez, Ã© dita ser diagonalizÃ¡vel. Os vetores-coluna da matriz ğ‘† sÃ£o
os autovetores de ğ´ e as entradas da diagonal principal de ğ· sÃ£o os autovalores de ğ´
associados aos respectivos autovetores.

Como exemplo dessa decomposiÃ§Ã£o, consideremos a matriz

ğ´ =

â›

âœâœâœâœâœâœâœâœâœâ

0 0 âˆ’2
1
1 2
3
1 0

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

.

Seu polinÃ´mio caracterÃ­stico Ã© ğ‘ƒğ´(ğœ†) = (ğœ† âˆ’ 2)2(ğœ† âˆ’ 1) e disso obtemos os autovalores
â
âˆ’1
0
1

, e, para ğœ† = 1, o autovetor

ğœ†1 = 2 com autovetores associados âƒ—ğ‘£ =

e âƒ—ğ‘¢ =

0
1
0

âŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

âœâœâœâœâœâœâœâœâœâ

âœâœâœâœâœâœâœâœâœâ

âŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

â

â›

â›

CAPÃTULO 1. PRELIMINARES

37

âƒ—ğ‘¤ =

â›

âœâœâœâœâœâœâœâœâœâ

âˆ’2
1
1

forma,

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

. Assim, fazendo ğ‘† =

â›

âœâœâœâœâœâœâœâœâœâ

âˆ’1 0 âˆ’2
1
1
0
1
0
1

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

, temos ğ‘†

âˆ’1 =

â›

âœâœâœâœâœâœâœâœâœâ

2
0
1
1
1
1
âˆ’1 0 âˆ’1

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

, e dessa

âˆ’1ğ´ğ‘† =

ğ‘†

â›

âœâœâœâœâœâœâœâœâœâ

2
0
1
1
1
1
âˆ’1 0 âˆ’1

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

â›

âœâœâœâœâœâœâœâœâœâ

Â·

0 0 âˆ’2
1
1 2
3
1 0

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

â›

âœâœâœâœâœâœâœâœâœâ

Â·

âˆ’1 0 âˆ’2
1
1
0
1
0
1

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

â›

âœâœâœâœâœâœâœâœâœâ

=

2 0 0
0 2 0
0 0 1

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

= ğ·

donde concluÃ­mos que,

ğ´ = ğ‘†ğ·ğ‘†

âˆ’1 =

â›

âœâœâœâœâœâœâœâœâœâ

âˆ’1 0 âˆ’2
1
1
0
1
0
1

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

â›

âœâœâœâœâœâœâœâœâœâ

Â·

2 0 0
0 2 0
0 0 1

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

â›

âœâœâœâœâœâœâœâœâœâ

Â·

2
0
1
1
1
1
âˆ’1 0 âˆ’1

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

.

2 Matrizes de Markov

Neste capÃ­tulo mostraremos uma importante ferramenta no estudo e na soluÃ§Ã£o de
problemas que surgem a partir da anÃ¡lise do comportamento de certos sistemas fÃ­si-
cos que evolvem constantemente, sob a inï¬‚uÃªncia de um conjunto ï¬nito de variÃ¡veis
cujos valores mudam com o passar do tempo. Esses sistemas sÃ£o chamados de Siste-
mas DinÃ¢micos e seu estudo compreende campos de pesquisa importantes da matemÃ¡-
tica como a Teoria das Probabilidades, Ãlgebra Linear, FÃ­sica MatemÃ¡tica e EquaÃ§Ãµes
Diferenciais; tendo aplicaÃ§Ãµes relevantes nas engenharias, biologia, economia, dentre
outras ciÃªncias sociais. Deï¬niremos as Matrizes de Markov, fruto de estudos do ma-
temÃ¡tico russo Andrei Andreyevich Markov que descreveu matematicamente, sob o
ponto de vista do produto matricial, o cÃ¡lculo de probabilidades de ocorrÃªncia de cer-
tos eventos que dependem apenas do estado em que o fenÃ´meno se encontra para que
seja calculada a probabilidade de estar num estado seguinte.

2.1 Deï¬niÃ§Ã£o e exemplos iniciais

Deï¬niÃ§Ã£o 2.1.1. Uma matriz quadrada â‰¥ 0 diremos ser uma Matriz de Markov se a soma
das entradas de cada vetor-coluna Ã© sempre igual a 1.

Na Teoria da Probabilidade essas matrizes sÃ£o tradicionalmente chamadas de ma-
trizes de transiÃ§Ã£o de probabilidades (ou simplesmente matriz de transiÃ§Ã£o) e, cada
vetor-linha tem como soma de suas entradas o valor 1. Em nosso trabalho considera-
remos a soma das entradas de cada vetor-coluna sempre igual a 1.

Como exemplo trivial, a matriz ğ´ =

identidade sÃ£o matrizes de Markov.

â›

âœâœâœâœâ

â

âŸâŸâŸâŸâ 

2
3
1
3

1
2
1
2

Ã© uma matriz de Markov; matrizes

As matrizes de Markov tambÃ©m sÃ£o conhecidas como matrizes estocÃ¡sticas. Cada
coluna de uma matriz de Markov Ã© um vetor cujas entradas sÃ£o â‰¥ 0 e com soma 1.
Cada um desses vetores sÃ£o chamados vetor de probabilidade (ou simplesmente, vetor
estocÃ¡stico). Vejamos alguns resultados acerca desse tipo especial de matriz.

ProposiÃ§Ã£o 2.1.1. Se âƒ—ğ‘ Ã© um vetor estocÃ¡stico e ğ´ Ã© uma matriz de Markov, entÃ£o, âƒ—ğ´ğ‘ Ã© um
vetor estocÃ¡stico.

DemonstraÃ§Ã£o: Suponha que âƒ—ğ‘1, âƒ—ğ‘2, . . . , âƒ—ğ‘ğ‘› sejam os vetores coluna de ğ´ e ğ‘1, ğ‘2, . . . , ğ‘ğ‘›
as entradas nÃ£o negativas do vetor estocÃ¡stico âƒ—ğ‘. O produto ğ´âƒ—ğ‘ pode ser expresso como
uma combinaÃ§Ã£o linear dos vetores-coluna de ğ´, com os coeï¬cientes sendo as entradas

40

CAPÃTULO 2. MATRIZES DE MARKOV

de âƒ—ğ‘, isto Ã©, denotando por ğ‘ğ‘– com 1 â‰¤ ğ‘– â‰¤ ğ‘›, as entradas de âƒ—ğ´ğ‘, segue que

ğ´âƒ—ğ‘ =

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

ğ‘1
ğ‘2
...
ğ‘ğ‘›

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

para ğ‘1 =

âˆ‘ï¸

1â‰¤ğ‘˜â‰¤ğ‘›

ğ‘ğ‘˜ğ‘1ğ‘˜; ğ‘2 =

âˆ‘ï¸

1â‰¤ğ‘˜â‰¤ğ‘›

ğ‘ğ‘˜ğ‘2ğ‘˜; Â· Â· Â· ; ğ‘ğ‘› =

âˆ‘ï¸

1â‰¤ğ‘˜â‰¤ğ‘›

ğ‘ğ‘˜ğ‘ğ‘›ğ‘˜.

Ora, se somarmos ğ‘1 +ğ‘2 +Â· Â· Â·+ğ‘ğ‘› obtemos ğ‘1
que ğ´âƒ—ğ‘ Ã© um vetor estocÃ¡stico.

Â·1+ğ‘2

Â·1+Â· Â· Â·+ğ‘ğ‘›

Â·1 = 1, donde concluÃ­mos

ProposiÃ§Ã£o 2.1.2. O produto de duas matrizes de Markov de uma mesma ordem Ã© uma
matriz de Markov.

DemonstraÃ§Ã£o: Sejam ğ´ e ğµ duas matrizes de Markov de uma mesma ordem, e âƒ—ğ‘ğ‘– com
1 â‰¤ ğ‘– â‰¤ ğ‘› os vetores-coluna de ğµ. A matriz ğ´ğµ Ã© tal que, suas colunas sÃ£o os vetores ğ´âƒ—ğ‘ğ‘–
com 1 â‰¤ ğ‘– â‰¤ ğ‘›. Note que cada ğ´âƒ—ğ‘ğ‘– Ã© um vetor estocÃ¡stico, conforme provamos acima,
para todo ğ‘–. ConcluÃ­mos que ğ´ğµ Ã© uma matriz de Markov.

Veriï¬camos com facilidade que a soma de duas matrizes de Markov nÃ£o Ã© uma
matriz de Markov, pois, considerando as matrizes ğ´ e ğµ, como da proposiÃ§Ã£o acima, a
soma das entradas de cada vetor-coluna da matriz ğ´ + ğµ serÃ¡ igual a 2. A inversa de
uma matriz de Markov nÃ£o Ã©, necessariamente, uma matriz de Markov. Com efeito,

pois, a matriz ğ´ =

tem como inversa a matriz ğ´

âˆ’1 =

que, embora

â›

âœâœâœâœâ

4 âˆ’2
âˆ’3
3

â

âŸâŸâŸâŸâ 

â›

âœâœâœâœâ

1
2
1
2

â

âŸâŸâŸâŸâ 

1
3
2
3

tenha soma das entradas dos vetores-coluna iguais a 1, possui entradas negativas.

No que diz respeito a autovalores e autovetores de uma matriz de Markov, temos o

seguinte resultado.

ProposiÃ§Ã£o 2.1.3. Uma matriz de Markov tem sempre um autovalor igual a 1. Qualquer
outro Ã©, em valor absoluto, menor do que ou igual a 1.

DemonstraÃ§Ã£o: Seja ğ´ uma matriz de Markov de ordem ğ‘›. A soma das entradas de
cada vetor-linha de sua transposta ğ´ğ‘‡ Ã© igual a 1. Dessa forma, a matriz ğ´ğ‘‡ tem,
portanto, o autovetor

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

1
1
...
1

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

.

âƒ—ğ‘£ =

Como matrizes transpostas tÃªm o mesmo determinante, como vimos no capÃ­tulo 1, as
âˆ’ğ´ğ‘‡ ) tÃªm o mesmo determinante, de maneira que os autovalo-
matrizes (ğœ†ğ¼ğ‘›
res de ğ´ e de ğ´ğ‘‡ sÃ£o os mesmos. Assim, como ğ´ğ‘‡ tem autovalor 1, a matriz ğ´ tambÃ©m
possui autovalor 1.

âˆ’ğ´) e (ğœ†ğ¼ğ‘›

CAPÃTULO 2. MATRIZES DE MARKOV

41

Suponha agora que âƒ—ğ‘¢ Ã© um autovetor associado a um autovalor com |ğœ†| > 1. En-
tÃ£o, ğ´ğ‘˜ âƒ—ğ‘¢ = |ğœ†|ğ‘˜ âƒ—ğ‘¢ tem entradas que crescem exponencialmente para ğ‘˜ suï¬cientemente
grande. Isso implica que existe (ğ´ğ‘˜)ğ‘–ğ‘—>1. Mas, ğ´ğ‘˜ Ã© uma matriz de Markov e tem todas
as suas entradas â‰¤ 1. E assim, a suposiÃ§Ã£o de um autovalor maior do que 1 nÃ£o pode
ser vÃ¡lida, e entÃ£o concluÃ­mos que todos os outros autovalores sÃ£o, em valor absoluto,
menor do que ou igual a 1.

Vamos ilustrar a deï¬niÃ§Ã£o e os resultados acima.

â›

â

Exemplo 2.1.1. A matriz de Markov

1
5
1
5
1
5
1
5
1
5
vetor-linha igual a 1. EntÃ£o, pela proposiÃ§Ã£o acima, tem um autovalor ğœ†1 = 1 e um autovetor

tem a soma das entradas de cada

1
5
1
5
1
5
1
5
1
5

1
5
1
5
1
5
1
5
1
5

1
5
1
5
1
5
1
5
1
5

1
5
1
5
1
5
1
5
1
5

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

associado âƒ—ğ‘¢ =

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

1
1
1
1
1

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

.

Ademais, essa matriz tem polinÃ´mio caracterÃ­stico ğ‘ƒ (ğœ†) = ğœ†4(ğœ† âˆ’ 1), donde tiramos seu
outro autovalor ğœ†2 = 0 com multiplicidade 4. Os autovetores associados a esse autovalor
sÃ£o:

âƒ—ğ‘£1 =

,

âƒ—ğ‘£2 =

,

âƒ—ğ‘£3 =

,

âƒ—ğ‘£4 =

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

âˆ’1
0
0
0
1

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

âˆ’1
0
0
1
0

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

âˆ’1
0
1
0
0

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

âˆ’1
1
0
0
0

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

.

2.2 Matrizes IrredutÃ­veis

Nesta seÃ§Ã£o, trataremos de conceitos que sÃ£o menos estudados e trabalhados nos
cursos de Ãlgebra Linear, como os de matriz â‰¥ 0, positiva, redutÃ­vel e irredutÃ­vel.
Nosso objetivo Ã© conhecer os elementos e propriedades relevantes que utilizaremos na
demonstraÃ§Ã£o do Teorema de Perron-Frobenius. Faremos entÃ£o, uma breve exposiÃ§Ã£o
destes conceitos e de suas propriedades.

Deï¬niÃ§Ã£o 2.2.1. Uma matriz real ğ´ de tamanho ğ‘›âˆ’porâˆ’ğ‘› Ã© uma matriz nÃ£o-negativa, a
â‰¥ 0 para cada ğ‘– e ğ‘— em {1, 2, ..., ğ‘›}. Por outro lado, se ğ´ Ã©
qual denota-se por ğ´ â‰¥ 0, se ğ´ğ‘–ğ‘—

42

CAPÃTULO 2. MATRIZES DE MARKOV

uma matriz positiva entÃ£o ela tem ğ´ğ‘–ğ‘— > 0 para cada ğ‘– e ğ‘— em {1, 2, ..., ğ‘›}, e por conseguinte,
a denotamos por ğ´ > 0.

Notamos da deï¬niÃ§Ã£o acima que, se ğ´ â‰¥ 0 entÃ£o, para todo ğ‘˜ â‰¥ 1, veriï¬ca-se que
ğ´ğ‘˜ â‰¥ 0, e ainda, que ğ´ğ‘˜ > 0 sempre que a matriz ğ´ for positiva ou tiver a maior parte
das entradas de cada linha e de cada coluna nÃ£o-nula. Como exemplos de matrizes
â‰¥ 0 que utilizaremos neste trabalho, destacamos as matrizes de permutaÃ§Ã£o, que sÃ£o
matrizes quadradas na qual cada linha e cada coluna tem uma Ãºnica entrada igual a
1 e, todas as outras, nulas. Outra maneira de concebermos as matrizes de permuta-
Ã§Ã£o Ã© veriï¬carmos que elas resultam de um reordenamento das linhas(ou colunas) das
matrizes identidade. Nesse contexto, utilizando os vetores da base canÃ´nica do espaÃ§o
Rğ‘› com ğ‘› â‰¥ 2, temos que, para ğ‘› = 2 existem duas matrizes de permutaÃ§Ãµes possÃ­veis,
a saber: (ğ‘’1, ğ‘’2) e (ğ‘’2, ğ‘’1). De modo anÃ¡logo, veriï¬camos que para ğ‘› = 3 existem seis
matrizes de permutaÃ§Ãµes possÃ­veis, para ğ‘› = 4 existem vinte e quatro, etc. Nessas con-
diÃ§Ãµes, existem ğ‘›! matrizes de permutaÃ§Ãµes possÃ­veis a partir da identidade de ordem
ğ‘›. As matrizes

â›

âœâœâœâœâ

0 1
1 0

â

âŸâŸâŸâŸâ  ,

â›

âœâœâœâœâœâœâœâœâœâ

0 0 1
1 0 0
0 1 0

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

,

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

0 1 0 0
1 0 0 0
0 0 1 0
0 0 0 1

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

,

sÃ£o exemplos de algumas matrizes de permutaÃ§Ãµes de ordens 2, 3 e 4, respectivamente.

Pela deï¬niÃ§Ã£o das matrizes de permutaÃ§Ã£o, podemos veriï¬car facilmente que:

(i) O produto matricial entre matrizes de permutaÃ§Ã£o Ã© uma matriz de permutaÃ§Ã£o;

(ii) Elas sÃ£o matrizes ortogonais, isto Ã©, sua inversa Ã© igual Ã  sua transposta (ğ‘ƒ

âˆ’1 = ğ‘ƒ ğ‘‡ ).
Em particular, o determinante de uma matriz de permutaÃ§Ã£o Ã© sempre igual a Â±1.

De fato, para (i) a conclusÃ£o Ã© imediata, pois, Ã© suï¬ciente notarmos que, se trata da
permutaÃ§Ã£o das linhas da identidade. Para (ii), note que ğ‘ƒ ğ‘‡ ğ‘ƒ = ğ¼ (onde ğ¼ Ã© a identidade
de ordem correspondente). Ademais, det(ğ‘ƒ ğ‘‡ ğ‘ƒ ) = det(ğ‘ƒ ğ‘‡ ) det(ğ‘ƒ ) = (det(ğ‘ƒ ))2 = 1, donde
concluÃ­mos que det(ğ‘ƒ ) = Â±1.

Dada uma matriz ğ´, o produto ğ‘ƒ ğ´ troca as linhas da matriz ğ´, enquanto o produto
âˆ’1ğ´ğ‘ƒ troca as linhas e as colunas de ğ´ do mesmo

ğ´ğ‘ƒ troca as colunas. JÃ¡ o produto ğ‘ƒ
modo.

Deï¬niÃ§Ã£o 2.2.2. Uma matriz quadrada ğ´ Ã© dita redutÃ­vel se existe uma matriz de permu-
taÃ§Ã£o ğ‘ƒ , tal que

âˆ’1ğ´ğ‘ƒ =

ğ‘ƒ

â›

âœâœâœâœâ

ğ‘‹ ğ‘Œ
0 ğ‘

â

âŸâŸâŸâŸâ  ,

CAPÃTULO 2. MATRIZES DE MARKOV

43

onde ğ‘‹ e ğ‘ sÃ£o matrizes quadradas; a matriz 0 Ã© nula e ğ‘Œ qualquer. Se uma matriz quadrada
ğ´ nÃ£o Ã© redutÃ­vel, entÃ£o ela Ã© irredutÃ­vel, o que signiï¬ca dizer que a matriz ğ´ nÃ£o Ã© similar,
por uma matriz de permutaÃ§Ã£o, a uma matriz bloco-triangular-superior.

Nesse contexto, Ã© natural entÃ£o, questionarmos: HÃ¡ alguma caracterÃ­stica ou pro-
priedade comum Ã s matrizes redutÃ­veis? E Ã s irredutÃ­veis? Caso exista, Ã© possÃ­vel ilus-
trarmos tais caracterÃ­sticas ou propriedades? Mencionamos que matrizes irredutÃ­veis
sÃ£o fundamentais nos estudos de grupos lineares e representaÃ§Ãµes de grupos [Za93].

Para toda matriz ğ´ â‰¥ 0 de tamanho ğ‘›âˆ’porâˆ’ğ‘› podemos associar um grafo orientado
Î“ (ğ´), onde seus vÃ©rtices sÃ£o os nÃºmeros 1, 2, ..., ğ‘› e para cada par (ğ‘–, ğ‘—) de vÃ©rtices existe
uma aresta orientada de ğ‘– para ğ‘— se, e somente se ğ´ğ‘–ğ‘— > 0. Em um grafo como este, um
caminho orientado deï¬ne uma sequÃªncia de vÃ©rtices conectados por arestas orientadas.
âˆ’1ğ´ğ‘ƒ ) =
A partir desta deï¬niÃ§Ã£o e do efeito do produto ğ‘ƒ
Î“ (ğ´) a menos da ordem dos vÃ©rtices.

âˆ’1ğ´ğ‘ƒ sobre ğ´, temos que Î“ (ğ‘ƒ

Vamos ilustrar alguns exemplos de matrizes ğ´ â‰¥ 0 e seus respectivos grafos asso-
ciados que nos ajudarÃ£o a identiï¬car caracterÃ­sticas ou propriedades comuns e que
sirvam tambÃ©m como conveniente critÃ©rio de redutibilidade e de irredutibilidade.

(1) Matrizes com algumas entradas nulas

ğ´ =

â›

âœâœâœâœâœâœâœâœâœâ

1 2 0
4 5 0
7 8 9

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

,

ğµ =

â›

âœâœâœâœâœâœâœâœâœâ

0 2 0
4 5 6
7 8 0

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

,

Î“ (ğ´) :

1

(cid:47) 2

3(cid:69)

Î“ (ğµ) : 1

(cid:47) 2

3

Veriï¬camos com os exemplos acima, a existÃªncia de laÃ§os nos vÃ©rtices 1, 2 e 3 da
matriz ğ´ e no vÃ©rtice 2 da matriz ğµ, que ilustram uma entrada positiva na diagonal
principal dessas matrizes. Veriï¬camos tambÃ©m facilmente que, em relaÃ§Ã£o Ã  matriz ğ´,
nÃ£o hÃ¡ caminhos orientados dos vÃ©rtices 1 e 2 para o vÃ©rtice 3, o que ilustra um grafo
nÃ£o fortemente conexo.

O resultado seguinte garante que a transposiÃ§Ã£o de matrizes preserva a irredutibi-

lidade.

ProposiÃ§Ã£o 2.2.1. Se ğ´ Ã© uma matriz irredutÃ­vel, entÃ£o ğ´ğ‘‡ tambÃ©m Ã© irredutÃ­vel.

DemonstraÃ§Ã£o: Se ğ´ğ‘‡ fosse redutÃ­vel, entÃ£o, existiria uma matriz de permutaÃ§Ã£o ğ‘„
tal que,

âˆ’1ğ´ğ‘‡ ğ‘„ =

ğ‘„

â¡

â¢â¢â¢â¢â£

ğ‘‹ ğ‘Œ
0 ğ‘

â¤

â¥â¥â¥â¥â¦

,

(cid:23)
(cid:23)
(cid:47)
(cid:7)
(cid:7)
(cid:111)
(cid:111)
(cid:69)
(cid:64)
(cid:64)
(cid:79)
(cid:79)
(cid:47)
(cid:23)
(cid:23)
(cid:111)
(cid:111)
(cid:0)
(cid:0)
(cid:64)
(cid:64)
(cid:79)
(cid:79)
44

CAPÃTULO 2. MATRIZES DE MARKOV

para ğ‘‹ e ğ‘ quadradas. Com isso, e do fato de ğ‘„ ser ortogonal, seguiria que,

âˆ’1ğ´ğ‘‡ ğ‘„ = [ğ‘„

âˆ’1ğ´ğ‘„]ğ‘‡

ğ‘„

e disso terÃ­amos, ğ‘„

âˆ’1ğ´ğ‘„ =

â¡

â¢â¢â¢â¢â£

ğ‘‹ğ‘‡
0ğ‘‡
ğ‘Œ ğ‘‡ ğ‘ğ‘‡

â¤

â¥â¥â¥â¥â¦

.

Considere a matriz de permutaÃ§Ã£o ğ‘ƒ cujas entradas sÃ£o todas nulas salvo aquelas
na diagonal nÃ£o-principal que sÃ£o iguais a 1. Em outros termos, se ğ‘ƒ possui ordem
ğ‘›-por-ğ‘› entÃ£o ğ‘ğ‘–ğ‘— = 1 se ğ‘– + ğ‘— = ğ‘› + 1, ğ‘ğ‘–ğ‘— = 0 sempre que ğ‘– + ğ‘— (cid:44) ğ‘› + 1. Notemos que se ğ´
possui ordem ğ‘›-por-ğ‘› entÃ£o ğ´ğ‘ƒ Ã© obtida de ğ´ escrevendo-se as colunas de ğ´ na ordem
inversa, e ğ‘ƒ ğ´ por sua vez, obtida de ğ´ escrevendo-se as linhas de ğ´ na ordem inversa.
Em particular, ğ‘ƒ

âˆ’1 = ğ‘ƒ . Assim

ğ‘ƒ

âˆ’1(ğ‘„

âˆ’1ğ´ğ‘„)ğ‘ƒ =

â¥â¥â¥â¥â¦
â€² quadradas. Entretanto, isso contradiz a hipÃ³tese de ğ´ ser irre-

â¢â¢â¢â¢â£

â€²

ğ‘Œ
ğ‘‹

â€², ğ‘‹

â¤

â€²

â¡

â€²

ğ‘
0â€²

com submatrizes ğ‘
dutÃ­vel. Portanto, se ğ´ Ã© irredutÃ­vel, entÃ£o ğ´ğ‘‡ tambÃ©m Ã© irredutÃ­vel.

Podemos obter exemplos mais evidentes acerca das matrizes redutÃ­veis e irredutÃ­-

veis atravÃ©s do resultado seguinte.

ProposiÃ§Ã£o 2.2.2. Seja ğ´ uma matriz â‰¥ 0 de tamanho ğ‘›âˆ’porâˆ’ğ‘›. EntÃ£o, as seguintes aï¬r-
maÃ§Ãµes sÃ£o equivalentes:

(1) ğ´ Ã© irredutÃ­vel;

(2) O conjunto {1, 2, ..., ğ‘›} nÃ£o pode ser dividido em dois subconjuntos nÃ£o-vazios ğ¼ e ğ½ com

a propriedade: ğ´ğ‘–ğ‘— = 0 se ğ‘– âˆˆ ğ¼ e ğ‘— âˆˆ ğ½;

(3) O grafo Î“ (ğ´) Ã© fortemente conexo, isto Ã©, para quaisquer dois vÃ©rtices distintos ğ‘– e ğ‘— de

Î“ (ğ´), existe um caminho orientado de ğ‘– para ğ‘— em Î“ (ğ´).

DemonstraÃ§Ã£o: (1) â‡’ (2) : Suponhamos que o conjunto {1, 2, ..., ğ‘›} seja particionado
em dois subconjuntos nÃ£o-vazios ğ¼ e ğ½, tal que, ğ´ğ‘–ğ‘— = 0 sempre que (ğ‘–, ğ‘—) âˆˆ ğ¼ Ã— ğ½. En-
tÃ£o a matriz ğ´ tem em alguma linha (ou coluna), no mÃ­nimo, (ğ‘› âˆ’ 1) entradas nulas
abaixo (ou acima) da sua diagonal principal conforme ğ‘– > ğ‘— (ou ğ‘– < ğ‘—, respectivamente).
Dessa forma, e pela proposiÃ§Ã£o anterior, conseguimos uma matriz de permutaÃ§Ã£o ğ‘ƒ
âˆ’1ğ´ğ‘ƒ Ã© similar, por ğ‘ƒ a uma matriz bloco-triangular-superior, o que
de tal sorte que ğ‘ƒ
contradiz a irredutibilidade de ğ´.

(2) â‡’ (3) Por outro lado, nÃ£o sendo possÃ­vel uma partiÃ§Ã£o do conjunto {1, 2, ..., ğ‘›}
em dois subconjuntos ğ¼ e ğ½ com ğ´ğ‘–ğ‘— = 0, entÃ£o devemos ter ğ´ğ‘–ğ‘— > 0 para cada par (ğ‘–, ğ‘—),
ou ao menos, para os pares consecutivos dois a dois, tomados numa sequÃªncia como
ğ‘–1 < ğ‘–2 < Â· Â· Â· < ğ‘–ğ‘˜ < ğ‘—. Pela deï¬niÃ§Ã£o de grafo, temos associado Ã  matriz ğ´ um grafo Î“ (ğ´)
fortemente conexo.

CAPÃTULO 2. MATRIZES DE MARKOV

45

(3) â‡’ (1) Se Î“ (ğ´) Ã© fortemente conexo entÃ£o todos os seus vÃ©rtices estÃ£o dois a
dois, conectados direta ou indiretamente. Em outros termos, se ocorrer ğ´ğ‘–ğ‘—ğ›¼ = 0, entÃ£o
ocorrerÃ¡ ğ´ğ‘–ğ‘—ğ›½ > 0 e ğ´ğ‘—ğ›½ğ‘—ğ›¼ > 0. Nessas condiÃ§Ãµes, o conjunto {1, 2, ..., ğ‘›} nÃ£o pode ser
particionado em dois subconjuntos nÃ£o vazios ğ¼ e ğ½ com ğ´ğ‘–ğ‘— = 0 sempre que (ğ‘–, ğ‘—) âˆˆ ğ¼ Ã— ğ½,
e consequentemente a matriz ğ´ Ã© irredutÃ­vel.

A partir do exposto atÃ© aqui, podemos elencar algumas caracterÃ­sticas e proprieda-

des referentes Ã s matrizes redutÃ­veis e Ã s irredutÃ­veis por meio de alguns exemplos.

2.2.1 Exemplos de matrizes redutÃ­veis

â€¢ Matriz Identidade. De fato, pois, Ã© suï¬ciente observarmos que todas as entradas
nulas estÃ£o distribuÃ­das acima e abaixo da diagonal principal, e nesses casos, seus
grafos nÃ£o sÃ£o fortemente conexos e o conjunto {1, 2, ..., ğ‘›} pode ser facilmente
dividido em subconjuntos ğ¼ e ğ½ com ğ´ğ‘–ğ‘— = 0 sempre que ğ‘– âˆˆ ğ¼ e ğ‘— âˆˆ ğ½.
â

â›

ğ¼2 =

âœâœâœâœâ

1 0
0 1

âŸâŸâŸâŸâ  ,

â›

âœâœâœâœâœâœâœâœâœâ

1 0 0
0 1 0
0 0 1

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

,

ğ¼3 =

Î“ (ğ¼2) :

1

2

Î“ (ğ¼3) :

1

2

3(cid:69)

O que estende aos demais casos de dimensÃ£o superior a 3.

â€¢ Matrizes Triangulares (inferiores e superiores). Ã‰ um caso anÃ¡logo ao das ma-
trizes identidade, porÃ©m, aqui com entradas nulas inferiores (ou superiores) Ã 
diagonal principal, o que nos possibilita veriï¬car.
â›

â

ğ‘‡ğ‘– =

âœâœâœâœâœâœâœâœâœâ

0
0
ğ›¼1
0
ğ›¼2 ğ›¼3
ğ›¼4 ğ›¼5 ğ›¼6

âŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

ğ‘‡ğ‘  =

â›

âœâœâœâœâœâœâœâœâœâ

ğ›½1 ğ›½2 ğ›½3
0 ğ›½4 ğ›½5
0 ğ›½6
0

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

Î“ (ğ‘‡ğ‘–) : 1

2

3(cid:69)

Î“ (ğ‘‡ğ‘ ) : 1

2

3 (cid:89)

(cid:23)
(cid:23)
(cid:7)
(cid:7)
(cid:23)
(cid:23)
(cid:7)
(cid:7)
(cid:69)
(cid:23)
(cid:23)
(cid:7)
(cid:7)
(cid:111)
(cid:111)
(cid:69)
(cid:64)
(cid:64)
(cid:79)
(cid:79)
(cid:7)
(cid:7)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:23)
(cid:23)
(cid:0)
(cid:0)
(cid:89)
46

CAPÃTULO 2. MATRIZES DE MARKOV

Em ambos os casos, veriï¬camos que os grafos nÃ£o sÃ£o fortemente conexos, pois,
em Î“ (ğ‘‡ğ‘–) nÃ£o hÃ¡ caminho orientado do vÃ©rtice 1 para o vÃ©rtice 2, por exemplo.
Assim como ocorre em Î“ (ğ‘‡ğ‘ ) que nÃ£o hÃ¡ caminho orientado do vÃ©rtice 2 para o
vÃ©rtice 1. Nestes casos, o mesmo se aplica Ã s matrizes de ordens superiores.

â€¢ Matrizes com ï¬las nulas (linha ou coluna). Notamos com facilidade que um
vetor-linha (ou vetor-coluna) nulo em uma matriz ğ´ğ‘›Ã—ğ‘› nos dÃ¡ uma divisÃ£o do
conjunto {1, 2, ..., ğ‘›} em dois subconjunto ğ¼ e ğ½ com a propriedade: ğ´ğ‘–ğ‘— = 0 se ğ‘– âˆˆ ğ¼
e ğ‘— âˆˆ ğ½, pois, ï¬xando uma linha ğ‘˜ para 1 â‰¤ ğ‘˜ â‰¤ ğ‘›, isso nos dÃ¡ ğ´ğ‘˜ğ‘— = 0 com ğ‘—
percorrendo os valores {1, 2, ..., ğ‘›}, e entÃ£o, tomamos os subconjuntos ğ¼ = {ğ‘˜} e
ğ½ = {1, 2, ..., ğ‘›} âˆ– {ğ‘˜}. Ilustrando

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

ğ´ =

ğ´11
ğ´21
...

ğ´12
ğ´22
...

Â· Â· Â·
Â· Â· Â·

Â· Â· Â·

ğ´1ğ‘›
ğ´2ğ‘›
...

ğ´ğ‘˜1 = 0 ğ´ğ‘˜2 = 0 Â· Â· Â· ğ´ğ‘˜ğ‘› = 0

...
ğ´ğ‘›1

...
ğ´ğ‘›2

Â· Â· Â·
Â· Â· Â·

...
ğ´ğ‘›ğ‘›

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

ConcluÃ­mos de forma anÃ¡loga tomando nula uma coluna arbitrÃ¡ria de ğ´.

2.2.2 Exemplos de matrizes irredutÃ­veis

â€¢ Matrizes Positivas. Com efeito, pois, dada uma matriz ğ´ğ‘›Ã—ğ‘› positiva, a matriz
âˆ’1ğ´ğ‘ƒ nÃ£o serÃ¡ bloco-triangular-superior, visto que nÃ£o possui entradas nulas.
ğ‘ƒ
Temos ainda, associado a essa matriz um grafo fortemente conexo. Vejamos um
caso que ilustra essa caracterÃ­stica.

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

ğ´ =

ğ´11 ğ´12 ğ´13 ğ´14
ğ´21 ğ´22 ğ´23 ğ´24
ğ´31 ğ´32 ğ´33 ğ´34
ğ´41 ğ´42 ğ´43 ğ´44

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

,

Î“ (ğ´) : 1

3 (cid:89)

2

4(cid:69)

Analogamente estende-se para matrizes de ordem menor e maior do que 4.

â€¢ Matrizes com diagonal principal nula e demais entradas positivas. Dada uma
matriz ğ´ com essa caracterÃ­stica, Ã© imediata a sua irredutibilidade uma vez que
ğ´ğ‘–ğ‘— = 0 se, e somente se ğ‘– = ğ‘—. Nessas condiÃ§Ãµes, ilustramos.

(cid:7)
(cid:7)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:30)
(cid:30)
(cid:23)
(cid:23)
(cid:111)
(cid:111)
(cid:15)
(cid:15)
(cid:0)
(cid:0)
(cid:89)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:64)
(cid:64)
(cid:69)
(cid:79)
(cid:79)
(cid:111)
(cid:111)
(cid:94)
(cid:94)
CAPÃTULO 2. MATRIZES DE MARKOV

47

ğ´ =

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

0 ğ´12 ğ´13 ğ´14
0 ğ´23 ğ´24
ğ´21
0 ğ´34
ğ´31 ğ´32
0
ğ´41 ğ´42 ğ´43

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

Î“ (ğ´) : 1

3

2

4

â€¢ Matrizes com diagonal nÃ£o-principal nula e demais entradas positivas. Dada
uma matriz ğ´ğ‘›Ã—ğ‘› com essa caracterÃ­stica, basta veriï¬carmos que ğ´ğ‘–ğ‘— = 0 se, e
somente se ğ‘– + ğ‘— = (ğ‘› + 1). Esse fato nos diz que, para cada ğ‘– tem um Ãºnico ğ‘— que
satisfaz essa igualdade, e portanto, o conjunto {1, 2, ..., ğ‘›} nÃ£o pode ser dividido
em dois subconjuntos ğ¼ e ğ½. Vale tambÃ©m salientar, que o grafo associado a uma
matriz nessas condiÃ§Ãµes, Ã© fortemente conexo. Vejamos um exemplo.

ğ´ =

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

0
ğ´11 ğ´12 ğ´13
0 ğ´24
ğ´21 ğ´22
0 ğ´33 ğ´34
ğ´31
0 ğ´42 ğ´43 ğ´44

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

Î“ (ğ´) : 1

3(cid:69)

2

(cid:54) 4 (cid:89)

2.3 Perron-Frobenius

O teorema de Perron-Frobenius emerge em diversos ramos da matemÃ¡tica, por
exemplo nas teorias de automorï¬smos de grupos livres [BH12], de sistemas dinÃ¢micos
e ergÃ³dicos [PY98], Sec. 3.2; e de categorias tensoriais [EGNO15], Sec. 3.2. Nesta seÃ§Ã£o,
utilizaremos os conceitos de matriz â‰¥ 0, positiva, de irredutibilidade e propriedades
dessas matrizes, que tratamos na seÃ§Ã£o anterior, para a demonstraÃ§Ã£o desse teorema
que Ã© o principal resultado desse capÃ­tulo.

Deï¬niÃ§Ã£o 2.3.1. Seja ğ´ uma matriz irredutÃ­vel â‰¥ 0, nÃ£o-nula. O autovalor denotado por
ğ‘ƒ ğ¹(ğ´) Ã© chamado de Autovalor Perron-Frobenius de ğ´. Se ğ´ Ã© irredutÃ­vel, nula de or-
dem 1, deï¬nimos ğ‘ƒ ğ¹(ğ´) = 0. O vetor-coluna positivo âƒ—ğ‘£ com ğ´âƒ—ğ‘£ = ğ‘ƒ ğ¹(ğ´)âƒ—ğ‘£ Ã© chamado de
Autovetor Direito Perron-Frobenius de ğ´.

Teorema 2.3.1. (Perron-Frobenius) Seja ğ´ uma matriz real â‰¥ 0 nÃ£o-nula, irredutÃ­vel.
EntÃ£o,

(1) ğ´ tem um Ãºnico autovetor > 0, real, a menos da multiplicaÃ§Ã£o por um nÃºmero real

positivo; o autovalor ğ‘ƒ ğ¹(ğ´) associado Ã© positivo;

(cid:40)
(cid:40)
(cid:21)
(cid:21)
(cid:30)
(cid:30)
(cid:118)
(cid:118)
(cid:9)
(cid:9)
(cid:0)
(cid:0)
(cid:72)
(cid:72)
(cid:54)
(cid:54)
(cid:64)
(cid:64)
(cid:86)
(cid:86)
(cid:104)
(cid:104)
(cid:94)
(cid:94)
(cid:23)
(cid:23)
(cid:40)
(cid:40)
(cid:21)
(cid:21)
(cid:7)
(cid:7)
(cid:118)
(cid:118)
(cid:9)
(cid:9)
(cid:69)
(cid:72)
(cid:72)
(cid:54)
(cid:89)
(cid:86)
(cid:86)
(cid:104)
(cid:104)
48

CAPÃTULO 2. MATRIZES DE MARKOV

(2) ğ‘ƒ ğ¹(ğ´) = ğ‘ƒ ğ¹(ğ´ğ‘‡ );

(3) Para cada autovalor ğœ† de ğ´, vale | ğœ† |â‰¤ ğ‘ƒ ğ¹(ğ´);

(4) Para o autovalor ğ‘ƒ ğ¹(ğ´) corresponde um Ãºnico autovetor real Ã  direita, a menos da

multiplicaÃ§Ã£o por um nÃºmero real;

AlÃ©m disso, sendo âƒ—ğœ” um vetor-coluna real, nÃ£o-nulo , nÃ£o-negativo, e ğ›¼ um nÃºmero real
nÃ£o-negativo, segue que

(5) Se ğ´ âƒ—ğœ” â‰¤ ğ‘ƒ ğ¹(ğ´) âƒ—ğœ”, entÃ£o ğ´ âƒ—ğœ” = ğ‘ƒ ğ¹(ğ´) âƒ—ğœ”;

(6) Se ğ´ âƒ—ğœ” â‰¤ ğ›¼ âƒ—ğœ”, entÃ£o ğ‘ƒ ğ¹(ğ´) â‰¤ ğ›¼.

Sugerimos a quem nÃ£o queira ler a demonstraÃ§Ã£o dele que prove-o, Ã  guisa de exer-

cÃ­cio, no caso 2-por-2. Comecemos por demonstrar o lema a seguir.

Lema 2.3.2. Seja ğ´ uma matriz irredutÃ­vel â‰¥ 0, nÃ£o-nula de tamanho ğ‘›-por-ğ‘›. EntÃ£o a
matriz ğµ =

ğ‘–=0 ğ´ğ‘– Ã© positiva. Em particular, se ğ‘¥ âˆˆ Rğ‘›, com 0 (cid:44) ğ‘¥ â‰¥ 0, entÃ£o ğµğ‘¥ > 0.

âˆ‘ï¸€ğ‘›âˆ’1

DemonstraÃ§Ã£o: Note que cada entrada da diagonal principal de ğµ Ã© positiva, isto Ã©,
ğµğ‘–ğ‘– > 0. Note ainda que a matriz ğµ pode ser expressa como

ğµ = ğ¼ğ‘› +

ğ‘›âˆ’1âˆ‘ï¸

ğ‘–=1

ğ´ğ‘–.

Devemos provar que ğµğ‘–ğ‘— > 0 para ğ‘– (cid:44) ğ‘—. Como por hipÃ³tese, ğ´ Ã© irredutÃ­vel, nÃ£o-
negativa e nÃ£o-nula, o grafo Î“ (ğ´) de ğ´ Ã© fortemente conexo, isto Ã©, conforme provamos
na ProposiÃ§Ã£o 2.2.2, para quaisquer dois vÃ©rtices distintos ğ‘–, ğ‘— existe um caminho ori-
entado de ğ‘– para ğ‘— formado por arestas orientadas entre dois vÃ©rtices consecutivos. Se
todos esses vÃ©rtices forem distintos, segue ainda da deï¬niÃ§Ã£o do grafo Î“ (ğ´) de ğ´ que
ğ´ğ‘–0ğ‘–1ğ´ğ‘–1ğ‘–2...ğ´ğ‘–ğ‘˜âˆ’1ğ‘–ğ‘˜ > 0. Portanto, (ğ´ğ‘˜)ğ‘–ğ‘— > 0 e consequentemente, ğµğ‘–ğ‘— > 0 como querÃ­amos
provar.

Deï¬niÃ§Ã£o 2.3.2. Sejam ğ‘ e ğ‘€ matrizes reais. Escrevemos ğ‘ â‰¤ ğ‘€ para signiï¬car que ğ‘
â‰¤ ğ‘€ğ‘–ğ‘— para todo par de Ã­ndices (ğ‘–, ğ‘—). NÃ³s tambÃ©m
e ğ‘€ tÃªm o mesmo tamanho e que ğ‘ğ‘–ğ‘—
dizemos que a matriz ğ‘ Ã© dominada pela matriz ğ‘€ se ğ‘ â‰¤ ğ´, onde ğ´ Ã© uma submatriz da
matriz ğ‘€.

âˆ‘ï¸€ğ‘›
Deï¬niÃ§Ã£o 2.3.3. Para âƒ—ğ‘¥ = (ğ‘¥1, . . . , ğ‘¥ğ‘›) âˆˆ Rğ‘› deï¬nimos | âƒ—ğ‘¥ |=
norma da soma. Deï¬nimos tambÃ©m ğ›¥ = {ï¸€âƒ—ğ‘¥ âˆˆ Rğ‘› | âƒ—ğ‘¥ â‰¥ 0; | âƒ—ğ‘¥ |= 1}ï¸€.

ğ‘–=1

| ğ‘¥ğ‘–

| que Ã© denominada

Temos uma representaÃ§Ã£o geomÃ©trica desse conjunto quando associamos seus ele-
mentos a pontos do plano ou do espaÃ§o, como um segmento de extremos (1, 0) e (0, 1)
em R2, ou uma regiÃ£o triangular com vÃ©rtices (1, 0, 0), (0, 1, 0) e (0, 0, 1) em R3.

CAPÃTULO 2. MATRIZES DE MARKOV

49

Figura 1 â€“ RepresentaÃ§Ã£o geomÃ©trica de âˆ† em R2

Figura 2 â€“ RepresentaÃ§Ã£o geomÃ©trica de âˆ† em R3

Deste ponto em diante, apresentamos uma demonstraÃ§Ã£o do Teorema de Perron-
Frobenius que segue uma ideia sugerida pelo matemÃ¡tico alemÃ£o Helmut Wielandt,
em Unzerlegbare nicht negative Matrizen, cuja traduÃ§Ã£o livre Ã© "Matrizes nÃ£o-negativas
irredutÃ­veis".

DemonstraÃ§Ã£o Perron-Frobenius: (1) e (2) Primeiro provamos que existe um vetor-
coluna âƒ—ğ‘¦ > 0 e um nÃºmero ğœ† > 0, tal que, ğ´âƒ—ğ‘¦ = ğœ†âƒ—ğ‘¦. Seja ğ´ uma matriz quadrada de
ordem ğ‘›. Seja âƒ—ğ‘¢ um vetor-linha de tamanho ğ‘› com cada entrada igual a 1. Conside-
rando um vetor âƒ—ğ‘¥ âˆˆ ğ›¥, aï¬rmamos que sup{ğœŒ | ğ‘’ğ‘¥ğ‘–ğ‘ ğ‘¡ğ‘’ âƒ—ğ‘¥ âˆˆ ğ›¥, ğ‘¡ğ‘ğ‘™ ğ‘ğ‘¢ğ‘’, ğ´âƒ—ğ‘¥ â‰¥ ğœŒâƒ—ğ‘¥} Ã© um
nÃºmero real. De fato, pois, se ğ´âƒ—ğ‘¥ â‰¥ ğœŒâƒ—ğ‘¥ para algum âƒ—ğ‘¥ âˆˆ ğ›¥, entÃ£o, claramente teremos

ğœŒ = ğœŒâƒ—ğ‘¢âƒ—ğ‘¥ â‰¤ âƒ—ğ‘¢ğ´âƒ—ğ‘¥ â‰¤ âƒ—ğ‘¢ğ´(âƒ—ğ‘¢)ğ‘‡ .

Denotando o supremo acima por ğœ†, segue do fato de ğ›¥ ser compacto que existe
um vetor âƒ—ğ‘¦ âˆˆ ğ›¥, tal que, ğ´âƒ—ğ‘¦ â‰¥ ğœ†âƒ—ğ‘¦. Suponhamos por contradiÃ§Ã£o, que ğ´âƒ—ğ‘¦ (cid:44) ğœ†âƒ—ğ‘¦. EntÃ£o,

50

CAPÃTULO 2. MATRIZES DE MARKOV

â€², e portanto, ğœ† = ğœ†
âƒ—ğ‘¦â€²
| âƒ—ğ‘¦â€² |

âˆ‘ï¸€ğ‘›âˆ’1

ğµğ´âƒ—ğ‘¦ > ğœ†ğµâƒ—ğ‘¦, onde ğµ = ğ¼ğ‘› +

ğ‘–=1 ğ´ğ‘– Ã© positiva conforme provamos no Lema 2.3.2. Assim,
âˆˆ ğ›¥, entretanto,

sendo fato que ğ´ğµ = ğµğ´, conseguimos obter ğ´âƒ—ğ‘¥ > ğœ†âƒ—ğ‘¥ para âƒ—ğ‘¥ =

ğµâƒ—ğ‘¦
| ğµâƒ—ğ‘¦ |
ğ‘–=0 ğœ†ğ‘– âƒ—ğ‘¦, e sendo ğµâƒ—ğ‘¦ > 0
isso contradiz a maximalidade de ğœ†. Consequentemente, ğµâƒ—ğ‘¦ =
obtemos tambÃ©m do lema anterior que âƒ—ğ‘¦ > 0. Novamente, de ğ´âƒ—ğ‘¦ = ğœ†âƒ—ğ‘¦ segue que ğœ† > 0.
Por simetria, existe um vetor-linha âƒ—ğ‘§ > 0 e um nÃºmero real ğœ‡ > 0, tais que, âƒ—ğ‘§ğ´ = ğœ‡âƒ—ğ‘§.
EntÃ£o, ğœ‡âƒ—ğ‘§âƒ—ğ‘¦ = âƒ—ğ‘§ğ´âƒ—ğ‘¦ = ğœ†âƒ—ğ‘§âƒ—ğ‘¦ e, âƒ—ğ‘§âƒ—ğ‘¦ > 0, donde segue que ğœ‡ = ğœ†.
Agora, seja âƒ—ğ‘¦â€² um autovetor positivo arbitrÃ¡rio de ğ´, e seja ğœ†
Como anteriormente, obteremos ğœ‡ = ğœ†

â€² o autovalor associado.
â€². Suponhamos que âƒ—ğ‘¦â€² nÃ£o

âˆ‘ï¸€ğ‘›âˆ’1

e

seja um mÃºltiplo escalar de âƒ—ğ‘¦. EntÃ£o os pontos

âƒ—ğ‘¦
| âƒ—ğ‘¦ | pertencem a ğ›¥, sÃ£o distintos
e a reta que os contÃ©m intercepta a borda de ğ›¥ em algum ponto âƒ—ğ‘£. Como âƒ—ğ‘£ Ã© autovetor
de ğ´, e portanto, de ğµ, concluÃ­mos que ğµâƒ—ğ‘£ Ã© um mÃºltiplo escalar de âƒ—ğ‘£. Assim, alguma
coordenada do vetor âƒ—ğµğ‘£ Ã© nula, o que contradiz o Lema 2.3.2. Novamente, obtemos
entÃ£o que âƒ—ğ‘¦â€² Ã© um mÃºltiplo escalar e assim, ï¬ca provado (1). Da identidade acima
ğœ‡ = ğœ†, obtemos a prova da aï¬rmaÃ§Ã£o (2). Em particular, como ğ´ e ğ´ğ‘‡ tÃªm o mesmo
polinÃ´mio caracterÃ­stico, possuem os mesmos autovalores.
(3) Seja ğ´âƒ—ğ‘¢ = ğœ âƒ—ğ‘¢ com ğœ âˆˆ C e âƒ—ğ‘¢ âˆˆ Cğ‘›, para âƒ—ğ‘¢ (cid:44) 0. Deï¬na âƒ—ğ‘¢â€² = (| ğ‘¢1
|)ğ‘‡ . EntÃ£o
ğ´ âƒ—ğ‘¢â€² â‰¥| ğœ | âƒ—ğ‘¢â€². Seja âƒ—ğ‘ > 0 o vetor-linha, tal que, âƒ—ğ‘ğ´ = ğœ†âƒ—ğ‘ onde ğœ† = ğ‘ƒ ğ¹(ğ´). EntÃ£o ğœ†âƒ—ğ‘ âƒ—ğ‘¢â€² =
âƒ—ğ‘ğ´ âƒ—ğ‘¢â€² â‰¥| ğœ | âƒ—ğ‘ âƒ—ğ‘¢â€² e âƒ—ğ‘ âƒ—ğ‘¢â€² > 0, de modo que ğœ† â‰¥| ğœ |.
(4) Seja âƒ—ğ‘ um autovetor positivo e âƒ—ğ‘ um autovetor arbitrÃ¡rio associado a ğ‘ƒ ğ¹(ğ´). EntÃ£o
para um nÃºmero suï¬cientemente grande ğ‘Ÿ > 0 o vetor âƒ—ğ‘ + ğ‘Ÿâƒ—ğ‘ tambÃ©m Ã© um autovetor
positivo associado ao autovalor ğ‘ƒ ğ¹(ğ´). Pela aï¬rmaÃ§Ã£o (1) este vetor, e portanto o vetor
âƒ—ğ‘ Ã© um mÃºltiplo escalar do vetor âƒ—ğ‘.
(5) Suponha que ğ´ âƒ—ğœ” â‰¤ ğœ† âƒ—ğœ” e ğ´ âƒ—ğœ” (cid:44) ğœ† âƒ—ğœ”, onde ğœ† = ğ‘ƒ ğ¹(ğ´). Seja âƒ—ğ‘§ > 0 um vetor-linha tal
que, âƒ—ğ‘§ğ´ = ğœ†âƒ—ğ‘§. EntÃ£o ğœ†âƒ—ğ‘§ âƒ—ğœ” = âƒ—ğ‘§ğ´ âƒ—ğœ” < ğœ†âƒ—ğ‘§ âƒ—ğœ” Ã© uma contradiÃ§Ã£o. Portanto, se ğ´ âƒ—ğœ” â‰¤ ğœ† âƒ—ğœ” entÃ£o
ğ´ âƒ—ğœ” = ğœ† âƒ—ğœ”.
(6) Suponhamos que ğ´ âƒ—ğœ” â‰¤ ğ›¼ âƒ—ğœ” e ğœ† > ğ›¼, onde ğœ† = ğ‘ƒ ğ¹(ğ´). Tomando o vetor-linha âƒ—ğ‘§ > 0
tal que âƒ—ğ‘§ğ´ = ğœ†âƒ—ğ‘§, segue que ğœ†âƒ—ğ‘§ âƒ—ğœ” = âƒ—ğ‘§ğ´ âƒ—ğœ” â‰¤ ğ›¼âƒ—ğ‘§ âƒ—ğœ”, o que Ã© um absurdo. Logo, se ğ´ âƒ—ğœ” â‰¤ ğ›¼ âƒ—ğœ”,
entÃ£o ğ‘ƒ ğ¹(ğ´) â‰¤ ğ›¼, como querÃ­amos demonstrar.

|, ..., | ğ‘¢ğ‘›

Teorema 2.3.3. Sejam ğ‘€ e ğ‘€1 matrizes quadradas reais, â‰¥ 0, e seja ğ‘€1 irredutÃ­vel e do-
minada por ğ‘€. Suponha que ğ‘€ âƒ—ğ‘¤ â‰¤ ğœ† âƒ—ğ‘¤ para algum nÃºmero ğœ† > 0 e vetor âƒ—ğ‘¤ > 0. EntÃ£o ou
ğ‘ƒ ğ¹(ğ‘€1) < ğœ† ou ğ‘ƒ ğ¹(ğ‘€1) = ğœ† e, a menos da conjugaÃ§Ã£o por uma matriz de permutaÃ§Ã£o,

ğ‘€ =

â›

âœâœâœâœâ

ğ‘€1 0
ğ¶ ğ·

â

âŸâŸâŸâŸâ  .

DemonstraÃ§Ã£o: Sem perda de generalidade, vamos supor que

ğ‘€ =

â›

âœâœâœâœâ

ğ´ ğµ
ğ¶ ğ·

â

âŸâŸâŸâŸâ  ,

CAPÃTULO 2. MATRIZES DE MARKOV

51

e ğ‘€1

â‰¤ ğ´. Pondo âƒ—ğ‘¤ =

triz ğ´. EntÃ£o,

DaÃ­,

â›

âœâœâœâœâ

âƒ—ğ‘¢
âƒ—ğ‘£

â

âŸâŸâŸâŸâ 

, onde o tamanho de âƒ—ğ‘¢ corresponde ao tamanho da subma-

â›

âœâœâœâœâ

ğœ†

â

âŸâŸâŸâŸâ 

âƒ—ğ‘¢
âƒ—ğ‘£

â‰¥ ğ‘€

â›

âœâœâœâœâ

âƒ—ğ‘¢
âƒ—ğ‘£

â

âŸâŸâŸâŸâ 

â›

âœâœâœâœâ

=

ğ´âƒ—ğ‘¢ + ğµâƒ—ğ‘£
ğ¶ âƒ—ğ‘¢ + ğ·âƒ—ğ‘£

â

âŸâŸâŸâŸâ  ,

ğ‘€1âƒ—ğ‘¢ â‰¤ ğ´âƒ—ğ‘¢ â‰¤ ğ´âƒ—ğ‘¢ + ğµâƒ—ğ‘£ â‰¤ ğœ†âƒ—ğ‘¢.

Como ğ‘€1âƒ—ğ‘¢ â‰¤ ğœ†âƒ—ğ‘¢, entÃ£o segue da aï¬rmaÃ§Ã£o (6) do Teorema 2.3.1 que ğ‘ƒ ğ¹(ğ‘€1) â‰¤ ğœ†. Caso
ğ‘ƒ ğ¹(ğ‘€1) = ğœ†, entÃ£o pela aï¬rmaÃ§Ã£o (5), ğ‘€1âƒ—ğ‘¢ = ğœ†âƒ—ğ‘¢. Portanto, ğµâƒ—ğ‘£ = 0. Sendo âƒ—ğ‘£ > 0, como
ğµ â‰¥ 0 entÃ£o ğµ = 0.

CorolÃ¡rio 2.3.4. Sejam ğ‘€ e ğ‘€1 matrizes quadradas reais, irredutÃ­veis, â‰¥ 0, e sendo ğ‘€1
dominada por ğ‘€ e ğ‘€ (cid:44) ğ‘€1. EntÃ£o ğ‘ƒ ğ¹(ğ‘€1) < ğ‘ƒ ğ¹(ğ‘€).

DemonstraÃ§Ã£o: Pelo Teorema 2.3.3 anterior, segue diretamente que ğ‘€1âƒ—ğ‘¢ â‰¤ ğœ†âƒ—ğ‘¢. Supo-
â›
âœâœâœâœâ

â
, e isso contraria a hipÃ³tese de ğ‘€

nha que ğ‘ƒ ğ¹(ğ‘€1) = ğœ†. EntÃ£o terÃ­amos ğ‘€ =

âŸâŸâŸâŸâ 

ğ´ 0
ğ¶ ğ·

ser irredutÃ­vel. Portanto, ğ‘ƒ ğ¹(ğ‘€1) < ğ‘ƒ ğ¹(ğ‘€) como querÃ­amos mostrar.

Teorema 2.3.5. Seja ğ´ uma matriz â‰¥ 0, nÃ£o-nula, irredutÃ­vel, com entradas inteiras, e
autovalor Perron-Frobenius ğœ†. EntÃ£o, sÃ£o vÃ¡lidos

(a) ğœ† â‰¥ 1;

(b) Se ğœ† = 1, entÃ£o ğ´ Ã© uma matriz de permutaÃ§Ã£o;

(c) ğ´ğ‘–ğ‘—

â‰¤ ğœ†ğ‘› para todo ğ‘–, ğ‘— em {1, . . . , ğ‘›} .

DemonstraÃ§Ã£o: Pelo Teorema de Perron-Frobenius, existe um vetor-coluna âƒ—ğ‘£ > 0 tal
que, ğ´âƒ—ğ‘£ = ğœ†âƒ—ğ‘£. Seja âƒ—ğ‘¢ o vetor-linha de tamanho ğ‘› e cada entrada igual a 1. Assim, para
provarmos (a) e (b) calculamos diretamente

âƒ—ğ‘¢ğ´âƒ—ğ‘£ = ğœ†âƒ—ğ‘¢âƒ—ğ‘£ =

â›
ğ‘›âˆ‘ï¸
âœâœâœâœâœâ

ğ‘–=1

â

âŸâŸâŸâŸâŸâ 

ğ´ğ‘–1

ğ‘£1 + Â· Â· Â· +

â›
ğ‘›âˆ‘ï¸
âœâœâœâœâœâ

ğ‘–=1

â

âŸâŸâŸâŸâŸâ 

ğ´ğ‘–ğ‘›

ğ‘£ğ‘› = ğœ†ğ‘£1 + Â· Â· Â· + ğœ†ğ‘£ğ‘›,

âˆ‘ï¸€ğ‘›

onde notamos que para cada ğ‘— a soma
ğ‘–=1 ğ´ğ‘–ğ‘— Ã© pelo menos 1, pois, a matriz ğ´ Ã© nÃ£o-
negativa, nÃ£o-nula, irredutÃ­vel e tem entradas inteiras. Portanto, ğœ† â‰¥ 1. Se ğœ† = 1, entÃ£o
cada uma dessas somas serÃ¡ igual a 1. Dessa forma, cada coluna da ğ´ Ã© constituÃ­da
inteiramente de zeros, com exceÃ§Ã£o de uma Ãºnica entrada igual a 1. Com isso, ğ´ nÃ£o
contÃ©m linha nula e Ã© uma matriz de permutaÃ§Ã£o.

Para (c), ï¬xamos ğ‘–, ğ‘— âˆˆ {1, . . . , ğ‘›}. Na demonstraÃ§Ã£o do Lema 2.3.2 mostramos que
existe um nÃºmero natural ğ‘˜ tal que (ğ´ğ‘˜)ğ‘–ğ‘— > 0; alÃ©m disso, 0 â‰¤ ğ‘˜ â‰¤ ğ‘› âˆ’ 1. Visto que ğ´
tem suas entradas inteiras, segue que (ğ´ğ‘˜)ğ‘–ğ‘—

â‰¥ 1. De ğ´ğ‘˜âƒ—ğ‘£ = ğœ†ğ‘˜âƒ—ğ‘£ temos

52

CAPÃTULO 2. MATRIZES DE MARKOV

Ademais, de ğ´âƒ—ğ‘£ = ğœ†âƒ—ğ‘£ deduzimos que

ğ‘£ğ‘—

â‰¤ (ğ´ğ‘˜)ğ‘–ğ‘—ğ‘£ğ‘—

â‰¤ ğœ†ğ‘˜ğ‘£ğ‘—.

ğ´ğ‘–ğ‘—ğ‘£ğ‘—

â‰¤ ğœ†ğ‘£ğ‘—

â‰¤ ğœ†ğ‘˜+1ğ‘£ğ‘—

â‰¤ ğœ†ğ‘›ğ‘£ğ‘—,

o que prova (c).

CorolÃ¡rio 2.3.6. Seja ğ‘Ÿ um nÃºmero real e ğ‘› um nÃºmero natural. EntÃ£o existe apenas um
nÃºmero ï¬nito de matrizes quadradas â‰¥ 0, irredutÃ­veis, com entradas inteiras e autovalores
Perron-Frobenius nÃ£o excedendo ğ‘Ÿ.

DemonstraÃ§Ã£o: De fato, pelo teorema anterior como cada entrada de uma tal matriz
ğ´ Ã© inteira, ela estÃ¡ no intervalo [0, ğ‘Ÿğ‘›].

3 AplicaÃ§Ãµes

Neste capÃ­tulo, apresentaremos 4 exemplos com problemas onde podemos veriï¬-
car aplicaÃ§Ãµes das matrizes de Markov e do Teorema de Perron-Frobenius. SÃ£o eles:
Problema 4 da 23Âº OBM; Problema dos ğ‘› mentirosos; O PageRank e O Modelo de
DifusÃ£o de Ehrenfest.

3.1 XXIII OBM - OlimpÃ­ada Brasileira de MatemÃ¡tica

Um ratinho ocupa inicialmente a gaiola ğ´ e Ã© treinado para mudar de gaiola atravessando
um tÃºnel sempre que soa um alarme. Cada vez que soa o alarme o ratinho escolhe qualquer
um dos tÃºneis incidentes a sua gaiola com igual probabilidade e sem ser afetado por escolhas
anteriores. Qual Ã© a probabilidade de que apÃ³s o alarme soar 23 vezes, o ratinho ocupe a
gaiola ğµ?

Figura 3 â€“ Um esquema das gaiolas

Este Ã© o problema 4 proposto na 23Âª OlimpÃ­ada Brasileira de MatemÃ¡tica, [Eur02].

ResoluÃ§Ã£o: Diferentemente do que Ã© apresentado nas resoluÃ§Ãµes jÃ¡ publicadas ante-
riormente, utilizaremos a matemÃ¡tica subjacente a partir do instrumental referente Ã 
Cadeia de Markov [Dia09] e diagonalizaÃ§Ã£o de matrizes. Para tanto, comeÃ§amos por
considerar o conjunto ğ¾ = {1, 2, 3, 4, 5, 6}, que representa os estados de transiÃ§Ãµes or-
denadas das gaiolas de A atÃ© F, respectivamente. De acordo com as informaÃ§Ãµes do
problema, â€œcada vez que o alarme soa o ratinho escolhe qualquer um dos tÃºneis inci-
dentes a sua gaiola com igual probabilidade e sem ser afetado por escolhas anterioresâ€,
isso entÃ£o nos diz que as transiÃ§Ãµes satisfazem a uma Cadeia de Markov. A partir das
disposiÃ§Ãµes das gaiolas, veriï¬camos as probabilidades condicionais ğ‘ƒ (ğ‘‹ğ‘›+1 = ğ‘—|ğ‘‹ğ‘› = ğ‘–)
associadas Ã s transiÃ§Ãµes, e disso, os valores ğ‘ = 1
2 quando estiver saindo das gaiolas A,
C, F e D, e, ğ‘ = 1
Com isso, podemos construir a seguinte matriz ğ‘€ de transiÃ§Ã£o:

3 quando estiver saindo das gaiolas B e E.

54

CAPÃTULO 3. APLICAÃ‡Ã•ES

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

ğ‘€ =

2 0 1

0 1
3 0 1
1
0 1
2 0 0 0 1
1
3 0 1
0 1
0 0 1

2 0 0
3 0 1
3 0
2 0 0 0 1
2
2 0
3 0 1
3
2 0

2 0 1

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

)ï¸
(ï¸
1 0 0 0 0 0

onde ğ‘€ğ‘‡ Ã© uma Matriz de Markov. Considerando âƒ—ğ‘€0 =
o vetor-
linha estocÃ¡stico inicial de distribuiÃ§Ã£o de probabilidade (posto que o ratinho estÃ¡ ini-
(23)
cialmente na gaiola A(cid:79)) precisamos calcular ğ‘€
12 , isto Ã©, a probabilidade do ratinho
ocupar a gaiola B apÃ³s o alarme soar 23 vezes, e que por sua vez, Ã© o mesmo que a
segunda coordenada do vetor âƒ—ğ‘€0ğ‘€23. Para isso, devemos calcular a potÃªncia ğ‘€23 o
que pode parecer intimidador, computacionalmente. Entretanto, se pudermos lanÃ§ar
âˆ’1, onde a matriz ğ‘† Ã© invertÃ­vel
mÃ£o da decomposiÃ§Ã£o da matriz ğ‘€ na forma ğ‘€ = ğ‘†ğ·ğ‘†
e a matriz ğ· Ã© diagonal, obteremos entÃ£o, com facilidade

ğ‘€23 = (ğ‘†ğ·ğ‘†

âˆ’1)23 = ğ‘†ğ·23ğ‘†

âˆ’1 = ğ‘†.

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

ğ·

(23)
11
0

0

0

0

0

ğ·

0
(23)
22
0

0

0

0

ğ·

0

0
(23)
33
0

0

0

0

0

ğ·

0
(23)
44
0

0

0

0

0

ğ·

0
(23)
55
0

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

0

0

0

0

0
(23)
66

ğ·

âˆ’1

.ğ‘†

uma vez que, para elevar uma matriz diagonal a uma potÃªncia ğ‘› basta elevar cada
entrada de sua diagonal principal a ğ‘›. Assim, a matriz ğ‘€ serÃ¡ diagonalizÃ¡vel e, as
entradas da diagonal principal da matriz ğ· serÃ£o os autovalores de ğ‘€. Uma pergunta
surge naturalmente: A matriz ğ‘€ Ã© diagonalizÃ¡vel? De acordo com o que mostramos
em SeÃ§Ã£o 1.3 a resposta Ã© sim, pois, ğ‘€ tem seis autovalores distintos. Considerando os
vetores-linha de ğ‘€, notamos que para cada um deles a soma de suas entradas Ã© igual 1.
Esse fato, de imediato nos dÃ¡ que ğœ†1 = 1 Ã© um autovalor de ğ‘€ que tem como autovetor

direito associado âƒ—ğ‘£1 =

. Do fato do traÃ§o da matriz ğ‘€ ser nulo, isso nos garante

que, ou âˆ’1 Ã© um dos outros cinco autovalores, ou, Ã© a soma de ao menos, dois outros.
Buscando os demais autovalores encontramos o polinÃ´mio caracterÃ­stico

â›
â
1
1
1
1
1
1

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

CAPÃTULO 3. APLICAÃ‡Ã•ES

55

ğ‘‘ğ‘’ğ‘¡(ğœ†ğ¼6

âˆ’ ğ‘€) =

âƒ’âƒ’âƒ’âƒ’âƒ’âƒ’âƒ’âƒ’âƒ’âƒ’âƒ’âƒ’âƒ’âƒ’âƒ’âƒ’âƒ’âƒ’âƒ’âƒ’

2 0 1

ğœ† 1
3 ğœ† 1
1
0 1
2 0 0 ğœ† 1
1
3 0 1
0 1
0 0 1

2 0 0
3 0 1
3 0
2 ğœ† 0 0 1
2
2 0
3 ğœ† 1
3
2 ğœ†

2 0 1

âƒ’âƒ’âƒ’âƒ’âƒ’âƒ’âƒ’âƒ’âƒ’âƒ’âƒ’âƒ’âƒ’âƒ’âƒ’âƒ’âƒ’âƒ’âƒ’âƒ’

= ğœ†6 âˆ’ 23
18

ğœ†4 +

41
144

ğœ†2 âˆ’ 1
144

o qual notamos facilmente a partir dos expoentes pares da variÃ¡vel ğœ†, que tem raÃ­zes
com valores simÃ©tricos. Podemos, entÃ£o, aï¬rmar que ğœ†2 = âˆ’1 Ã© outro autovalor de ğ‘€.
Calculando as demais raÃ­zes do polinÃ´mio equivalente 144ğœ†6 âˆ’ 184ğœ†4 + 41ğœ†2 âˆ’ 1 = 0,
estas sÃ£o ğœ† = Â±1
2

. Com os demais autovetores, obtemos:

e ğœ† = Â±1
6

â€¢ ğœ†1 = 1

associado a

âƒ—ğ‘£1 =

)ï¸
(ï¸
;
1 1 1 1 1 1

â€¢ ğœ†2 = âˆ’1

associado a

âƒ—ğ‘£2 =

(ï¸

âˆ’1 1 âˆ’1 1 âˆ’1 1

)ï¸
;

â€¢ ğœ†3 =

1
2
â€¢ ğœ†4 = âˆ’1
2
â€¢ ğœ†5 = âˆ’1
6

â€¢ ğœ†6 =

1
6

associado a

âƒ—ğ‘£3 =

)ï¸
(ï¸
âˆ’1 0 1 âˆ’1 0 1
;

associado a

âƒ—ğ‘£4 =

(ï¸

)ï¸
1 0 âˆ’1 âˆ’1 0 1
;

associado a

âƒ—ğ‘£5 =

associado a

âƒ—ğ‘£6 =

(ï¸‚

1 âˆ’4
3
(ï¸‚
âˆ’1 âˆ’4
3

1 1 âˆ’4
3

âˆ’1 1

4
3

)ï¸‚

1

;

)ï¸‚

1

.

A partir desses resultados, e do fato de que todos os autovalores sÃ£o distintos e com
multiplicidade algÃ©brica 1, e que portanto, seus autovetores sÃ£o linearmente indepen-
dentes, a matriz ğ‘€ Ã© diagonalizÃ¡vel. Formando uma matriz ğ‘† com os seis autovetores
de ğ‘€ sendo seus vetores-coluna, temos de imediato que ğ‘† Ã© invertÃ­vel. Com isso, en-
contramos a decomposiÃ§Ã£o espectral de ğ‘€ = ğ‘†ğ·ğ‘†

âˆ’1 como segue,

â›
âˆ’1
1
âˆ’1 âˆ’1
1 âˆ’1
âˆ’1
1

1
0 âˆ’ 4
âˆ’ 4
3
3
1 âˆ’1
1
0 âˆ’ 4
3
1
1

1 âˆ’1 âˆ’1 1
1
0
1
1
1 âˆ’1 1
4
1
0
3
1
1
1

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

â
â›
âˆ’1
0 0 0
0
0
0 âˆ’ 1
0 0 0
0
2
0 âˆ’ 1
6 0 0 0
0
1
6 0 0
0
0
0
0 1
2 0
0
0
0
0 0 1
0
0
0

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

.

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

âˆ’ 1
7
1
4
3
28
âˆ’ 3
28
âˆ’ 1
4
1
7

.

3
14
0
âˆ’ 3
14
âˆ’ 3
14
0
3
14

âˆ’ 1
7
âˆ’ 1
4
3
28
âˆ’ 3
28
1
4
1
7

1
7
âˆ’ 1
4
3
28
3
28
âˆ’ 1
4
1
7

âˆ’ 3
14
0
âˆ’ 3
14
3
14
0
3
14

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

1
7
1
4
3
28
3
28
1
4
1
7

.

Finalmente, calculamos

âƒ—ğ‘€0.ğ‘€23 =

(ï¸ƒ
0

)ï¸ƒ

(ï¸ƒ

Â·

3
7

623 + 1
623

0

2
7

+

1
224

âˆ’ 3
14

Â· 1
623 0

2
7

+

1
224

âˆ’ 3
14

Â· 1
623

)ï¸ƒ

56

CAPÃTULO 3. APLICAÃ‡Ã•ES

donde tiramos que ğ‘€

mada de 42, 85%.

(23)
12 =

)ï¸ƒ

(ï¸ƒ

Â·

3
7

623 + 1
623

(cid:27) 0, 4285, isto Ã©, uma probabilidade aproxi-

3.2 O Problema dos ğ‘› mentirosos

O Problema dos ğ‘› mentirosos e a Cadeia de Markov [Fel67] "Se cada uma das pessoas
ğ´, ğµ, ğ¶, ğ· dizem a verdade uma vez em trÃªs vezes (independentemente), e ğ´ aï¬rma que ğµ
nega que ğ¶ declara que ğ· Ã© um mentiroso, qual a probabilidade de que ğ· estava dizendo a
verdade?"

Este problema foi primeiramente tratado por A.S. Eddington e teve sua soluÃ§Ã£o

publicada em Monthly [4288, Vol.57(1950) pp. 43-45].

ResoluÃ§Ã£o: Note inicialmente que, existem apenas oito declaraÃ§Ãµes distintas que po-
dem ser feitas pela pessoa ğ´. Na formulaÃ§Ã£o original, as pessoas ğ¶, ğµ e ğ´ declaram
sucessivamente trÃªs aï¬rmaÃ§Ãµes que podem ser verdadeiras ou falsas, e que podem se
contradizer. Tem-se ainda que, apenas ğ· e ğ¶ conhecem o estado inicial. No entanto,
tomado pelo valor nominal, cada aï¬rmaÃ§Ã£o implicaria ou que ğ· estÃ¡ dizendo a ver-
dade ou que ele mente. Assim, por exemplo, se ğµ negar que ğ¶ aï¬rma que ğ· Ã© um
mentiroso, ele implica que ğ· diz a verdade. Se, por outro lado, ğ´ nega que ğµ negou,
etc., entÃ£o isso implica que ğ· Ã© um mentiroso. Utilizaremos uma terminologia neutra,
e falaremos de um processo casual com dois estados possÃ­veis onde, eventualmente, o
estado observado Ã© 1 ou 2 conforme a Ãºltima aï¬rmaÃ§Ã£o de que ğ· Ã© honesto ou menti-
roso. Inicialmente, ou seja, no instante 0, o estado observado Ã© 1 ou 2 de acordo com o
que ğ· diz verdade ou mentira, respectivamente, e, cada sequÃªncia de amostra possÃ­vel
do processo acaba sendo representada por uma sucessÃ£o dos dÃ­gitos 1 e 2.

Fundamentalmente, o processo de Markov caracteriza-se pelo fato de que cada pes-
soa conhece apenas a aï¬rmaÃ§Ã£o da Ãºltima falante. Ainda, no instante ğ‘› o estado obser-
vado muda ou permanece o mesmo de acordo com a ğ‘›-Ã©sima falante diz a verdade ou
mente. Nessas condiÃ§Ãµes, obtemos um modelo que representa uma Cadeia de Markov
mais simples:

â€¢ Temos dois estados possÃ­veis 1 e 2 nos quais, inicialmente, as suas respectivas
probabilidades sÃ£o ğ›¼ e ğ›½, naturalmente com ğ›¼ + ğ›½ = 1. Independentemente do
que ocorra atÃ© o tempo ğ‘›, temos probabilidade ğ‘ de que exatamente no tempo ğ‘›, o
estado observado nÃ£o sofra alteraÃ§Ã£o, e ğ‘ = (1âˆ’ğ‘) de que haja alteraÃ§Ã£o. Buscamos
as probabilidades condicionais ğ‘¥ğ‘› e ğ‘¦ğ‘› que o processo realmente comeÃ§ou a partir
do estado 1 dado que, no tempo ğ‘› o estado observado Ã©, respectivamente, 1 ou 2.

Na versÃ£o original, tem-se ğ›¼ = ğ‘ =

, ğ‘› = 3 e somente ğ‘¥ğ‘› Ã© requerida;

1
3

CAPÃTULO 3. APLICAÃ‡Ã•ES

57

â€¢ Em cada etapa temos quatro transiÃ§Ãµes possÃ­veis 1 â†¦â†’ 1, 1 â†¦â†’ 2, 2 â†¦â†’ 1, 2 â†¦â†’ 2 e
as correspondentes probabilidades de transiÃ§Ã£o, por suposiÃ§Ã£o, sÃ£o ğ‘11 = ğ‘22 =
ğ‘ e ğ‘12 = ğ‘21 = (1 âˆ’ ğ‘). Assim, supondo que em um determinado momento o
(ğ‘›)
ğ‘—ğ‘˜ a probabilidade de que ğ‘› etapas depois, o
sistema estÃ¡ num estado ğ‘— e sendo ğ‘
(ğ‘›)
ğ‘—ğ‘˜ Ã© chamada a probabilidade de transiÃ§Ã£o
estado observado seja ğ‘˜, temos que ğ‘
(1)
ğ‘— â†¦â†’ ğ‘˜ da ğ‘›-Ã©sima etapa, onde claramente ğ‘
ğ‘—ğ‘˜ = ğ‘ğ‘—ğ‘˜ e, a partir das possibilidades
de transiÃ§Ã£o do estado ğ‘— para o estado ğ‘˜ em duas etapas e de suas respectivas
probabilidades,

ğ‘

(2)
ğ‘—ğ‘˜ = ğ‘ğ‘—1ğ‘1ğ‘˜ + ğ‘ğ‘—2ğ‘2ğ‘˜.

(3.1)

O que de modo geral, a partir de igualdades recursivas, pode ser expresso por,

ğ‘

(ğ‘›+1)
ğ‘—ğ‘˜

= ğ‘

(ğ‘›)
ğ‘—1 ğ‘1ğ‘˜ + ğ‘

(ğ‘›)
ğ‘—2 ğ‘2ğ‘˜.

(3.2)

As igualdades (3.1) e (3.2) acima podem ser obtidas de forma direta por meio das
potÃªncias da matriz

ğ‘ƒ =

â›

ğ‘
âœâœâœâœâ
1 âˆ’ ğ‘

â

âŸâŸâŸâŸâ 

1 âˆ’ ğ‘
ğ‘

â›

âœâœâœâœâ

=

ğ‘11 ğ‘12
ğ‘21 ğ‘22

â

âŸâŸâŸâŸâ  .

(3.3)

â€¢ Sendo ğ›¼ e ğ›½ as probabilidades iniciais respectivas aos estados 1 e 2, entÃ£o, deno-

tando por ğ‘’

(ğ‘›)
ğ‘˜ a probabilidade de observar no instante ğ‘› o estado ğ‘˜, segue que,

ğ‘’

(ğ‘›)
ğ‘˜ = ğ›¼ğ‘

(ğ‘›)
1ğ‘˜ + ğ›½ğ‘

(ğ‘›)
2ğ‘˜ .

Encontramos, portanto, para as probabilidades condicionais ğ‘¥ğ‘› e ğ‘¦ğ‘›, que

ğ‘¥ğ‘› =

ğ›¼ğ‘

(ğ‘›)
11
(ğ‘›)
ğ‘’
1

,

ğ‘¦ğ‘› =

ğ›¼ğ‘

(ğ‘›)
12
(ğ‘›)
ğ‘’
2

.

(3.4)

(3.5)

â€¢ Retomando Ã  matriz ğ‘ƒ em (3.3). Precisamos calcular ğ‘ƒ ğ‘›, e para isso, conforme
utilizamos na resoluÃ§Ã£o do problema (3.1), escreveremos a sua decomposiÃ§Ã£o es-
pectral. Como ğ‘ƒ Ã© uma matriz de Markov, e seus vetores-linha tem a soma de
suas coordenadas iguais a 1, temos que ğœ†1 = 1 Ã© um autovalor associado ao auto-
)ï¸
. Buscando o outro autovalor e autovetor associado encontramos,
vetor âƒ—ğ‘£1 =
ğœ†2 = (2ğ‘ âˆ’ 1) e âƒ—ğ‘£2 =

)ï¸
. Assim, escrevemos a decomposiÃ§Ã£o

(ï¸
1 âˆ’1

(ï¸
1 1

â›

â
1
1
âœâœâœâœâ
âˆ’1 1

âŸâŸâŸâŸâ  .

â›

âœâœâœâœâ

ğ‘ƒ =

2ğ‘ âˆ’ 1 0
1

0

â

âŸâŸâŸâŸâ  .

â›

âœâœâœâœâ

1
2
1
2

â

âŸâŸâŸâŸâ  .

âˆ’ 1
2
1
2

58

CAPÃTULO 3. APLICAÃ‡Ã•ES

Consequentemente,

ğ‘ƒ ğ‘› =

â›

âœâœâœâœâ

â
1
1
âˆ’1 1

âŸâŸâŸâŸâ  .

â›
(2ğ‘ âˆ’ 1)ğ‘› 0
âœâœâœâœâ
1

0

â

âŸâŸâŸâŸâ  .

â›

âœâœâœâœâ

1
2
1
2

â

âŸâŸâŸâŸâ 

âˆ’ 1
2
1
2

=

â›

âœâœâœâœâœâœâœâœâ

(2ğ‘ âˆ’ 1)ğ‘› + 1
2
1 âˆ’ (2ğ‘ âˆ’ 1)ğ‘›
2

1 âˆ’ (2ğ‘ âˆ’ 1)ğ‘›
2
(2ğ‘ âˆ’ 1)ğ‘› + 1
2

â

âŸâŸâŸâŸâŸâŸâŸâŸâ 

.

(3.6)

Finalmente, para explicitarmos as probabilidades condicionais conforme exibidas
em (3.5) basta substituirmos as expressÃµes (3.6) e (3.4) nesta igualdade, o que nos dÃ¡,

ğ‘¥ğ‘› =

ğ›¼. [(2ğ‘ âˆ’ 1)ğ‘› + 1]
1 + (2ğ‘ âˆ’ 1)ğ‘›.(ğ›¼ âˆ’ ğ›½)

,

ğ‘¦ğ‘› =

ğ›¼. [1 âˆ’ (2ğ‘ âˆ’ 1)ğ‘›]
1 âˆ’ [(2ğ‘ âˆ’ 1)ğ‘›.(ğ›¼ âˆ’ ğ›½)]

.

(3.7)

1
3

13
41

Como na versÃ£o original, estamos interessados em determinar ğ‘¥ğ‘› para ğ‘› = 3 e
. Substituindo os valores em questÃ£o, concluÃ­mos que a probabilidade pro-

ğ›¼ = ğ‘ =

7
20

(cid:27) 31, 71%. AlÃ©m disso, temos ğ‘¦ğ‘› =

. ConvÃ©m ainda veriï¬carmos
curada Ã© ğ‘¥3 =
â†’ ğ›¼ conforme ğ‘› â†’ âˆ, isto Ã©, a probabilidade
que, como 0 < ğ‘ < 1, entÃ£o claramente ğ‘¥ğ‘›
condicional de que o processo comeÃ§ou a partir do estado 1, dado que para um nÃºmero
suï¬cientemente grande de etapas o estado observado Ã© novamente 1, converge para a
probabilidade inicial ğ›¼.
Mentiroso Preferencial. AtÃ© aqui, as chances de uma pessoa dizer a verdade nÃ£o de-
pendem da aï¬rmaÃ§Ã£o que ele deve transmitir. Suponhamos entÃ£o, que cada pessoa
tem a preferÃªncia de aï¬rmar que ğ· diz a verdade. Assim, uma transiÃ§Ã£o 1 â†¦â†’ 1 Ã© mais
provÃ¡vel que 2 â†¦â†’ 2, enquanto 1 â†¦â†’ 2 Ã© menos provÃ¡vel que 2 â†¦â†’ 1. Podemos tratar
este caso geral da mesma forma que tratamos o problema especÃ­ï¬co anterior. Para isso,
â€². Naturalmente, todas as igualdades
â€² e ğ‘22 = ğ‘
faÃ§amos ğ‘11 = ğ‘, ğ‘12 = 1 âˆ’ ğ‘, ğ‘21 = 1 âˆ’ ğ‘
acima sÃ£o aplicadas, exceto a igualdade (3.6) que deve ser substituÃ­da por

ğ‘ƒ ğ‘› =

â›

ğ‘
âœâœâœâœâ
1 âˆ’ ğ‘

â€²

â
ğ‘›

âŸâŸâŸâŸâ 

1 âˆ’ ğ‘
â€²
ğ‘

=

â›

âœâœâœâœâœâœâœâœâœâœâ

(ğ‘ + ğ‘

(ğ‘ + ğ‘

â€² âˆ’ 1)ğ‘›(ğ‘ âˆ’ 1) + ğ‘
ğ‘ + ğ‘â€² âˆ’ 2
â€² âˆ’ 1)ğ‘›(1 âˆ’ ğ‘
ğ‘ + ğ‘â€² âˆ’ 2

â€²) + ğ‘

â€² âˆ’ 1

(ğ‘ + ğ‘

â€² âˆ’ 1)ğ‘›(1 âˆ’ ğ‘) + ğ‘ âˆ’ 1

â€² âˆ’ 1

(ğ‘ + ğ‘

â€² âˆ’ 1)ğ‘›(ğ‘

â€² âˆ’ 1) + ğ‘ âˆ’ 1

ğ‘ + ğ‘â€² âˆ’ 2

ğ‘ + ğ‘â€² âˆ’ 2

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

.

Nessas condiÃ§Ãµes, o resultado ï¬nal Ã© agora

ğ‘¥ğ‘› =

ğ›¼ Â· [(1 âˆ’ ğ‘

â€²) + (1 âˆ’ ğ‘) Â· (ğ‘ + ğ‘
(1 âˆ’ ğ‘â€²) + [ğ›¼(1 âˆ’ ğ‘) âˆ’ ğ›½(1 âˆ’ ğ‘â€²)] Â· (2ğ‘â€² âˆ’ 1)ğ‘›

â€² âˆ’ 1)ğ‘›]

3.3 O PageRank

Um conjunto de nÃ³s com conexÃµes Ã© um grafo. Qualquer rede pode ser descrita
por um grafo. A estrutura de links da web forma um grafo, onde os sites individuais
sÃ£o os nÃ³s e existe uma seta do site ğ‘ğ‘– para o site ğ‘ğ‘— se ğ‘ğ‘– se liga a ğ‘ğ‘—. A matriz de

CAPÃTULO 3. APLICAÃ‡Ã•ES

59

adjacÃªncia ğ´ deste grafo Ã© chamada de grafo da web. Se houver ğ‘› sites, entÃ£o a matriz
de adjacÃªncia Ã© uma matriz ğ‘› Ã— ğ‘› com entradas ğ´ğ‘–ğ‘— = 1 se existir um link de ğ‘ğ‘– para ğ‘ğ‘—.
Se dividirmos cada coluna pela quantidade de nÃºmeros 1 dessa coluna, obtemos uma
matriz de Markov A que Ã© chamada de matriz da web normalizada. Deï¬na a matriz

1
ğ‘›

para todo ğ‘–, ğ‘—. Os estudantes de pÃ³s-graduaÃ§Ã£o e posteriores
ğ¸ que satisfaz ğ¸ğ‘–ğ‘— =
empresÃ¡rios Sergey Brin e Lawrence Page tiveram em 1996 a seguinte ideia de um
bilhÃ£o de dÃ³lares:

A matriz do Google Ã© a matriz ğº = ğ‘‘ğ´ + (1 âˆ’ ğ‘‘)ğ¸, onde 0 < ğ‘‘ < 1 Ã© um parÃ¢metro
chamado fator de amortecimento e ğ´ Ã© a matriz de Markov obtida da matriz de adja-
cÃªncia escalonando as linhas para se tornar matriz estocÃ¡stica. Esta Ã© uma matriz de
Markov ğ‘› Ã— ğ‘› com autovalor 1. Seu autovetor Perron-Frobenius âƒ—ğ‘£ escalado de forma
que o maior valor seja 10 Ã© chamado de PageRank do fator de amortecimento ğ‘‘. A
equaÃ§Ã£o do PageRank Ã© [ğ‘‘ğ´ + (1 âˆ’ ğ‘‘)ğ¸]âƒ—ğ‘£ = âƒ—ğ‘£.

O fator de amortecimento pode parecer um pouco misterioso. Brin e Page escre-
veram: â€œO PageRank pode ser tambÃ©m considerado um modelo de comportamento
do usuÃ¡rio. Assumimos que hÃ¡ um â€™surï¬sta aleatÃ³rioâ€™ que recebe uma pÃ¡gina da Web
aleatoriamente e continua clicando nos links, nunca atingindo â€™de voltaâ€™, mas eventu-
almente ï¬ca entediado e comeÃ§a em outra pÃ¡gina aleatÃ³ria. A probabilidade de que
o surï¬sta aleatÃ³rio visite uma pÃ¡gina Ã© seu PageRank. O fator de amortecimento Ã© a
probabilidade em cada pÃ¡gina de o â€™surï¬sta aleatÃ³rioâ€™ ï¬car entediado e solicitar outra
pÃ¡gina aleatÃ³ria. Uma variaÃ§Ã£o importante Ã© adicionar apenas o fator de amorteci-
mento ğ‘‘ a uma Ãºnica pÃ¡gina ou a um grupo de pÃ¡ginas. Isso permite personalizaÃ§Ã£o
e pode tornar quase impossÃ­vel enganar deliberadamente o sistema para obter uma
classiï¬caÃ§Ã£o mais alta. Temos vÃ¡rias outras extensÃµes para o PageRank.â€

Vamos resolver um caso simples que ilustra essas condiÃ§Ãµes.

Considere uma rede com quatro sites ğ‘†1, ğ‘†2, ğ‘†3 e ğ‘†4 conectados por meio de links conforme

ilustrado no grafo a seguir.

ğ‘†1

ğ‘†2

ğ‘†3

ğ‘†4

Vamos encontrar o pagerank para o fator ğ‘‘ = 0, 1.

ResoluÃ§Ã£o: A matriz de adjacÃªncia do grafo acima e sua matriz da web normalizada
sÃ£o respectivamente,

ğ‘€ =

â
â›
0 1 1 1
0 0 1 1
1 0 0 0
1 0 1 0

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

,

ğ‘’ ğ´ =

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

0 1 1
1
3
2
1
0 0 1
3
2
1
2 0 0 0
2 0 1
1
3 0

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

.

(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:31)
(cid:31)
(cid:111)
(cid:111)
(cid:47)
(cid:47)
(cid:63)
(cid:63)
(cid:79)
(cid:79)
(cid:95)
(cid:95)
60

CAPÃTULO 3. APLICAÃ‡Ã•ES

Com isso, e tendo ainda a matriz

ğ¸ =

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

1
4
1
4
1
4
1
4

1
4
1
4
1
4
1
4

1
4
1
4
1
4
1
4

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

1
4
1
4
1
4
1
4

,

obtemos a matriz do Google

ğº =

Â·

1
10

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

0 1 1
1
3
2
0 0 1
1
3
2
1
2 0 0 0
2 0 1
1
3 0

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

+

Â·

9
10

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

1
4
1
4
1
4
1
4

1
4
1
4
1
4
1
4

1
4
1
4
1
4
1
4

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

1
4
1
4
1
4
1
4

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

9
40
9
40
11
40
11
40

=

13
40
9
40
9
40
9
40

31
120
31
120
9
40
31
120

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

11
40
11
40
9
40
9
40

.

O Pagerank do fator de amortecimento ğ‘‘ serÃ¡ o autovetor Perron-Frobenius âƒ—ğ‘£, tal

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

ğ‘¥
ğ‘¦
ğ‘§
ğ‘¤

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

obtemos o sistema

que, ğºâƒ—ğ‘£ = âƒ—ğ‘£ e sua maior entrada serÃ¡ 10. Assim, pondo âƒ—ğ‘£ =

â§

âªâªâªâªâªâªâªâªâªâ¨
âªâªâªâªâªâªâªâªâªâ©

9
40 ğ‘¥ + 13
40 ğ‘¥ + 9
9
40 ğ‘¥ + 9
11
11
40 ğ‘¥ + 9

40 ğ‘¦ + 31
40 ğ‘¦ + 31
40 ğ‘¦ + 9
40 ğ‘¦ + 31

120ğ‘§ + 11
120ğ‘§ + 11
40ğ‘§ + 9
120ğ‘§ + 9

40 ğ‘¤ = ğ‘¥
40 ğ‘¤ = ğ‘¦
40 ğ‘¤ = ğ‘§
40 ğ‘¤ = ğ‘¤

que possui inï¬nitas soluÃ§Ãµes, sendo que, a sua soluÃ§Ã£o relevante para o fator ğ‘‘ Ã©

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

10
443100
48741
63300
7161
2110
231

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

âƒ—ğ‘£ =

que Ã© o seu Pagerank, e nos diz que o site ğ‘†1 Ã© o mais acessado direta-

mente ou, por meio de links nos sites ğ‘†2, ğ‘†3 e ğ‘†4 que levam o usuÃ¡rio atÃ© ğ‘†1.

Esse mesmo caso visto sob outra interessante abordagem nos dÃ¡ a distribuiÃ§Ã£o das
probabilidades de acesso aos sites ğ‘†1, ğ‘†2, ğ‘†3 e ğ‘†4. Para este ï¬m, consideremos a ma-
triz de transiÃ§Ã£o ğ‘ƒ de uma cadeia de Markov, associada Ã s probabilidades do usuÃ¡rio
estando no site ğ‘– acessar o site ğ‘—,

ğ‘ƒ =

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

0 0 1 1
2
1
3 0 0 0
1
2 0 1
1
3
2
1
1
2 0 0
3

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

.

CAPÃTULO 3. APLICAÃ‡Ã•ES

61

Seu autovetor Perron-Frobenius Ã© a sua distribuiÃ§Ã£o de equilÃ­brio estÃ¡vel. Nesse
ğ‘˜=1 ğ‘£ğ‘˜ = 1 sÃ£o as probabilidades

contexto, as entradas ğ‘£ğ‘˜ desse autovetor âƒ—ğ‘£, tais que,
que buscamos e revelam qual site tem o maior Ã­ndice de acessos. Assim, o sistema

âˆ‘ï¸€4

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

0 0 1 1
2
1
3 0 0 0
2 0 1
1
1
3
2
1
1
2 0 0
3

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

=

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

ğ‘¥
ğ‘¦
ğ‘§
ğ‘¤

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

ğ‘¥
ğ‘¦
ğ‘§
ğ‘¤

Â·

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

â›

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ
âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

â

2
2
3
3
2
1

tem inï¬nitas soluÃ§Ãµes caracterizadas por ğ‘¡ Â·

, obviamente para ğ‘¡ âˆˆ R.

Escolhendo ğ‘¡ =

6
31

de modo conveniente a satisfazer a condiÃ§Ã£o dada, obtemos

âƒ—ğ‘£ =

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

12
31
4
31
9
31
6
31

.

(cid:27) 38, 71%,
DaÃ­, temos que as probabilidades de acesso aos respectivos sites sÃ£o ğ‘†1 = 12
31
(cid:27) 19, 35%, donde concluÃ­mos que quanto
(cid:27) 29, 03% e ğ‘†4 = 6
ğ‘†2 = 4
31
31
mais a rede Ã© acessada, o site ğ‘†1 tem maior probabilidade de ser visitado com 38, 71%.

(cid:27) 12, 9%, ğ‘†3 = 9
31

3.4 O Modelo de DifusÃ£o de Ehrenfest

Este modelo foi estudado e apresentado pelos fÃ­sicos teÃ³ricos Tatyana e Paul Eh-
renfest ainda na primeira dÃ©cada do sÃ©culo XX. Trata-se da sua versÃ£o considerada
bÃ¡sica e fornece explicaÃ§Ãµes relevantes fundamentadas na mecÃ¢nica estatÃ­stica - que
tem como objetivo estudar e explicar o comportamento macroscÃ³pico a partir das leis
microscÃ³picas - para um problema inquietante, o da irreversibilidade. As leis da fÃ­sica
baseiam-se em mecÃ¢nica quÃ¢ntica (que Ã© probabilÃ­stica), portanto, um tratamento es-
tocÃ¡stico faz-se necessÃ¡rio nesse contexto. O casal Ehrenfest conseguiu 20 anos antes
do desenvolvimento da mecÃ¢nica quÃ¢ntica moderna, introduzir um modelo probabi-
lÃ­stico para descrever a origem da irreversibilidade em sistemas fÃ­sicos. Este modelo
vem sendo usado extensivamente para estudar aplicaÃ§Ãµes do Teorema H de Boltzmann
(sobre a entropia termodinÃ¢mica e o caos molecular) e entender como a simetria do
tempo subjacente nas leis do movimento geram a assimetria de tempo dos processos
de difusÃ£o.

62

CAPÃTULO 3. APLICAÃ‡Ã•ES

Considere um sistema formado por dois recipientes idÃªnticos ğ´ e ğµ ligados por uma pe-
quena abertura, e que o recipiente ğ´ contÃ©m um gÃ¡s constituÃ­do por um nÃºmero ï¬nito de
molÃ©culas indistinguÃ­veis. Tais molÃ©culas transitam de ğ´ para ğµ regidas pelas leis da me-
cÃ¢nica e da termodinÃ¢mica atÃ© que num dado momento, os recipientes encontram-se em
equilÃ­brio macroscÃ³pico (quando suas pressÃµes internas sÃ£o iguais). Considere tambÃ©m que
nÃ£o hÃ¡ interferÃªncias externas ao sistema e que, as velocidades e as posiÃ§Ãµes das molÃ©culas
no interior dos recipientes sÃ£o irrelevantes. Qual a probabilidade de mesmo num instante de
tempo arbitrariamente remoto, cada molÃ©cula regressar ao recipiente ğ´? Caso esse regresso
ocorra, Ã© possÃ­vel determinar o tempo necessÃ¡rio?.

ResoluÃ§Ã£o: Como neste trabalho nÃ£o hÃ¡ a pretensÃ£o de apresentar e nem discutir os
fundamentos da mecÃ¢nica estatÃ­stica, apresentaremos de forma introdutÃ³ria um mo-
delo de evoluÃ§Ã£o estocÃ¡stica que responde satisfatoriamente ao modelo em questÃ£o.

Seja ğ‘š o nÃºmero total de molÃ©culas. Estamos interessados em observar do ponto de
vista probabilÃ­stico o trÃ¢nsito das molÃ©culas, entÃ£o, consideraremos que apenas uma
molÃ©cula passa de um recipiente para o outro a cada instante de tempo, e que todas
as molÃ©culas tÃªm a mesma probabilidade de passar de um recipiente para o outro.
Temos um nÃºmero ï¬xo de molÃ©culas, por isso, deduzimos o nÃºmero de molÃ©culas de
um recipiente a partir do nÃºmero de molÃ©culas do outro. O estado desse sistema serÃ¡
determinado por

ğ‘‹ğ‘› := nÃºmero de molÃ©culas em B no instante n,

onde, como o recipiente ğµ pode conter 0, 1, 2, etc. atÃ© no mÃ¡ximo ğ‘š molÃ©culas, entÃ£o,
âˆˆ ğ¸ğ‘š. Suporemos a
para ğ¸ğ‘š := {0, 1, 2, . . . , ğ‘š}, que Ã© o espaÃ§o dos estados, temos ğ‘‹ğ‘›
condiÃ§Ã£o inicial ğ‘‹0 = 0, isto Ã©, todas as molÃ©culas estÃ£o no recipiente ğ´. Precisamos
determinar a evoluÃ§Ã£o do nÃºmero de molÃ©culas em ğµ em funÃ§Ã£o do tempo ğ‘›, ou seja,
ğ‘‹0, ğ‘‹1, ğ‘‹2, . . . , a partir do fato de que apenas uma molÃ©cula passa de um recipiente
Â± 1, e assim, quando ğ‘‹ğ‘› = ğ‘˜ temos
para o outro. Nessas condiÃ§Ãµes, temos ğ‘‹ğ‘›+1 = ğ‘‹ğ‘›
ğ‘˜ molÃ©culas em ğµ e consequentemente, ğ‘š âˆ’ ğ‘˜ em ğ´. Dessa forma, no tempo ğ‘› + 1 a
molÃ©cula estarÃ¡ ou em ğ‘˜ âˆ’ 1: uma molÃ©cula passou de ğµ para ğ´; ou em ğ‘˜ + 1: uma
molÃ©cula passou de ğ´ para ğµ.

AtÃ© aqui, temos uma Cadeia de Markov com ğ‘š + 1 estados ğ¸(0) = 0, ğ¸(1) = 1, Â· Â· Â· ,
ğ¸(ğ‘š) = ğ‘š e transiÃ§Ãµes possÃ­veis apenas para os estados, imediatamente, anterior e pos-
terior, pois, em cada instante uma molÃ©cula move-se aleatoriamente de seu recipiente
para o outro enquanto o estado do sistema Ã© determinado pelo nÃºmero de molÃ©culas
em ğµ. Para ğ‘‹1, o primeiro estado imediatamente apÃ³s a condiÃ§Ã£o inicial, segue que
ğ‘‹1 = 1, pois, dado que em ğ‘› = 0 o recipiente ğµ estava vazio, em ğ‘› = 1 devemos ter
um molÃ©cula em ğµ, o que probabilisticamente, representamos por ğ‘(0 â†¦â†’ 1) = 1, isto Ã©,
estando ğµ vazio, uma molÃ©cula passa de ğ´ para ğµ com probabilidade 1. Supondo entÃ£o
ğ‘‹ğ‘› = ğ‘˜, a probabilidade da molÃ©cula a ser movida estar em ğµ serÃ¡

CAPÃTULO 3. APLICAÃ‡Ã•ES

63

consequentemente, se for uma molÃ©cula que esteja em ğ´, a probabilidade serÃ¡

ğ‘(ğ‘˜ â†¦â†’ ğ‘˜ âˆ’ 1) =

ğ‘˜
ğ‘š

,

ğ‘(ğ‘˜ â†¦â†’ ğ‘˜ + 1) =

ğ‘š âˆ’ ğ‘˜
ğ‘š

.

Essas probabilidades correspondentes acima, sÃ£o chamadas de probabilidades de
transiÃ§Ã£o e podem ser escritas como entradas da matriz quadrada ğ‘€ com ordem (ğ‘š+1),
tais que, ğ‘€ğ‘–ğ‘— := ğ‘(ğ‘– â†¦â†’ ğ‘—). A matriz ğ‘€ construÃ­da como segue, Ã© chamada de matriz de
, conforme respectivamente, ğ‘— = ğ‘– âˆ’1 ou ğ‘— = ğ‘– +1,

transiÃ§Ã£o, onde ğ‘€ğ‘–ğ‘— =

ğ‘—
ğ‘š

ou ğ‘€ğ‘–ğ‘— = 1âˆ’ ğ‘—
ğ‘š

ğ‘€ =

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

0
1
ğ‘š
0
...
0

0

1
0
0 1 âˆ’ 1
ğ‘š
2
ğ‘š
...
0

0
...
0

0

0
1 âˆ’ 2
ğ‘š
...
0

0

0

0

Â· Â· Â· 0 0
Â· Â· Â· 0 0

Â· Â· Â·

Â· Â· Â· 0 0
...
...
1
Â· Â· Â· 0
ğ‘š
Â· Â· Â· 1 0

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

Temos claramente para todo ğ‘–, ğ‘—, que ğ‘€ğ‘–ğ‘—

â‰¥ 0, e que

âˆ‘ï¸€

ğ‘˜=ğ‘–Â±1 ğ‘€ğ‘–ğ‘˜ = 1 para todo ğ‘–. Essas

propriedades fazem com que ğ‘€ğ‘‡ seja uma matriz de Markov.

Note que temos o vetor-coluna âƒ—ğ‘¢(0) =

com ğ‘š + 1 entradas, representando a

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

1
0
...
0
0

condiÃ§Ã£o inicial ğ‘‹0 = 0, e signiï¬cando que no tempo ğ‘› = 0 suas entradas sÃ£o,

(0)
ğ‘¢
ğ‘˜

:= ğ‘(ğ‘‹0 = ğ‘˜) =

â§
âªâªâªâ¨
1, se k=1,
âªâªâªâ©
0, caso contrÃ¡rio.

Dado que, a probabilidade contida em âƒ—ğ‘¢(0) concentrada em 0 no tempo ğ‘› = 0,

espalha-se pelo sistema, segue para ğ‘› = 1, que

âƒ—ğ‘¢(1) =

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

0
1
...
0
0

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

= ğ‘€ğ‘‡ âƒ—ğ‘¢(0),

64

e

CAPÃTULO 3. APLICAÃ‡Ã•ES

âƒ—ğ‘¢(2) =

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

1
ğ‘š
0
ğ‘šâˆ’1
ğ‘š
0
...
0

= ğ‘€ğ‘‡ âƒ—ğ‘¢(1),

e de modo generalizado, notamos que

âƒ—ğ‘¢(ğ‘›) =

â›

âœâœâœâœâœâœâœâœâœâœâœâ

(ğ‘›)
ğ‘¢
0

...

(ğ‘›)
ğ‘š

ğ‘¢

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

= ğ‘€ğ‘‡ âƒ—ğ‘¢(ğ‘›âˆ’1)

(ğ‘›)

âˆ‘ï¸€

(ğ‘›)
sÃ£o nÃ£o-
Ã© o vetor de distribuiÃ§Ã£o de probabilidade no tempo ğ‘›, onde as entradas ğ‘¢
ğ‘˜
ğ‘˜ = 1 para ğ‘˜ percorrendo o conjunto {0, 1, 2, . . . , ğ‘š} com a interpretaÃ§Ã£o
negativas e
ğ‘¢
(ğ‘›)
de que ğ‘¢
ğ‘˜ = ğ‘ƒ (a molÃ©cula estar em k no instante n). De fato, estando a molÃ©cula em ğ‘˜
no instante ğ‘›, isto signiï¬ca que no instante ğ‘› âˆ’ 1 devia estar em ğ‘˜ âˆ’ 1 ou mesmo, em
ğ‘˜ + 1. Disso, tiramos que

(ğ‘›)

ğ‘¢

ğ‘˜ = ğ‘ƒ (ğ‘˜ âˆ’ 1 â†¦â†’ ğ‘˜).ğ‘¢

(ğ‘›âˆ’1)
ğ‘˜âˆ’1 + ğ‘ƒ (ğ‘˜ + 1 â†¦â†’ ğ‘˜).ğ‘¢

(ğ‘›âˆ’1)
ğ‘˜+1 ,

o que equivalentemente representamos de forma vetorial como âƒ—ğ‘¢(ğ‘›) = ğ‘€ğ‘‡ âƒ—ğ‘¢(ğ‘›âˆ’1).

Para a condiÃ§Ã£o inicial dada, a evoluÃ§Ã£o do sistema converge para um estado de
equilÃ­brio em termos de nÃºmero de molÃ©culas em cada recipiente. Estamos interessa-
dos em provar esse comportamento a partir da matriz ğ‘€ğ‘‡ , mais precisamente, veriï¬-
cando o que acontece com (ğ‘€ğ‘‡ )ğ‘› quando ğ‘› â†’ âˆ. Tal veriï¬caÃ§Ã£o serÃ¡ mais interessante
se a matriz ğ‘€ğ‘‡ satisï¬zer as condiÃ§Ãµes do Teorema de Perron-Frobenius, entretanto,

isso nÃ£o ocorre. Consideremos, entÃ£o, uma matriz ğ‘€ğœ€, para 0 < ğœ€ <
sendo uma pe-
quena variaÃ§Ã£o associada Ã  fraÃ§Ã£o de tempo em que a molÃ©cula permanece num estado,
conforme deï¬nimos a seguir:

1
ğ‘š

ğ‘€ğœ€ =

â›

âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ

2ğœ€

1 âˆ’ 2ğœ€

0
...
...

0

1
ğ‘š

âˆ’ ğœ€

2ğœ€
1 âˆ’ 1
ğ‘š
0

âˆ’ ğœ€

0

0

0

2
ğ‘š

âˆ’ ğœ€

âˆ’ ğœ€

2ğœ€
1 âˆ’ 2
ğ‘š
0

Â· Â· Â·

Â· Â· Â·

Â· Â· Â·

3
ğ‘š

âˆ’ ğœ€

2ğœ€
1 âˆ’ 3
ğ‘š
0

âˆ’ ğœ€

â

âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 

.

0

0
...

0

1 âˆ’ 2ğœ€

2ğœ€

0

0

0

Â· Â· Â·

. . .

1
ğ‘š

âˆ’ ğœ€

CAPÃTULO 3. APLICAÃ‡Ã•ES

65

Com isso, dadas as matrizes quadradas ğ‘€ğ‘‡ e ğ‘€ğœ€ de ordem ğ‘š + 1, deï¬nimos a dis-

tÃ¢ncia entre ğ‘€ğ‘‡ e ğ‘€ğœ€ por

ğ‘‘(ğ‘€ğ‘‡ , ğ‘€ğœ€) := max

1â‰¤ğ‘–,ğ‘—â‰¤ğ‘š+1

| (ğ‘€ğ‘‡ )ğ‘–ğ‘—

âˆ’ (ğ‘€ğœ€)ğ‘–ğ‘—

|,

| â‰¤ 2ğœ€ para todo ğ‘–, ğ‘—. Nessas condiÃ§Ãµes, a distÃ¢n-
donde veriï¬camos que | (ğ‘€ğ‘‡ )ğ‘–ğ‘—
cia ğ‘‘(ğ‘€ğ‘‡ , ğ‘€ğœ€) entre as matrizes ğ‘€ğ‘‡ e ğ‘€ğœ€ torna-se tÃ£o pequena quanto se queira, isto
Ã©,

âˆ’ (ğ‘€ğœ€)ğ‘–ğ‘—

ğ‘€ğœ€ = ğ‘€ğ‘‡ .

lim
ğœ€â†’0

AlÃ©m disso, temos associado Ã  matriz ğ‘€ğœ€ um grafo Î“ (ğ‘€ğœ€) tal que, todas as entradas
de ğ‘€ğœ€ cujos Ã­ndices sÃ£o nÃºmeros consecutivos, sÃ£o positivas. Consequentemente, para
quaisquer dois vÃ©rtices distintos ğ‘–, ğ‘— de Î“ (ğ‘€ğœ€) deve existir uma aresta orientada de ğ‘–
para ğ‘—, e assim, o grafo Î“ (ğ‘€ğœ€) Ã© fortemente conexo e a matriz ğ‘€ğœ€ Ã© irredutÃ­vel, de
sobremaneira que (ğ‘€ğ‘š
ğœ€ )ğ‘–ğ‘— > 0 e a matriz satisfaz as condiÃ§Ãµes do Teorema de Perron-
Frobenius.

ConcluÃ­mos daÃ­ que, como âƒ—ğ‘¢(ğ‘›) = ğ‘€ğ‘‡ âƒ—ğ‘¢(ğ‘›âˆ’1), isto Ã©, a distribuiÃ§Ã£o para um tempo ğ‘›
Ã© calculada recursivamente a partir de ğ‘› âˆ’ 1; existe no equilÃ­brio, uma distribuiÃ§Ã£o âƒ—ğ‘£
com entradas

ğ‘£ğ‘˜ := lim

ğ‘›â†’âˆ ğ‘¢

(ğ‘›)
ğ‘˜ = lim

ğ‘›â†’âˆ ğ‘ƒ (ğ‘‹ğ‘› = ğ‘˜)

qualquer que seja a distribuiÃ§Ã£o inicial âƒ—ğ‘¢.

Dessa forma, obtemos âƒ—ğ‘£ fazendo

(ğ‘€ğ‘‡ ğ‘£)ğ‘˜ = ( lim

ğ‘›â†’âˆ ğ‘€ğ‘‡ ğ‘¢(ğ‘›))ğ‘˜ = ( lim

ğ‘›â†’âˆ ğ‘¢(ğ‘›+1))ğ‘˜ = (ğ‘£)ğ‘˜ = ğ‘£ğ‘˜

e assim, ğ‘€ğ‘‡ âƒ—ğ‘£ = âƒ—ğ‘£, o que mostra que âƒ—ğ‘£ Ã© uma distribuiÃ§Ã£o invariante em relaÃ§Ã£o ğ‘€ğ‘‡ .
Com outras palavras, âƒ—ğ‘£ Ã© o Ãºnico autovetor associado ao autovalor de Perron-Frobenius
ğ‘ƒ ğ¹(ğ‘€ğ‘‡ ) = 1. Portanto, hÃ¡ uma probabilidade de regresso das molÃ©culas para o recipi-
ente ğ´, ainda que para um tempo humanamente impossÃ­vel de ser vivido.

ReferÃªncias

[BH12]

M. Bestvina, and M. Handel, Train tracks and automorphisms of free
groups, Annals of Mathematics (2), vol. 135 (1992), no. 1, pp. 1â€“51.

[Bog08]

Bogopolski, O. Introduction to Group Theory, EMS: European Mathema-
tical Society, English edition; Dortmund, Germany 2008.

[Bol78]

Boldrini, J. L.; Costa, S. I. R.; Ribeiro, V. L. F. F.; Wetzler, H. G. Ãlgebra
Linear - Harper & Row do Brasil, SÃ£o Pulo, 1Âªed. 1978.

[Dia09]

Diaconis, P. The Markov chain Monte Carlo revolution, Bulletin of the
AMS, Vol. 46, 2009, p. 179â€“205.

[Doo53] Doob, J.L., Stochastic Processes. New York, John Wiley, 1953.

[EGNO15] Etingof, P.; Gelaki, Sh.; Nikshych, D.; Ostrik, V. Tensor categories, Mathe-
matical Surveys and Monographs, Vol. 205, American Mathematical Soci-
ety, 2015.

[Eur02]

Revista Eureka! nÂº13, p. 41-44, Sociedade Brasileira de MatemÃ¡tica, 2002.

[Fel67]

Feller, W. An Introduction to Probability Theory and Its Applications.
JOHN WILEY & SONS, New York, 3nd Ed.,1967.

[GM18]

Google Matrix: fundamentals, applications and beyond; Workshop, 15-
18 October 2018, https://indico.math.cnrs.fr/event/3475/overview.

[Jam15]

James, B. R. Probabilidade: Um Curso em NÃ­vel IntermediÃ¡rio, Projeto
Euclides, IMPA, Rio de Janeiro, 2015.

[Lim04]

Lima, E. L. Ãlgebra Linear, ColeÃ§Ã£o MatemÃ¡tica UniversitÃ¡ria, IMPA, 7Âª
ed., Rio de Janeiro, 2004.

[Mag15] MagalhÃ£es, M. N. Probabilidade e VariÃ¡veis AleatÃ³rias. EDUSP: Editora
da Universidade de SÃ£o Paulo, SÃ£o Paulo, 3Âª EdiÃ§Ã£o, 2Âª reimpressÃ£o, 2015.

[MCCF06] Morgado, A. C.; de Carvalho, J. B. P.; Carvalho, P. C. P.; e Fernandez, P.
AnÃ¡lise CombinatÃ³ria e Probabilidade, SBM; ColeÃ§Ã£o do Professor de Ma-
temÃ¡tica, 9Âª ed., Rio de Janeiro, 2006.

[PY98]

Pollicott, M.; Yuri, M. Dynamical Systems and Ergodic Theory, London
Mathematical Society Student Texts 40, Cambridge University Press.

68

[Ser02]

[Za93]

REFERÃŠNCIAS

Serre, D. Matrices : Theory and Applications, Springer-Verlag New York,
2002.

Zalesskii, A.E. Linear Groups, Em Algebra IV, Encyclopaedia of Mathema-
tical Sciences, Vol. 37, Springer-Verlag, 1993.

