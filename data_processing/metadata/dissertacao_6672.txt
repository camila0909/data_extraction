Programa de Pós-Graduação em
Matemática em Rede Nacional

A INVERSA GENERALIZADA DE UMA
MATRIZ E APLICAÇÕES

José Norberto Reinprecht

Rio Claro - SP
2022

Universidade Estadual Paulista “Júlio de Mesquita Filho”
Instituto de Geociências e Ciências Exatas
Câmpus de Rio Claro

A INVERSA GENERALIZADA DE
UMA MATRIZ E APLICAÇÕES

José Norberto Reinprecht

Dissertação apresentada como parte dos requisitos para
obtenção do título de Mestre em Matemática, junto ao
Programa de Pós-Graduação – Mestrado Profissional em
Matemática em Rede Nacional, do Instituto de Geociên-
cias e Ciências Exatas da Universidade Estadual Paulista
“Júlio de Mesquita Filho”, Câmpus de Rio Claro.

Orientadora
Profa. Dra. Marta Cilene Gadotti

Rio Claro - SP
2022

R373iReinprecht, José Norberto    A inversa generalizada de uma matriz e aplicações / José NorbertoReinprecht. -- Rio Claro, 2022    106 p.    Dissertação (mestrado profissional) - Universidade EstadualPaulista (Unesp), Instituto de Geociências e Ciências Exatas, RioClaro    Orientadora: Marta Cilene Gadotti    1. Matemática Aplicada. 2. Decomposição em Valores Singulares.3. Pseudo-inversa de uma matriz. 4. Sistemas Lineares. 5. Métodosdos Mínimos Quadrados. I. Título.Sistema de geração automática de fichas catalográficas da Unesp. Biblioteca do Instituto deGeociências e Ciências Exatas, Rio Claro. Dados fornecidos pelo autor(a).Essa ficha não pode ser modificada.TERMO DE APROVAÇÃO

José Norberto Reinprecht

A INVERSA GENERALIZADA DE UMA MATRIZ E
APLICAÇÕES

Dissertação aprovada como requisito parcial para a obtenção do
grau de Mestre no Curso de Pós-Graduação – Mestrado Profissional
em Matemática em Rede Nacional, do Instituto de Geociências e Ci-
ências Exatas da Universidade Estadual Paulista “Júlio de Mesquita
Filho”, pela seguinte banca examinadora:

Profa. Dra. Marta Cilene Gadotti
Orientadora

Profa. Dra. Carina Alves Severo
IGCE/UNESP/Rio Claro (SP)

Prof. Dr. Luciano Aparecido Magrini
IFSP/Votuporanga (SP)

Rio Claro, 14 de outubro de 2022

Aos meus pais (em memória).

Agradecimentos

Em primeiro lugar agradecer a minha orientadora, Profa. Dra. Marta Cilene Gadotti,
pela sua disponibilidade, apoio e incentivo. Pela paciência e inestimável ajuda durante
todo o desenvolvimento deste trabalho.

Ao professor Thiago e ao colega do curso André, pelo auxílio na parte computacional

durante a realização deste trabalho.

Aos professores que aceitaram participar da banca examinadora deste trabalho.
Aos professores do Profmat pela dedicação em suas aulas e pelos incentivos na busca

de qualificação.

Aos colegas do curso e a todos que, diretamente ou indiretamente, contribuíram para

a realização desta dissertação de Mestrado.

E finalmente, a minha esposa pela compreensão da minha ausência em vários momen-

tos durante estes últimos anos.

A todos a minha gratidão.

Se você quer ser bem sucedido, precisa ter dedicação total, buscar seu último limite e
dar o melhor de si.
Ayrton Senna.

Resumo

A resolução de sistemas lineares tem aplicação nos mais diversos campos da ciência.
Em geral, as resoluções de sistemas lineares envolvendo a inversa de uma matriz estão
restritas às matrizes quadradas e não singulares. O objetivo deste trabalho é abordar
a busca de soluções de sistemas lineares envolvendo matrizes não quadradas ou mesmo
matrizes quadradas mas singulares, através do uso das inversas de Moore-Penrose, também
denominadas de pseudo-inversas.

Palavras-chave: Matemática Aplicada. Decomposição em Valores Singulares. Pseudo-
inversa . Método dos Mínimos Quadrados. Sistemas Lineares .

Abstract

The resolution of linear systems has application in the most diverse fields of science.
In general, the resolutions of linear systems involving the inverse of a matrix are restricted
to square and non-singular matrices. The objective of this work is to approach the search
for solutions of linear systems involving non-square matrices or even square but singu-
lar matrices, through the use of inverse Moore-Penrose matrices, also known as pseudo
inverses.

Keywords: Applicated Math. Singular Value Decomposition. Pseudo-inverse. Minimum
Squares Method. Linear Sistems.

Sumário

1 Introdução

17

2 Preliminares

19
. . . . . . . . . . . . . . . . . . . 19
2.1 Um breve relato sobre a Álgebra Linear.
2.2 Espaços vetoriais
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
2.3 Base, dimensão e coordenadas . . . . . . . . . . . . . . . . . . . . . . . . . 25
2.4 Produto interno e ortogonalidade . . . . . . . . . . . . . . . . . . . . . . . 29
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
2.5 Transformações lineares

3 Teoria Espectral

43
3.1 Autovalores e autovetores de uma transformação linear . . . . . . . . . . . 43
3.2 Autovalores e autovetores de uma matriz . . . . . . . . . . . . . . . . . . . 44
. . . . . . . . . . . . . . . . . . . . . . . . . 53
3.3 Diagonalização de operadores

4 Decomposição em Valores Singulares

65
4.1 Um breve relato histórico . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
4.2 A DVS de uma matriz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66

5 A Inversa Generalizada

75
5.1 Caracterização da inversa generalizada de uma matriz . . . . . . . . . . . . 75
5.2 Algoritmo de obtenção de uma inversa generalizada . . . . . . . . . . . . . 77
5.2.1 Algoritmo de Searle . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
5.3 A inversa de Moore-Penrose . . . . . . . . . . . . . . . . . . . . . . . . . . 79
. . . . . . . . . . . . . . . . . . . . . . . 82
5.4 Obtenção de uma pseudo-inversa.
. . . . . . . . . . . . . . . 87
5.5 Algoritmos de obtenção de uma pseudo-inversa.
5.5.1 Algoritmo de Penrose.
. . . . . . . . . . . . . . . . . . . . . . . . . 87
5.5.2 Algoritmo de Greville . . . . . . . . . . . . . . . . . . . . . . . . . . 90

6 Aplicações da Inversa Generalizada

95
6.1 A inversa generalizada na resolução de sistemas lineares . . . . . . . . . . . 95
. . . . . . . . . . . . . . . . . . . . . . . . 98
6.2 Solução de mínimos quadrados.

7 Considerações Finais

Referências

103

105

1 Introdução

O objetivo deste trabalho consiste no estudo da inversa de Moore-Penrose ou pseudo-
inversa de uma matriz, e algumas aplicações na resolução de sistemas lineares. A escolha
deste tema teve por motivação a ausência de tal assunto nos conteúdos dos cursos de
graduação do Ensino Superior. Além disso, o tema pode ser explorado no Ensino Médio
como está descrito nas considerações finais.

O texto apresentado está desenvolvido de modo objetivo e didático. Dentre as apli-
cações, o leitor tomará conhecimento da existência: de inversas de matrizes retangulares,
de inversas de matrizes quadradas e singulares e de sistemas lineares inconsistentes apre-
sentando solução (uma melhor solução) assuntos não estudados em cursos regulares de
graduação.

O conteúdo deste trabalho está organizado na seguinte sequência:
Capítulo 1. Preliminares - apresenta uma revisão de conceitos básicos da Álgebra
Linear, indispensáveis ao desenvolvimento deste trabalho. E por se tratar de uma revisão
deixaremos de apresentar as provas dos teoremas e proposições. Em algumas situações os
conceitos são ilustrados através de exemplos.

Capítulo 2. Teoria Espectral - apresenta o estudo de autovalores e autovetores de
um operador linear, suas propriedades e resultados importantes (dando uma particular
importância à matriz associada ao operador linear); a sua aplicação na diagonalização de
matrizes; o conceito de operador autoadjunto, e finaliza com a demonstração do chamado
Teorema Espectral, um importante resultado envolvendo operador autoadjunto.

Capítulo 3. Decomposição em Valores Singulares (DVS) - apresenta de ma-
neira breve e prática a decomposição de qualquer matriz real (trabalho restrito somente
a estas matrizes), no produto de uma matriz ortogonal, uma matriz diagonal e uma ou-
tra matriz ortogonal. A DVS é de fundamental importância no estudo da inversa de
Moore-Penrose de uma matriz.

Capítulo 4. A Inversa Generalizada de uma Matriz - apresenta o estudo da
inversa generalizada de uma matriz, e em particular, uma das mais importantes inversas
generalizadas, a de Moore-Penrose ou pseudo-inversa. Contém um algoritmo de obtenção
de uma inversa generalizada,e também outros dois algoritmos algébricos e iterativos para
a determinação da pseudo-inversa de uma matriz.

Capítulo 5. Aplicações da Inversa Generalizada - apresenta o uso da inversa
generalizada em situações, tais como: em sistemas de equações lineares Ax = b, sendo
A uma matriz retangular ou quadrada e singular, na obtenção da solução de um sistema
linear, e no cálculo de uma solução de "melhor ajuste" (mínimos quadrados) para um
sistema de equações lineares inconsistentes.

Capítulo 6. Considerações Finais - apresenta relatos de alguns pontos relevantes

que devem ser destacados.

17

2 Preliminares

Neste capítulo trazemos as definições e resultados clássicos da Álgebra Linear necessá-
rios ao entendimento dos capítulos seguintes. O leitor poderá encontrar as demonstrações
dos teoremas e proposições que foram omitidas nas referências [1], [2], [3], [4], [5], [6] e
[13] que foram utilizadas para a elaboração do capítulo.

2.1 Um breve relato sobre a Álgebra Linear.

A Álgebra Linear é uma das Àreas mais importantes, versáteis e úteis da matemá-
tica, e tem se tornado nos últimos anos uma parte essencial da base matemática de que
necessitam matemáticos, engenheiros, físicos e outros cientistas. Na Matemática, sua
importância dificilmente pode ser subestimada, quando se compreende que é impossível
atacar qualquer problema sem perfeita compreensão dos fenômenos lineares.

A Álgebra Linear surgiu através do estudo detalhado de sistemas de equações lineares
algébricas ou diferenciais, e se utiliza de alguns conceitos e estruturas fundamentais da
matemática como vetores, espaços vetoriais, sistemas de equações lineares, matrizes e
transformações lineares.

A ideia abstrata de Espaços Vetoriais generaliza o conceito de vetores nos espaços
bidimensional e tridimensional de duas maneiras. Primeiro, espaços vetoriais pode ter
dimensão maior que 3. E segundo, definimos espaços vetoriais não só apenas para vetores
do plano ou do espaço mas com diferentes objetos matemáticos, por exemplo números,
matrizes, polinômios, funções.

Na sua forma atual, a Álgebra Linear começou a ser estudada em meados do século
XIX, evoluiu ao longo de muitos anos e teve a contribuições de várias pessoas. Apesar
do crédito ser dado, geralmente, ao matemático alemão H. Grassmann, como sendo o
primeiro a introduzir a idéia de Espaço Vetorial em 1862, devemos mencionar também a
contribuição do matemático italiano Giuseppe Peano, que em seu livro "Cálculo Geomé-
trico" tornou claro o trabalho de Grassmann e descreveu as propriedades para um Espaço
Vetorial da maneira como hoje o conhecemos. [5]

2.2 Espaços vetoriais

Os espaços vetoriais são um dos mais importantes exemplos de estruturas algébricas
(nome dado a um conjunto com algumas operações definidas sobre ele satisfazendo cer-
tas propriedades) e como a definição envolve a estrutura de um corpo, recordemos o
significado deste conceito.

19

20

Preliminares

Definição 2.1. Um corpo consiste de um conjunto não vazio munido de duas operações,
, que satisfazem as propriedades a seguir:
denotadas por + e ·

(a) As duas operações são comutativas;
(b) As duas operações são associativas;
(c) Vale a distributiva de · sobre + ;
(d) Existem elementos neutros 0 para a adição e 1 para a multiplicação;
(e) Todo elemento diferente de 0 tem inverso multiplicativo;

(Q, +, ·) , (R, +, ·) e (C, +, ·) são exemplos de corpos, pois os conjuntos dos racionais,
reais ou complexos com as operações usuais de adição e multiplicacão satisfazem todas as
propriedades citadas acima.

A definição de um espaço vetorial envolve um conjunto não vazio, um corpo arbitrário
K (daqui em diante, K = R ou K = C) e duas operações, denominadas de adição e
multiplicação por um escalar, satisfazendo algumas propriedades.

Definição 2.2. Um espaço vetorial V sobre o corpo K é um conjunto não vazio V
munido de duas operações:

Adição : ” + ”

Multiplicação : ” · ”

V × V → V .
(u, v)

7→ u + v.

K × V → V .
7→ av.
(a, v)
Estas operações devem satisfazer os axiomas a seguir:

Adição
A1 : u + v = v + u , ∀u, v ∈ V.
A2 : (u + v) + w = u + (v + w) , ∀u, v, w ∈ V.
A3 : Existe um vetor em V , denotado por 0, chamado de vetor nulo, tal que qualquer

que seja u em V , temos u + 0 = u.

A4 : Para todo vetor u ∈ V , existe um vetor, denotado por −u tal que u + (−u) = 0.

Multiplicação por escalar
M1 : (α.β)u = α.(β.u) , ∀α, β ∈ K e u ∈ V.
M2 : α.(u + v) = αu + αv , ∀α ∈ K e u, v ∈ V.
M3 : (α + β)u = αu + βu , ∀α, β ∈ K e u ∈ V.
M4 : Dado 1 ∈ K , então vale 1.u = u , ∀u ∈ V.
Os elementos do espaço vetorial V são chamados de vetores; e os do corpo K (podendo

ser R ou C) de escalares.

Quando consideramos o corpo dos escalares como sendo os reais, dizemos que (V, +, ·)
é um espaço vetorial real; no caso do corpo dos escalares ser os números complexos,
dizemos que (V, +, ·) é um espaço vetorial complexo.

A seguir apresentaremos alguns de exemplos de espaços vetoriais.

Exemplo 2.3. O conjunto Rn de todas n-uplas ordenadas de números reais, com as
operações usuais de adição e multiplicação por um escalar definidas por:

Adição : a + b = (a1, . . . , an) + (b1, . . . , bn) = (a1 + b1, · · · , an + bn) , ∀a, b ∈ Rn.
Multiplicação : λa = λ(a1, . . . , an) = (λa1, . . . , λan) , ∀a ∈ Rn e ∀λ ∈ R.

Espaços vetoriais

21

Exemplo 2.4. O conjunto Mm×n(R) das matrizes reais m × n com as operações
usuais de adição de matrizes e produto de um escalar por uma matriz, definidas abaixo:

Adição: A + B = (aij) + (bij) = (aij + bij) , ∀A, B ∈ Mm×n(R).
Multiplicação: k.A = k(aij) = (kaij) , ∀A ∈ Mm×n(R)

e ∀ k ∈ R.

Exemplo 2.5. O conjunto Pn(R) de todos os polinômios p(t) = a0 + a1t + · · · + antn
com coeficientes reais de grau menor ou igual a n (só igual a n não conteria o elemento
neutro da adição representado pelo polinômio nulo), com as operações usuais de adição
de polinômios e a multiplicação de um escalar por um polinômio.

É possível que um espaço vetorial esteja contido num outro espaço vetorial. Recorde-
mos como reconhecer dentro de um espaço vetorial V subconjuntos W que sejam também
espaços vetoriais.

Definição 2.6. Sejam V um espaço vetorial e W um subconjunto, não vazio, de V . Se
W é um espaço vetorial com as operações de adição e multiplicação por escalar definidas
em V , então dizemos que W é um subespaço vetorial de V .

Em geral, devemos verificar todos axiomas de espaço vetorial para mostrar que um
conjunto W com duas operações formam um espaço vetorial. Entretanto, se W é um
subconjunto do espaço vetorial V então alguns axiomas não precisam ser verificados, pois
estes "herdam" de V . Entretanto, é necessário verificarmos que W é fechado em relação
às duas operações definidas, pois é possível que a soma de dois vetores em W , ou a
multiplicação de um vetor de W por um algum escalar possa produzir um vetor que não
esteja em W .

O teorema a seguir estabelece um critério simples de identificarmos quando um sub-

conjunto W ⊆ V é um subespaço do espaço vetorial V .

Teorema 2.7. Seja V um espaço vetorial sobre um corpo K. Um subconjunto W é
subespaço de V se, e somente se,

(a) W é não vazio.
(b) Quaisquer que sejam u, v ∈ W , então u + v ∈ W .
(c) Para todo u ∈ W , temos que ku ∈ W , para todo k ∈ K.

As condições (b) e (c) garantem que W é fechado em relação as operações de adição

e multiplicação por escalar.

Corolário 2.8. W é um subespaço de V se, e somente se,

(a) 0 ∈ W ( ou seja, W ̸= ϕ ).
(b) v, w ∈ W implica av + bw ∈ W , para todo a, b ∈ K .

Utilizando o Corolário 1.8 podemos verificar facilmente que os subconjuntos definidos

nos exemplos a seguir constituem subespaços vetoriais.

Exemplo 2.9. O conjunto formado somente pelo vetor nulo e o próprio espaço vetorial
V são subespaços de V . Portanto, todo espaço vetorial admite pelo menos estes dois
subespaços chamados subespaços triviais.

22

Preliminares

Exemplo 2.10. Os conjuntos WS = { A ∈ Mn(R) : At = A } das matrizes reais simé-
tricas de ordem n e WS′ = { A ∈ Mn(R) : At = −A } das matrizes reais antissimétricas
de ordem n são subespaços do espaço vetorial Mn(R) das matrizes quadradas reais de
ordem n.



Exemplo 2.11. Consideremos o sistema linear homogêneo de m equações e n incógnitas
a11x1 + a12x2 + · · · + a1nxn = 0
a21x1 + a22x2 + · · · + a2nxn = 0
...
am1x1 + am2x2 + · · · + amnxn = 0
O conjunto W de todas as soluções deste sistema é um subespaço do espaço vetorial




.

do Rn, chamado espaço das soluções.

Teorema 2.12. Se S1 e S2 são subespaços vetoriais de um espaço vetorial V , então
S1 ∩ S2 é um subespaço de V .

Corolário 2.13. Seja V um espaço vetorial. Então, a intersecção de um conjunto arbi-
trário de subespaços de V é um subespaço vetorial de V .

Exemplo 2.14. Sejam W1, W2 e W3 subespaços do espaço vetorial Mn(R) das matrizes
reais de ordem n, definidos abaixo:

W1 : o subespaço vetorial das matrizes triangulares superiores de ordem n.
W2 : o subespaço vetorial das matrizes triangulares inferiores de ordem n.
W3 : o subespaço vetorial das matrizes diagonais de ordem n.
Verificamos facilmente que: W1 ∩ W2 = W3.

Como a intersecção de subespaços é sempre um subespaço, seria previsível admitir-
mos que a união de subespaços também seria um subespaço, mas isto, em geral, não é
verdadeiro, como mostra o enunciado do teorema a seguir.

Teorema 2.15. Sejam S1 e S2 subespaços vetoriais de V . Então, S1 ∪ S2 é subespaço
vetorial de V se, e somente se, S1 ⊂ S2 ou S2 ⊂ S1.

Definição 2.16. Sejam W1 e W2 subespaços vetoriais de um espaço vetorial V . Definimos
a soma de W1 e W2 como sendo

W1 + W2 = { w1 + w2 : w1 ∈ W1 e w2 ∈ W2 }.

Teorema 2.17. Sejam W1 e W2 subespaços de um espaço vetorial V . Então,

(a) W1 + W2 é um subespaço de V .
(b) W1 + W2 é o menor subespaço de V contendo W1 e W2.
(c) W1 ∪ W2 ⊂ W1 + W2.

Definição 2.18. Sejam W1 e W2 subespaços vetoriais de um espaço vetorial V . Dizemos
que:

(a) w1 + w2 é soma direta quando W1 ∩ W2 = {0}, e denotamos por W1 ⊕ W2.
(b) O espaço vetorial V é a soma direta dos subespaços W1 e W2 quando V = W1+W2

e W1 ∩ W2 = {0}.

Espaços vetoriais

23

Teorema 2.19. Sejam W1 e W2 subespaços vetoriais de um espaço vetorial V . Então,
V = W1 ⊕ W2 se, e somente se, todo elemento v ∈ V se escreve de maneira única na
forma v = w1 + w2, com w1 ∈ W1 e w2 ∈ W2.

Exemplo 2.20. Sejam W1 =

#

( " a b
0 0

)

: a, b ∈ R

e W2 =

#

( " 0 b
0 d

)

: b, d ∈ R

subespaços do espaço vetorial M2(R).
#

Então, W1 + W2 =

; a, b, d ∈ R

( " a b
0 d

)

, ou seja, consiste do conjunto das

matrizes cujo elemento da segunda linha e primeira coluna é igual a 0.

Portanto, o espaço vetorial M2(R) não é soma direta dos subespaços W1 e W2.

Exemplo 2.21. O espaço vetorial R3 é soma direta dos subespaços

W1 = {(a, b, c) ∈ R3 : b = c = 0}

e W2 = { (a, b, c) ∈ R3 : a + b + c = 0 }.

Exemplo 2.22. O espaço vetorial Mn(R) das matrizes reais de ordem n é a soma
direta dos subespaços WS e WS′ das matrizes simétricas e antissimétricas de ordem n,
respectivamente.

Exemplo 2.23. Sejam W1 , W2 ⊂ Mn(R),
sendo W1 o subespaço das matrizes
triangulares superiores e W2 o subespaço das matrizes triangulares inferiores. Então,
Mn(R) = W1 + W2. Mas, não é uma soma direta, pois W1 ∩ W2 é o subespaço formado
pelas matrizes diagonais (veja Exemplo 1.14.). Portanto, U ∩ W ̸= {0}.

A seguir recordemos as definições de combinação, dependência e independência linear,
usados para construir os conceitos de base e de coordenadas de vetores em diferentes
bases.

Definição 2.24. Seja V um espaço vetorial sobre um corpo K . Um vetor v ∈ V
é uma combinação linear dos vetores v1, v2,
. . . , vn de V , se existirem escalares
a1, a2, . . . , an ∈ K , também chamados de coeficientes da combinação linear , tais que
v = a1v1 + a2v2 + · · · + anvn .

Definição 2.25. Seja V um espaço vetorial sobre um corpo K . Um conjunto, não
vazio, de vetores {v1, v2, . . . , vn} ⊂ V é chamado de linearmente dependente sobre K,
e denotamos simplificadamente por LD, se existirem a1, a2, . . . , an ∈ K, nem todos nulos,
tais que a1v1 + a2v2 + · · · + anvn = 0.
Caso contrário, dizemos que o conjunto de vetores é linearmente independentes, e
denotamos simplificadamente por LI.

As definições acima equivalem a afirmar que: se a equação

a1v1 + a2v2 + · · · + anvn = 0.
admite somente a solução trivial a1 = a2 = · · · = an = 0, então o conjunto de vetores
v1, v2, . . . , vn é LI. Caso existam outras soluções além da trivial, o conjunto de vetores
é LD.

24

Preliminares

Proposição 2.26.
(a) Todo conjunto finito de vetores de um espaço vetorial que contém o vetor nulo é

linearmente dependente.

(b) O conjunto {v} formado por um único vetor, não nulo, de um espaço vetorial é

linearmente independente.

(c) Um conjunto formado por dois vetores {v1, v2} de um espaço vetorial é linearmente

dependente se, e somente se, um deles é múltiplo escalar do outro.

(d) Se {v1, . . . , vk} é um conjunto linearmente dependente, então qualquer conjunto finito

contendo v1, . . . , vk é também LD.

Teorema 2.27. Seja S = {v1, v2, . . . , vn} um conjunto de vetores de um espaço vetorial
V , com n ≥ 2. Então,
(a) S é LD se, e somente se, pelo menos um dos vetores de S pode ser escrito como

uma combinação linear dos demais vetores em S.

(b) Se S é LI, então qualquer subconjunto de S também é LI.
(c) Se S é LI e S ∪ {v} é LD, então v é uma combinação linear dos vetores de S.

Teorema 2.28. Seja S = {v1, v2, . . . , vn} um subcconjunto finito do espaço vetorial V .
O conjunto formado por todas as combinações lineares de S, que denotamos por h
i, é
S
um subespaço de V .

h

i = { a1v1 + a2v2 + · · · + anvn ; ai ∈ K }.

S

Corolário 2.29. O subespaço h

S

i é o menor subespaço de V que contém S.

Em outras palavras, se W é um outro subespaço de V contendo S, então h

S

i

⊂ W .

Definição 2.30. Sejam V um espaço vetorial e S = {v1, v2, . . . , vn} um subconjunto
i das combinações lineares dos vetores do conjunto S é chamado
de V . O subespaço h
S
de subespaço gerado por S, ou que, v1, v2 , . . . , vn geram o subespaço h
S

i.

Os elementos v1, v2, . . . , vn de S são chamados geradores de [S], ou que S é o

conjunto gerador de [S].

Definição 2.31. Um espaço vetorial V é finitamente gerado se existir um subcon-
junto finito S ⊂ V tal que h

i = V .

S

Por conveniência definimos que, se S = ∅ , então h

i = {0}.

∅

Exemplo 2.32. O conjunto S = { (1, 0, 0) , (0, 1, 0) } gera o subespaço

W = { (a, b, 0) : a, b ∈ R } ⊂ R3.

Exemplo 2.33. O subespaço W = {A ∈ M2(R) : At = A} das matrizes simétricas é um
subespaço de M2(R) gerado pelo subconjunto

S =

( " 1
0

0
0

#

" 0
0

,

0
1

#

" 0
1

,

# )
.

1
0

Base, dimensão e coordenadas

25

Teorema 2.34. Seja S = { v1, v2, . . . , vn } um conjunto de vetores LI em um espaço
i se escreve de maneira única como combinação
vetorial V . Então, cada vetor v ∈
linear dos vetores S.

S

h

Teorema 2.35. Seja V um espaço vetorial tal que V = h
conjunto com mais de n vetores em V é LD.

v1, v2, . . . , vn

i . Então, todo

Assim, todo conjunto de vetores LI em V possui no máximo n vetores.

2.3 Base, dimensão e coordenadas

O conceito de base de um espaço vetorial consiste em escolhermos um conjunto, com o
menor número de vetores, que possam gerar todo o espaço vetorial. Assim, caso retiremos
um vetor qualquer desse conjunto, os vetores restantes não geram mais o espaço todo.

Dimensão, usualmente, associamos a algo geométrico; mas, através do conceito de base

podemos dar uma definição algébrica para dimensão de um espaço vetorial.

Definição 2.36. Seja V ̸= ∅ um espaço vetorial sobre um corpo K. Uma base de V é
um subconjunto B de vetores linearmente LI que gera V .

Um espaço vetorial V ̸= {0V } sempre possui uma infinidade de bases diferentes, pois
se {v1, v2, . . . , vn} é uma base de V , então {αv1, αv2, . . . , αvn} com α ∈ K também
é uma base de V .

Teorema 2.37. Seja B = {v1, v2, . . . , vn} uma base de um espaço vetorial V . Então,
todo vetor em V pode ser escrito de maneira única como combinação linear de B.

Teorema 2.38. Seja B = {v1, v2, . . . , vn} uma base de um espaço vetorial V . Então,
qualquer conjunto com mais de n vetores é LD.

Este teorema garante que qualquer conjunto de n+1 ou mais vetores é LD, e portanto,
não forma uma base de V . Por outro lado, qualquer conjunto com n − 1 ou menos vetores
não é suficiente para gerar V , e então, também não é uma base de V .

Teorema 2.39. Se V um espaço vetorial gerado pelo conjunto S = {v1, v2, . . . , vn} ,
então podemos extrair do conjunto S uma base de V .

Definição 2.40. Seja V um espaço finitamente gerado. Definimos dimensão de V , e
denotamos por dim V ou dimKV para indicar também o corpo, como sendo o número
de vetores de uma base qualquer de V .

No caso de V = { 0 }, dizemos que o espaço vetorial V tem dimensão nula. E quando

um espaço vetorial V não é finitamente gerado, dizemos que possui dimensão infinita.

Teorema 2.41. Se V é um espaço vetorial de dimensão n. Então,

(a) qualquer base de V possui n elementos.
(b) todo conjunto de n vetores LI forma uma base de V .

26

Preliminares

Teorema 2.42. (Teorema do complementamento da base) Seja V um espaço vetorial
de dimensão n. Se os vetores u1, u2, . . . , uk
são linearmente independentes em V , com
k < n. Então existem vetores uk+1, . . . , un em V , tais que u1, . . . , uk, uk+1, . . . , un
formam uma base de V .

Exemplo 2.43. O conjunto B = { (1, 0, 0), (0, 1, 0, ), (0, 0, 1) } é linearmente indepen-
dente e gera o espaço R3. Portanto, B é uma base de R3, denominada base canônica, e
dim R3 = 3.

Exemplo 2.44. Consideremos o espaço vetorial real M2(R) das matrizes quadradas

reais de ordem 2. O conjunto B =

#

( " 1 0
0 0

#

" 0 1
0 0

,

#

" 0 0
1 0

,

" 0 0
0 1

,

# )

é

linearmente independente, gera M2(R).

Logo, B é uma base de M2(R), denominada canônica, e dim M2(R) = 4.

Teorema 2.45. Se W é um subespaço de um espaço vetorial V e dim V = n, então
dimW ≤ n.

No caso da dim W = n, então W = V .

Teorema 2.46. Sejam U e W subespaços de um espaço vetorial V de dimensão finita,
então

dim (U + W ) = dim U + dim V − dim (U ∩ V ).

Teorema 2.47. Sejam V um espaço vetorial de dimensão finita e W um subespaço de
V . Então, existe um subespaço U de V tal que V = U ⊕ W .

Definição 2.48. Uma base ordenada de espaço vetorial V é uma base em que é
considerada a ordem dos vetores da base.

Definição 2.49. Sejam V um espaço vetorial finitamente gerado e B uma base ordenada
de V formada pelos vetores v1, v2, . . . , vn. Todo vetor em V se escreve de maneira única
(T eorema 1.37) como v = c1v1 + c2v2 + · · · + cnvn. Os escalares
são
chamados coordenadas de v em relação à base B, e denotamos por


c1, c2, . . . , cn



[v]B =







= h

c1 c2 . . . cn

iT .







c1
c2
...
cn

Dada uma base de um espaço vetorial, uma troca na ordem em que escrevemos esses
vetores, acarreta uma troca na ordem das coordenadas, produzindo um vetor de coorde-
nadas diferentes. Então, para evitar tal situação introduzimos a convenção de ordenar a
base.

Exemplo 2.50. Consideremos a base B = { (1, 1, 1), (1, 1, 0), (1, 0, 0) } do R3. Deter-
minemos as coordenadas do vetor (2, 1, 4) com relação à base B.

Devemos obter a , b e c tais que

(2, 1, 4) = a(1, 1, 1) + b(1, 1, 0) + c(1, 0, 0) = (a + b + c , a + b , a).

Base, dimensão e coordenadas

27

Equivale ao sistema linear





a + b + c = 2
a + b = 1
a = 4

cuja solução é a = 4 , b = −3 e c = 1.

Desse modo, as coordenadas de (2, 1, 4) com relação à base B são dadas por (4, −3 , 1).











2
1
4

=






4
−3
1






B

ou (2, 1, 4)T = (4, −3, 1)T
B

.

Uma base conveniente para um determinado problema pode não ser conveniente para
um outro, de forma que um procedimento comum no estudo de Espaços Vetoriais é a mu-
dança de uma base para outra. Nesta seção recordaremos problemas relativos à mudança
de base.

Consideremos B = {v1, v2, ..., vn} e B′ = {u1, u2, ..., un} duas bases ordenadas de um
mesmo espaço vetorial V . Um vetor v em V pode ser escrito como combinação linear de
B e B ′.

Sejam v = x1v1 + x2v2 + · · · + bnxn

e v = y1u1 + y2u2 + · · · + ynun as representações

de v como combinação linear dos vetores das duas bases B e B ′, respectivamente .

Então,









i

h

v

B

=









e

i

h

v

B′

=









x1
x2
...
xn





.




y1
y2
...
yn

.
Como os vetores de B ′ são vetores de V , e B é uma base de V , então os vetores de B ′

se escrevem como combinação linear de B.

u1 = a11v1 + a21v2 + · · · + an1vn.
u2 = a12v1 + a22v2 + · · · + an2vn.
...
un = a1nv1 + a2nv2 + · · · + annvn.

v = y1u1 + y2u2 + · · · + ynun.

Assim,
Logo, v = y1(a11v1 +· · ·+an1vn)+y2(a12v1 +· · ·+an2vn)+· · ·+yn(a1nv1 +· · ·+annvn).
Mas,

v = x1v1 + x2v2 + · · · + bnxn.




x1 = a11y1 + a12b2 + · · · + a1nbn
x2 = a21y1 + a22y2 + · · · + a2nyn
...
xn = an1y1 + an2y2 + · · · + annyn

Então,

.

(1.1)

Portanto, podemos escrever que

h

v

i

B

=









x1
x2
...
xn

sendo a matriz h

M

i
B→B′

















=







a11 a12 . . . a1n
a21 a22 . . . a2n
...
an1 an2 . . . ann

y1
y2
...
yn
a transposta da matriz dos coeficientes do sistema (1.1).

B→B′ .

i
B′

= h



















M

v

i

h

,

28

Preliminares

Definição 2.51. A matriz h
é denominada matriz mudança de base B para
a base B ′. Se B e B′ são bases ordenadas de um espaço vetorial V , então para todo v
em V temos

i
B→B′

M

h

i

v

= h

M

i

B

B→B′ .

h

v

i
B′

.

A matriz h

i

M

B→B′

podemos expressa-la em termos de vetores colunas, em que as

colunas desta matriz são os vetores da base atual em relação à nova base, isto é

h

M

i
B→B′

= h [ u1 ]B [ u2 ]B · · ·

[ un ]B

i.

Exemplo 2.52. Determinemos a matriz mudança da base B para a base B ′ sendo
e B ′ = { v1 = (2, 1) , v2 = (3, 2) } bases de R2, e
B = { u1 = (1, 0) , u2 = (0, 1) }
também as coordenadas do vetor v = (1, −2)B na base B em relação à base B ′.

a) Matriz mudança de base B para B ′:

h
M

i
B→B′

= h [u1]

B′

[u2]

B′

i .

Escrevendo os vetores da base B como combinação linear dos vetores de B ′, temos:

(1, 0) = a11(2, 1) + a21(3, 2) = ( 2a11 + 3a21 , a11 + 2a21 ).
(0, 1) = a12(2, 1) + a22(3, 2) = ( 2a12 + 3a22 , a12 + 2a22 ).

Resolvendo os sistemas:











2a11 + 3a21 = 1
a11 + 2a21 = 0

2a12 + 3a22 = 0
a12 + 2a22 = 1

, obtemos a11 = 2 e a21 = −1.

, obtemos a12 = −3 e a22 = 2 .

Portanto,

h

M

i
B→B′

= h [u1]

B′

[u2]

B′

i =

" 2 −3
2
−1

#
.

b) Obtenção das coordenadas do vetor v = (1, −2)B na base B, para a base B ′:
i

h

i

h

v

B′
2 −3
2

−1

= h

M

# "

1
−2

B→B′ .
"
#

=

v

i
B′

.

2 + 6
−1 − 4

"

#

=

#

.

8
−5

"

=

i

h

v

B′

Portanto,

vB′ = (8, −5)T .

Teorema 2.53. Se M é a matriz mudança de base B para uma base B ′ de um espaço
vetorial V de dimensão finita, então M é invertível e M −1 é a matriz mudança de base
B ′ para B .

M −1i
h

= h

M

i

.

B→B′

B′ →B

Teorema 2.54. Sejam B ′ = {v1, v2, . . . , vn} e B = {e1, e2, . . . , en} bases de Rn ,
Se os vetores da base B ′
forem escritos em forma de
sendo B a base canônica .
colunas, então

h
M

i
B→B′

= h [v1]B [v2]B · · ·

[vn]B

i.

Produto interno e ortogonalidade

29

2.4 Produto interno e ortogonalidade

No estudo da Geometria Analítica no Rn define-se o comprimento de um vetor, e o
ângulo entre vetores através do produto escalar. O objetivo agora é estendermos estes
conceitos para os espaços vetoriais sobre um corpo K (reais ou complexos). Porém devido
ao objetivo deste trabalho e que se realizado nos próximos capítulos vamos considerar o
corpo dos reais.

O conceito de produto interno vai possibilitar que um espaço vetorial de estrutura
totalmente algébrica adquira uma estrutura geométrica, o que permitirá falarmos em ân-
gulo entre vetores, projeção de um vetor ortogonalmente sobre outro, comparar tamanho
entre vetores e outros conceitos a espaços vetoriais quaisquer.

Nesta seção, V representará um espaço vetorial de dimensão finita (a menos que seja
mencionado o contrário) pois alguns teoremas e proposições que serão mencionados não
serem verdadeiros para espaços vetoriais de dimensão infinita. E o corpo envolvido no
espaço vetorial V será o corpo dos reais.

Definição 2.55. Seja V um espaço vetorial sobre um corpo R. Uma aplicação

que satisfaz as seguintes propriedades:

⟨ , ⟩ : V × V
(u , v)

−→ R.
7−→ ⟨u, v⟩.

(1) Simetria: ⟨u, v⟩ = ⟨v, u⟩ , para todo u, v ∈ V .
(2) Distributividade: ⟨u + w, v⟩ = ⟨u, v⟩ + ⟨w, v⟩ , para todo u, v, w ∈ V .
(3) Homogeneidade: ⟨au, v⟩ = a⟨u, v⟩ , para todo u, v ∈ V e a ∈ R.
(4) P ositividade: ⟨v, v⟩ ⩾ 0 e ⟨v, v⟩ = 0 se, e somente se v = 0, para todo v ∈ V .

define um produto interno sobre V .

Um espaço vetorial sobre o corpo R com produto interno, denotamos por ( V , ⟨· , ·⟩ ),

e é chamado de espaço euclidiano.

Observação 2.56. O produto interno em Rn também é chamada de produto escalar.

Propriedade 2.57. Atráves das propriedades de simetria, distributividade e homoge-
neidade podemos verificar que:

(a) ⟨u, 0⟩ = ⟨0, u⟩ = 0, para todo u ∈ V .
(b) ⟨u, av⟩ = a⟨u, v⟩, para todo u, v ∈ V e a ∈ R.
(c) ⟨u, v + w⟩ = ⟨u, v⟩ + ⟨u, w⟩, para todo u, v ∈ V .

Exemplo 2.58. Sejam u = (u1, u2, . . . , un) e v = (v1, v2, ..., vn) vetores do espaço
vetorial Rn. O produto interno dado por

é chamado de produto interno euclidiano, ou canônico do Rn.

⟨u, v⟩ = u1.v1 + · · · + un.vn.

Exemplo 2.59. Consideremos o espaço vetorial R2, os vetores u = (u1, u2) e v = (v1, v2)
em R2. A aplicação definida por

⟨u, v⟩ = 3u1.v1 + 2u2.v2 , para todo u, v ∈ R2

30

Preliminares

define um produto interno em R2.

Este produto interno é diferente do produto canônico do R2. Assim, podemos observar

a existência de mais de um produto interno num mesmo espaço vetorial.

Definição 2.60. Definimos traço de uma matriz A = (aij) de ordem n, e denotamos
por trA, como a soma dos elementos da sua diagonal principal.

Exemplo 2.61. Consideremos V = Mm×n(R). A aplicação definida por

⟨A, B⟩ = tr(BtA), ∀A, B ∈ Mn(R).

é um produto interno Mm×n(R).

Definição 2.62. Seja V um espaço euclidiano. A norma de um vetor v em relação a
esse produto interno é definida por

∥ v ∥ = q

⟨v, v⟩ ≥ 0 , ∀v ∈ V .

Um espaço vetorial euclidiano de uma norma ∥ . ∥ é denominado espaço normado.
Quando ∥ v ∥= 1 dizemos que v é um vetor unitário.
Para todo v ̸= 0, o vetor u = v

∥v∥ ∈ V , e é chamado vetor normalizado, e

∥ u ∥ = 1. Pois,

Observação 2.63.

∥ u ∥=

s

⟨

v
∥ v ∥

,

v
∥ v ∥

⟩ =

s 1
∥ v ∥2 . ∥ v ∥2 = 1.

(a) A norma depende do produto interno considerado.
(b) É sempre possível extrair a raiz quadrada de ⟨v, v⟩ pois este número é não negativo.
(c) A norma de v representa o comprimento do vetor v.

Proposição 2.64. Em um espaço normado V são válidas as seguintes propriedades:
v = 0.

(a) ∥ v ∥= 0
se, e somente se,
(b) ∥ λv ∥ = |λ| ∥ v ∥ , ∀λ ∈ K e ∀u, v ∈ V .
(c) ∥ u + v ∥ ≤ ∥ u ∥ + ∥ v ∥ , ∀u, v ∈ V . (desigualdade triangular).

Proposição 2.65. (Desigualdade de Cauchy-Schwarz) Se V é um espaço normado, en-
tão,

| ⟨u, v⟩ | ≤ ∥ u ∥ ∥ v ∥ , ∀u, v ∈ V .

Exemplo 2.66. Consideremos o espaço vetorial V = Mm×n(R) e o produto interno
definido por

⟨A, B⟩ = tr(AtB) , ∀A, B ∈ Mn×n(R). Calculemos a norma da matriz







.



A =

1
0
2 −1
0
3
∥ A ∥2 = tr(AtA)

1
0
2 −1
3
0

0
3









#

.

At.A =

" 1

2
0 −1

=

"

5 −2
−2 10

#
.

Logo, ∥ A ∥2 = tr(AtA) = 5 + 10 = 15 . Portanto ∥ A ∥=

√

15.

Produto interno e ortogonalidade

31

Definição 2.67. Seja V um espaço vetorial. Consideremos uma aplicação d : V ×V → R,
que associa a cada par ordenado (u, v) ∈ V × V um número real d(u, v) satisfazendo as
seguintes condições:

(a) P ositividade : d(u, v) ⩾ 0
(b) Simetria : d(u, v) = d(v, u), ∀u, v ∈ V .
(c) Desigualdade triangular : d(u, v) ⩽ d(u, w) + d(v, w), ∀u, v, w ∈ V .
Nestas condições, dizemos que d é uma métrica sobre V e que d(u, v) é a distância

e d(u, v) = 0 ⇐⇒ u = v, ∀u, v ∈ V .

do vetor u ao vetor v.

Um espaço euclidiano munido de uma métrica é chamado de espaço métrico.

Observação 2.68. Em um espaço euclidiano V a métrica ou distância é dada por

d(u,v) = ∥ u − v ∥= q

⟨u − v , u − v⟩ , ∀u, v ∈ V .

Definição 2.69. Em espaço vetorial euclidiano V definimos ângulo entre dois veto-
res, não nulos, u e v em V como sendo o valor θ ∈ [0, π), tal que

cos θ = ⟨u,v⟩

∥u∥∥v∥.

Quando ⟨u, v⟩ = 0, ou seja, cos θ = 0, o que acarreta θ = π
2

e v são ortogonais.

, dizemos que os vetores u

Definição 2.70. Em um espaço vetorial euclidiano V definimos a projeção ortogonal
de um vetor u sobre um vetor não nulo v, denotado por projvu, como sendo o vetor

projvu = ⟨u , v⟩

∥v∥ v , u, v ∈ V .

Observe que a projeção ortogonal de um vetor u sobre um vetor v não nulo é um

múltiplo escalar do vetor v.

Teorema 2.71. Seja v ∈ Rn um vetor não nulo. Então, u − projvu é ortogonal a v,
para qualquer u ∈ Rn.

Demonstração. Calculando o produto escalar de v com u − projvu, temos:
⟨v , u − projvu⟩ = ⟨v , u⟩ − ⟨v , projvu⟩ = ⟨v , u⟩ − ⟨v ,

⟨v , u⟩

∥v∥ v⟩ =

Portanto, u − projvu é ortogonal a v.

= ⟨v , u⟩ − ⟨v , u⟩

∥v∥ ⟨v, v⟩ = 0.

Definição 2.72. Em um espaço euclidiano V dizemos que:

(a) um conjunto S = {v1, v2, . . . , vn} ⊂ V é ortogonal , se vi⊥vj para todo i ̸= j.
(b) um vetor u ∈ V é ortogonal a um conjunto não vazio S ⊂ V , se u for ortogonal a

todos os vetores de S, e denotamos por u ⊥ S.

(c) um conjunto S é chamado de ortonormal quando, além de ortogonal, satisfaz

∥ vi ∥= 1 , para todo 1 ⩽ i ⩽ n.

Definição 2.73. Sejam V um espaço euclidiano e S um conjunto não vazio de vetores
de V . O conjunto

é denominado ortogonal de S.

S⊥ = { u ∈ V : ⟨u, v⟩ = 0 , ∀v ∈ V }

32

Preliminares

Teorema 2.74. Seja V um espaço euclidiano. Se S = { v1, v2, . . . , vn } é um con-
junto ortogonal em V , tais que vi
̸= 0, para i = 1, 2, . . . , n. Então, S é linearmente
independente em V .

Propriedade 2.75. Num espaço vetorial euclidiano V valem as seguintes propriedades:

(a) 0⊥v, para todo v ∈ V .
(b) Se u⊥v, então v⊥u.
(c) Se v⊥u, para todo u ∈ V , então v = 0.
(d) Se u⊥w e v⊥w, então (u + v)⊥w.
(e) Se u⊥v e λ é um escalar, então u⊥λv.

Teorema 2.76. (Teorema de Pitágoras).
Então, os vetores u, v ∈ V são ortogonais se, e somente se,

Sejam V

um espaço vetorial euclidiano.

∥ u + v ∥2 = ∥ u ∥2 + ∥ v ∥2.

Proposição 2.77. Um conjunto ortonormal {u1, u2, . . . , un} é LI se, para qualquer v ∈
V , o vetor w = v − ⟨v, u1⟩u1 − ⟨v, u2⟩u2 − · · · − ⟨v, un⟩un é ortogonal a cada um dos ui.

As bases ortogonais são de fundamental importância em espaços com produto interno,

e o teorema a seguir mostra que sempre é possível obtermos tais bases.

Teorema 2.78. (Processo de Ortogonalização de Gram-Schmidt). Todo espaço vetorial
euclidiano de dimensão finita admite uma base ortonormal.

Demonstração. A prova é feita por indução sobre a dimensão do espaço.

Seja V um espaço vetorial euclidiano de dimensão finita.
Se dim V = 1 , então existe v1 ∈ V tal que V = h
Como v1 ̸= 0 , consideremos u1 = v1
∥v1∥
Se dim V = 2, então existem v1, v2 ∈ V , tais que V = h
Consideremos u1 = v1
∥v1∥
Pela Proposição 1.77. basta tomarmos o vetor não nulo u′
2

v1

i.

v1, v2

i.

e determinemos um vetor u2 ortogonal a u1 e de norma 1.

= v2 − ⟨v2, u1⟩u1, (pois v1

, e então {u1} é uma base ortonormal de V .

e v2 são LI) , e normalizá-lo obtendo u2.

Logo, {u1, u2} forma uma base ortonormal de V .
Por hipótese de indução, suponhamos que o teorema seja válido para todo espaço
euclidiano de dimensão n − 1, e vamos provar que o teorema é válido para todo espaço
euclidiano de dimensão n.

Se dim V = n ≥ 2, então existem v1, v2, . . . , vn ∈ V que forma uma base de V .
Mas, U = h

i é um subespaço de dimensão n − 1. Então, usando a
hipótese de indução, é possível tomarmos uma base {u1, u2, . . . , un−1} ortonormal de U .

v1, v2, . . . , vn

Como vn /∈ V , então pela Proposição 1.77. o vetor

= vn − ⟨vn, u1⟩u1 − · · · − ⟨vn, un−1⟩un−1
é não nulo e ortogonal a todos os vetores de U . Normalizando u′
n

u′
n

Portanto, {u1, u2, . . . , un−1, un} forma uma base ortonormal de V .

obtemos un.

Teorema 2.79. O conjunto S⊥ é um subespaço de V , mesmo que S não o seja.

No caso de S ser um subespaço, então S ∩ S⊥ = {0}.

Transformações lineares

33

Teorema 2.80. Sejam V um espaço vetorial euclidiano de dimensão finita e S um
subespaço de V . Então, V é soma direta de S e S⊥, isto é,

V = S ⊕ S⊥.

Assim, todo elemento em V pode ser escrito de modo único como sendo, v = u + u′,
com u ∈ S e u′ ∈ S⊥. E neste caso, em que S é um subespaço vetorial de V , o conjunto
S⊥ é denominado complemento ortogonal de S em V .

Corolário 2.81. Sejam V um espaço vetorial de dimensão finita munido de um produto
interno. Se S um subespaço de V , então dimV = dimS + dimS⊥.

Teorema 2.82. Sejam V um espaço vetorial com um produto interno, S e W subespaços
vetoriais de V . Então,
(a) (S⊥)⊥ = S.
(b) (S ∩ W )⊥ = S⊥ + W ⊥.
(c) (U + W )⊥ = U ⊥ ∩ W ⊥.

2.5 Transformações lineares

Uma transformação linear é um tipo particular de aplicação em que o seu domínio
e contradomínio são espaços vetoriais, e que leva combinações lineares de vetores de um
espaço em combinações lineares de vetores em outro espaço. Na Física, Engenharia e
várias áreas de exatas são inúmeras as aplicações envolvendo este conceito.

Nesta seção recordaremos os principais conceitos e resultados envolvendo as trans-
formações lineares em espaços vetoriais arbitrários, e a relação fundamental entre estes
espaços de dimensão n e o Rn (isomorf ismo) , permitindo efetuarmos cálculos vetori-
ais em espaços arbitrários aparentemente diferentes, porém dotados da mesma estrutura
algébrica do Rn, possibilitando restringirmos o estudo somente a este último.

Definição 2.83. Sejam U e V espaços vetoriais sobre um mesmo corpo K. A aplicação
T : U → V é denominada transformação linear se, satisfaz:

(a) T (u1 + u2) = T (u1) + T (u2) , ∀u1, u2 ∈ U .
(b) T (k.u1) = k.T (u1), ∀u1 ∈ U e ∀k ∈ K.
Estas duas condições podem ser condensadas em:

T (k1.u1 + k2.u2) = k1.T (u1) + k2.T (u2), ∀u1, u2 ∈ U e ∀k1, k2 ∈ K.

A transformação linear T : U → U é denominada de operador linear sobre o espaço

vetorial U .

Denotamos por L(U, V ) o conjunto de todas as transformações lineares T : U → V ; e

por L(U ) o conjunto de todos os operadores lineares T : U → U .

Exemplo 2.84. Consideremos o espaço vetorial Pn(R). A aplicação

T : Pn(R) → Pn(R) definida por T (p) = p ′ ,

34

Preliminares

sendo p ′ a derivada do polinômio p, para todo p ∈ Pn(R), é uma transformação linear.

Pois, para ∀ p, q ∈ Pn(R) e k ∈ R, temos:
(a) T (p + q)(x) = (p + q)′(x) = p′(x) + q′(x) = T [(p)(x)] + T [(q)(x)].
(b) T (kp)(x) = (kp)′(x) = kp′(x) = kT [p(x)].

Exemplo 2.85. Dada uma matriz A ∈ Mm×n(R) podemos sempre associá-la a uma
transformação linear TA : Rn → Rm cuja imagem TA(v) do vetor v ∈ Rn é dada pelo
produto da matriz [A]m×n com a matriz coluna [v]n×1
formada pelas coordenadas do
vetor v, ou seja,

TA : Rn → Rm tal que TA(v) = [A].[v] .

TA é uma transformação linear, pois ∀u, v ∈ Rn e λ ∈ R, temos:
(a) T (u + v) = A(u + v) = Au + Av = T (u) + T (v).
(b) T (λv) = A(λv) = λ(Av) = λT (v).

Propriedade 2.86. Sejam U , V espaços vetoriais sobre K, T : U → V uma transfor-
mação linear, 0u e 0v os vetores nulos dos espaços vetoriais U e V , respectivamente.
Então,

(a) T (0u) = 0v.
(b) T (−u) = −T (u), ∀u ∈ U .

n
P
i=1

n
P
i=1

(c) T (

kiui) =

kiT (ui), ∀ui ∈ U e ∀ki ∈ K, para i ∈ {1, . . . , n}.

Teorema 2.87. Sejam U , V espaços vetoriais, {v1, v2, . . . , vn} uma base de V e
{u1, u2, . . . , un} vetores de U . Então, existe uma única transformação linear T : V → U
tal que T (vi) = ui, para todo i ∈ {1, 2, . . . n}.

Exemplo 2.88. Consideremos B = {(1, 0), (1, 1)} uma base de R2, e os vetores (3, 1, 0)
e (1, 2, −1) de R3. Então, existe uma transformação linear

T : R2 → R3 tal que T (1, 0) = (3, 1, 0) e T (1, 1) = (1, 2, −1).

Escrevendo (x, y) como uma combinação linear da base dada, e aplicando T , encon-

tramos a transformação linear T .

Como (x, y) = (x − y)(1, 0) + y(1, 1) e T uma transformação linear, então

T (x, y) = (x − y).T (1, 0) + y.T (1, 1) = (x − y).(3, 1, 0) + y.(1, 2, −1) =

= (3x − 3y + y, x − y + 2y, −y).

Portanto, T (x, y) = (3x − 2y, x + y, −y).

Definição 2.89. Sejam U e V espaços vetoriais sobre um corpo K e T : U → V uma
transformação linear.

(a) O conjunto de todos os vetores em V que são imagens por T de pelo menos um
vetor em U é denominado de imagem da transformação linear T , e deno-
tamos por Im(T ). Ou seja,

Im(T ) = { v ∈ V : v = T (u) , u ∈ U }.

Transformações lineares

35

(b) O conjunto de todos os vetores u ∈ U tais que T (u) = 0 é denominado
núcleo da transformação linear de T , e denotamos por Ker(T ). Ou seja,

Ker(T ) = { u ∈ U : T (u) = 0 }.

No caso de Ker(T ) = {0}, dizemos que T é não-singular. Caso contrário, T é

singular.

Exemplo 2.90. Consideremos a transformação linear
T : P1(R) → R tal que p(x) = R 1

0 p1(x) dx .

0 p(x) dx = R 1

Então, dado p(x) ∈ P1(R) temos que p(x) = ax + b, com a , b ∈ R .
Mas , T ( p(x) ) = R 1
Como, Ker(T ) = { p(x) ∈ P1(x) : T (p(x)) = 0 } , então a
2
Logo, b = − a
.
2
Portanto, Ker(T ) = { p(x) ∈ P1(x) : p(x) = ax − a
Agora, como para todo a ∈ R, existe p(x) = a ∈ P1(R), tal que

, e temos então que p(x) = ax − a
2

= a
+ bx ]1
0
2
+ b = 0 .

(ax + b) dx = [ a x2
2

2 }.

0

+ b − 0 = a
2

+ b.

T ( p(x) ) = R 1

0 p(x) dx = R 1

0 a dx = ax|1

0

Logo , R ⊂ Im(T ).
Mas, pela definição de Im(T ) temos que Im(T ) ⊂ R.
Portanto, Im(T ) = R.

= a − 0 = a.

Teorema 2.91. Seja T : U → V uma transformação linear. Então,

(a) Ker(T ) é um subespaço de U .
(b) Im(T ) é um subespaço de V .

Definição 2.92. Seja T uma transformação linear entre espaços vetoriais de dimensão
finita. Definimos:

(a) O posto de T como sendo a dimensão de Im(T ), e denotamos por post(T ).

post(T ) = dim Im(T ).

(b) A nulidade de T como sendo a dimensão de Ker(T ), e denotamos por nul(T ).

nul(T ) = dim Ker(T ).

Teorema 2.93. Se T : U → V é uma transformação linear, sendo U um espaço vetorial
de dimensão finita, então

dim Ker (T ) + dim Im(T ) = dim U , ou seja, nul(T ) + post(T ) = dim U .

Definição 2.94. Sejam U e V espaços vetoriais sobre um corpo K e T uma transfor-
mação linear de U em V . Dizemos que:

(a) A transformação T é injetora se u ̸= v então T (u) ̸= T (v), para todo u, v ∈ U .
(b) A transformação T é sobrejetora se, Im (T ) = V .
(c) A transformação T é bijetora se, T é injetora e também sobrejetora.

Teorema 2.95. Seja T : U → V uma transformação linear. Então, Ker(T ) = {0}
(T é não singular) se, e somente se, T é injetora.

36

Preliminares

Teorema 2.96. Seja T : U → V uma transformação linear, e dimU = dimV . Então,
T é injetora (T é não singular) se, e somente se, T é sobrejetora.

Definição 2.97. Sejam U e V espaços vetoriais de dimensão finita sobre o corpo K.
Uma transformação linear T : U → V bijetora é denominada um isomorfismo de U
em V .

Quando existe um isomorfismo de U em V , dizemos que o espaço vetorial U é

isomorfo ao espaço vetorial V , e denotamos por U ≃ V .

Um isomorfismo T : U → U é denominado um automorfismo de U .

Teorema 2.98. Seja V um espaço vetorial real sobre o corpo K, com dimV = n, então
V é isomorfo ao espaço vetorial Kn.

Exemplo 2.99. O espaço vetorial dos polinômios Pn(R) é isomorfo ao Rn+1.

Exemplo 2.100. O espaço vetorial M2(R) das matrizes quadradas reais de ordem 2 é
isomorfo ao R4.

Teorema 2.101. Dois espaços vetoriais de dimensões finitas são isomorfos se, e somente
se, possuem a mesma dimensão.

No Exemplo 1.85 vimos que dada uma matriz A ∈ Mm×n(R)

sempre pode ser
associada a uma transformação linear TA : Rn → Rm cuja imagem TA(v) do vetor v do
Rn é dada pelo produto da matriz [A]m×n com a matriz coluna [v]n×1 das coordenadas
do vetor v com respeito à base canônica do espaço Rn.

TA(v) = [A].[v] .

O objetivo agora é estabecermos a recíproca, isto é, fixadas as bases, toda transfor-
mação linear T : U → V , sendo U e V espaços vetoriais finitamente gerado, sobre o
mesmo corpo K associa uma única matriz, que permite através dela calcularmos T (u)
para todo u em U , usando a forma matricial.

Estudamos matrizes de transformações lineares por duas razões básicas, uma teórica
e outra bastante prática. A primeira, muitas vezes, as respostas para as questões teóricas
sobre a estrutura de transformações lineares arbitrárias em espaços de dimensão finita po-
dem ser obtidas simplesmente estudando as transformações matriciais. E a segunda, que
estas matrizes tornam possíveis calcularmos as imagens de vetores usando multiplicação
matricial, e estes cálculos podem ser efetudados rapidamente através de computadores.

Definição 2.102. Sejam U e V espaços vetoriais de dimensão finita, sobre um corpo K,
com bases ordenadas B = {u1, u2, . . . , un} e B′ = {v1, v2, . . . , vm}, respectivamente.
Dada T : U → V uma transformação linear, então T (u1), T (u2), . . . , T (un)
são
vetores de V . Logo, cada um destes vetores podem ser escritos de maneira única como
combinação linear dos elementos da base B′. Ou seja,



T (u1) = a11v1 + a21v2 + · · · + am1vm

T (u2) = a12v1 + a22v2 + · · · + am2vm
...

T (un) = a1nv1 + a2nv2 + · · · + amnvm

,

Transformações lineares

37

com aij ∈ K, para i ∈ {1, 2, . . . , n} e j ∈ {1, 2, . . . , m} .

A transposta da matriz formada pelos coeficientes das combinações lineares acima é
chamada de representação matricial de T em relação às bases B e B′, e denotamos
por [T ]B→B′, isto é,

[T ]B→B′ =









a12
a11
a22
a21
...
...
am1 am2

a1n
. . .
a2n
. . .
...
. . .
. . . amn





.




No caso de U = V e B = B′ denotamos a matriz acima por [T ]B. Além disso, se
B e B′ são as bases canônicas dos espaços vetoriais U e V , respectivamente, denotamos
simplesmente por [T] .

Como a matriz [T ]B→B′ depende das bases B e B′, uma mesma transformação linear
pode ser representada por infinitas matrizes, bastando mudarmos as bases. Porém, uma
vez fixada as bases B e B′, respectivamente, do domínio e do contra-domínio, uma trans-
formação linear fica bem definida através de sua representação matricial, ou seja, dada
uma transformação linear só existe uma única matriz associada a essa transformação li-
near, e vice-versa, dada uma matriz existe uma única transformação linear associada a
ela. Assim, o estudo de uma transformação linear se restringe ao um estudo matricial;
e a transformação linear sobre um vetor u de U se comporta simplesmente como uma
multiplicação, pela direita, da representação matricial de T pela matriz das coordenadas
de u na base B, como garante o teorema a seguir.

Teorema 2.103. Sejam T : U → V uma transformação linear entre espaços finitamente
gerados, B e B′ as bases de U e V , respectivamente. Então, para qualquer vetor u ∈ U ,
temos

i

h
T

h

i
u

B

= h

T (u)i

.

B′

B→B′

















a11
a12
a21
a22
...
...
am1 am2

· · ·
a1n
· · ·
a2n
...
...
· · · amn

β1
β2
...
βm
e T (u) = ( β1, β2, · · · , βn )B′.

α1
α2
...
αn

























=









,

onde u = ( α1, α2, · · · , αn )B

Exemplo 2.104. Sejam B e B′ as bases canônicas de R2 e R3, respectivamente e T a
transformação linear definida por

T : R3 → R2, dada por T (x, y, z) = (2x + y, x − z) .

Como T está aplicada nas bases canônicas, a matriz da transformação linear se torna

mais fácil de ser obtida, pois podemos escrever a expressão T (x, y, z) na forma

T (x, y, z) = (2x , x) + (y , 0) + (0 , −z).
T (x, y, z) = x(2 , 1) + y(1 , 0) + z(0 , −1).

Assim, T pode ser escrita na forma matricial como sendo:
" 2
1

T (x, y, z) =

= h 2x + y x − z

1
0
0 −1











#

x
y
z

i.

38

Preliminares

Desse modo, a transformação T pode ser representada pela matriz

[ T ] =

" 2
1

1
0
0 −1

#

,

e através dela podemos calcular T (x, y, z) para qualquer (x, y, z) ∈ R3 .

Tomando-se, como exemplo, o vetor u = (3, −1, 2)
que também pode ser obtido aplicando o Teorema 1.103. .
" 5
1

[ T ][u]B′ =

" 2
1

1
0
0 −1

=






#

#


3
−1


2

= h 5 1 it.

então T (3 , −1 , 2) = (5, 1) ,

Exemplo 2.105. Seja T : R2 → R3 a transformação linear definida por

T (x, y) = (x − y, x + 2y, y),

sendo B = {(1, 0) , (1, 1)} e B′ = {(0, 0, 1) , (1, 1, 0) , (1, 0, 1)} as bases de R2 e R3,
respectivamente. Determinemos a matriz [T ]B→B′.

T (1, 0) = (1, 1, 0) = a11.(0, 0, 1) + a12.(1, 1, 0) + a13.(1, 0, 1).






a12 + a13 = 1
a12 = 1
a11 + a13 = 0

.

Logo, a12 = 1 , a13 = 0 e a11 = 0.

Então, T (1, 0) = 0.(0, 0, 1) + 1.(1, 1, 0) + 0.(1, 0, 1).

T (1, 1) = (0, 3, 1) = a21.(0, 0, 1) + a22.(1, 1, 0) + a23.(1, 0, 1).






a22 + a23 = 0
a22 = 3
a21 + a23 = 1

.

Logo, a22 = 3 , a23 = −3 e a21 = 4.

Então, T (1, 1) = (0, 3, 1) = 4.(0, 0, 1) + 3.(1, 1, 0) − 3.(1, 0, 1).

Portanto,

[T ]B→B′ =






a11 a21
a12 a22
a13 a23






=






4
0
3
1
0 −3






.

Exemplo 2.106. Seja T a transformação linear do exemplo anterior e consideremos as
bases canônicas B = {(1, 0), (0, 1)} e B′ = {(1, 0, 0), (0, 1, 0), (0, 0, 1)}.

T (1, 0) = (1, 1, 0) = a11.(0, 0, 1) + a12.(1, 1, 0) + a13.(1, 0, 1).






a12 + a13 = 1
a12 = 1
a11 + a13 = 0

.

Logo, a12 = 1 , a13 = 0 e a11 = 0.

Então, T (1, 0) = 0.(0, 0, 1) + 1.(1, 1, 0) + 0.(1, 0, 1).

T (0, 1) = (−1, 2, 1) = a21.(0, 0, 1) + a22.(1, 1, 0) + a23.(1, 0, 1)

a22 + a23 = −1

a22 = 2
a21 + a23 = 1

Logo, a22 = 2 , a23 = −3 e a21 = 4.



.

Transformações lineares

39

Então, T (0, 1) = 4.(0, 0, 1) + 2.(1, 1, 0) − 3.(1, 0, 1).

Portanto,

[T ]B→B′ =











a11 a21
a12 a22
a13 a23

=






4
0
1
2
0 −3






.

Podemos obter a matriz de qualquer transformação linear, desde que os espaços ve-
toriais envolvidos na transformação seja finitamente gerado. E utilizando o conceito de
coordenadas, podemos trabalhar com quaisquer bases dos espaços vetoriais envolvidos na
transformação linear.

Exemplo 2.107. Sejam B =

( " 1
0

#

0
0

" 0
0

,

#

1
0

" 0
1

,

#

0
0

" 0
0

,

0
1

# )

e

B′ = {1 , t, t2} , as bases canônicas de M2(R) e P2(R), respectivamente. Consideremos
a transformação linear T : M2(R) → P2(R) definida por

# !

  " a b
c d

T

= (a + b) + (c − d)t + (b + 2c + d)t2.

Assim, temos:

# !

# !

  " a b
c d
  " a b
c d

T

T

= T ( a, b, c, d )B = ( a + b, c − d, b + 2c + d )B′.

= a(1, 0, 0) + b(1, 0, 1) + c(0, 1, 2) + d(0, −1, 1).

Logo, podemos escrever

  " a b
c d

T

# !

=






0
1 1 0
0 0 1 −1
1
0 1 2













a
b
c
d








B

=






a + b
b − c
b + c + d






.

B′






1 1 0
0
0 0 1 −1
1
0 1 2


 ∈ M3×4(R), representa a transformação linear


E a matriz [ T ] =

dada.

Assim, por exemplo:

"

T

  " 2

0
−1 3

# !#

B′

=






1
0
0

1
0
1

0
0
1 −1
1
2



 .








2
0
−1
3








=






3
−1
−1






.

B

Portanto,

"
T

  " 2

0
−1 3

# !#

B′

= 2 − 4t + t2.

Definição 2.108. Dizemos que uma matriz A é equivalente por linha a uma matriz
B, e denotamos por A ∼ B, quando B pode ser obtida de A por uma sequência finita
de operações denominadas operações elementares com linhas, que são:

(a) Trocar duas linhas quaísquer.

Li ←→ Lj (troca da linhas i com a linha j).
(b) Substituir uma linha por ela mesma multiplicada por um escalar não nulo.
Lj −→ kLj (troca da linha j por k vezes a linha j).

40

Preliminares

(c) Substituir uma linha por ela mesma mais o produto de um escalar, não nulo, por

outra linha.

Lj −→ Lj + kLi (troca da linha j pela soma da linha j com k vezes a linha i).

Proposição 2.109. Dada uma matriz Am×n(K) podemos obter uma matriz escalonada
B equivalente a A.

Teorema 2.110. As linhas não-nulas (quando considerada como vetores de Rn) de uma
matriz na forma escalonada são linearmente independentes.

Definição 2.111. Podemos escrever os vetores com parênteses e vírgulas ou em forma
matricial como vetores linhas ou vetores colunas.

Assim, na matriz A =

, os vetores











a11
a12
a21
a22
...
...
am1 am2

· · ·
a1n
· · ·
a2n
...
. . .
· · · amn
v2 = (a21, a22, . . . , a2n),







v1 = (a11, a12, . . . , a1n) ,
, vn = (am1, am2, . . . , amn) em
Kn formados pelas linhas de A são chamados vetores linhas de A. Enquanto que, os
vetores u1 = (a11, a21, . . . , am1), u2 = (a12, a22, . . . , am2) , · · · , un = (am1, am2, . . . , amn)
em Kn são os vetores colunas de A.

· · ·

O subespaço de Kn gerado pelos vetores linhas de A é denominado espaço linha de
A; o subespaço de Kn gerado pelos vetores colunas de A é o espaço coluna de A, que
também é um subespaço de Kn.

O conjunto solução de um sistema homogêneo de equações AX = 0, é um subespaço

de Kn, denominado espaço nulo de A.

Teorema 2.112.
(a) As operações elementares com linhas não alteram o espaço linha, e nem o espaço nulo

de uma matriz, ou seja, geram o mesmo espaço vetorial.

(b) Se uma matriz está na forma escalonada por linhas, então os vetores linhas formam
uma base do espaço linha de A, e os vetores colunas formam uma base do espaço
coluna de A.

As operações elementares com linhas afetam o espaço coluna de uma matriz, mas não

alteram as relações de dependência linear entre os vetores colunas.

Exemplo 2.113. Consideremos a matriz A =

por linha .








1
0
0
0

0 −1
1
3
2
0
0
0








0
5
4
0

na forma escalonada

Pelo teorema anterior, os vetores v1 = (1, 0, −1, 0) , v2 = (0, 3, 1, 5) , v3 = (0, 0, 2, 4)
formam uma base do espaço linha de A; os vetores u1 = (1, 0, 0, 0) , u2 = (0, 3, 0, 0) ,
u3 = (−1, 1, 2, 0) , u4 = (0, 5, 4, 0) não formam base do espaço coluna de A, pois o vetor
u4 = (0, 5, 4, 0) é combinação linear dos demais.

Transformações lineares

41

Como operações elementares com linhas não alteram o espaço linha de uma matriz,
podemos encontrar uma base do espaço linha (ou uma base do espaço coluna) encontrando
uma base do espaço linha (ou uma base do espaço coluna) de qualquer forma escalonada
por linhas da matriz.

Teorema 2.114.
(a) O posto de uma matriz é o posto da transformação linear representada por ela.
(b) O posto de uma matriz é igual à quantidade de linhas (ou colunas) LI da matriz.
(c) Se uma matriz A de ordem n possui posto estritamente menor que n, então A é

singular.

Teorema 2.115. Seja T : U → V uma transformação linear e B e B′ bases de U e V ,
respectivamente. Então,

(a) dim Im(T ) = post [T ]B→B′.
(b) dim Ker(T ) = nul [T ]B→B′ = nº de colunas de [T ]B→B′ − post [T ]B→B′.

Exemplo 2.116. Consideremos B e B′ as bases canônicas de R4 e R3, e seja T : R4 → R3
a transformação linear tal que

T (x, y, z, t) = (x + y − z + 2t , y + z − t , 3x + 4y − 2z + 5t) .

Determinemos as bases para Im(T ) e Ker(T ).

Podemos escrever T como sendo:

T (x, y, z, t) = x(1, 0, 3) + y(1, 1, 4) + z(−1, 1, −2) + t(2, −1, 5) .
Im(T ) = [ (1, 0, 3), (1, 1, 4), (−1, 1, −2), (2, −1, 5) ].

Logo,

Determinemos uma base de Im(T ) escalonando a matriz [ T ]T

B→B′.

[ T ]B→B′ =








 ∼

1 −1
1
2
1
0
1 −1
4 −2
3
5
L3 → L3 − 3L1.






1 −1
1
1

2
1
1 −1
0
1 −1
0
L3 → L3 − L2.



 ∼






1
0
0

1 −2
1
0

2
1 −1
0
0


.



Portanto, uma base da imagem de T é formada pelos dois primeiros vetores da matriz

inicial, isto é, B = {(1, 0, 3), (1, 1, 4)} é uma base para Im(T ), e post(T ) = 2.

Para calcularmos uma base do núcleo de T temos que resolver o sistema homogêneo



[ T ]B→B′.[v] = 0,

ou seja,

x + y − z + 2t = 0
y + z − t = 0
3x + 4y − 2z + 5t = 0



.

Escalonando o sistema temos:






1
0
3

2
1 −1
5

1 −1
1
4 −2
L3 → L3 − 3L1.

0
0
0



 ∼






1
0
0

1 −1
2
0
1
1 −1
0
1
1 −1
0
L1 → L1 − L2.
L3 → L3 − L2.



 ∼






1
0
0

0 −2
1
0

3
1 −1
0
0


.



0
0
0

42

Preliminares

Então,






x − 2z + 3t = 0
y + z − t = 0

, (sistema indeterminado com duas variáveis livres).

Fazendo z = a e t = b, temos: x = 2a − 3b e y = b − a. E o conjunto solução do

sistema é dado por

(2a − 3b , b − a , a , b) = a(2, −1, 1, 0) + b(−3, 1, 0, 1) , a, b ∈ R.

Portanto, {(2, −1, 1, 0), (−3, 1, 0, 1)} formam uma base de Ker(T ).

Exemplo 2.117. Sejam B = { (0, 1, 1), (1, 0, 0), (0, 0, 1) }
e B′ = { (1, 0), (1, 1) }
bases de R3 e R2, respectivamente. Determinemos uma base do núcleo e uma da imagem
de T da transformação linear

T : R3 → R2 definida por T (x, y, z) = (x + y − z , 2x − y − z) .

T (0, 1, 1) = (0, −2) = 2.(1, 0) − 2.(1, 1) = (2, −2)B′.
T (1, 0, 0) = ( 1, 2) = −1.(1, 0) + 2.(1, 1) = (−1, 2)B′.
T (0, 0, 1) = (−1, −1) = 0(1, 0) − 1.(1, 1) = (0, −1)B′.
"

#

Portanto ,

[ T ]B→B′ =

2 −1

0
2 −1

−2

.

Determinemos uma base de Im(T ) escalonado a matriz [ T ]B→B′.

#

" 2 −1
0
−2
2 −1
L2 → L2 + L1.

∼

" 2 −1
0

#

0
1 −1
2.L1.

L1 → 1

∼

#

∼

" 1
0

0 − 1
2
1 −1

#

.

" 1 − 1
0

0
2
1 −1
L1 → L1 + 1

2.L2.

Logo, os dois primeiros vetores da matriz inicial formam uma base da imagem de T .

Portanto B = {(2, −2), (−1, 2)} forma uma base de Im(T ).



Escalonando o sistema

, temos:

x + y − z = 0
2x − y − z = 0



" 1

#

1 −1
2 −1 −1
L2 → L2 − 2.L1.

0
0

∼

" 1

0 −3

1 −1
0
1
0
3.L2.

L2 → − 1

#

∼

" 1
0

1 −1
1 − 1
3

#

.

0
0

Então,






x + y − z = 0
3z = 0
y − 1

(sistema indeterminado com uma variável livre).

Fazendo z = 3a, temos y = a. Logo, x + a − 3a = 0 e x = 2a, a ∈ R.
Portanto, B = {(2, 1, 3)} é uma base de Ker(T ).

3 Teoria Espectral

As classes de escalares e vetores conhecidas como “autovalores” e “autovetores” são
conceitos relacionados com transformações lineares especiais por suas características pe-
culiares. A ideia surgiu no estudo do movimento rotacional e, mais tarde, foi usada para
classificar tipos de superfícies e nas resoluções de certas equações diferenciais. No século
XX, foi aplicada à matrizes e às transformações matriciais e hoje é aplicada em diversas
áreas.

Neste capítulo apresentaremos os conceitos, propriedades e resultados importantes
envolvendo autovalores e autovetores de uma matriz, e sua aplicação no estudo da diago-
nalização de matrizes, baseados nas referências [1], [2], [3], [5], [6], e [22].

3.1 Autovalores e autovetores de uma transformação

linear

Definição 3.1. Sejam V um espaço vetorial sobre o corpo K e T : V → V um operador
linear. Um vetor v, não nulo, em V , é um autovetor de T , se existir um escalar λ ∈ K,
tal que

T (v) = λv.

Neste caso, dizemos que λ é um autovalor de T associado ao vetor v, ou v é um

autovetor de T associado a λ.

Vejamos agora algumas consequências da definição anterior e exemplos.

Propriedade 3.2. Se v é um autovetor de T associado ao autovalor λ, então qualquer
vetor u = av, não nulo, tambem é autovetor de T associado a λ.

Demonstração.

T (v) = λv,

então T (av) = aT (v) = a(λv) = λ(av).

Portanto, av é autovetor associado ao autovalor λ.

Proposição 3.3. O conjunto de todos os autovetores associados ao autovalor λ, acrescido
do vetor nulo, denotado por V (λ), forma um subespaço do espaço vetorial V , denominado
autoespaço de λ.

V (λ) = { v ∈ V : T (v) = λv , λ ∈ K }.

Demonstração.

Sejam v1, v2 ∈ V (λ) , então T (v1) = λv1 e T (v2) = λv2.
Assim, para quaisquer a, b ∈ K, temos:

43

44

Teoria Espectral

T (av1 + bv2) = T (av1) + T (bv2) = aT (v1) + bT (v2) = a(λv1) + b(λv2) = λ(av1 + bv2).
Logo, av1 + bv2 é um autovetor associado à λ, ou seja, av1 + bv2 ∈ V (λ).
Portanto, V (λ) é um subespaço de V .

Exemplo 3.4. O operador linear T : R3 → R3 definido por

T (x, y, z) = (2x + y, y − z, 2y + 4z), para todo (x, y, z) ∈ R3,

admite os autovalores λ1 = 2 e λ2 = 3.

De fato, λ1 = 2 é um autovalor de T associado ao autovetor v1 = (1, 0, 0), pois
T (v1) = T (1, 0, 0) = (2, 0, 0) = 2(1, 0, 0) = 2v1.
Os vetores da forma v = (a, 0, 0), a em R∗, são autovetores de T associados ao

autovalor 2.

E o autoespaço V (2) = { v ∈ R3 : v = (a, 0, 0), a ∈ R} = h(1, 0, 0)i.

λ2 = 3 é um autovalor de T associado ao autovetor v2 = (1, 1, −2), pois

T (v2) = T (1, 1, −2) = (3, 3, −6) = 3(1, 1, −2) = 3v2 .
Os vetores da forma v = (a, a, -2a), a em R∗, são autovetores de T associados ao

autovalor 3.

E o autoespaço V (3) = { v ∈ R3 : v = (a, a, −2a), a ∈ R} = h(1, 1, −2)i.

Exemplo 3.5. Seja T : C2 → C2 um operador linear definido por T (x, y) = (−y , x),
para todo (x, y) ∈ C2.

λ1 = −i é um autovalor de T associado ao autovetor v1 = (1 , i), pois

T (v1) = T (1 , i) = (−i , 1) = (−i , −i2) = −i(1 , i) = −iv1 .
Todo vetor da forma (a, ai), a em C∗ é um autovetor do operador T associado ao

autovalor −i.

V (−i) = {v ∈ C2 : v = (a , ai) , ∀a ∈ C} = h(1, i)i.

λ2 = i é um autovalor de T associado ao autovetor v2 = (1 , −i), pois

T (v2) = T (1 , −i) = (i , 1) = (i , −i2) = i(1 , −i) = iv2 .
Todo vetor da forma (a, −ai), a em C∗ é um autovetor do operador T associado ao

autovalor i.

V (i) = {v ∈ C2 : v = (a , −ai) , ∀a ∈ C} = h(1, −i)i

.

3.2 Autovalores e autovetores de uma matriz

Também podemos definir autovalor e autovetor para uma matriz.

Definição 3.6. Dada uma matriz quadrada A de ordem n, dizemos que λ é um autovalor
associado ao autovetor v de A, se λ e v forem, respectivamente, o autovalor e autovetor
do operador linear TA : Cn → Cn cuja matriz correspondente é a matriz A, em relação
à base canônica, isto é, T (v) = Av.

Assim, um autovalor λ em C e um autovetor v em Cn de A são soluções da equação

Av = λv , v ̸= 0.

Autovalores e autovetores de uma matriz

45

Usando o fato de que a matriz identidade In é tal que Inv = v, então podemos escrever

a equação anterior na forma matricial como sendo

ou seja ,

Av = λInv,
(A − λIn)v = 0 .

Esta última igualdade resulta em um sistema homogêneo de n equações com n incóg-

nitas, sendo n a ordem da matriz A, isto é, num sistema compatível.

As n incógnitas representam as coordenadas do vetor v, e a matriz A−λI representa

os coeficientes das equações.

Pelo fato do sistema ser homogêneo, caso o determinante da matriz A − λI seja dife-
rente de 0, obteremos uma única solução para o sistema, a trivial, que não nos interessa,
pois teríamos v = 0 .

O sistema (A − λIn)v = 0 admite solução não trivial, isto é, vetores não nulos se, e

somente se,

det (A − λIn) = 0 .

E esta igualdade é denominada equação característica da matriz A, e expandindo
o primeiro membro desta equação obtemos um polinômio de grau n em λ denominado
polinômio característico.

P (λ) = λn + a1λn−1 + a2λn−2 + · · · + an−1λ + an.

Proposição 3.7. Seja A uma matriz quadrada de ordem n. Então, os autovalores de A
são as raízes do polinômio característico P (λ) = det (A − λIn).

Definição 3.8.

Sejam A uma matriz de ordem n e λ um autovalor de A, definimos:
(a) Multiplicidade algébrica do autovalor λ como sendo a multiplicidade da

raiz λ no polinômio característico.

(b) Multiplicidade geométrica do autovalor λ como sendo a dimensão do auto-

espaço associado ao autovalor λ.

Observação 3.9. A multiplicidade geométrica de um autovalor λ não excede sua mul-
tiplicidade aritmética.

Nos exemplos a seguir veremos que os autovalores de uma matriz podem ser obtidos
utilizando-se da Proposição 2.7. ; e os autovetores associados a estes autovalores são ob-
tidos resolvendo-se os sistemas lineares homogêneos indeterminados extraídos da equação
matricial Av = λ.v , sendo v não nulo, para cada um dos autovalores encontrados.

Exemplo 3.10. Determinemos todos os autovalores e autovetores, e uma base de cada
autoespaço da matriz A =

#

.

" 1 2
3 2

Obtemos os autovalores de A encontrando as raízes do polinômio característico
P (λ) = det(A − λI2).
" λ 0
0 λ

" 1 − λ
3

" 1 2
3 2

A − λI2 =

2
2 − λ

#
.

=

#

#

-

46

Teoria Espectral

Assim, P (λ) =

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1 − λ
3

2
2 − λ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= (1−λ)(2−λ)−6 = λ2 −3λ−4 = (λ+1)(λ−4).

Logo, P (λ) = (λ + 1)(λ − 4) admite os valores λ1 = −1 e λ2 = 4 como raízes.
Portanto, −1 e 4 são autovalores da matriz A, ambos de multiplicidade algébrica 1.
Os autovetores (x, y) em R2 associados a estes autovalores são encontrados resolvendo-

se os sistemas obtidos em Av = λv, v ̸= 0, para cada um destes autovalores, isto é,
#

#

" 1 2
3 2

" x
y

.

= λ.

#
.

" x
y

Para λ = −1 , temos:

" 1 2
3 2

# " x
y

#

= −1

#

.

" x
y

" x + 2y
3x + 2y

"

#

=

−x
3x + 2y + y

#

=

#

" 0
0

⇐⇒

" 2x + 2y
3x + 3y

#

=

#

.

" 0
0

Logo,






2x + 2y = 0
3x + 3y = 0

⇐⇒

n

x + y = 0

.

Resolvendo o sistema homogêneo indeterminado, temos y = −x.
Logo, todo vetor da forma v = (x, −x) em R2, com x ̸= 0, é um autovetor associado

ao autovalor −1.

Mas, v = (x, −x) = x(1, −1). Assim, BV (−1) = {(1, −1)} constitui uma base do

autoespaço V (−1), ou seja V (−1) = h(1, −1)i.

E o autovalor −1 possui multiplicidade geométrica igual a 1.

" 1 2
3 2

#

.

" x
y

#

= 4

# " x
y
" x + 2y − 4x
3x + 2y − 4x

Para λ = 4 , temos:

" x + 2y
3x + 2y

#

=

#

" 4x
4y

⇐⇒

Logo,






−3x + 2y = 0
3x − 2y = 0

⇐⇒

n 3x − 2y = 0

.

#

=

#

" 0
0

⇐⇒

" −3x + 2y
3x − 2y

#

=

#
.

" 0
0

Resolvendo o sistema linear homogêneo indeterminado, temos 2y = 3x. Assim, y = 3
Logo, todo vetor da forma u = (x, 3

2x.
2x) em R2, com x ̸= 0, é um autovetor associado

ao autovalor 4.

Mas, u = (x, 3

2x) = x

2

autoespaço V (4), ou seja V (4) = h(2, 3)i.

(2, 3). Assim, BV (4) = {(2, 3)} constitui uma base do

E o autovalor 4 possui multiplicidade geométrica igual a 1.
Neste exemplo podemos observamos que:
Os vetores v = (1, −1) e u = (2, 3) das bases dos autoespaços BV (−1) e BV (4), respec-

tivamente, são LI.

Logo, B ′ = {(1, −1), (2, 3)} forma uma base do espaço vetorial V = R2. Além disso,

V (−1) ⊕ V (4) = R2.

Isto nem sempre ocorre, e é interessante sabermos quais as condições para que isto

aconteça.

Aproveitando o exemplo, determinemos a matriz de T com relação à base B ′:

Autovalores e autovetores de uma matriz

47

T (v) = T (1, −1) = −(1, −1) = (−1, 1) = -1.(1, −1) + 0.(2, 3) = (−1, 0)
T (u) = T (2, 3) = 4(2, 3) = (8, 12) = 0.(1, −1) + 4.(2, 3) = (0, 4)
#

B′ .

B′ .

Observamos que, h
T

i

=

"−1 0
0 4

B′

é uma matriz diagonal formada pelos autovalores

de A.

Mais adiante, veremos que as matrizes

#

" 1 2
3 2

e

#

"−1 0
0 4

são semelhantes. Porém,

trabalharmos com a matriz diagonal se torna mais interessante, pois ela é mais simples
de ser manipulada.

Exemplo 3.11. Determinemos todos os autovalores e autovetores, e uma base de cada
autoespaço do operador linear T : R3 → R3 definido por

T (x, y, z) = (2x + y, y − z, 2y + 4z).

Inicialmente, encontremos a representação matricial de T , em relação à base canônica

do R3:

T (x, y, z) = (2x + y, y − z, 2y + 4z) = x(2, 0, 0) + y(1, 1, 2) + z(0, −1, 4).

A = h
T

i

=

B






2
0
0

1
0
1 −1
4
2


.



Obtenção dos autovalores da matriz A:

A − λI3 =






2
0
0

P (λ) = det(A − λI3).

1
0
1 −1
4
2

0
λ
0

λ
0
0














-

0
0
λ

=






2 − λ
0
0

1

0
1 − λ −1
4 − λ

2


.



Então,

P (λ) =

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

2 − λ
0
0

1

0
1 − λ −1

2

4 − λ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= (2 − λ)(1 − λ)(4 − λ) + 2(2 − λ) =

= (2 − λ)(λ2 − 5λ + 6) = (2 − λ)(λ − 2)(λ − 3) .
Logo, P (λ) admite duas raízes: λ1 = 3 e λ2 = 2 (raiz dupla).
Portanto, os autovalores de T são: 3 de multiplicidade algébrica 1 e 2 de multiplici-

dade algébrica 2 .

Obtenção dos autovetores (x, y, z) ∈ R3 associados aos autovalores encontrados:



Av = λv


1
0
x
1 −1
y
4
2
z










= λ


.








x
y
z






2
0
0

Para λ = 3, temos:

2
0
0

0
1
1 −1
4
2






 .











x
y
z

= 3



 ⇐⇒






x
y
z






2x + y
y − z
2y + 4z






=


.








3x
3y
3z

48

Teoria Espectral






2x + y − 3x
y − z − 3y
2y + 4z − 3z
−x + y = 0
−2y − z = 0
2y + z = 0






=






0
0
0



 ⇐⇒






−x + y
−2y − z
2y + z






=


.








0
0
0

⇐⇒






x − y = 0 (1)
2y + z = 0 (2)

.

Então,






Resolvendo o sistema linear acima temos: da equação (1) que y = x, que substítuido

na equação (2) resulta z = −2x.

Assim, o sistema admite como solução: y = x , z = −2x e x é qualquer número real

diferente de 0.

Os vetores da forma v = (x, x, −2x) ∈ R3, x ̸= 0, são autovetores de T associado

ao autovalor 3.

Mas, v = (x, x, −2x) = x(1, 1, −2). Logo, BV (3) = { (1, 1, −2) } é uma base do

autoespaço V (3), ou seja, V (3) = h(1, 1, −2)i.

E o autovalor 3 possui multiplicidade geométrica igual a 1.

Para λ = 2, temos:






2
0
0

1
0
1 −1
4
2



 .











x
y
z

= 2



 ⇐⇒











2x + y − 2x
y − z − 2y
2y + 4z − 2z
y = 0
−y − z = 0
2y + 2z = 0

Então,











=



 ⇐⇒






⇐⇒






y = 0
y + z = 0

.






x
y
z






0
0
0






2x + y
y − z
2y + 4z


y
−y − z
2y + 2z

=









=






0
0
0


.



2x
2y
2z

.



Resolvendo o sistema linear acima temos: y = 0 , z = 0 e x é qualquer real, x ̸= 0.
Os vetores da forma u = (x, 0, 0) ∈ R3, x ̸= 0,

são autovetores de T associado ao

autovalor 2 .

Mas, u = (x, 0, 0) = x(1, 0, 0). Logo, BV (2) = { (1, 0, 0) } é uma base do autoespaço

V (2), ou seja, V (2) = h(1, 0, 0)i.

E o autovalor 2 possui multiplicidade geométrica igual a 1.

Analisando este exemplo como fizemos com o exemplo anterior, observamos que:
os autoespaços encontrados forneceram somente dois vetores linearmente independentes
(1, 2, −4) e (1, 0, 0) , e portanto o conjunto formado por eles não formam uma base de
V = R3.

Exemplo 3.12. Determinemos os autovalores e autovetores, e a base de cada auto espaço

da matriz triangular superior A =






1 0 2
0 1 1
0 0 3


.



Obtenção dos autovalores da matriz A:

P (λ) = det(A − λI3).

Autovalores e autovetores de uma matriz

49

A − λI3 =

Então,






1 0 2
0 1 1
0 0 3











-

λ 0
0 λ
0

0
0
0 λ






=






3 − λ
0
0

0
3 − λ
0


.



2
1
2 − λ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

P (λ) =

1 − λ
0
0

0
1 − λ
0

2
1
3 − λ
Logo, P (λ) admite raízes: λ1 = 1 (raiz dupla) e λ2 = 3.
Portanto, os autovalores de T são: 1 de multiplicidade algébrica 2 e 3 de multiplici-

= (1 − λ)(1 − λ)(3 − λ) = (1 − λ)2.(3 − λ) .

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

dade algébrica 1 .

Obtenção dos autovetores (x, y, z) ∈ R3

associados aos autovalores encontrados:

Av = λv.












= λ




x
y
z


.








x
y
z






1 0 2
0 1 1
0 0 3

Para λ = 1, temos:





1 0 2
0 1 1
0 0 3















x
y
z

= 1






x
y
z



 ⇐⇒











=


.








x
y
z

x + 2z
y + z
3z











x + 2z − x
y + z − y
3z − z

2z
z
2z
O sistema linear apresenta duas variáveis livres x e y. E admite a solução z = 0 , x e



 ⇐⇒

Então, n

z = 0


.



0
0
0

0
0
0

=

=




















.

y números reais quaisquer, não ambos nulos.

Assim, todo vetor da forma v = ( x, y, 0 ) em R3, com x ̸= 0 e y ̸= 0 é autovetor

da matriz A associado ao autovalor 1 .

Mas, v = ( x, y, 0 ) = x(1, 0, 0) + y(0, 1, 0). Logo, BV (1) = { (1, 0, 0), (0, 1, 0) } é

uma base do autoespaço V (1), ou seja, V (1) = h(1, 1, 0), (0, 1, 0)i.
E o autovalor 1 possui multiplicidade geométrica igual a 2.

Para λ = 3, temos:














1 0 2
0 1 1
0 0 3

x
y
z
x + 2z − 3x
y + z − 3y
3z − 3z











= 3



 ⇐⇒






x
y
z






=






0
0
0



 ⇐⇒


.








x + 2z
y + z
3z
−2x + 2z
−2y + z
0






=

=






3x
3y
3z

0
0
0


















.



.

Então ,




−2x + 2z = 0
−2y + z = 0

⇐⇒

−x + z = 0 (1)
−2y + z = 0 (2)


Resolvendo o sistema linear obtemos da equação (1) que z = x, e da equação (2) que



z = 2y. Portanto, x = z = 2y.

Assim, todo vetor da forma u = (2y, y, 2y), com y ̸= 0, é uma autovetor da matriz

A associado ao autovalor 3.

50

Teoria Espectral

Mas, u = (2y, y, 2y) = y(2, 1, 2). Logo, BV (3) = { (2, 1, 2) } é uma base do

autoespaço V (3), ou seja, V (3) = h(2, 1, 2)i.

E o autovalor 3 possui multiplicidade geométrica igual a 1.

Neste exemplo, observamos que o conjunto de vetores (1, 1, 0) , (0, 1, 0) e (2, 1, 2)
obtidos das bases dos autoespaços V (1) e V (3) são LI, e formam uma base do espaço
vetorial V = R3. Além disso, V (1) ⊕ V (3) = R3.

Observação 3.13. Se A é uma matriz triangular (superior ou inferior), então os seus
autovalores são os elementos de sua diagonal principal.

De fato, pois o determinante de uma matriz triangular A é dado pelo produto dos
elementos de sua diagonal principal, e como a matriz A − λI é triangular sendo sua
diagonal formada pelos elementos da forma aii − λ, para i = 1, 2, . . . n. Então,

p(λ) = det(A − λIn) = (a11 − λ)(a22 − λ) · . . . · (ann − λ).

Logo, a11, a22, · · · , ann são raízes de p(λ).
Portanto, os autovalores de uma matriz triangular são os elementos de sua diagonal

principal.

Proposição 3.14. Sejam A uma matriz de ordem n com autovalor λ associado ao
autovetor v. Então,

(a) A não é invertível se, e somente se, 0 é autovalor de A.
(b) Se A é invertível, então λ−1 é um autovalor de A−1 associado ao vetor v.
(c) λn é um autovalor de An e v é o seu autovetor associado.

Demonstração.
(a) Se A é uma matriz invertível, então detA ̸= 0.

Suponhamos por contraposição que A admita um autovalor nulo.
Assim, da equação det(A − λIn) = 0, temos det A = 0.
Logo, A não invertível, que é absurdo, pois admitimos que A fosse invertível.
Portanto, 0 não é autovalor de A.

(b) Com A é invertível. Logo, pela (i) , λ ̸= 0.

Seja λ um autovalor de A associado ao autovetor v, então Av = λv.
Multiplicando por A−1 a ambos os lados desta última igualdade resulta:
v = A−1λv = λA−1v.

Portanto, A−1v = λ−1v, ou seja, λ−1 é autovalor de A−1.

(c) Demonstremos por indução sobre n.

Para n = 1 a proposição é válida, pois Av = λv.
Suponhamos por indução que a proposição seja válida para todo n − 1, com n > 2,
ou seja, An−1v = λn−1v.
Multiplicando à esquerda ambos os membros desta última igualdade por A, temos:
Anv = A(An−1v) = A(λn−1v) = λn−1(Av) = λn−1(λv) = (λn−1λ)v = λnv

Portanto, Anv = λnv.

Teorema 3.15. Se v1, v2, . . . , vn são autovetores de uma matriz A associados a autova-
lores distintos λ1, λ2, . . . , λn, respectivamente, então v1, v2, . . . , vn são LI.

Autovalores e autovetores de uma matriz

51

Demonstração. Demonstraremos por indução sobre n.

Se n = 1, temos a1v1 = 0 , então que v1 é LI, pois v1 ̸= 0. Logo, o teorema é válido.
Suponhamos por hipótese de indução que o teorema seja válido para n − 1 vetores,
ou seja, os vetores v1, v2, · · · vn−1 são LI, e vamos mostrar que também é válido para n
vetores.
Seja
Aplicando T na relação (2.5), obtemos pela linearidade:

a1v1 + a2v2 + · · · + anvn = 0, onde ai ∈ K, i = 1, 2, . . . , n.

(2.5)

a1T (v1) + a2T (v2) + · · · + anT (vn) = 0.

Mas, por hipótese, T (vi) = λivi , para i = 1, 2, . . . , n. Logo,

a1λ1v1 + a1λ2v2 + · · · + anλnvn.

Por outro lado, multiplicando a relação (2.5) por λn, temos:

a1λnv1 + a2λnv2 + · · · + anλnvn = 0.

(2.6)

(2.7)

Agora, subtraindo (2.7) de (2.6):

a1(λ1 − λn)v1 + a2(λ2 − λn)v2 + · · · + an−1(λn−1 − λn)vn + an(λn − λn)vn = 0.
a1(λ1 − λn)v1 + a2(λ2 − λn)v2 + · · · + an−1(λn−1 − λn)vn = 0.
Por indução, cada um dos coeficientes acima é 0. Como os λi são distintos, então

λi − λn ̸= 0, i ̸= n , para i = 1, 2, . . . , n − 1.

Portanto, a1, a2, . . . , an−1 = 0.
Logo, an = 0 , pois vn ̸= 0 e, portanto, v1, v2, . . . , vn são LI.

Substituindo em (2.5), temos, anvn = 0.

Corolário 3.16. Seja T : V → V um operador linear. Se dim V = n e T possui n
autovalores distintos, então V possui uma base formada por autovetores de T .

Demonstração. Pelo teorema anterior, n autovalores distintos implicam na existência de
um conjunto de n autovetores { v1, v2, . . . , vn} LI.
i

h

v1, v2, . . . , vn

⊂ V e dim

v1, v2, . . . , vn

i = n = dim V , temos que

v1, v2, . . . , vn

i = V . Logo, { v1, v2, . . . , vn} é uma base de V .

Como h
h

Teorema 3.17. Se a matriz A, de ordem n, possui autovalores λ1, λ2, . . . , λn, distintos.
Então, a soma dos autoespaços de A é direta, isto é, para cada j = 1, 2, . . . , n, temos
V (λi) ∩ [ V (λ1) + · · · + V (λi−1) + V (λi+1) + · · · + V (λn) ] = {0}.

Demonstração. Provaremos por indução sobre o número de autovalores.

Inicialmente mostremos que V (λ1) ∩ V (λ2) = {0}.
Fixemos u(1)
uma base de V (λ1) e u(2)
Se v ∈ V (λ1) ∩ V (λ2 , então
v = a(1)

1 , . . . , u(1)
m1

+ · · · + a(1)

= a(2)

1 u(2)

1

m1u(1)
m1

1 u(1)
1
Logo, T (u) é dado por
a(1)
1 T (u(1)

1

ou seja,

) + · · · + a(1)

m1T (u(1)
m1

) = a(2)

1 T (u(2)

1

) + · · · + a(2)

m2T (u(2)
m2

)

1 , . . . , u(2)
m2

uma base de V (λ2).

+ · · · + a(2)

m2u(2)
m2.

(3.1)

1 λ1u(1)
a(1)

1

+ · · · + a(1)

m1λ1u(1)
m1

= a(2)

1 λ2u(2)

1

+ · · · + a(2)

m2λ2u(2)
m2.

(3.2)

Multiplicando a equação (2.1) por λ1 e subtraindo-a da equação (2.2), obtemos:

a(2)
1

(λ2 − λ1)u(2)
1

+ · · · + a(2)
m2

(λ2 − λ1)u(2)
m2

= 0.

52

Teoria Espectral

Mas, u(2)

é uma base de V (λ2) , então:
(λ2 − λ1) = · · · = a(2)
m2
= · · · = a2
m2

1 , . . . , u(2)
m2
a(2)
1
E como λ1 ̸= λ2 , resulta que a(2)
1
Suponhamos agora, por indução, que a soma de n − 1 autoespaços de T referentes
a n − 1 autovetores distintos seja direta. E vamos mostrar que este resultado é válido
quando T apresentar n autovalores distintos.

= 0. Segue-se de (2.1) que v = 0.

(λ2 − λ1) = 0.

Para cada j = 1, 2, . . . , n escolhemos uma base Bj de V (λj) formada por vetores que
. Observe que, cada v(j)
é uma autovetor associado ao

denotaremos por, v(j)
2 , . . . , v(j)
mj
autovalor λj e que mj é a multiplicidade geométrica deste autovalor.

1 , v(j)

i

Se u ∈ V (λj) ∩ [ V (λ1) + · · · + V (λj−1) + V (λj+1) + · · · + V (λn) ].
Então,
u = a(j)

= a(1)

+· · ·+a(j−1)
mj−1

+· · ·+a(j)
mj

+a(j+1)
1

1 v(1)

1

1 v(j)

1

v(j+1)
1

v(j−1)
mj−1

v(j)
mj

+· · ·+a(n)

mnv(n)

mn. (3.3)

Assim, T (u) é dado por
1 T (v(j)
a(j)
T (v(j)
) + · · · + a(j)
mj
mj

1

) = a(1)

1 T (v(1)

1

Ou seja,

1 λjv(j)
a(j)

1

+ · · · + a(j)
mj

λjv(j)
mj

) + · · · + a(j−1)
mj−1
T (v(j+1)
1

+ a(j+1)
1

T (v(j−1)
mj−1
) + · · · + a(n)

) +

mnT (v(n)
mn

= a(1)

1 λ1v(1)

+ · · · + a(j−1)
mj−1
λj+1v(j+1)
1

1
+a(j+1)
1

λmj−1v(j−1)
mj−1
+ · · · + a(n)

+

mnλnv(n)
mn.

).

(3.4)

a(j)
1

(λ1 − λj)v(1)
1

Multiplicando a equação (2.3) por λj e subtraindo-a da equação (2.4), temos:
(λj−1 − λj)v(j−1)
mj−1
(λj+1 − λj)v(j+1)
Usando a hipótese de indução e o fato que λj ̸= λi, quando i ̸= j, obtemos:

+ · · · + a(j−1)
mj−1
+ a(j+1)
1

(λn − λj)v(n)
mn

+ · · · + a(n)
mn

+

1

= 0.

= · · · = a(i)
mi
Deste resultado e da equação (2.3) resulta que u = 0.

0 , para todo i = 1, 2, . . . , n.

a(i)
1

Exemplo 3.18. A matriz A =

admite dois autovalores
λ1 = −1 e λ2 = 4 e os respectivos autoespaços V (−1) = h(1, −1)i e

(Exemplo 2.10.)

distintos:
V (4) = h(2, 3)i. E vimos que, V (−1) ⊕ V (4) = R2.

#

" 1 2
3 2

Convém observar que o teorema não garante a recíproca, ou seja, que se a soma dos

autoespaços de uma matriz é direta então todos os seus autovalores são distintos.

Esse fato, percebemos no Exemplo 2.12., com a matriz A =






1 0 2
0 1 1
0 0 3






em que a

soma dos autoespaços V (1) = h(1, 1, 0), (0, 1, 0)i e V (3) = h(2, 1, 2)i é direta, isto é,
V (1)⊕V (3) = R3. No entanto, nem todos os autovalores de A apresentam multiplicidade
algébrica igual a 1, ou seja, não temos três autovalores distintos, apenas 2.

Definição 3.19. Sejam A e B matrizes de ordem n . Dizemos que, B é semelhante
ou similar a A ; ou que B é obtida de A por uma transformação de semelhança, que
denotamos por A ∼ B, se existir uma matriz invertível P tal que B = P −1AP.

Diagonalização de operadores

53

Teorema 3.20. Sejam A, B ∈ Mn×n(K). Se B é semelhante a A, então o polinômio
característico de A é idêntico ao de B.

Demonstração. Sejam PA(λ) e PB(λ) os polinômios característicos da matrizes A e B,
respectivamente.

Como A ∼ B, existe uma matriz invertível P tal que A = P −1BP.
Mas, PA(λ) = det (A − λI) = det (P BP −1 − λI) = det (P BP −1 − λP P −1) =

= det (P B − P λ)P −1 = det [P (B − λI)P −1] =
= det P. det (B − λI). det P −1 = det P. det (B − λI).
= det (B − λI) = PB(λ).

1
detP

=

Corolário 3.21. Sejam A, B ∈ Mn×n(K) matrizes semelhantes. Então,

(a) A e B representam o mesmo operador linear.
(b) det A = det B.
(c) possuem os mesmos autovalores, com a mesma multiplicidade; e também os

mesmos autovetores associados a estes autovalores.

(d) A é invertível se, e somente se, B é invertível.
(e) trA = trB.
(f) post(A) = post(B).

Observação 3.22. Dados um operador T : V → V , B e C duas bases distintas de V ,
com dimV < ∞, então as matrizes h
T

são semelhantes e vale:

e h

T

i

i

h
T
C
onde MB→C é a matriz de mudança de base de B para C e MC→B é a matriz de mudança
de base de C para B.

T

B

h

i

i

B
= MB→C

C
MC→B.

3.3 Diagonalização de operadores

Muitos sistemas podem ser representados por matrizes, e em determinadas situações,
não é fácil de serem manipuladas, então devemos encontrar matrizes mais simples possível
para representar tais sistemas de modo a minimizar tempo e custo operacional. Uma
das formas de obter esta simplificação consiste em diagonalizar a matriz dada, quando
possível.

A diagonalização de matrizes quadradas consiste na obtenção de uma matriz diagonal
que seja semelhante à matriz original dada. O interessante da obtenção de uma matriz
diagonal é que todos os seus elementos fora da sua diagonal principal são iguais a zero, o
que implica, matematicamente, em uma redução significativa no tempo de processamento
utilizando estas matrizes. Além disso, uma matriz diagonal semelhante preserva todas
as condições mencionadas no Corolário 2.21 em relação à matriz original; dentre essas
condições, uma importante é preservar os autovalores, e veremos que estes irão compor a
diagonal dessa matriz, o que significa que podemos determinar facilmente essas matrizes.
Dado um operador linear T em V , vimos que, a cada base B de V corresponde uma
matriz [T ]B que representa T nesta base. O objetivo agora será obter, quando possível,

54

Teoria Espectral

uma outra base B ′ de V de modo que a matriz do operador linear T nesta nova base seja
uma matriz diagonal.

Definição 3.23. Dizemos que um operador linear T definido sobre um espaço vetorial
V de dimensão finita sobre um corpo K é diagonalizável, se for possível representá-lo
por uma matriz diagonal em alguma base de V .

Dado um operador T , os teoremas a seguir caracterizam base associada ao operador

que se pretende diagonalizar.

Teorema 3.24. Um operador T : V → V admite uma base B em relação à qual a matriz
h
é diagonal se, e somente se, essa base B for formada por autovetores de T .

T

i

B

Demonstração. Suponhamos que T é diagonalizável, ou seja, exista uma base
B = {v1, v2, . . . vn} de V tal que h
é diagonal.
T
B

0
0
λ1
· · ·
0
0 λ2
· · ·
...
...
...
. . .
0
0
· · · λn



.










=

T



B

i

h

i

Tomando-se um elemento vi da base B, observe que, da maneira como a matriz que
é formada pelos coeficientes

representa T com relação à base B, a coluna i da matriz h
T (vi) escrito como combinação linear dos elementos da base B, isto é,

T

B

i

T (vi) = 0v1 + · · · + λivi + · · · + 0vn = λivi.

Assim, T (vi) = λivi, para i = 1, 2, . . . , n. Logo, vi é um autovetor de T associado ao

autovalor λi.

Portanto, a base B é formada por autovetores de T .
Agora, suponhamos que B = {v1, v2, . . . vn} seja uma base para V formada por autove-
tores de T , ou seja, T (vi) = λivi, para i = 1, 2, . . . , n , sendo λ1, λ2, . . . , λn os respectivos
autovalores do operador T .

Como as imagens dos elementos vi da base B pelo operador linear T , escritos como

combinação linear dos elementos de B, são da forma T (vi) = λivi, resulta que
0
λ1
0 λ2
...
...
0
0

0
· · ·
0
· · ·
...
. . .
· · · λn



.










h
T

=





B

i

é a matriz que representa T com relação à base B, que é uma matriz diagonal. Portanto,
T é um operador diagonalizável.

Exemplo 3.25. A matriz A =

#

" 1 2
3 2

(Exemplo 2.10.) admite a base

B ′ = {(1, −1), (2, 3)} de R2 formada de autovetores associados pelos autovalores −1 e 4,
respectivamente. Logo, pelo teorema anterior, A é diagonalizável, e a matriz diagonal
h
formada pelos seus autovalores diagonaliza A.

=

T

#

i

"−1 0
0 4

B′

As matrizes

#

" 1 2
3 2

e

#

"−1 0
0 4

são semelhantes. Porém, a matriz diagonal se

torna mais interessante, pois é mais simples de ser manipulada.

Diagonalização de operadores

55

Exemplo 3.26. A matriz A =

0
1
1 −1
4
2
autovetores LI. Logo, não existe uma base de R3 constituída só de autovetores.

2
0
0







do Exemplo 2.11. admite somente dois





Portanto, A não é diagonalizável.

Exemplo 3.27. Consideremos a matriz A =

Seu polinômio característico:






4
0
0
0 −1 −5
1
1
0






.

P (λ) =

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

4 − λ
0
0

0
−1 − λ
1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
= (4 − λ)(−1 − λ)(1 − λ) + 5(4 − λ) = (4 − λ)(λ2 + 4).

0
−5
1 − λ

= (4 − λ)(−1 − λ)(1 − λ) + 5(4 − λ) =

Portanto, λ1 = 4 , λ2 = 2i e λ3 = −2i são raízes de P (λ).
Se A é uma matriz sobre o corpo R, então A tem somente o autovalor 4.
Logo, um único autovetor e, portanto, A não é diagonalizável.
No caso de A ser uma matriz sobre o corpo C, então A têm três autovalores distintos:

4 , 2i e -2i.

Então, A admite três auvetores LI e, portanto, A é diagonalizável.

Definição 3.28. Uma matriz A de ordem n é chamada de diagonalizável se existir
alguma matriz invertível P tal que P −1AP é uma matriz diagonal. Neste caso, dizemos
que a matriz P diagonaliza A.

Teorema 3.29. Seja V um espaço vetorial de dimensão finita sobre um corpo K e T um
operador linear sobre V . Então, T é diagonalizável se, e somente se:

(a) o polinômio característico de T possui todas as suas raízes em K.
(b) a multiplicidade algébrica de cada autovalor de T é igual a sua multiplicidade

geométrica.

Demonstração. (a) Se T é um operador diagonalizável, então V possui uma base formada
por autovetores de T (Teorema 2.24).

Seja B = {v11, v12, . . . , v1r1, v21, v22, . . . , v2r2, . . . , vk1, vk2, . . . , vkrk} essa base, de modo
que em cada {vi1, vi2, . . . , viri} estão todos os autovetores associados ao autovalor λi , para
i = 1, 2, . . . , k

A matriz que representa T em relação à base B é a matriz diagonal:

i

h
T

B

=


















λ1
...
0
...
0
...
0

0
. . .
...
. . .
. . . λ1
...
0
...
0

. . .

. . .

. . .

0
...
0
. . .
...
. . .
. . . λk
...
0

. . .

. . .

. . .

0
...
0
...
0
. . .
...
. . .
. . . λk









.









56

Teoria Espectral

P (λ) = det (h
T
de T cujas raízes estão todas em K.

B

i

−λI) = (λ1 −λ)r1(λ2 −λ)r2 · · · (λk −λ)rk é o polinômio característico

Para cada índice i, seja Ui , i = 1, 2, . . . , k, o subespaço gerado por {vi1, vi2, . . . , viri}.
e, portanto, é um
Um elemento u ∈ Ui

é combinação linear de vi1, vi2, . . . , viri

autovetor associado ao autovalor λi, ou seja, u ∈ V (λi). Logo, Ui ⊂ V (λi).
Agora, tomando u ∈ V (λi) como combinação linear dos elementos de B:
u = a11v11 + · · · + akrkvkrk

.

Multiplicando por λi e utilizando que T (u) = λiu, uma vez que u ∈ V (λi), temos:

λia11v11 + · · · + λia1r1v1r1

+ · · · + λiak1vk1 + · · · + λiakrkvkrk

=

= λiu = T (u) = a11T (u11) + · · · + akrkT (ukrk
= λ1a11v11 + · · · + λ1a1r1v1r1

) =
+ · · · + λkak1vk1 + · · · + λkakrkvkrk

.

Comparando a primeira e a última combinação linear acima, obtemos:

λia11 = λ1a11, . . . , λia1r1

...

= λ1a1r1

.

λiak1 = λkak1, . . . , λiakrk

= λkakrk

.

E assim,

a11 = · · · = a1r1

= · · · = a(i−1)1 = · · · = a(i−1)r(i−1)

=

= a(i+1)1 = · · · = a(i+1)r(i+1)

= · · · = ak1 = · · · = akrk

= 0.

o que implica que u ∈ Ui.

Logo, u = ai1vi1 + · · · + airiviri
Assim, V (λi) ⊂ Ui .
Dessa forma, mostramos que Ui = V (λi), para i = 1, 2, . . . , k.
Portanto, os autovalores de T têm a mesma multiplicidade algébrica e geométrica.
Reciprocamente, sabemos que os autovalores de T são as raízes do polinômio caracte-

rístico de T .

Como V tem dimensão n, o polinômio característico de T terá grau n, e se ele possuir
todas as suas raízes em K, então pelo Teorema Fundamental da Álgebra temos que T
possui n autovalores λ1, λ2, . . . , λn, não necessariamente, distintos.

Mas a multiplicidade algébrica de cada autovalor de T é igual a sua multiplicidade
geométrica, então é possível associarmos a um mesmo autovalor λ de T um conjunto LI
formado por autovetores, que será uma base para o autoespaço V (λ).

Além disso, os autovetores associados a autovalores distintos também serão LI, e como

a dimensão de V é n, temos que {v1, v2, . . . vn} é uma base para V .

Portanto, T é um operador diagonalizável.

Exemplo 3.30. A matriz A =






1 0 2
0 1 1
0 0 3






(Exemplo 2.12) possui autovalor 1 com

multiplicidade 2, auto espaço V (1) = h(1, 0, 0), (0, 1, 0)i , um autovalor 3 e autoespaço
V (3) = h(2, 1, 2)i.

Observamos que, os autovalores possuem multiplicidades aritmética iguais às geomé-

tricas. Portanto, pelo teorema anterior, A é diagonalizável.

Diagonalização de operadores

57



Exemplo 3.31. A matriz A =

0
1
1 −1
4
2
multiplicidade algébrica 2, mas o autoespaço associado V (2) = h(1, 1, 0)i possui multipli-
cidade geométrica 1. Portanto, pelo teorema anterior, a matriz A não é diagonalizável.

(Exemplo 2.11.) admite autovalor 2, de

2
0
0









Exemplo 3.32. Consideremos a matriz A =








2
0
0
0

5
1
0
0

7
4
9
3
5
6
0 −1




.




Como a matriz A é triangular, pela Observação 2.13., então os seus autovalores são

os elementos da diagonal principal, λ1 = 2 , λ2 = 1 , λ3 = 8 e λ4 = −1.

Como os autovalores são todos distintos, então A é diagonalizável. (Corolário 2.16. e

Teorema 2.15.).

Exemplo 3.33. Determinemos a matriz P que diagonaliza a matriz A =






4
0
1

2
3
2



 .

1
0
4

Calculemos os autovalores de A:


A − λI =




4 − λ
0
1

2
3 − λ
2


.



1
0
4 − λ

P (λ) = det(A − λI) = (4 − λ)(3 − λ)(4 − λ) − (3 − λ) = (3 − λ)(λ2 − 8λ + 15) =

= (3 − λ)(λ − 3)(λ − 5) = (λ − 3)2(λ − 5).

Logo, os autovalores da matriz A são λ1 = λ2 = 3 e λ3 = 5 que são as raízes do

polinômio característico .

(A − λI3)v = 0 , ou seja ,






4 − λ
0
1

2
3 − λ
2

1
0
4 − λ
















x
y
z

=


.








0
0
0

Para λ = 3 ,

temos






4 − 3
0
1

2
3 − 3
2



1
0


4 − 3






x
y
z






=


.








0
0
0






1
0
1

2
0
2











1
0
1






x
y
z

=






0
0
0



 ⇐⇒






x + 2y + z = 0
x + 2y + z = 0

⇐⇒

n

x + 2y + z = 0.

Resolvendo o sistema linear homogêneo, temos como solução: z = −x−2y , e x, y ∈ R,

não ambos nulos.

Assim, todo vetor v = (x, y, −x − 2y) ∈ R3 com x ̸= 0 e y ̸= 0 é um autovetor da

matriz A associado ao autovalor 3.

Mas, v = (x, y, −x − 2y) = x(1, 0, −1) + y(0, 1, −2).
Logo, BV (3) = {(1, 0, −1), (0, 1, −2)} é uma base do autoespaço V (3).

Para λ = 5 ,

temos






4 − 5
0
1

2
3 − 5
2



1
0


4 − 5






x
y
z






=


.








0
0
0

58

Teoria Espectral


−1



2
0 −2
1

1
0
2 −1
















x
y
z

=






0
0
0



 ⇐⇒






−x + 2y + z = 0
−2y = 0
x + 2y − z = 0

⇐⇒






x − z = 0
y = 0

.

Resolvendo o sistema linear homogêneo, obtemos x = z e y = 0 .
Assim, todo vetor u = (x, 0, x) ∈ R3 e x ̸= 0 é um autovetor da matriz A associado

ao autovalor 5.

Mas, u = (x, 0, x) = x(1, 0, 1).
Logo, BV (5) = {(1, 0, 1)} é uma base do autoespaço V (5).
Dessa forma, temos três autovetores v1 = (1, 0, −1) , v2 = (0, 1, −2) e v3 = (1, 0, 1)

linearmente independentes pois pertencem às bases dos autoespaços de A.

Construção da matriz P :

cuja inversa é P −1 =








1
0

1
0
1

i =

v1 v2 v3

P = h

0
1


−1 −2
Obtenção da matriz diagonal D a partir dos autovalores de A.
1
2 −1 − 1
1
2
0
1
0
0
1
1
1
2
2
1
0

D = P −1AP =

4
0
1

2
3
2

1
0
4













=





















.

.

.

2 −3 − 3
3
2
3
0
0
5
5
5
2
2




0
1


−1 −2

1
0
1




0
1


−1 −2

0
0
0
3
.


5
0

3
0
0




D =









2 −1 − 1
1
2
0
1
0
1
1
1
2
2


.




.



1
0
1

Em geral, não existe uma ordem preferencial para as colunas de P . Se trocarmos a
ordem das colunas na construção da matriz P , alteramos a ordem dos autovalores na
diagonal da matriz P −1AP . Assim, por exemplo, se tivéssemos construído
3
0
0

então D = P −1AP =

v1 v3 v2

P = h



 .

0
1
0
1
1 −2

1
0
−1

i =

0
5
0

0
0
3
















Observação 3.34. A seguir provaremos importantes resultados envolvendo matrizes
simétricas. Tais matrizes desempenham papel relevante no estudo da diagonalização de
operadores, sendo uma de suas aplicações no reconhecimento de cônicas e quadráticas.
Resumiremos alguns resultados marcantes entre os casos simétrico e não simétrico.
(a) se A é uma matriz simétrica todos os seus autovalores são reais, cada autovalor possui

multiplicidade aritmética igual a geométrica, e A é diagonalizável.

(b) se A é uma matriz não-simétrica seus autovalores nem sempre são todos reais, e mes-
mo que todos sejam reais é possível que a matriz A não seja diagonalizável. É o caso
em que um autovalor tenha multiplicidade geométrica menor que à aritmética.
(c) se a matriz A é simétrica então autovetores associados a autovalores distintos são or-

togonais, o que não acontece sendo A não-simétrica.

Definição 3.35. Um operador T : V → V sobre um corpo K admite um operador
adjunto T ∗ : V → V quando

⟨ T (u) , v ⟩ = ⟨ u , T ∗(v) ⟩ , ∀u, v ∈ V .

Dizemos que T é autoadjunto quando T ∗ = T .

Diagonalização de operadores

59

Observação 3.36. Seja B uma base ortonormal de V e T : V → V um operador
autoadjunto, então
(a) Quando K = R, h

é simétrica e denominamos o operador T autoadjunto de

T

i

simétria.

(b) Quando K = C, h

i

T

B
é hermitiana, isto é, h

B

T

iT

B

= h
T

B

iT

e denominamos o operador

T autoadjunto hermitiano.
Portanto, temos um critério prático para determinarmos se um dado operador T é
autoadjunto. Basta considerarmos qualquer base ortonormal B de V e verificarmos se a
matriz h

é simétrica (caso real) ou é hermitiana (caso complexo).

i

T

B

Exemplo 3.37. Consideremos B a base canônica do R3, que é uma base ortornormal, e
o operador T : R3 → R3 definido por

T (x, y, z) = (3x + y − 2z, x + 2y + 4z, −2x + 4y + 5z).





h

i =

Então,

3
1
−2
Portanto, T é um operador autoadjunto.

1 −2
4
2
5
4







T

é uma matriz simétrica.

Exemplo 3.38. Determinemos um operador em R2 que seja autoadjunto.

De acordo com a definição, basta tomarmos qualquer matriz simétrica de ordem 2. Por

#

" 5 3
3 2

exemplo, considerando a matriz simétrica A =

, podemos definir um operador

autoadjunto como sendo TA : R2 → R2 tal que TA(x, y) = (5x + 3y, 3x + 2y), sendo B
a base canônica de R2.

Teorema 3.39. Se T : V → V é um operador autoadjunto, então os seus autovalores
são reais.

Demonstração. Seja λ um autovalor associado ao autovetor v, então Av = λv.
Suponhamos λ ∈ C seja um autovalor de T associado ao autovetor v, ou seja,

Como T é um operador autoadjunto, temos:

T (v) = λv.

⟨T (v), v⟩ = ⟨v, T (v)⟩.
e

⟨T (v), v⟩ = ⟨λv, v⟩ = λ⟨v, v⟩

Mas,
Assim, λ⟨v, v⟩ = λ⟨v, v⟩, então (λ − λ)⟨v, v⟩ = 0.
Como ⟨v, v⟩ ̸= 0, temos que, (λ − λ) = 0. Logo, λ = λ.
Portanto, λ é real.

⟨v, T (v)⟩ = ⟨v, λv⟩ = λ⟨v, v⟩.

Teorema 3.40.

Seja T é um operador autoadjunto. Se λ1, λ2, . . . , λn são autovalores de T dois a dois

distintos, então os autovetores associados v1, v2, . . . , vn são dois a dois ortogonais.
Demonstração. Seja i ̸= j, então

λi⟨vi, vj⟩ = ⟨λivi, vj⟩ = ⟨T (vi), vj⟩ = ⟨vi, T (vj)⟩ = ⟨vi, λjvj⟩ = λj⟨vi, vj⟩.

Logo, λi⟨vi, vj⟩ = λj⟨vi, vj⟩, então λi⟨vi, vj⟩ − λj⟨vi, vj⟩ = 0.
Assim, (λi − λj)⟨vi, vj⟩ = 0.
Como, λi ̸= λj, então ⟨vi, vj⟩ = 0.
Portanto, vi é ortogonal a vj, para todo i ̸= j.

60

Teoria Espectral

Teorema 3.41. Seja T um operador autoadjunto. Então, a dimensão do autoespaço
associado a um autovalor é igual à multiplicidade deste autovalor.

Proposição 3.42. Se T é um operador autoadjunto e λ1, λ2, . . . , λk são os autovalores
de T . Então, V = V (λ1) ⊕ V (λ2) ⊕ · · · ⊕ V (λk). Além disso, para todo i ̸= j, V (λi) e
V (λj) são ortogonais.

Demonstração. Para provarmos que V é soma direta dos autoespaços

V (λ1), V (λ2), . . . , V (λk),

vamos mostrar que se v1 + v2 + · · · + vk = 0, com vj ∈ V (λj) , para j = 1, 2, . . . , k ,
então todos os vj são nulos, para todo j = 1, 2, . . . , k.

Suponhamos que 0 = v1 + v2 + · · · + vk, vj ∈ V (λj), para i = 1, 2, . . . , n. Mas,
⟨0, vj⟩ = ⟨v1 + v2 + · · · + vk , vj⟩ = ⟨v1, vj⟩ + ⟨v2, vj⟩ + · · · + ⟨vj, vj⟩ + · · · + ⟨vk, vj⟩.
Como ⟨vi, vj⟩ = 0 , para todo i ̸= j.
Então, ⟨vj, vj⟩ = 0, o que implica que vj = 0 , ∀j.
Portanto, V é soma direta dos autoespaços V (λ1), V (λ2), . . . , V (λk).
E todo vetor v em V , pode ser escrito de maneira única como sendo v = v1+v2+· · ·+ vk

com vj ∈ V (λj).

O próximo teorema, um dos mais importantes da Álgebra Linear, garante que todo

operador autoadjunto é diagonalizável.

Teorema 3.43. (Teorema Espectral) Sejam V um espaço vetorial de dimensão n e
T : V → V
um operador autoadjunto. Então, existe uma base B ortonormal de V
formada por autovetores de T . Em particular, T é ortogonalmente diagonalizável.

Demonstração. Demonstraremos por indução sobre a dimensão do espaço vetorial V .

Se dim V = 1, então, existe v ̸= 0 em V , e todo vetor de V é um múltiplo escalar de

v, ou seja, V = h

v

i.

Então T : V → V é tal que T (v) = kv. Logo, v é um autovetor de T . Portanto,

B = { v

∥v∥} é uma base ortonormal de V .

Suponhamos por hipótese de indução que o teorema seja verdadeiro para todo espaço

vetorial com dimensão menor que a dimensão de V .

Seja v1 ∈ V , v1 ̸= 0, um autovetor de T , e consideremos o subespaço S = h

i, cuja

v1

dimensão é 1.

Como V = S ⊕ S⊥, então dim S⊥ = n − 1 < dim V = n.
Consideremos T |S⊥ : S⊥ → S⊥ , a restrição do operador auto-adjunto T no subespaço

S⊥, e verifiquemos que para todo v ∈ S⊥ temos T (v) ∈ S⊥.

Seja u ∈ S , então u = cv1.
Como T é operador autoadjunto segue que

⟨T (v), u⟩ = ⟨T (v), cv1⟩ = c⟨v, T (v1)⟩ =

= c ⟨v, λ1v1⟩ = cλ1⟨v, v1⟩ = 0 , pois v ∈ S⊥ e v1.

Assim, T |S⊥ é um operador linear em S⊥, também autoadjunto possuindo os mesmos

autovalores e autovetores de T .

Pela hipótese de indução o espaço vetorial S⊥ tem uma base B = {v2, v3, . . . , vn}

ortonormal de autovetores de T .

Como V = S ⊕ S⊥ , então B ′ = { v1

∥v1∥, v2, . . . , vn} é uma base ortonormal de

autovetores de T .

Diagonalização de operadores

61

Em outras palavras, toda matriz simétrica é diagonalizável. Caso seus autovalores
λ1, λ2, . . . , λn sejam todos distintos, os autovetores associados {vi} serão ortogonais, isto
é, ⟨vi , vj⟩ = 0, se i ̸= j, e a matriz P será neste caso formada pelos n autovetores {vi}
normalizados (para garantir que P seja ortogonal), e na mesma ordem dos autovalores.

Observação 3.44. O Teorema Espectral nos permite escrever uma matriz simétrica real
A de ordem n na forma A = P DP T , sendo P uma matriz ortogonal e D uma matriz
diagonal. Os elementos na diagonal de D são os autovalores de A, e as colunas de P são
os vetores ortonormais u1, u2, . . . , un. Usando a representação linha-coluna do produto,
temos:

A = P DP T = h

u1 u2

· · · un

i







λ1
...
0

0
· · ·
...
. . .
· · · λn





.


















uT
1
uT
2...
uT
n

A = h

λ1u1

λ2u2

· · · λnvn





.




i









uT
1
uT
2...
uT
n

A = λ1u1uT
1

matriz A.

+ λ2u2uT
2

+ · · · + λnunuT
n

, que é chamada decomposição espectral da

Podemos trocar a matriz A por um operador T : V → V autoadjunto, que teremos a

mesma conclusão.

Exemplo 3.45. Determinemos a decomposição espectral da matriz simétrica

A =






2 0 1
0 2 0
1 0 2


.



Polinômio característico de A é dado por

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

P (λ) =

2 − λ
0
1

0
2 − λ
0

1
0
2 − λ
Logo, os autovalores de A são: λ1 = 2 , λ2 = 1 e λ3 = 3.
Determinando os autovetores de A obtemos:
v1 = (0, 1, 0) associado ao autovalor 2; v2 = (1, 0, −1) associado ao autovalor 1; e

= (2 − λ)(λ2 − 4λ + 3) = (λ − 2)(λ − 1)(λ − 3).

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

v3 = (1, 0, 1) associado ao autovalor 3.

Então,

u1 = v1
∥v1∥

=











0
1
0

;

u2 = v2
∥v2∥

Logo,

uT
1

= h 0 1 0 i

;

uT
2

= h 1√
2

=







1√
2
0
−1√
2
0 −1√
2







i

e

u3 = v3
∥v3∥

=







e

uT
3

= h 1√
2

0




.



i.

1√
2
0
1√
2

1√
2

A = λ1u1uT
1

+ λ2u2uT
2

+ λ3u3uT
3

.

Portanto,

A = 2.



 .






0
1
0

h 0 1 0 i + 1.













1√
2
0
−1√
2

h 1√
2

.

0 −1√
2

i + 3.













1√
2
0
1√
2

h 1√
2

.

0

i.

1√
2

62

Teoria Espectral

é a decomposição espectral da matriz A.

A =






0 0 0
0 2 0
0 0 0











+

1
2
0
− 1
2

0 − 1
2
0
0
0
1
2






+











0 3
3
2
2
0 0 0
0 3
3
2
2

=






2
0
1

0
2
0


.



1
0
2

A decomposição espectral expressa explicitamente uma matriz simétrica em termos de
seus autovalores e autovetores. Isto nos fornece uma maneira de se construir uma matriz
com autovalores e autovetores ortonormais dados.

Exemplo 3.46. Determinemos uma matriz simétrica A de ordem 2 com autovalores
λ1 = 2 e λ2 = −3, e autovetores ortogonais associados v1 = (1, −2) e v2 = (2, 1),
respectivamente.

u1 = v1
∥v1∥

=

#

" 1√
5
−2√
5

⇒ uT
1

= h 1√
5

i

−2√
5

e

u2 = v2
∥v2∥

=

#

⇒ uT
2

h 2√
5

i.

1√
5

A = λ1u1uT
1

+ λ2u2uT
2

= 2.

#

" 1√
5
−2√
5

h 1√
5

.

i

− 3.

−2√
5

" 2√
5
1√
5

h 2√
5

i =

1√
5

" 2√
5
1√
5
#

=

" 2
− 4
5

5 − 4

#

−

" 12
5
6
5

#
.

6
5
3
5

Portanto,

A =

" −2 −2
1
−2

#

.

5
8
5

Para verificarmos quando um operador linear pode ser diagonalizável ou não, vimos que
isto pode ser feito através da obtenção de uma base de autovetores, ou mostrando a inexis-
tência desta base. Para espaços vetoriais de baixa dimensão este processo é conveniente.
Entretanto, para espaços de dimensão elevada este procedimento pode ser trabalhoso, e
nestes casos vamos utilizar um outro método que veremos a seguir.

Definição 3.47. Se P (x) = anxn +· · ·+a1x+a0 um polinômio e A uma matriz quadrada
de ordem n. Definimos P (A) como sendo a matriz P (A) = anAn + · · · + a1A + a0In.

Definição 3.48. Seja A uma matriz quadrada de ordem n. O polinômio

é chamado de polinômio minimal de A, quando:

m(x) = xk + ak−1xk−1 + · · · + a1x + a0

(a) m(A) = 0, ou seja, a matriz A anula m(x).
(b) m(x) é o polinômio de menor grau entre aqueles que são anulados pela matriz A.

Os próximos teoremas não serão provados, mas as provas podem ser encontradas na

referência [2].

Teorema 3.49. Sejam V um espaço vetorial de dimensão n, T : V → V um operador
linear e B uma base qualquer de V . Então, T é diagonalizável se, e somente se, o
polinômio minimal de h
com
λ1, λ2, · · · , λr autovalores distintos.

é da forma m(x) = (x − λ1)(x − λ2) · · · (x − λr),

T

B

i

Desta forma, o problema da determinação se um operador linear T é diagonalizável se

reduz a encontrar o polinômio minimal de T .

Teorema 3.50. (Cayley-Hamilton) Sejam T : V → V um operador linear, B uma base
de V e P (λ) o polinômio característico de T . Então, P ( [T ]B) = 0.

Diagonalização de operadores

63

O Teorema Cayley-Hamilton garante que o polinômio característico é um candidato a

ser o polinômio minimal, pois ele satisfaz a Definição 2.48.(a) .

Exemplo 3.51. Consideremos a matriz A =
autovalores: λ1 = −1 e λ2 = 4, associados aos autovetores v1 = (1, −1) e v2 = (2, 3),
respectivamente.

(Exemplo 2.10 ) que admite

#

" 1 2
3 2

O polinômio característico de A é dado por P (λ) = λ2 − 3λ − 4 = (λ + 1)(λ − 4), e o

Teorema de Cayley-Hamilton garante que a matriz A é um zero de P (λ).
#
" 1 0
1
0

De fato, P (A) = A2 − 3A − 4I2 =

" 1 2
2
3

" 1 2
2
3

− 4

− 3

#2

#

=

=

#

" 7

6
9 10

#

" 3 6
9 6

#

" 4 0
0 4

−

−

=

" 0 0
0 0

#
.

Neste caso, P (λ) = m(λ) = (λ + 1)(λ − 4), e a matriz A é diagonalizável (Teorema

2.49.).

Considerando a base B = {(1, −1), (2, 3)} formada pelos autovetores A temos que

nesta base h
T

i

=

B

" −1 0
4
0

#

.

Teorema 3.52. Os polinômios característicos e minimal têm os mesmos fatores irredu-
tíveis. Em particular, m(λ) divide o polinômio característico P (λ) de A.

Assim, este teorema garante que: qualquer fator irredutível de um dos polinômios
deve dividir o outro polinômio. Em particular, como um fator linear é irredutível, m(x)
e P (x) têm os mesmos fatores lineares. Logo, têm as mesmas raízes.

Teorema 3.53. Um escalar λ é um autovalor de uma matriz A se, e somente se, λ é
uma raiz do polinômio minimal de A.

Exemplo 3.54. Consideremos T : V → V um operador linear e B uma base de V .
Suponhamos que o polinômio característico de T seja P (λ) = (λ − 2)3(λ − 1)2(λ − 5).
Então, seu polinômio minimal deverá ser um dos polinômios da forma:
m(λ) = (λ − 2)r(λ − 1)s(λ − 5) , r ∈ {1, 2, 3} e s ∈ {1, 2}.

Como o polinômio minimal é o de menor grau, verifiquemos inicialmente para

r = s = 1 , ou seja, P1(λ) = (λ − 2)(λ − 1)(λ − 5).

Se P1([T ]B) = 0, então ele é o minimal. Caso contrário, testamos o próximo de menor

grau dos polinômios que restaram.

Teorema 3.55. Sejam λ1, λ2, . . . , λr autovalores distintos de um operador linear T .
Então, T é diagonalizável se, e somente se, o polinômio (x − λ1)(x − λ2) · · · (x − λr)
anular a matriz de T .

Exemplo 3.56. Verifiquemos se o operador linear T : R4 → R4 definido por
T (x, y, z, t) = (x, y + z, 3z, 3t) é diagonalizável.

T (x, y, z, t) = (x, y + z, 3z, 3t) = x(1, 0, 0, 0) + y(0, 1, 0, 0) + z(0, 1, 3, 0) + t(0, 0, 0, 3)

64

Teoria Espectral



1 0 0 0
0 1 1 0
0 0 3 0
0 0 0 3
Como a matriz é triangular, então os autovalores de T são: λ1 = 1 , λ2 = 3, ambos de


.









=

T



B

h

i

multiplicidades algébricas 2.

Logo, o polinômio característico de T é dado por P (λ) = (λ − 1)2(λ − 3)2.
Desta forma, os possíveis candidatos a polinômio minimal são:

P1(λ) = (λ − 1)(λ − 3),
ou P4(λ) = (λ − 1)2(λ − 3)2.
Testando a matriz h

i

T

B

em P1(λ), temos:

P2(λ) = (λ − 1)2(λ − 3),

P3(λ) = (λ − 1)(λ − 3)2

P1( [T ]B ) =

















P1( [T ]B ) = ( [T ]B − 1.I )( [T ]B − 3.I ).
1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1

1 0 0 0
0 1 1 0
0 0 3 0
0 0 0 3

1 0 0 0
0 1 1 0
0 0 3 0
0 0 0 3































−











.








−








3 0 0 0
0 3 0 0
0 0 3 0
3
0 0 0











.




P1( [T ]B ) =








0 0 0 0
0 0 1 0
0 0 2 0
0 0 0 3








.

−2






0 0
0
0 −2 1 0
0 0
0
0
0 0
0
0








=








0 0 0 0
0 0 0 0
0 0 0 0
0 0 0 0




.




Logo, P1(λ) = (λ − 1)(λ − 3) é o polinômio minimal de T . Portanto, a matriz é

diagonalizável.

4 Decomposição em Valores
Singulares

Neste capítulo apresentaremos de uma forma breve e prática a decomposição em va-

lores singulares de uma matriz não quadrada, conhecida como DV S.

A abordagem será com respeito a matrizes reais, porém também pode ser feita para o
caso complexo. Este capítulo é baseado nas referências [5], [6], [11], [16], [18], [19], e [24].

4.1 Um breve relato histórico

A Decomposição em Valores Singulares é um assunto relativamente recente. Os mate-
máticos Eugenie Beltrami e Camille Jordan podem ser considerados os pais da DV S. Em
1873, Beltrami publica um trabalho relativo a esta decomposição considerando apenas
matrizes quadradas, não singulares e com valores singulares distintos. Em 1874, Jordan,
independentemente de Beltrami, publica um trabalho em que considera os valores singula-
res para forma bilineares, em que reduz uma forma bilinear para uma forma diagonal por
substituições ortogonais. A generalização da DV S para matrizes retangulares e comple-
xas somente aparecem nos trabalhos de Carl Eckart e Gale Young, em 1936. A utilização
deste método na computação ocorre a partir de 1960. Mais recentemente, o matemático
norte-americano Gene Golub também estudou a aplicabilidade deste método em diversas
áreas, produzindo um algoritmo estável e eficaz para calculá-la.

Sabemos que toda matriz simétrica A pode ser fatorada como A = P DP T , em que P é
uma matriz ortogonal de ordem n×n de autovetores de A e D é uma matriz diagonal cujas
entradas são os autovalores associados aos autovetores colunas de P . Esta decomposição
podemos chamá-la de DAV "Decomposição em Autovalores da matriz A".

A Decomposição em Valores Singulares (DV S) conhecida também, abreviamente por
SV D - Singular Value Decomposition - consiste numa extensão da teoria de diagonalização
de matrizes simétricas n × n (DAV ) para matrizes arbitrárias real ou complexa de ordem
m × n. Formalmente, a decomposição em valores singulares de uma matriz A de ordem
m×n é uma fatoração na forma A = U ΣV T , onde U e V são matrizes quadradas ortogonais
e ortonormais de ordem m × m e n × n, respectivamente. A matriz Σ = h
i é retangular
de ordem m×n (mesma ordem da matriz A), em que as entradas diagonais σii são números
reais não negativos, arranjados em ordem decrescente de magnitude chamados de valores
singulares de A. As m colunas de U são os autovetores de AAT , denominadas de vetores
singulares à esquerda; as n colunas V são os autovetores de AT A, chamados de vetores
singulares à direita.

δii

A importância da decomposição em valores singulares é a sua variedade de aplicações,
tais como: na compressão, no armazenamento e na transmissão de informações digitali-

65

66

Decomposição em Valores Singulares

zadas que formam a base de muitos dos melhores algoritmos computacionais disponíveis
para resolução de sistemas lineares; no cálculo da pseudo-inversa de uma matriz, no ajuste
de dados utilizando mínimos quadrados. De um modo geral, em todas as aplicações a
utilização da decomposição em valores singulares (DV S), consiste no fato que através
desta fatoração matricial podemos reduzir significativamente a dimensão do espaço ini-
cial, podendo reter apenas os dados que são importantes, desprezando os demais.

4.2 A DVS de uma matriz

Definição 4.1. Sejam A ∈ Mm×n(R)
e λ1 ≥ λ2 ≥ · · · ≥ λp > 0 , para algum
p ≤ min{m, n}, os autovalores de AT A ou AAT . Definimos valores singulares não
nulos de A como sendo σk =

λk , para k ∈ {1, 2, . . . , p}.

√

Proposição 4.2. Os valores singulares de A são os comprimentos dos vetores
Av1, Av2, . . . , Avn.

Demonstração. De fato,

∥ Avj ∥2 = ⟨Avj, Avj⟩ = (Avj)T Avj = vT
j
Assim, ∥ Avj ∥2= σ2
j
Portanto, ∥ Avj ∥= σj , para todo j = 1, 2, . . . , p.

.

(AT A)vj = vT

j λjvj = vT

j σ2

j vj = σ2

j vT

j vj = σ2

j

Definição 4.3. A diagonal principal de uma matriz retangular A de ordem m × n
é definida como sendo a fileira de entradas que começa no canto superior esquerdo a11 e
se estende diagonalmente até onde fôr possível. Dizemos que as entradas nessa diagonal
principal são as entradas diagonais da matriz.

A =









a11
a12
a21
a22
...
...
am1 am2

· · ·
a1m · · · a1n
· · ·
a2m · · · a1n
...
...
. . .
. . .
· · · amm · · · amn









B =















b11
b21
...
bn1
...
bm1

b12
b22
...
bn2
...
bm2

· · ·
· · ·
. . .
· · ·
. . .
· · ·















b1n
b2n
...
bnn
...
bmn

{ a11, a22, . . . , amm} forma a diagonal principal da matriz A e { b11, b22, . . . , bnn} a

diagonal principal da matriz B.

O próximo teorema garante a existência da decomposição em valores singulares de

qualquer matriz na chamada forma reduzida de decomposição.

Teorema 4.4. (Existência da DVS, na forma reduzida, de uma matriz) Toda matriz real
retangular A de ordem n × m e posto r pode ser decomposta em

An×m = Un×rΣr×rV T

r×m

,

em que U ∈ Mn×r(R) e V ∈ Mm×r(R) são ortonormais nas colunas, e Σr×r = diag(σi)
(matriz diagonal de valores singulares de A), com λi > 0. Sendo λ1, λ2, . . . , λr os auto-
valores não nulos da matriz AT A ou AAT , U e V matrizes de r autovetores ortonormais
por coluna, respectivamente das matrizes AT A ou AAT .

Demonstração. Suponhamos a matriz A seja decomposta em A = U ΣV T , sendo U e V
matrizes quadradas ortonormais nas colunas de ordem n e m, respectivamente, e Σ = [σij]
λi , para todo i = 1, 2, 3. . . . , m.
uma matriz diagonal de ordem n × m, tal que σii =

√

A DVS de uma matriz

67

Por conveniência e sem perda de generalidade admitamos que m ≤ n.
Construindo matrizes ortogonais U e V com acréscimos de n − r e m − r vetores,
respectivamente, às colunas U e V , ortonormais aos primeiros r deles. Assim, uma nova
representação para Σ é dada por
√



Σ =













λ1
0
...
0
...
0

0
√
λ2
...
0
...
0

· · ·
· · ·
. . .
· · ·
. . .
· · ·

√

0
0
...
λm
...
0








.







A matriz AT A é simétrica, pois (AT A)T = AT A, e possui os autovalores λ1, λ2, . . . , λm.
Por suposição A = U ΣmV T , então

AT A = (U ΣmV T )T (U ΣmV T ) = V ΣmU T U ΣmV T = V ΣmΣmV T = V Σ2

mV T .

Como V é ortogonal, segue que, V T AT AV = V T V Σ2
Assim, V T AT AV = Σ2
m

.

mV T V = Σ2

m

.

Portanto,

V T AT AV = Σ2
m

=









λ1
0
...
0

0
λ2
...
0

autovetores de AT A em suas colunas.

0
· · ·
0
· · ·
...
. . .
· · · λm





, em que V é a matriz dos




Assumindo uma situação restrita, em apenas r dos λi sejam não nulos, então a matriz

Σn×m será dada por
√



Σ =













λ1
0
...
0
...
0

0
√
λ2
...
0
...
0

· · ·
· · ·
. . .
· · ·
. . .
0

0
0
...
√
λr
...
0

· · ·
· · ·
. . .
· · ·
. . .
· · ·















0
0
...
0

0
0

=









Σr 0 · · ·
0
0 · · ·
...
...
. . .
0 · · ·
0





.




0
0
...
0

Essencialmente, Σ e Σr são idênticas nas primeiras r linhas e colunas. Ocorre que a

matriz Σ possui n − r linhas e m − r colunas de zeros a mais que a matriz Σr.

Podemos ver que ΣT Σ = Σ2
m

, sendo Σ2
m

=















0
λ1
0 λ2
...
...
0
0
...
...
0
0

0
· · ·
0
· · ·
...
. . .
· · · λr
...
. . .
0
0

· · ·
· · ·
. . .
· · ·
. . .
· · ·















0
0
...
0

0
0

.

E podemos verificar que (V T AT )AV = ΣT Σ , ou seja, (AV )T AV = ΣT Σ. Fazendo

W = AV , temos W T W = ΣT Σ , sendo W = h
Como W T W = ΣT Σ, conclui-se que

w1 w2

· · · wm

i.

wT

i wi =






λi , se i ≤ r
0 , se i > r

e wT

i wi = 0, se i ̸= j, para j = 1, 2, . . . , m.

68

Decomposição em Valores Singulares

Se wi = 0, para i > r, as primeiras colunas de W são LI. Logo, concluímos que

r ≤ n.

Agora definindo ui = 1√
λi

i = 1, 2, . . . , r, e tomando
se r < m, ortogonais entre si, e os demais ui tais que a matriz

· wi = 1√
λi

· Avi, para

ur+1, ur+2, . . . , um,
quadrada U = h

u1 u2

· · · un

i de ordem n seja ortogonal.
√

Assim, pela definição de ui , vemos que a expressão ui ·

λi = Avi pode ser expressa

matricialmente por U Σ = AV .

Como V é ortogonal, temos V −1 = V T , o que resulta em

AV = U Σ, o que implica, A = U AV T .

Considerando apenas as r primeiras linhas de U , as r primeiras linhas e colunas de Σ
e as r primeiras de V , o resultado ainda continua válido, o que finaliza a demonstração.

Observação 4.5. A Decomposição em Valores Singulares de uma matriz A ∈ Mm×n(R)
sendo m ≥ n e posto r, na forma reduzida, pode ser obtida efetuando-se a sequência
descrita a seguir:

1. Construímos a matriz AT A e determinamos os seus autovalores λ1, λ2, . . . , λr.
2. Calculamos os valores singulares σi da matriz A, sendo σi =

λi , para i = 1, 2, . . . , r,

√

com λ1 ≥ λ2 ≥ · · · ≥ λr > 0.

3. Construímos a matriz diagonal Σr = h

i quadrada de ordem r cujas entradas da
diagonal principal são os valores singulares, sendo σii =
λi , para i = 1, 2, . . . , r ,
com λ1 ≥ λ2 ≥ · · · ≥ λr > 0. No caso de não existir r valores singulares não nulos,
completamos com linhas e colunas nulas até formamos uma matriz de dimensão r × r.
4. Obtemos os autovetores v1, v2, . . . , vr de AT A associados aos autovalores λ1 , λ2, . . . , λr,

σij

√

respectivamente.

5. Construímos a matriz V = h

v1 v2

· · ·

vr

lizamos esta matriz.

i

n×r

e caso não seja ortonormal, norma-

6. Construímos a matriz ortonormal U = h

determinado pelos
vetores ui = 1
.Avi, para i = 1, 2, . . . , r (ou encontramos os autovetores ui da matriz
σi
AAT ). Caso o número de vetores ui seja menor que r, determinemos através do pro-
cesso de ortogonalização de Gram-Schmidt vetores ui necessários para obtermos uma
matriz m × r, e normalizamos ui, caso não estejam.

· · · ur

u1 u2

m×r

i

7. Escrevemos a matriz Am×n = Um×rΣr×rV T

.

r×n
No caso da decomposição em valores singulares de uma matriz A de ordem m × n
sendo n ≥ m o processo sofre alterações na construção das matrizes U e V . Inicialmente,
obtemos os autovalores e autovetores da matriz AAT . Estes autovetores formaram a
matriz U = h
i será construída
i, e a matriz V = h
sendo vi = 1
σi

AT ui (ou obtendo-se os autovetores da matriz AT A).

· · · un

u1 u2

v1 v2

· · ·

vn

Exemplo 4.6. Determinemos a DV S, na forma reduzida, da matriz A =






1 1
0 1
1 0


.



O post(A) = 2, então a DVS da matriz A tem a forma A3×2 = U3×2Σ2×2V T
2×2

.

A DVS de uma matriz

69

Calculemos de AT A.

AT A =

" 1 0 1
1 1 0

#



.









1 1
0 1
1 0

=

" 2 1
1 2

#
.

Determinemos os autovalores de AT A.
2 − λ
1

P (λ) =

1
2 − λ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= (2 − λ)(2 − λ) − 1 = λ2 − 4λ + 3 = 0.

Logo, os autovalores de AT A (em ordem decrescente) são: λ1 = 3 e λ2 = 1.

Construção da matriz Σ2×2:
Determinemos os valores singulares σi =
√
σ2 =

σ1 =

λ1 =

√

3

√

λi da matriz A.
√
√
λ2 =

1 = 1.

Portanto, Σ =

" σ1

0
0 σ2

#

=

e
" √
3 0
1
0

#

.

Construção da matriz V2×2:
Determinemos os autovetores associados aos autovalores obtidos.

Consideremos o sistema (AT A − λI2)X = 0.
" 0
0

" 2 − λ
1

1
2 − λ

" x1
x2

=

#

#

.

#
.

"−1

Fazendo λ = 3 no sistema temos:
1 | 0
1 −1 | 0
L2 → L2 + L1

∼

#

"−1 1 | 0
0 0 | 0

#

.

Então, n

−x1 + x2 = 0.

Escolhendo x2 = a (variável livre), temos x1 = a.

Logo,

" x1
x2

#

=

#

" a
a

= a

#

" 1
1

, a ∈ R∗.

Assim, w1 =

#

" 1
1

é um autovetor de AT A associado ao autovalor λ1 = 3.

Fazendo λ = 1 no sistema temos:
#

" 1 1 | 0
1 1 | 0

∼

" 1 1 | 0
0 0 | 0

#
.

Então, n

x1 + x2 = 0.

L2 → L2 − L1

Escolhendo x2 = a (variável livre), temos x1 = −a.

Logo,

" x1
x2

#

=

#

"−a
a

= a

#

"−1
1

, a ∈ R∗.

Assim, w2 =

#

"−1
1

é um autovetor de AT A associado ao autovalor λ2 = 1.

Os vetores w1 e w2 são ortogonais, mas não ortonormais, normalizando-os temos:

70

Decomposição em Valores Singulares

v1 = 1

∥w1∥.w1 = 1√

2

#

" 1
1

.

=

Portanto, V = h

v1 v2

i =

Construção da matriz U3×2:
Determinemos ui = 1
Avi.
σi

v2 = 1

∥w2∥.w2 = 1√

2

#

"−1
1

.

=

#
.

" −1√
2
1√
2

" 1√
2
1√
2
" 1√
2
1√
2

#

e

#

.

−1√
2
1√
2

u1 = 1
σ1

.Av1 = 1√
3

.






1 1
0 1
1 0


 . 1√


2

" 1
1

#

= 1√
6











2
1
1

=







.







2√
6
1√
6
1√
6

u2 = 1
σ2

.Av2 = 1
1.






1 1
0 1
1 0


 . 1√


2

" −1
1

#

= 1√
2








0
1


−1

=







.







0
1√
2
−1√
2

Portanto, U = h

u1 u2

i =







2√
6
1√
6
1√
6




.



0
1√
2
−1√
2

Podemos facilmente verificar que, A = U ΣV T .






U ΣV T =





2√
6
1√
6
1√
6

0
1√
2
−1√
2

#

"√
3 0
1
0

.

.





" 1√
2
−1√
2

#

1√
2
1√
2

=





2√
6
1√
6
1√
6











√
3√
2
−1√
2

0
1√
2
−1√
2


 =






√
3√
2
1√
2






1 1
0 1
1 0

= A.

Exemplo 4.7. Fatoremos, na forma reduzida, a matriz A =
decomposição em valores singulares.

#

" 1 1 0
0 1 1

usando a

O post(A) = 2, então a DVS da matriz A tem a forma A2×3 = U2×2Σ2×2V T
2×3

.

Calculemos de AAT .

AAT =

" 1 1 0
0 1 1

#



.









1 0
1 1
0 1

=

" 2 1
1 2

#
.

Determinemos os autovalores de AAT .
1
2 − λ
Logo, os autovalores de AAT (em ordem decrescente) são: λ1 = 3 , λ2 = 1.

P (λ) = (cid:12)
(cid:12)AAT − λI
(cid:12)

2 − λ
1

= (2 − λ)2 − 1 = λ2 − 4λ + 3 = 0.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

(cid:12)
(cid:12)
(cid:12)

Construção da matriz Σ2×2:
Determinemos os valores singulares σi =

√

Logo,

Σ =

√

σ1 =
" σ1

0
0 σ2

λ1 =
#

=

e

√
3
" √
3 0
1
0

#

.

λi de A.
λ2 =

√

σ2 =

√

1 = 1.

A DVS de uma matriz

71

Construção da matriz U2×2:
Determinemos dos autovetores associados aos autovalores obtidos.

Consideremos o sistema

(AAT − λI3)X = 0.

" 2 − λ
1

1
2 − λ

#

" x1
x2

.

#

=

#

.

" 0
0

Fazendo λ1 = 3 no sistema, obtemos:

"−1

1 | 0
1 −1 | 0

#

"−1
0

∼

1 | 0
0 | 0

#

.

Então,

n

−x1 + x2 = 0.

L2 → L2 − L1

Escolhendo x2 = a (variável livre), temos x1 = a.

Logo,

#

=

" x1
x2

Assim, w1 =

" 1
1

" a
a
#

#

= a

#

" 1
1

, a ∈ R∗.

é um autovetor de AAT associado ao autovalor λ1 = 3.

Fazendo λ2 = 1 no sistema temos:
#

" 1
1

1 | 0
1 | 0

" 1
0

∼

1 | 0
0 | 0

#

.

Então, n

x1 + x2 = 0.

Escolhendo x2 = a (variável livre), temos x2 = a e x1 = −a.
"−1
1

Logo,

= a

#
.

=

#

#

"−a
a

" x1
x2

Assim, w2 =

#

"−1
1

é um autovetor de AAT associado ao autovalor λ2 = 1.

Os vetores w1 e w2 e são ortogonais, mas não ortornormais. Normalizando-os temos:

u1 = 1

∥w1∥.w1 = 1√

2

#

" 1
1

.

=

Portanto, U = h

u1 u2

i =

u2 = 1

∥w2∥.w2 = 1√

2

#

"−1
1

.

=

#

.

" −1√
2
1√
2

" 1√
2
1√
2
" 1√
2
1√
2

#

e

#

.

−1√
2
1√
2

Construção da matriz V3×2:
Determinemos

vi = 1
σi

.AT ui.

v1 = 1
σ1

.Av1 = 1√
3

.






1 0
1 1
0 1


 . 1√


2

" 1
1

#

= 1√
6

v2 = 1
σ1

.Av2 = 1
1.






1 0
1 1
0 1


 . 1√


2

#

"−1
1

= 1√
2











1
2
1

=







.







1√
6
2√
6
1√
6


−1
0


1






=







.







−1√
2
0
1√
2

72

Decomposição em Valores Singulares

Assim, V = h

v1 v2

i =

Portanto, A = U ΣV T =







1√
6
2√
6
1√
6

" 1√
2
1√
2

−1√
2
0
1√
2

−1√
2
1√
2




.



#

" √
3 0
1
0

.

#

.

" 1√
6
−1√
2

2√
6
0

#

.

1√
6
1√
2

A seguir apresentamos o teorema da DV S de uma matriz A de ordem m × n na
chamada forma completa, em que U e V são matrizes quadradas de ordem m e n, respec-
tivamente. A matriz Σ é retangular da mesma ordem da matriz A.

Teorema 4.8. (DVS na forma completa) Toda matriz A de ordem m × n e posto r pode
ser fatorada em

Am×n = Um×mΣm×nV T

n×n

, em que

Um é uma matriz quadrada de ordem m ortonormal nas colunas formada pelos auto-
vetores de AAT , Vn éuma matriz quadrada de ordem n ortonormal nas colunas formada
pelos autovetores de AT A. A matriz Σm×n é retangular diagonal de ordem m × n cu-
λi , para
jas entradas diagonais são os valores singulares da matriz A, ou seja, σii =
i = 1, 2, . . . , r , com λ1 ≥ λ2 ≥ · · · ≥ λr > 0.

√

A prova deste teorema pode ser encontrada nas referências:

[24], (p.367 − 368)

e

[6],(p.101).

Observação 4.9. A Decomposição em Valores Singulares de uma matriz A ∈ Mm×n(R)
sendo m ≥ n e posto r, na forma completa, pode ser obtida de modo análogo ao descrito
na Observação 3.5. As matrizes Um×m , Vn×n
e Σm×n poderão ser completadas, se
necessário, com linhas e colunas nulas de modo a serem representadas nas formas m × m,
n × n e m × n, respectivamente, como veremos no exemplo a seguir.

Exemplo 4.10. Determinemos a DV S, na forma completa, da matriz A =


−2



1 −1
2
2 −2


.



O post(A) = 1 e a DV S da matriz A tem a forma A3×2 = U3×3Σ3×2V T
2×2

.

Calculemos de AT A.

AT A =

" 1 −2
−1

2
2 −2

#

.


−2



1 −1
2
2 −2






=

" 9 −9
9
−9

#

.

Determinemos os autovalores de AT A.

P (λ) = (cid:12)
(cid:12)AT A − λI
(cid:12)

(cid:12)
(cid:12)
(cid:12)

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

9 − λ −9
9 − λ
−9

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= (9 − λ)2 − 81 = λ(λ − 18) = 0.

Logo, os autovalores de AT A (em ordem decrescente) são: λ1 = 18 e λ2 = 0.

Construção da matriz Σ3×2:
Determinemos o único valor singular σ1 da matriz A.
√
√
18 = 3

√

√

λi = σ1 =

λ1 =

σi =

2 .

A DVS de uma matriz

73

Neste caso, devemos completar a matriz Σ3×2 com 2 linhas e 1 coluna de zeros.

Portanto,

Σ =






√
3


.



2 0
0
0

0
0

Construção da matriz V2×2:
Determinemos os autovetores associados aos autovalores obtidos.

Consideremos o sistema

(AT A − λI2)X = 0.
#
" 0
" x1
0
−9
x2
Fazendo λ1 = 18 no sistema obtemos:

" 9 − λ −9
9 − λ

=

#

#

.

.

#

∼

"−9 −9 | 0
−9 −9 | 0
L1 → 1

9L1
x1 + x2 = 0.

Então, n

#

" 1 1 | 0
−9 9 | 0
L2 → 9L1 + L2

∼

" 1 1 | 0
0 0 | 0

#

.

Escolhendo x2 = a (variável livre), temos x1 = −a.

Logo,

" x1
x2

#

=

#

"−a
a

= a

#

"−1
1

, a ∈ R∗.

Assim, w1 =

#

"−1
1

é um autovetor de AT A associado ao autovalor λ1 = 18.

Fazendo λ2 = 0 no sistema temos:

#

" 9 −9 | 0
−9
9 | 0
L1 → 1

9L1

∼

#

" 1 −1 | 0
−9
9 | 0
L2 → 9L1 + L2

Então, n

x1 − x2 = 0.

∼

" 1 −1 | 0
0 | 0

0

#

.

Escolhendo x2 = a (variável livre), temos x1 = a.

Logo,

#

=

" x1
x2

Assim, w2 =

" 1
1

" a
a
#

#

= a

#
.

" 1
1

é um autovetor associado ao autovalor λ2 = 0.

Os vetores w1 e w2 são ortogonais, mas não são ortonormais, Normalizando-os temos:

v1 = 1

∥w1∥.w1 = 1√

2

#

"−1
1

.

=

#

" −1√
2
1√
2

;

v2 = 1

∥w2∥.w2 = 1√

2

#

" 1
1

.

=

#

.

" 1√
2
1√
2

Portanto, V = h

v1 v2

i =

" −1√
2
1√
2

#
.

1√
2
1√
2

Construção da matriz U3×3:

ui = 1
σi

· Avi.

74

Decomposição em Valores Singulares

u1 = 1
σ1

· Av1 = 1
√
3

2

.


−2



1 −1
2
2 −2


 · 1√


2

#

" −1
1

= 1
6











−2
4
−4

=



 .






− 1
3
2
3
− 2
3

Neste caso devemos determinar u2 e u3 de modo que {u1, u2, u3} forme uma base
⟨u1, u3⟩ = 0. Determinemos os vetores

ortonormal de R3, ou seja, ⟨u1, u2⟩ = 0 e
x = (x1, x2, x3) tais que ⟨uT

1 , x⟩ = 0 .

), (x1, x2, x3)⟩ = 0 ⇒ − 1

3x1 + 2

3x2 − 2

3x3 = 0. Logo, −x1 +2x2 −2x3 = 0.

3

3, 2

3, − 2

⟨(− 1
Portanto, x1 = 2x2 − 2x3.






Assim, x =







=




x1
x2
x3

2x2 − 2x3
x2
x3






=






2x2
x2
0











+






= x2











2
1
0

+ x3


−2
0


1


.



−2x3
0
x3


Escolhendo os vetores

2
1
0
formam uma base de R3, porém não são ortogonais, pois ⟨u2, u3⟩ = −4.

e u3 =

−2
0
1

u2 =



















observamos que u1 , u2 e u3

Aplicando-se o processo de ortogonalização de Gram-Schmidt, determinemos vetores

1, u′
u′

2 e u′
3

de modo a formar uma base ortonormal de R3.

).

Seja

= 1

= (− 1

u′
1
Logo, u′
1

∥u1∥ · u1.
3, − 2
3, 2
Como r2 = u2 − ⟨ u2, u′
= 1
Então, u′
∥r2∥ · r2 = 1√
2
, 0).
Logo, u′
, 1√
2
5

= ( 2√
5

3

Como r3 = u3 − ⟨ u3, u′

= (2, 1, 0) − 0 · u1 = (2, 1, 0).

1 ⟩ · u′
1
· (2, 1, 0).

5

1 ⟩ · u′
r3 = (−2, 0, 1) − 0 · u′
r3 = (−2, 0, 1) − (− 4√
5
· (− 2
∥r3∥ · r3 = 5
,
,

= 1

2⟩ · u′
2

.
1 − ⟨u3, u′
1 − ⟨(−2, 0, 1), ( 2√
) · ( 2√
, 1√
5
5
5, 1).

5, 4

).

4
√

5
√

√

3

5

= (− 2
√
3

5

5

3

5

3

Então, u′
3
Logo, u′
3

, 0)⟩ · ( 2√
, 1√
5
5
, 0) = (−2, 0, 1) + ( 8

5

, 0)
, 1√
5
5, 0) = (− 2

5, 4

5, 4

5, 1).

Assim, U = h

1 u′
u′

2 u′
3

i =







− 1
3
2
3
− 2
3

2√
5
1√
5
0




.



− 2
√
3
4
√

5

5

5

3

3

5
√

Portanto, A = U ΣV T =







− 1
3
2
3
− 2
3

2√
5
1√
5
0

− 2
√
3
4
√

5

5

5

3

3

5
√

3












·

√

2 0
0
0

0
0



 ·

"− 1√
2
1√
2

#

.

1√
2
1√
2

5 A Inversa Generalizada

Neste capítulo trataremos do conceito de inversa generalizada de uma matriz, e es-
tudaremos um dos tipos mais importantes deste conceito por suas fortes propriedades
algébricas, com a denominação de inversa de Moore-Penrose, ou simplesmente, pseudo-
inversa, em que é possível garantir sua unicidade, e consequentemente manter todas as
propriedades de matriz inversa no sentido clássico.

O termo inversa generalizada apareceu nos trabalhos do matemático sueco Erik Ivar
Fredholm em 1903, em que chamou de "pseudo-inversa" a inversa generalizada de um
operador integral. O conceito de inversa generalizada de matrizes surgiu em 1920 nos
trabalhos do matemático norte-americano Eliakim Hasting Moore que define a inversa
generalizada por meio de projeção de matriz e provou sua unicidade. Pouco se fez nos
próximos anos, até aproximadamente 1950, quando surgiram alguns trabalhos envolvendo
relações entre inversa generalizada e solução de problemas de mínimos quadrados. Em
1955, o britânico Sir Roger Penrose, físico, matemático e filósofo da ciência (ganhador do
Prêmio Nobel de Física de 2020) prova que toda matriz admite uma única inversa gene-
ralizada desde que satisfaça algumas equações, denominadas de "condições de Penrose".
Mostra também que sua definição de inversa generalizada coincide com a de Moore, daí
hoje ser conhecida como "inversa de Moore-Penrose", ou simplesmente pseudo-inversa.
([7] e [8]).

Este capítulo é baseado nas referências: [9], [10], [14], [15], [17], [18], [20], [21] e [22].

5.1 Caracterização da inversa generalizada de uma

matriz

A ideia de se obter a inversa de uma matriz A de ordem m × n também é utilizada
em importantes aplicações de Álgebra Linear, como por exemplo, na determinação da
solução de Mínimos Quadrados de sistemas de equações lineares inconsistentes.

O estudo de matrizes nos cursos básicos estabelece que uma matriz quadrada A de
ordem n admite inversa, quando existir uma outra matriz A−1 também quadrada de
ordem n, tal que A.A−1 = A−1.A = In, sendo In a matriz identidade de ordem n. Assim,
a existência da inversa está restrita somente às matrizes quadradas e não-singulares, ou
equivalente a dizer, matrizes que possuem posto igual a n (posto completo). A partir
destas condições verificam-se a validade das seguintes propriedades:

P1. Se A−1 existe, então ela é única.
P3. (A−1)−1 = A .
P5. (AB)−1 = B−1A−1 .

P2. detA−1 = 1
detA
P4. (AT )−1 = (A−1)T .

.

75

76

A Inversa Generalizada

Em situações problemas, como por exemplo, que envolvam a solução de sistemas de
equações lineares do tipo Ax = b , sendo A uma matriz quadrada de ordem n e não-
singular, sabemos que o sistema é consistente e determinado (admite uma única solução)
que é dada por

x = A−1b .

Entretanto, existem situações que esses requisitos não se cumprem, isto é, quando a
matriz A não é quadrada, ou mesmo sendo quadrada apresenta posto incompleto. Uma
abordagem para tratar destas situações irá envolver a utilização do conceito de inversa
generalizada de uma matriz.

Uma matriz A, não quadrada, do tipo m × n não possui inversa nas condições
do tipo n × m, tal que
e A = In. Estas
recebem os nomes de inversa generalizada de A à direira, e inversa

que vimos anteriormente. Mas, pode admitir uma matriz A−
d
AA−
d
matrizes A−
d
generalizada de A à esquerda, respectivamente.

= Im, e também uma outra matriz A−
e

do tipo n × m tal que A−

e A−
e

Exemplo 5.1. Consideremos a matriz A =

" 1 2 3
0 1 2

#

.

2×3

Uma inversa generalizada à direita de A é a matriz A−
d

=






1 −2
1
0
0
0






3×2

, pois

A.A−
d

=

#

" 1 2 3
0 1 2



.




1 −2
1
0
0
0

2×3






3×2

=

" 1 0
0 1

#

2×2

= I2.

No exemplo a seguir baseado no que é feito para o caso tradicional faremos o cálculo

de obtenção de uma inversa generalizada.

Exemplo 5.2. Calculemos uma inversa generalizada à esquerda para a matriz
1 0
0 2
2 5


.



A =











1 0 | 1 0 0
0 2 | 0 1 0
2 5 | 0 0 1



 ∼






L3 → L3 − 2L1


1 0 | 1 0 0
0
0 1 | 0 1
2
0 0 | 0 5
1
2

∼









.

1 0 | 1 0 0
0 2 | 0 1 0
0 5 | 0 5 1
L2 → 1
2L2


1 0 0
0
0 1
2
0 5
1
2

Logo,






 ∼






1 0 | 1 0 0
0
0 1 | 0 1
2
0 5 | 0 5 1



 ∼

L3 → L3 − 5L2



.



1 0
0 1
0 0

=







1 0
0 2
2 5



 .






#

" 1 0 0
0
0 1
2

2×3



.




1 0
0 2
2 5






3×2

=

#

" 1 0
0 1

2×2

. Consequentemente,

Assim, A−
e

=

#

" 1 0 0
0 1
0
2

é a inversa generalizada de A à esquerda.

Algoritmo de obtenção de uma inversa generalizada

77

Convém destacar que uma matriz pode admitir mais de uma inversa generalizada à

direita (ou à esquerda).

A matriz A′−
e

= 1
45

esquerda, pois

" 29 −20
10
−10

#

8
5

também é uma inversa generalizada de A à

A′−

e .A = 1

45

" 29 −20
10
−10

#



.




8
5






1 0
0 2
2 5

= 1
45

" 45
0

0
45

#

=

" 1 0
0 1

#

.

A seguir a definição que caracteriza uma inversa generalizada de uma matriz.

Definição 5.3. Seja A uma matriz de ordem m × n e posto não nulo. Uma inversa
generalizada de A ou g-inversa de A, denotada por A−, é uma matriz de ordem
n × m tal que x = A−b é uma solução do sistema de equações lineares Ax = b, para
todo b que torne o sistema consistente.

Proposição 5.4. A inversa generalizada A− de uma matriz A existe se, e somente se,
AA−A = A.

Demonstração. Escolhemos y como sendo a i − ésima coluna ai da matriz A. A equação
Ax = ai
claramente consistente e, portanto, x = A−ai é uma solução, ou seja,
AA−ai = ai , para todo i, o que equivale a dizer que AA−A = A.

é

Por outro lado, se A− é tal que AA−A = A e Ax = y é consistente, então
AA−Ax = x ou A(A−y) = y. Portanto, x = A−y é uma solução, o que prova a
proposição.

A seguir, do resultado desta proposição, uma definição equivalente de inversa genera-

lizada de uma matriz.

Definição 5.5. A matriz A− é uma inversa generalizada de A se AA−A = A.

Definição 5.6. A matriz A− é chamada de inversa generalizada reflexiva de A
quando A−AA− = A−.

5.2 Algoritmo de obtenção de uma inversa generali-

zada

5.2.1 Algoritmo de Searle

A inversa generalizada de uma matriz A pode ser obtida através do algoritmo descrito

abaixo, denominado de Algoritmo de Searle (1971).

1. Encontre uma submatriz quadrada B de posto igual ao da matriz A.
2. Obtenha a matriz transposta da inversa de B, isto é, (B−1)T .
3. Construa a matriz C substituindo a submatriz B escolhida em A pela matriz
(B−1)T completando com "zeros" o restante da matriz até chegar na dimensão da
matriz A.

4. A transposta da matriz C é a inversa generalizada de A, isto é, A− = C T .

78

A Inversa Generalizada

Exemplo 5.7. Determinemos uma inversa generalizada da matriz A =






1
1
1 −1
1
3


.



0
2
2

Como det A = 0, e o determinante da submatriz B =

#

" 1

1
1 −1

= −2 ̸= 0, então

temos post(A) = 2.
Calculemos

(B−1)T :

B−1 = 1

detB .adjB = − 1

2

"−1 −1
1
−1

#

=

" 1
2
1

1
2

#

.

2 − 1

2

Assim,

(B−1)T =

1
2

#

.

" 1
2
1

2 − 1

2

Substítuindo (B−1)T na matriz A no lugar da submatriz B e completando com zeros

os demais elementos, temos:






1
2

1
2
1
2 − 1
2
0
0


.



0
0
0

Determinando a transposta da matriz anterior, temos: A− =






1
2

1
2
2 − 1
1
2
0
0


.



0
0
0

De fato, A− é a inversa generalizada de A, pois
0
0
0

1
1
1 −1
1
3

1
2
2 − 1
1
2
0
0

AA−A =



 .

0
2
2







1
2







 .






1
1
1 −1
1
3






0
2
2

=

=






1
0
2

0
1
1



 .






0
0
0

1
1
1 −1
1
3






0
2
2

=






1
1
1 −1
1
3






0
2
2

= A.

Exemplo 5.8. Determinemos uma inversa generalizada da matriz A =

" 3 2 2
5 1 1

#
.

O posto de A é igual a 2, pois o determinante da submatriz B =

#

" 3 2
5 1

= −7 ̸= 0.

Calculemos

(B−1)T :

B−1 = 1

detB .adjB = − 1

7

" 1 −2
3
−5

#

=

"− 1

2
7

#
.

7
5

7 − 3

7

Assim,

(B−1)T =

"− 1

5
7

#
.

7
2

7 − 3

7

Substítuindo (B−1)T na matriz A no lugar da submatriz B e completando com zeros

os demais elementos, temos:

"− 1

5
7

7
2

7 − 3

7

#
.

0
0

Achando a transposta da matriz anterior, temos: A− =

2
7


− 1
7
7 − 3
5


7
0
0


.



A inversa de Moore-Penrose

79

De fato, A− é a inversa generalizada de A, pois
" 3 2 2
5 1 1

AA−A =



 .


− 1
7
7 − 3
5


7
0
0

2
7

#

.

" 3 2 2
5 1 1

#

=

=

" 1
0

0
1

#

" 3 2 2
5 1 1

.

#

=

#

" 3 2 2
5 1 1

= A.

Proposição 5.9. Seja A uma matriz m × n. Então, A− existe, e a classe de inversas
generalizadas a partir de qualquer inversa A− é dada por

sendo U uma matriz arbitrária n × m.

A− + U − A−A U A− ,

sendo V e W matrizes arbitrárias n × m.

ou

A− + V (I − AA−) + (I − A−A)W ,

5.3 A inversa de Moore-Penrose

Definição 5.10. Seja A uma matriz real de ordem m × n. Dizemos que a matriz real A†
de ordem n×m é a inversa de Moore-Penrose de A, ou simplesmente pseudo-inversa
de A, se as condições a seguir, chamadas de equações de Penrose, forem satisfeitas:

(1) AA†A = A.
(2) A†AA† = A†.
(3) AA† é simétrica, ou seja, (AA†)T = AA†.
(4) A†A é simétrica, ou seja, (A†A)T = A†A.

Embora, AA† desempenha o papel de matriz identidade na pré multiplicação por A e
na pós-multiplicação por A†, respectivamente, nas propriedades (1) e (2), estas matrizes
só serão iguais à identidade em situações especiais, no caso da matriz A for quadrada e
não-singular. E nestes casos, A−1 = A† .

Exemplo 5.11. Mostremos que a matriz A† = 1
3

" 2
1

#

1 −1
1
2

é a pseudo-inversa

da matriz A =



1 0
0 1


−1 1


.



Devemos mostrar que A† e A satisfazem as quatro condições da definição de inversa

de Moore-Penrose.

(1) AA†A =






0
1
0
1
−1 1


 . 1


3

" 2
1

1 −1
1
2

#



.




0
1
0
1
−1 1






=






0
1
0
1
−1 1


 . 1


3

" 3
0

#

.

0
3

Logo, AA†A =






0
1
0
1
−1 1






= A

(2) A†AA† = 1
3

" 2
1

1 −1
1
2

#






0
1
0
1
−1 1


 . 1


3

" 2
1

1 −1
1
2

#

=

80

A Inversa Generalizada

" 2
1

= 1
9

1 −1
1
2

#



2
1


−1

1 −1
1
2
2
1






" 6
3

= 1
9

3 −3
3
6

#

" 2
1

= 1
3

1 −1
1
2

#

.

Logo, AA†A = A†

(3) AA† =






0
1
1
0
−1 1


 . 1


3

" 2
1

#

1 −1
1
2

= 1
3.



2
1


−1

1 −1
1
2
2
1






é simétrica.

(4) A†A = 1
3

" 2
1

1 −1
1
2

#






0
1
0
1
−1 1






#

" 3 0
0 3

= 1
3

é simétrica.

Portanto, A† é a matriz inversa de Moore-Penrose de A.

Teorema 5.12. Se A ∈ Mm×n(R), então A† = V Σ+U T existe é única, sendo U e
V matrizes quadrada ortonormais de ordem m, e n, respectivamente, e Σ+ uma matriz
diagonal m × n cujas entradas diagonais δii são os inversos dos valores singulares de A.

Demonstração. Existência: Utilizando-se da decomposição em valores singulares da
matriz A podemos construir uma possível pseudo-inversa de A.

Seja A = U ΣV T , em que U e V são matrizes quadradas ortonormais de ordem m e

n, respectivamente, sendo V T a matriz transposta de V .

Sejam λ1, λ2, . . . , λp os autovalores não nulos de AT A e σi =

os valores singulares de A. Lembremos que

√

λi (i = 1, 2, . . . , p) ,

Σ =

#

" C 0
0 0

m×n

em que

C =









0
σ1
0 σ2
...
...
0
0

0
0

· · ·
· · ·
. . .
0
· · · σp









.

Consideremos Σ+ =

#

" C −1 0
0
0

n×m

então C −1 =










1
σ1
0
...
0

0
1
σ2

...
0

· · ·
· · ·
. . .
· · ·

0
0

0
1
σp










p×p

.

Como os autovalores escolhidos λi de AT A são tais que λi > 0, então σi > 0. Logo,

as matrizes Σ e Σ+ estão bem definidas.

É evidente que a matriz C é invertível, pois σ1, σ2, . . . , σp são números reais positivos

e p ≤ min{m, n}, e C −1 é a sua inversa.

Mostremos que Σ+ e Σ satisfazem as condições de Penrose, ou seja, uma é pseudo-

inversa da outra.

Σ Σ+Σ =









0
σ1
0 σ2
...
...
0
0

0
0

· · ·
· · ·
. . .
0
· · · σp


















1
σ1
0
...
0

0
1
σ2

...
0

· · ·
· · ·
. . .
· · ·










0
0

0
1
σp

Σ = Ip Σ = Σ.

A inversa de Moore-Penrose

81

Σ+Σ Σ+ =










1
σ1
0
...
0

0
1
σ2

...
0

· · ·
· · ·
. . .
· · ·


















0
σ1
0 σ2
...
...
0
0

0
0

0
1
σp

0
0

· · ·
· · ·
. . .
0
· · · σp









Σ+ = Ip Σ+ = Σ+.

Σ Σ+ = Ip

e Σ∗ Σ = Ip . Logo, Σ Σ+ e Σ+ Σ são simétricas.

Portanto, satisfazem as condições de Penrose.

Mostremos que a pseudo-inversa de A é dada por X = V Σ+U T , pois A e X também

satisfazem as condições de Penrose.

AXA = (U ΣV T )( V Σ+U T ) U ΣV T = U ΣΣ+ΣV T = U ΣV T = A .

XAX = V Σ+U T U ΣV T V Σ+U T = V Σ+ΣΣ+U T = V Σ+U T = X .

(AX)T = (U ΣV T V Σ+U T )T = ( U ΣΣ+U T )T = U (ΣΣ+)T U T = U ΣΣ+U T

= U ΣV T V Σ+U T = AX.

(XA)T = (V Σ+U T U ΣV T )T = (V Σ+ΣV T )T = V (Σ+Σ)T V T = V Σ+ΣV T =

= V Σ+U T U ΣV T = XA.

Assim, X é a pseudo-inversa de A . Portanto, podemos dizer que A† = V Σ+U T .

Unicidade: Suponhamos que X e Y sejam pseudo-inversas de A. Logo, satisfazem as

condições de Penrose. Em particular que AXA = A , XAX = X e AY A = A.
Assim, AX = (AX)T = X T AT = X T (AY A)T = X T AT Y T AT = (AX)T (AY )T =

= AXAY = (AXA)Y = AY .

Logo, AX = AY .

Por outro lado, temos:

XA = (XA)T = AT X T = (AY A)T X T = AT Y T AT X T = (Y A)T (XA)T =

= Y AXA = Y (AXA) = Y A .

Logo, XA = Y A.

Então, X = XAX = XAY = Y AY = Y .
Portanto, a pseudo-inversa de A é única.

Seja A ∈ Mm×n(R) e consideremos a matriz quadrada A.AT simétrica de ordem m.
Suponhamos que A.AT seja invertível, então posto (AAT ) = m (número de linhas de A).

Logo, a matriz A.AT admite inversa à direita (A.AT )−1, e

(A.AT )(A.AT )−1 = Im.
A.[ AT (AAT )−1 ] = Im.

Portanto, AT (AAT )−1 é a pseudo-inversa de A à direita, ou seja,

A† = AT (AAT )−1.

De modo análogo, considerando a matriz quadrada AT A simétrica de ordem n, e

supondo-a invertível, então posto (AT A) = n (número de colunas de A).

Logo, a matriz AT A admite inversa à esquerda (AT A)−1, e

(AT A)−1(AT A) = In.
[ (AT A)−1AT ].A = In.

Portanto, (AT A)−1AT é a pseudo-inversa de A à esquerda, ou seja,

A† = (AT A)−1AT .

82

A Inversa Generalizada

É fácil de verificarmos que A† = (AT A)−1AT satisfaz as condições de Penrose. De fato,
(1) AA†A = A[(AT A)−1AT ]A = AA−1(AT )−1AT A = Im.ImA = A.

Logo, AA†A = A.

(2) A†AA† = [(AT A)−1AT ]AA† = A−1(AT )−1AT AA† = In.In.AL = A†.

Logo, A†AAL = AL.

(3) (AA†)T = [A(AT A)−1AT ]T = [AA−1(AT )−1AT ]T = (Im.Im)T = Im.
(4) (A†A)T = [(AT A)−1AT A]T = [A−1(AT )−1AT A]T = [A−1ImA]T = [A−1A]T = In.

Logo, A†A e AA† são simétricas.
Portanto, A† = (AT A)−1AT é a inversa de Moore-Penrose de A à esquerda.
Analogamente, podemos mostrar que A† = AT (AAT )−1 também satisfaz as condições

de Penrose e, portanto é a inversa de Moore-Penrose à direita.

5.4 Obtenção de uma pseudo-inversa.

Dada uma matriz A ∈ Mm×n(R), a inversa de Moore-Penrose ou pseudo-inversa

A† ∈ Mn×m(R) pode ser obtida da seguinte forma:
(a) Se post(A) = n, então A† = (AT A)−1AT .
(b) Se post(A) = m, então A† = AT (AAT )−1.
(c) Se post(A) < min{m, n}, então A† = (U ΣV T )−1 = (V T )−1Σ−1U −1 = V Σ+U T .
(d) Se m = n e post(A) = n, então A† = A−1.

É fácil observamos que a matriz A† = 0 , de ordem n × m, é a pseudo-inversa da

matriz A = 0 de ordem m × n.

No caso de uma matriz A ter posto 1, sua inversa A† também pode ser obtida como

sendo A† =

1

tr(AT A) AT .

Exemplo 5.13. Determinemos a inversa de Moore-Penrose da matriz

A =






1 0
0 2
3 1


.



A é uma matriz real 3 × 2 e post(A) = 2 = n, então A† = (AT A)−1AT .




Como AT A =

#

" 1 0 3
0 2 1

.




" 10 3
3 5

#

.

=




1 0
0 2
3 1

Logo, A† = (AT A)−1AT =

#−1

" 10 3
3 5






1 0
0 2
3 1

T





= 1
41

" 5 −3
10
−3

#

" 1 0 3
0 2 1

.

#

.

Portanto, A† = 1
41

" 5 −6 12
1
20
−3

#

.

Obtenção de uma pseudo-inversa.

83

Exemplo 5.14. Determinemos, a inversa de Moore-Penrose da matriz

A =

" 1
0

1
1

#

.

0
2

A é uma matriz real 2 × 3 e post(A) = 2 = m, então A† = AT (AAT )−1.




Como

AAT =

#

" 1 1 0
0 1 2

.




" 2 1
1 5

#

.

=




1 0
1 1
0 2

Logo, A† = AT (AAT )−1 =

" 1 1 0
0 1 2

#T " 2 1
1 5

#−1

=






1 0
1 1
0 2


 . 1


9

" 5 −1
2
−1

#

.

Portanto, A† = 1
9



5 −1
1
4


4
−2


.



Exemplo 5.15. Determinemos a pseudo-inversa da matriz A =

" 1 2 3
2 4 6

#
.

A é uma matriz real 2 × 3 e post(A) = 1 < min{m, n}, então , A† = V Σ+U T .
A DVS da matriz A é da forma A2×3 = U2Σ2×3V T
3
Calculemos de AT A.




.

AT A =






 .

1 2
2 4
3 6

#

" 1 2 3
2 4 6

=




5
10 15
10 20 30
15 30 45


.



Determinemos os autovalores de AT A:
5 − λ
10
15

P (λ) = (cid:12)
(cid:12)AT A − λI3
(cid:12)

=

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

10
20 − λ
30

15
30
45 − λ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

= − λ3 + 70λ2 − 1225λ + 13500 + 225λ + 900λ + 100λ − 13500 =
= − λ3 + 70λ2 = λ2(70 − λ) = 0.

Logo, os autovalores de AT A (em ordem decrescente) são: λ1 = 70 , λ2 = 0 e λ3 = 0.

Os valores singulares de A: σi =

√

√

σ1 =
Construção da matriz Σ:

λ1 =

√

λi.
√

70 ; σ2 =

λ2 =

√

0 = 0 e σ3 =

√

λ3 =

√

0 = 0.

Σ2×3 =

" σ1 0 0
0 0

0

#

=

" √
70 0 0
0 0
0

#

.

Então, A+

3×2

=






1√
70
0
0

0
0
0


.



Construção da matriz V:

Determinemos os autovetores associados aos autovalores obtidos.

Consideremos o sistema






5 − λ
10
15

(AT A − λI3)X = 0.

15
10
x1
30
20 − λ
x2
45 − λ
30
x3



 .









=


.








0
0
0

84

A Inversa Generalizada

Fazendo λ1 = 70 no sistema, obtemos:

15 | 0
−65
30 | 0


30 −25 | 0


−13





 ∼

10
10 −50
15
L1 → 0, 2.L1
L2 → 0, 1.L2
L3 → 0, 2.L3


3 | 0
3 | 0
6 −5 | 0

2
1 −5
3
L2 ↔ L1



 ∼



 ∼

3 | 0
1 −5
2
3 | 0
6 −5 | 0


−13


3
L2 → 13.L1 + L2
L3 → L3 − 3.L1

∼




Então,



 ∼

3
| 0
42 | 0
21 −14 | 0
21L2

1 −5
0 −63
0
L2 → − 1
L2 → 1
7L2

x1 − 5x2 + 3x3 = 0

3x2 − 2x3 = 0








1 −5
0
0

3 | 0
3 −2 | 0
3 −2 | 0



 ∼






1 −5
0
0

3 | 0
3 −2 | 0
0 | 0
0


.



L3 → L3 − L2

.



Escolhendo x3 = 3a (variável livre), temos x2 = 2a e x1 = a.

1
2
3


 , a ∈ R∗.


Logo,

= a

=






















a
2a
3a

x1
x2
x3

Assim, w1 =











1
2
3

é um autovetor de AT A associado ao autovalor λ1 = 70.

Fazendo λ2 = 0 no sistema temos:







10
20
30

15 | 0
30 | 0
45 | 0

5
10
15
L2 → L2 − 2.L1
L3 → L3 − 3.L1



 ∼




5
0
0

10
0
0

15 | 0
| 0
0
| 0
0



 ∼






1
0
0

2
1
0

3 | 0
0 | 0
0 | 0


.



L1 → 1

5.L1

Então,

n

x1 + 2x2 + 3x3 = 0

.

Escolhendo x3 = 0 e x3 = a (variáveis livres), temos x1 = −2a.
−2
1
0


−2a

a

0

Logo,


.



= a

=





















x1
x2
x3

Assim, w2 =


−2
1


0






é um autovetor de AT A associado ao autovalor λ2 = 0.

Determinemos o vetor w3 de modo que os vetores w1, w2 e w3 formem uma base
ortogonal de R3. Neste caso, podemos obter w3 efetuando o produto vetorial de w2 e w3.

w1 ∧ w2 =

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

i
j k
1
2 3
−2 1 0

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= −6j + k + 4k − 3i = −3i − 6j + 5k.

Obtenção de uma pseudo-inversa.

85

Portanto, w3 =


−3
−6


5


.



Os vetores w1 , w2 e w3

temos:

são ortogonais, mas não ortornormais. Normalizando-os

v1 = 1

∥w1∥.w1 = 1√

14



.









1
2
3







=

1√
2√
3√

14

14

14







;

v2 = 1

∥w2∥.w2 = 1√

5


−2
1


0

.






=







−2√
5
1√
5
0




.



v3 = 1

∥w3∥.w3 = 1√

70


−3
−6


5

.






=




.









−3√
70
−6√
70
5√

70

Portanto, V = h

v1 v2 v3

i =







1√
2√
3√

14

14

14

−2√
5
1√
5
0




.



−3√
70
−6√
70
5√

70

Construção da matriz U : ui = 1
σi
#

Avi.

u1 = 1
σ1

Av1 = 1√
70

.

" 1 2 3
2 4 6



. 1√
14

.









1
2
3

= 1√

980

#

" 14
28

.

= 1
√
14

5

.

" 14
28

#

.

Logo,

u1 = 1√
5

.

#

.

" 1
2

Determinemos o vetor u2 tal que { u1, u2} forme uma base ortonormal de R2. Neste

caso, basta tomarmos u2 = 1√
5

"−2
1

#

.

Assim,

U = 1√
5

" 1 −2
1
2

Então,

A† = V Σ+U T =

#







e

U T = 1√
5

" 1 2
−2 1

#

.

−2√
5
1√
5
0









.




1√
70
0
0

−3√
70
−6√
70
5√

70


 . 1√


5

0
0
0

" 1 2
−2 1

#
.

1√
2√
3√

14

14

14







A† = 1√
5

1√
2√
3√

14

14

14

−2√
5
1√
5
0









.




1√
70
0
0

−3√
70
−6√
70
5√

70






2√
70
0
0







= 1√
5

1√
2√
3√

980

980

980

2√
4√
6√

980

980

980







= 1√
5







1
√
14
2
√
14
3
√
14

5

5

5




.



2
√
14
4
√
14
6
√
14

5

5

5

Portanto,

A† = 1
70






1 2
2 4
3 6


.



Neste caso, convém observarmos, como post(A) = 1, então A† pode ser facilmente

tr(AT A) AT .
obtida através de A† =




1

AT A =




1 2
2 4
3 6

" 1 2 3
2 4 6




#

=






10 15
5
10 20 30
15 30 45


,



então tr(AT A) = 70 .

86

A Inversa Generalizada

Portanto,

A† = 1
70






1 2
2 4
3 6






.

Propriedade 5.16. Seja A ∈ Mm×n(R), com A† = V Σ+U T sua pseudo-inversa, então
são válidas as seguintes propriedades:

(A†)† = A.
(A†)T = (AT )†.

(a)
(b)
(c) Se A é invertível, então A† = A−1.

Demonstração.

(a) (A†)† = (V Σ†U T )† = U (Σ†)†V T = U ΣV T = A.
(b) (A†)T = (V Σ†U T )T = U (Σ†)T V T = U (ΣT )†V T = (V ΣT U T )† =

= [(U ΣV T )T ]† = (AT )† .

(c) Como A é invertível, então m = n e posto(A) = m. Então, a matriz Σm×m é
uma matriz diagonal sem zeros na diagonal principal. Logo, A é invertível.
AA† = U ΣV T V Σ†U T = U ΣΣ†U T .
Mas,
Como ΣΣ† = Im, então AA† = Im.
Portanto, A† = A−1.

Observação 5.17. A inversa de Moore-Penrose é sem dúvida a principal inversa gene-
ralizada de matrizes, porém as condições de Penrose pode ser usadas para definir outras
inversas generalizadas, as quais não possuem a garantia de unicidade, mas podem ser úteis
em diversas aplicações. Existem casos de matrizes cuja inversa generalizada não satisfaz
todas as condições de Penrose. Nestes casos, denotamos por inversa − { ·, ·, · } colocando
entre as chaves separados por vírgula as condições de Penrose que ela satisfaz. Assim, se
a matriz X é inversa de Moore-Penrose da matriz A que satisfaz somente as condições 1
e 2, denotamos por inversa − {1, 2}.

#

" 1 0 0
0
0 1
2

Assim a matriz A− =

A =






1 0
0 2
2 5



. De fato,




(Exemplo 4.2.) é uma inversa-{1,2,4} da matriz

(1) AA−A =






1 0
0 2
2 5






" 1 0 0
0 1
0
2

#











1 0
0 2
2 5

=











1 0
0 2
2 5






=






1 0
0 2
2 5






= A.






#

1 0 0
0 1 0
0
2 5
2
" 1 0
0 1

=



1 0
0 2
2 5






" 1 0 0
0 1
0
2

# " 1 0 0
0 1
0
2

#

=

(2) A−AA− =

=






(3) AA− =

#

#




" 1 0 0
0 1
0
2
" 1 0 0
0 1
0
2

" 1 0 0
0
0 1
2

1 0
0 2
2 5




= A−.

#

=











1 0 0
0 1 0
2 5
0
2

não é simétrica.

Algoritmos de obtenção de uma pseudo-inversa.

87

(4) A−A =

" 1 0 0
0
0 1
2

#











1 0
0 2
2 5

=

#

" 1 0
0 1

é simétrica.

Portanto, A− satisfaz apenas as condições (1) , (2) e (4) de Penrose.

5.5 Algoritmos de obtenção de uma pseudo-inversa.

Existem vários algoritmos algébricos para a determinação da matriz inversa de Moore-
Penrose. Abordaremos dois desses algoritmos: o primeiro bastante simples proposto pelo
próprio Penrose em 1956, e o segundo, denominado de algoritmo de Greville. Esta seção
foi baseada nas referências [9] e [25].

5.5.1 Algoritmo de Penrose.

Consideremos uma matriz A do tipo m × n e posto igual a r , r ≤ min{ m, n } . A
inversa de Moore-Penrose A† de ordem n × m pode ser obtida através de um algoritmo
cujos passos estão descritos a seguir:

1. Calculamos a matriz B = AT A .
2. Definimos de C1 = In .
3. A partir de agora,

inicia-se um processo iterativo de r − 1 passos, calculando-se

C2, C3, . . . , Cr utilizando-se da expressão:

Ci+1 = tr(CiB)

i

.In − CiB ,

para i = 1, 2, . . . , r − 1.

4. A inversa de Moore-Penrose da matriz A é dada por A† = r
A matriz Cr+1B = 0 e trCrB ̸= 0.

tr(CrB).CrAT .

Exemplo 5.18. Utilizando o algoritmo de Penrose, determinemos a inversa de Moore-

Penrose da matriz A =






1 2 1
1 1 0
0 1 1


.



Observamos que det A = 0 e o determinante da submatriz

#

" 1 2
1 1

̸= 0. Então,

posto(A) = r = 2. Logo, a obtenção da inversa Moore-Penrose de A através do algoritmo
de Penrose é dada por

Calculemos B = AT A =

A† = r



tr(CrB).CrAT = 2
1 1 0
2 1 1
1 0 1

tr(C2B).C2AT .


1 2 1
1 1 0
0 1 1



 .










=






2 3 1
3 6 3
1 3 2


.



Iniciando o processo de iteração temos:

C1 = I3 =






1 0 0
0 1 0
0 0 1






; C1.B = I3.






2 3 1
3 6 3
1 3 2






=






2 3 1
3 6 3
1 3 2






e

tr(C1B) = 10 .

88

A Inversa Generalizada

Logo,

C2 = tr(C1B)

1

.I2 − C1B = 10
1 .






1 0 0
0 1 0
0 0 1



 −






2 3 1
3 6 3
1 3 2


.



Então,

C2 =


−3


−1 −3

8 −3 −1
4 −3
8






e C2B =


−3


−1 −3

8 −3 −1
4 −3
8



 .






2 3 1
3 6 3
1 3 2






=



6
3


−3

3 −3
6 −3
6
3






e

tr(C2B) = 18 .

Assim,

tr(C2B) .C2AT .

A† = 2

−3


−1 −3

8 −3 −1
4 −3
8

A† = 2
18.



 .






1 1 0
2 1 1
1 0 1


.



Portanto, A† = 1
9






1 −5 −4
1
2
1
5
1 −4






.

Exemplo 5.19. Utilizando o algoritmo de Penrose, determinemos a inversa de Moore-

Penrose da matriz A =






0
1
2
1
0 −1


.



2
1
3

Observamos que det A = 0 e o determinante da submatriz

#

" 1 0
2 1

̸= 0. Então,

post(A) = r = 2. Assim, a obtenção da inversa de Moore-Penrose de A através do
algoritmo de Penrose é dada por

Calculemos B = AT A =

A† = r



tr(CrB) .CrAT = 2

0
2
1 −1
3
1

tr(C2B) .C2AT .

0
1
2
1

 .
0 −1

1
0
2







Iniciando o processo de iteração temos:

C1 = I3 =

C1.B = I3.






4
2
5
2
2 −2
4 −2 14






=






4
2
5
2
2 −2
4 −2 14






2
1
3











=






4
2
5
2
2 −2
4 −2 14


.



1 0 0
0 1 0
0 0 1






;

e

tr(C1B) = 21 .






1
0
0

0
1
0



 −






0
0
1

4
2
5
2
2 −2
4 −2 14


.



Logo,

C2 = tr(C1B)

1

.I3 − C1B = 21
1 .

Então,

C2 =






16 −2 −4
2
19
−2
7
2
−4


.



Algoritmos de obtenção de uma pseudo-inversa.

89

C2B =






16 −2 −4
2
19
−2
7
2
−4



 .






5
2
4 −2

4
2
2 −2
14











=

60
36
12 −18

12
36
30 −18
78






e tr(C2B) = 168 .

Assim, A† = 2

tr(C2B) .C2AT = 2
168.






16 −2 −4
2
19
−2
7
2
−4



 .






1
0
2

0
2
1 −1
3
1


.



Portanto, A† = 1
84.






8
2
10

26 −10
17 −13
19
1


.



Exemplo 5.20. Utilizando o algoritmo de Penrose, determinemos a inversa de Moore-

Penrose da matriz A =






1
0
2

1
1
1


.



0
1
0

Observamos que det A ̸= 0 então posto(A) = r = 3. Logo, a obtenção da inversa

de Moore-Penrose de A através do algoritmo de Penrose é dada por

Calculemos B = AT A =

A† = r



tr(C2B).C2AT .
tr(CrB).CrAT = 2



0
1
1
0
1
1
0
1
0
1
2
1

2
1
0

1
1
0













=






5
3
0

3
3
1

Iniciando o processo de iteração temos:

C1 = I3 =






1 0 0
0 1 0
0 0 1







.



0
1
1

;

C1.B = I3.






5
3
0

3
3
1






0
1
1

=






5
3
0

3
3
1






0
1
1

e

tr(C1B) = 9 .

Logo,

C2 = tr(C1B)

1

.I3 − C1B = 9
1.






1
0
0

0
1
0

0
0
1



 −






5
3
0

3
3
1


.



0
1
1

Então,

C2 =


−3


0

4 −3

0
6 −1
1
1

4 −3


−3



0 −1

0
6 −1
8



 .






5
3
0

C2B =

Assim,






3
3
1

e






0
1
1

=






11
3
−3

3 −3
5
8
7
5






e

tr(C2B) = 26 .

C3 = tr(C2B)

2

.I3 − C2B = 26
2 .






1
0
0

0
1
0



 −






0
0
1

11
3
−3

3 −3
5
8
7
5






=


−3



2 −3

3 −5

3
5 −5
6


.



90

A Inversa Generalizada

2 −3


−3



3 −5

3
5 −5
6











5
3
0

3
3
1






0
1
1

=






1
0
0

0
1
0






0
0
1

e

tr(C3B) = 3.

C3B =

Logo,

A† = 3

tr(C3B) .C3AT = 3
3.

2 −3


−3



3 −5

3
5 −5
6



 .






1
1
0

0
1
1






2
1
0

=


−1
2


−2

1
0
0 −1
1
1


.



Portanto, A† =


−1
2


−2

0
1
0 −1
1
1


.



5.5.2 Algoritmo de Greville

O algoritmo Greville é um outro processo iterativo desenvolvido em 1965 que per-
mite calcular a pseudo-inversa de uma matriz A a partir das matrizes pseudo-inversas
correspondentes a algumas de suas submatrizes.









a11
a12
a21
a22
...
...
am1 am2

· · ·
a1k
· · ·
a2k
...
a1k
· · · amk

· · ·
· · ·

a1n
a2n
...
· · ·
· · · amn









é a k − ésima coluna de A.

Seja A =

αk =

















a1k
a2k
...
amk

= h

α1 α2

· · · αk

· · · αn

i em que

Consideremos Ak a submatriz formada pelas primeiras k colunas da matriz A. Então,

Ak = h

α1 α2

· · · αk

i =









a11
a12
a21
a22
...
...
am1 am2

· · ·
a1k
· · ·
a2k
...
. . .
· · · amk





.




Então, Ak = h

Ak−1 αk

i.

A condição inicial da iteração é dada como sendo:

Se α1 = 0 (coluna nula), então A†
1
Se α1 ̸= 0 , então A†
1

1 .α1)−1.αT
Definimos os vetores colunas δk e γk como sendo:

= (αT

1

= 0.
.

k−1.αk
E um outro vetor Bk dado por:

δk = A†

e

γk = αk − Ak−1δk.

Se γk ̸= 0, então Bk = γ†
= (γT
k
Se γk = 0, então Bk = (1 + γT

k .γk)−1.γT
k .δk)−1.δT

k

.

k Ak−1.

Então, a matriz A†
k

da matriz Ak é calculada como sendo A†
k

=

"

A†

k−1 − δk.Bk
Bk

#

.

Algoritmos de obtenção de uma pseudo-inversa.

91

E o processo continua para k = 2, 3 . . . , n.

Exemplo 5.21. Determinemos a pseudo-inversa da matriz A =



0
1


−1

0
1
0
2
2 −1






2
1
0

utilizando o algoritmo de Greville .

As matrizes colunas αi são: α1 =

1ª iteração:


 , α2 =









0
1


−1


 , α3 =


1
0
2



0
2


−1


 e α4 =








.



2
1
0

A1 = h

α1

i =

Então, A†
1


 ̸= 0.




0
1


−1
= ( αT






A†
1

=

h0 1 −1i

.

1

1 α1 )−1αT



0
1
−1










.

−1

−

h 0 1 −1 i = h2i−1 h 0 1 −1 i = h 1

2

i h 0 1 −1 i.

Logo, A†
1

= h 0 1

2 − 1

2

i.

δ2 = A†

1.α2 = h 0 1

2 − 1

2

i



.









1
0
2

= h

−1i.

γ2 = α2 − A1δ2 =






1
0
2



 −



 .



0
1


−1

−1i =
h






1
0
2



 −


0
−1


1






=


 ̸=












1
1
1


.



0
0
0

Daí, B2 = γ†
2

= (cid:16)

γT
2 .γ2

(cid:17)−1

. γT
2

=

h 1 1 1 i








.




1
1
1

= h 3 i−1

h 1 1 1 i = h 1
3

.

i

.

h 1 1 1 i.





−1

h 1 1 1 i =

.







Então, B2 = h 1
"

3

Assim, A†
2

=

1
3

i.

1
3

A†

1 − δ2B2
B2

#

.

−1i
δ2B2 = h
1 − δ2B2 = h 0
A†

h 1
3

.

Portanto, A†
2

=

" 1
3
1
3

i = h

1
3

1
3

1

2 − 1

2

i

h

−

− 1

3 − 1
− 1
3 − 1

3 − 1
3 − 1

3

3

i.

i = h 1
3

5

6 − 1

6

i.

5

6 − 1

#
.

6
1
3

1
3

2ª iteração:

δ3 = A†

2α3 =

" 1
3
1
3

5

6 − 1

6
1
3

1
3

#

.








0
2


−1

=

#

.

" 11
6
1
3

92

A Inversa Generalizada

γ3 = α3 − A2γ3 =



0
2


−1



 −



0 1
1 0


−1 2



 .

" 11
6
1
3

#

=



0
2


−1



 −






1
3
11
6
− 7
6






=


− 1
3
1


6
1
6


 ̸=








.



0
0
0

Daí, B3 = γ†
3

= (cid:16)

γT
3 .γ3

(cid:17)−1

. γT
3

=


h



− 1
3

1
6

i

.

1
6

= h 1
6

i−1

h

.

− 1
3

1
6

1
6

i = h 6 i = h

− 1
3

1
6


− 1
3
1


6
1
6
i.

1
6





−1







h

.

− 1
3

i =

1
6

1
6

−2 1 1 i.
Então, B3 = h
#
"
A†
.

Assim, A†
3

2 − δ3B3
B3

=

δ3B3 =

#

" 11
6
1
3

−2 1 1 i =
h

.

A†

2 − δ3B3 =

" 1
3
1
3

Portanto, A†
3

=

"− 11
3
− 2
3
"− 11
3
− 2
3

11
6
1
3

11
6
1
3

#

.

#

11
6
1
3

11
6
1
3

=

" 4 −1 −2
0
0

1

#

.

#

−

5

6 − 1

6
1
3

1
3

"
A†

2 − δ3B3
B3

#

=



4 −1 −2
0
0
1


1
1
−2


.



3ª iteração:

δ4 = A†

3.α4 =



4 −1 −2
0
0
1


1
1
−2



 .











2
1
0

=


.





7
2


−3

γ4 = α4 − A3.δ4 =






2
1
0



 −



0
1


−1

0
1
0
2
2 −1



 .



7
2


−3






=



 −











2
1
0






2
1
0

=


.








0
0
0

Daí, B4 = (cid:16)1 + δT

4 .δ4

(cid:17)−1

4 .A3.
δT


1 + h 7 2 −3 i



.

B4 =



7
2


−3





−1

h 7 2 −3 i

.

.









0 1
1 0

0
2


−1 2 −1






=

= (cid:16)1 + h 62 i(cid:17)−1

.

h 5 1 7 i = h 63 i−1

h 5 1 7 i = h 1
63

.

i

.

h 5 1 7 i.

Então, B4 = h 5

63

1
63

i.

1
9

Mas, A†
4

=

"
A†

3 − δ4B4
B4

#

.

δ4B4 =



 .



7
2


−3

h 5
63

1
63

i =

1
9



5
9
10


63
− 5

1
9
2
63

7
9
2
9


.



21 − 1

21 − 1

3

Algoritmos de obtenção de uma pseudo-inversa.

93

A†

3 − δ4B4 =



4 −1 −2
0
0
1


1
1
−2



 −



5
9
10


63
− 5

1
9
2
63



7
9
2
9






=

31

9 − 10
63 − 2

9 − 25
63 − 2

9


.



53


− 37
21

22
21

9
4
3

31

9 − 10
63 − 2

9 − 25
63 − 2

9



53


− 37


21
5
63

22
21
1
63

21 − 1
21 − 1



31

3

3 − 10
21 − 2

3

3 − 25
21 − 2
3
4
1
3

22
7
1
21

53


− 37


7
5
21






= 1
3

9
4
3
1
9








é a inversa de Moore-

Portanto A† =

Penrose da matriz A.

Exemplo 5.22. Determinemos a pseudo-inversa da matriz A =

lizando o algoritmo de Greville.






0
0
0

1
0
1 −1
1
0






1
1
2

uti-

As matrizes colunas αi são: α1 =


 , α2 =












0
0
0

1
1
0


 , α3 =



0
−1


1


 e α4 =








.



1
1
2

1ª iteração:

A1 = h

α1

i =











0
0
0

= 0.

Então, A†
1

= h 0 0 0 i.

Assim, δ2 = A†

1.α2 = h 0 0 0 i



.









1
1
0

= h 0 i.

γ2 = α2 − A1δ2 =



 −











1
1
0



 .

0
0
0

h 0 i =


 ̸=












1
1
0



0
0
0



Daí, B2 = γ†
2

= (cid:16)

γT
2 .γ2

(cid:17)−1

. γT
2

=

h 1 1 0 i




.





.



1
1
0





−1

h 1 1 0 i =

.







h 1 1 0 i = h 1
2

.

i

h 1

.

1 0 i.

0 i.

1
2

= h 2 i−1
Então, B2 = h 1
"

2

Assim, A†
2

=

A†

1 − δ2B2
B2

#

.

1
2

h 1
2

δ2B2 = h 0 i
.
1 − δ2B2 = h 0 0 0 i
A†
−
" 0 0 0
0
1
2

Portanto, A†
2

=

1
2

0 i = h 0 0 0 i.

h 0 0 0 i = h 0

0 0 i.

#

.

2ª iteração:

δ3 = A†

2α3 =

" 0
1
2

0 0
0
1
2

#

.


0
−1


1






=

#
.

" 0
− 1
2

94

A Inversa Generalizada

γ3 = α3 − A2γ3 =


0
−1


1



 −






0 1
0 1
0 0



 .

" 0
− 1
2

#

=


0
−1


1



 −


− 1
2
− 1


2
0






=


1
2
− 1


2
1


 ̸=








.



0
0
0

Daí, B3 = γ†
3

= (cid:16)

γT
3 .γ3

(cid:17)−1

. γT
3

=






h 1

2 − 1

2

1 i

.

#

.

δ3B3 =

h 1

3 − 1

3

1 i = h 2
3

i = h 1

2 − 1

2

= h 3
2
Então, B3 = h 1
A†

"

Mas, A†
3

=

i−1

h 1

.

3 − 1

3

2 − δ3B3
B3

2 − 1
2
i.

2
3

Logo, A†

2 − δ3B3 =

" 0
1
2

0
1
2

#

0
0

−

" 0
− 1
6

Portanto, A†
3

=

"
A†

2 − δ3B3
B3

#

=






0
2
3
1

0
1
3

3 − 1

3

#

.

" 0
− 1
2

3

0
0
6 − 1
1

0
.
1


3
2
3

#

=

" 0
2
3

0
1
3

#
.

0
1
3


1
2
− 1


2
1

1 i.





−1







.

h 1

2 − 1

2

1 i =

i =

2
3

" 0
− 1
6

#
.

0
0
6 − 1
1

3

3ª iteração:

δ4 = A†

3.α4 =






γ4 = α4 − A3.δ4 =

0
2
3
1

0
1
3

3

3 − 1

1
1
2




0
1
3
2
3



 .











1
1
2

=


.








0
5
3
4
3



 −






0
0
0

0
1
1 −1
1
0



 .











0
5
3
4
3

=



 −











1
1
2

5
3
1
3
4
3






=


− 2
3
2


3
2
3


 ̸=








.



0
0
0

Então, B4 = (cid:16)

δT
4 .δ4

(cid:17)−1

.

δT
4


h



B4 =

− 2
3

2
3

i

.

2
3

= h 3
4

h

i

.

− 2
3

2
3


− 2
3
2


3
2
3
i .

2
3

Mas, A†
4

=

"
A†

3 − δ4B4
B4

#

.





−1







h
− 2
3

.

2
3

i = h 4
3

2
3

i−1

h

.

− 2
3

i =

2
3

2
3

Então, B4 = h

− 1
2

1
2

i.

1
2

δ4B4 =



 .






0
5
3
4
3

h
− 1
2

i =

1
2

1
2


0
− 5


6
− 2
3

0
5
6
2
3


.



0
5
6
2
3

A†

3 − δ4B4 =






0
2
3
1

0
1
3

3 − 1

3



 −

0
1
3
2
3


0
− 5


6
− 2

0
5
6

0
5
6











=

0
0
0
2 − 1
3
2 − 1
2
0
1 −1


.



3 − 2

3 − 2

3

Portanto, A† =

da matriz A.



0
0
0
3
2 − 1
2 − 1
2
0
1 −1
1
1
2
2





− 1
2








= 1
2



0
0
0
3 −1 −1
2 −2
0
1
1





−1








é a inversa de Moore-Penrose

6 Aplicações da Inversa
Generalizada

Neste capítulo apresentaremos duas aplicações envolvendo o conceito de inversa ge-
neralizada; uma aplicação a ser abordada envolve o cálculo de solução de sistemas não
quadrados e a outra envolve problemas de mínimos quadrados. As referências utilizadas
foram [9], [10], [11], [15], [20] , [22] e [24].

6.1 A inversa generalizada na resolução de sistemas

lineares

Nesta seção veremos uma aplicações da inversa generalizada aos sistemas lineares com
coeficientes constantes. Num sistema de equações lineares da forma Ax = b , quando a
matriz A é não singular ela admite inversa A−1, e o sistema admite uma única solução.
Neste caso, a solução é obtida bastando multiplicar à esquerda por A−1 ambos os membros
da equação por Ax = b .

A−1Ax = A−1b ⇒ x = A−1b

No caso da matriz A ser singular, ou não ser uma matriz quadrada podemos utilizar

a inversa generalizada de A para a resolução do sistema.

Consideremos o sistema de equações lineares Ax = b, em que A é uma matriz m × n
de números reais, X um vetor coluna n × 1 de variáveis reais e b um vetor coluna m × 1 de
números reais. Lembremos que: dado o sistema de equações lineares Ax = b, se existir ao
menos um vetor x∗ que satisfaça o sistema, dizemos que o sistema é consistente. Caso
contrário, o sistema é inconsistente.

Os teoremas a seguir estabelecem a aplicação da inversa generalizada A− na resolução

de sistemas de equações lineares.

Teorema 6.1. Seja Ax = b um sistema linear consistente. Então,

x∗ = A−b é uma solução do sistema se, e somente se, AA−b = b.

Demonstração. Consideremos x∗ = A−b uma solução da equação Ax = b.

Pré multiplicando por AA− ambos os membros da equação Ax = b, temos:
AA−Ax = AA−b.
Ax = AA−b.

b = AA−b.

Logo,
Agora, consideremos x∗ = A−b, então Ax∗ = AA−b.
Mas, por hipótese AA−b = b. Assim, Ax∗ = b.

95

96

Aplicações da Inversa Generalizada

Portanto, x∗ = A−b é solução do sistema Ax = b.

Teorema 6.2. A solução geral de um sistema consistente Ax = b é dada por

sendo A− qualquer inversa generalizada de A, In a matriz identidade de ordem n e h um
vetor coluna n-dimensional chamado vetor parâmetro.

x = A−b + (In − A−A)h ,

Demonstração.

Ax = A[ A−b + (In − A−A)h ] = AA−b + A(In − A−A)h = AA−b + (AIn − AA−A)h.
Pelo fato do sistema Ax = b ser consistente, então AA−b = b (Teorema 5.1.), e como

AA−A = A (Definição 4.5.), então temos

Ax = b + (A − A)h = b.

Logo, x é solução do sistema Ax = b, para todo h.

Caso AA− = In, o sistema tem solução única, caso contrário o sistema possui infinitas

soluções, parametrizadas pelo vetor h.

Exemplo 6.3. Vamos resolver o sistema linear






x1 + x2 = 1
x2 + x3 = 3
2x1 + x2 = 0

.

Então temos:

1
1
1

A =

1
0
2









0
1
0

; A− = A† =


−1
2


−2

0
1
0 −1
1
1






(Exemplo 4.20)

e

b =






.






1
3
0

Condição de consistência: AA−b = b.

AA−b =






1
0
2

1
1
1

0
1
0







−1
2


−2

0
1
0 −1
1
1
















1
3
0

=






1
0
0

0
1
0

0
0
1
















1
3
0

=











1
3
0

= b.

Logo, o sistema é consistente, e como AA− = I3 , então sua solução é dada por

x∗ =


−1
2


−2

x∗ = A−b



1
0
1
3
0 −1
0
1
1










=


−1
2


1


.



Portanto, x1 = −1 , x2 = 2 e x3 = 1 .

Exemplo 6.4. Vamos resolver o sistema linear

Então temos:






x1 + 2x3 = 1
2x1 + x2 + x3 = 2
−x2 + 3x3 = 0

.

A inversa generalizada na resolução de sistemas lineares

A =






0
1
2
1
0 −1






2
1
3

; A− = A† = 1
84






8
2
10

26 −10
17 −13
19
1






(Exemplo 4.19.) e b =

97


.








1
2
0

Condição de consistência: AA−b = b.


0
1
2
1
0 −1


 . 1


AA−b =

2
1
3







84

8
2
10

26 −10
17 −13
19
1
















1
2
0

=











1
2
0

= b.

Logo, o sistema é consistente e sua solução é dada por



x∗ = A−b + (I3 − A−A)h.


26 −10
0
1
17 −13
2
1
19
1
0 −1
Assim, o sistema admite infinitas soluções.

A−A = 1
84

8
2
10

2
1
3















=






5
7
3
7
1

3
7
5

14 − 3

1
7

14
13
14

7 − 3

14


 ̸= I3.


I3 − A−A =






1
0
0

0
1
0



 −






0
0
1

5
7
3
7
1

3
7
5

14 − 3

1
7

14
13
14






=


2
− 3


7
− 1
7

7 − 3

14

7 − 3

7 − 1


.



7
3
14
1
14

9
14
3
14

Então,

x∗ = 1
84






8
2
10

26 −10
17 −13
19
1
















1
2
0

+

x∗ = 1
84











60
36
12

+


2
− 3


− 1

Portanto,

x∗ =

Assim,
x3 = 1

x1 = 5
7
7h1 + 3

+ 2
7h1 − 3
14h2 + 1
14h3

7 − 1

5
7
3
7
1
7

.








5
7
3




=

7h1 − 3
7h1 + 9
7h1 + 3

+ 2
7 − 3
7 − 1
7h2 − 1

7h2 − 1
7h3
14h2 + 3
14h3
14h2 + 1
14h3
7h2 − 1
7h1 − 3
7h3
14h2 + 3
7h1 + 9
14h3
14h2 + 1
7h1 + 3
14h3
x2 = 3
,
, com h1 e h3 ∈ R.

7h3




1


2
− 3


7
− 1
7


7 − 3

7 − 1

9
14
3
14


+





.













h1
h2
h3
7h1 − 3
7h1 + 9
7h1 + 3

7
3
14
1
14


2
− 3


− 1

7h2 − 1
7h3
14h2 + 3
14h3
14h2 + 1
14h3


.



7 − 3

7h1 + 9

14h2 + 3

14h3

e

Exemplo 6.5. Vamos resolver o sistema linear






x1 + 2x2 + x3 = 3
x1 + x2 = 2
x2 + x3 = 0

.

Então temos:


A =




1
1
0

2
1
1






1
0
1

; A− = A† = 1
9.






5 −4
1
1
1
2
5
1 −4






(Exemplo 4.18)

e

b =


.








3
2
0

Condição de consistência: AA−b = b.

AA−b =






1
1
0

2
1
1











1
9

1
0
1

5 −4
1
1
2
1
5
1 −4
















3
2
0

= 1
9






6
3
3 −3

3
3
6 −3
6
















3
2
0

=

98

Aplicações da Inversa Generalizada

= 1
9






24
21
3


 ̸=












3
2
0

= b . Portanto, o sistema é inconsistente.

6.2 Solução de mínimos quadrados.

Em várias situações, os sistemas lineares aparecem como modelos matemáticos de
vários fenômenos. Ocorre que alguns sistemas lineares não possuem soluções, isso acontece
em aplicações nas quais erros de medição afetam os coeficientes das equações de um
sistema consistente de tal modo que o sistema se torna inconsistente.

Suponhamos que o sistema Ax = b seja inconsistente de m equações e n incógnitas
em que essa inconsistência ocorreu por erros de medição nos coeficientes de A. Como
não é possível obtermos uma solução exata, vamos procurar um solução que seja a mais
"próxima possível" de ser uma solução, no sentido de que ∥ b−Ax ∥ (em relação ao produto
euclidiano de Rm) seja mínimo. Sendo Ax uma aproximação de b e ∥ b − Ax ∥ o erro
dessa aproximação, então quanto menor o erro, melhor a aproximação. Esse processo é
denominado Método dos Mínimos Quadrados, e é uma técnica que permite obter
informações de sistemas inconsistentes, de forma aproximada.

A terminologia mínimos quadrados se deve ao fato de que, esse método minimiza a

soma dos quadrados dos erros obtidos na aproximação.
e1
e2
...
em

Suponhamos que na forma matricial

b − Ax =











, entao podemos observar que:







minimizar ∥ b − Ax ∥ implica também minimizar ∥ b − Ax ∥2 = e2
1

+ e2
2

+ · · · + e2
m

.

Definição 6.6. Seja Ax = b um sistema linear de m equações e n incógnitas. Um vetor
x que minimiza ∥ b − Ax ∥ em relação ao produto interno canônico de Rm é chamado de
uma solução de mínimos quadrados do sistema linear, e que b − Ax é o vetor erro
de mínimos quadrados e ∥ b − Ax ∥ é o erro de mínimos quadrados.

Teorema 6.7. Dado um sistema de equações lineares Ax = b, com Am×n, então x = A†b
é um minimizador de ∥ Ax − b ∥.

Além disso é o minimizador da norma euclidiana ∥ . ∥2, ou seja, A†b é a solução do

sistema Ax = b.

Portanto, a melhor solução aproximada do sistema linear inconsistente Ax = b é dada
por x∗ = A†b. A prova deste teorema a seguir pode ser encontrado nas referências [15] e
[20].

Exemplo 6.8. Encontremos a melhor solução aproximada do sistema inconsistente






2x1 + 3x2 = 1
3x1 + 5x2 = −2
−2x1 − 4x2 = −1

.

Solução:

Solução de mínimos quadrados.

99

A =



2
3

3
5


−2 −4






, post(A) = 2

e

b =







1
−2


−1

.

Então, A† = (AT A)−1AT .

AT A =

" 2
3

#

3 −2
5 −4



2
3

3
5


−2 −4






=

#

" 17 29
29 50

e

(AT A)−1 = 1
9

" 50 −29
17
−29

#

.

Logo, A† = 1
9

" 50 −29
17
−29

# " 2
3

#

3 −2
5 −4

= 1
9

" 13
16
5
−7 −2 −10

#

.

Assim, a melhor solução aproximada x∗ do sistema é dado por

x∗ = A†b = 1
9

" 13
16
5
−7 −2 −10

#







1
−2


−1

= 1
9

"−13
7

#
.

Portanto, x∗ =

#

.

"− 13

9
7
9

O teorema a seguir permite calcularmos uma melhor aproximação de um sistema de

uma outra maneira.

Teorema 6.9. Se W é um subespaço de dimensão finita de um espaço vetorial V com
produto interno e b um vetor de V , então projW b é a melhor aproximação de b à W ,
no sentido de que ∥ b − projW b ∥ < ∥ b − w ∥ qualquer que seja o vetor w em W distinto
de projW b .

Demonstração. Dado qualquer vetor w ∈ W podemos escrever

b − w = (b − projW b) + (projW b − w) .
Sendo uma diferença de vetores de W , o vetor (projW b−w) ∈ W e, como b−projW b

é ortogonal a W , então b − projW b e projW b − w são ortogonais.

Assim, pelo Teorema de Pitágoras (T eorema 1.76) , temos:

∥ b − w ∥2 = ∥ b − projW b ∥2 + ∥ projW b − w ∥2.

Como b ̸= projW b segue que ∥ projW b − w ∥ > 0 e, portanto,

∥ b − projW b ∥2 < ∥ b − w ∥2.

Como as normas são não negativas, segue que

∥ b − projW b ∥ < ∥ b − w ∥.

Uma maneira de encontrarmos alguma solução de um sistema linear Ax = b pelo
método dos mínimos quadrados consiste em calcularmos os vetores p , sendo p = projW b,
em que W é o espaço coluna de A , e resolvermos o sistema linear Ax = p , denominado
sistema normal associado ao sistema Ax = b. Portanto, a ideia é considerarmos os vetores
pertencentes ao espaço coluna de A que sejam "mais próximos" possíveis de b e cujo sistema
linear associado ao Ax = b possua solução.

O teorema a seguir permite evitarmos o cálculo da projeção reescrevendo Ax = p na

forma AT Ax = AT b.

100

Aplicações da Inversa Generalizada

Teorema 6.10. Dado qualquer sistema linear Ax = b, o sistema normal associado
AT Ax = AT b
são soluções de
é consistente, e todas as soluções de AT Ax = AT b
mínimos quadrados de Ax = b. Além disso,
se W for o espaço coluna de A e x uma
solução de mínimos quadrados qualquer de Ax = b, então a projeção ortogonal de B em
W é projW b = Ax.

Demonstração. Pelo teorema anterior projW b é a melhor aproximação de b em W ,
então Ax = projW b.

Assim,
Multiplicando-se ambos os membros da equação por AT , temos:

b − Ax = b − projW b.

AT (b − Ax) = AT (b − projW b).

Como b − projW b é o componente de b que é ortogonal ao espaço coluna de A, segue

que este vetor está no espaço nulo de AT . Então,

Assim,
Portanto,

AT (b − projW b) = 0.
AT ( b − Ax ) = 0.
AT Ax = AT b.

Assim, dado qualquer sistema linear Ax = b, o sistema normal associado dado por
AT Ax = AT b é consistente, e todas as suas soluções são soluções de mínimos quadrados
de Ax = b.

Exemplo 6.11.

(a) Encontremos todas as soluções de mínimos quadrados do sistema inconsistente






2x1 + 3x2 = 1
3x1 + 5x2 = −2
−2x1 − 4x2 = −1

.

(b) Encontremos o vetor erro e o erro.

Solução:

(a) É conveniente expressarmos o sistema na forma matricial.

A =



2
3

3
5


−2 −4






Segue que,

AT A =

AT b =

" 2
3

" 2
3

3 −2
5 −4

3 −2
5 −4

e

#

#

b =







1
−2


−1

.



2
3

3
5


−2 −4



1
−2


−1

=









=

" 17 29
29 50

#

.

" −2
−3

#
.

Utilizando o sistema normal associado AT Ax = AT b temos:
# " x1
x2

" 17 29
29 50

" −2
−3

#
.

=

#

Solução de mínimos quadrados.

101

Resolvendo este matricial consistente AT Ax = b temos que x = (AT A)−1b .

Assim,

x =

" 19 29
29 50
"−13
7

#

x = 1
9

#−1 " −2
−3
"− 13

#

=

1
850−841

" 50 −29
19
−29

# " −2
−3

#

.

#
.

=

9
7
9
e x2 = 7
9

.

Portanto, x1 = − 13
9

(b) O vetor erro é dado por


 −

b − Ax =

Portanto,

b − Ax =


1
−2


−1

14
9
− 14


9
− 7
9






"− 13

9
7
9

#

=



 −







1
−2


−1


.



− 5
9
− 4
9
− 2
9



2
3

3
5


−2 −4

.



O erro é dado por

∥ b − Ax ∥ = q( 14

9

)2 + ( −14
9

)2 + ( −7
9

)2 = q 441
81

.

Portanto,

∥ b − Ax ∥ ≈ 2, 33 .

Consideremos uma tabela de n pontos (xi, yi), em que yi

foram obtidos experimen-
talmente, e queremos expressar estes pontos por uma reta y = a1x + a2 que melhor se
ajuste a eles. A obtenção deste ajuste, pelo método dos mínimos quadrados, consiste em
resolvermos um sistema linear inconsistente de n equações à duas incógnitas a1 e a2 a
serem determinadas, como veremos no exemplo a seguir.

Exemplo 6.12. Seja o conjunto de pontos

{ (1 , 3.8), (2 , 7.3), (3 , 10.2), (4 12.7), ( 5, 15.5) }.
Determinemos a função g(x) = a1.x + a2 que melhor se ajusta no sentido de mínimos
quadrados aos pontos desse conjunto.

Assim, temos:

g(1) = 1.a1 + a2 = 3.8 ;
g(4) = 4.a1 + a2 = 12.7 ;

Resolvendo o sistema linear

g(x) = a1.x + a2.

g(2) = 2.a1 + a2 = 7.3 ; g(3) = 3.a1 + a2 = 10.2 ;
g(5) = 5.a1 + a2 = 15.5.

a1 + a2 = 3.8
2a1 + a2 = 7.3
3a1 + a2 = 10.2
4a1 + a2 = 12.7
ax1 + a2 = 15.5




temos:

,

A =











1 1
2 1
3 1
4 1
5 1











e

AT =

" 1 2 3 4 5
1
1 1 1 1

#

.

102

Aplicações da Inversa Generalizada

Utilizando o sistema normal associado AT Ax = AT b, temos:

AT A =

" 1 2 3 4 5
1
1 1 1 1

#









=

" 55 15
5
15

#
.













1 1
2 1
3 1
4 1
5 1

(AT A)−1 =

1
275−225

#

"

5 −15
55

−15

= 1
50

"
−15

5 −15
55

#

=

" 0.1 −0.3
1.1
−0.3

#
.

AT b =

" 1 2 3 4 5
1
1 1 1 1

#











Assim,

" 0.1 −0.3
1.1
−0.3

#

" a1
a2

.

3.8
7.3
10.2
12.7
15.5

#

=











=

" 177.3
49.5

#
.

" 177.3
49.5

#

.

Como a solução do sistema normal é dado por x = (AT A)−1AT b. Então,

" a1
a2

#

=

" 0.1 −0.3
1.1
−0.3

#

" 177.3
49.5

.

#

=

" 2.88
1.26

#
.

Assim,

a1 = 2.88 e a2 = 1.26.

Portanto,

g(x) = 2.88x + 1.26.

O vetor erro é dado por

b − Ax =











3.8
7.3
10.2
12.7
15.5





















1 1
2 1
3 1
4 1
5 1











−

#

" 2.88
1.26

=











3.8
7.3
10.2
12.7
15.5































4.14
7.02
9.90
12.78
15.66

−

=











−0.34
0.28
0.30
−0.08
−0.16






.





O erro é dado por
∥ b − Ax ∥ = q(−0.34)2 + (0.28)2 + (0, 30)2 + (−0.08)2 + (−0.16)2 =

√

0.316 .

Portanto,

∥ b − Ax ∥ ≈ 0.56 .

7 Considerações Finais

O conceito da pseudo-inversa é um assunto relativamente recente e nas últimas décadas
este conceito encontrou uma série de aplicações em muitas áreas da ciência e tornou-se
uma ferramenta útil para matemáticos aplicados, engenheiros e físicos que trabalham, por
exemplo, com análise de dados, problemas de otimização, solução de equações integrais
lineares e outras.

O tema pseudo-inversa é pouco explorado em trabalhos de conclusão de curso de
graduação e dissertação de mestrado, talvez pelo assunto não ser ministrado no curso
regular de gradução.

Este trabalho teve por objetivo apresentar de modo didático e ilustrado através de
exemplos aplicações da pseudo-inversa na resolução de sistemas lineares. E este contéudo
também pode ser aplicado aos alunos da 2a. série do Ensino Médio como uma atividade
complementar ao estudo das matrizes e na resolução de sistemas lineares consistentes
através da inversa de uma matriz. O cálculo da pseudo-inversa para resolução do sistema
linear por estes alunos, seria feito através do que vimos na Seção 4.4. (a),(b),(d), e no
caso do item (c) trocaria-se a DVS pelo algoritmo de Penrose.

103

Referências

[1] PULINO, P. Álgebra Linear e suas Aplicações (Notas de aulas). Campinas: Edição

Eletrônica, Unicamp, 2012.

[2] LIPSCHUTZ, S. Álgebra Linear. São Paulo: Editora McGraw-Hill, 1972.

[3] BOLDRINI J.L. ; COSTA S.I.R. ; FIGUEIREDO V.L. ; WETZLER H.G. . Álgebra

Linear. São Paulo: Harper Row do Brasil, 1980.

[4] STEINBRUCH, A. ; WINTERLE P. Introdução à Álgebra Linear. São Paulo: Editora

McGraw-Hill, 1987.

[5] HOWARD, A. ; RORRES C. Álgebra Linear com Aplicações. Porto Alegre: Bookman,

2012.

[6] MALAJOVICH, G. Álgebra Linear. Rio de Janeiro: Edição Eletrônica Preliminar,

2010.

[7] MOORE, E. H. On the reciprocal of the general algebraic matrix, Bull, Amer. Math.

Soc. , v.26, p. 394 - 395, 1920.

[8] PENROSE, R. A generalized inverse for matrices. Proc. Cambridge Philos Soc., v.51,

p. 406-413, 1955.

[9] BEN-ISRAEL, A. ; GREVILLE, T.N.E. . Generalized inverses: Theory and Aplica-

tions. New Jersey: Rutcor-Rutgers Rd, 2001.

[10] THOME, N. La inversa generalizada de Moore-Penrose y aplicaciones, Vol.21. Valèn-

cia: Publicaciones Electrónicas Sociedade Matemática Mexicana, 2019.

[11] BHAYA, A. Decomposição em valores singulares, pseudoinversa, norma. Rio de Ja-

neiro: Edição Eletrônica UFRJ.

[12] SALAS, J. ; TORRENTE, A. ; VILLASEÑOR E.J.S. . Álgebra Lineal - Tema 14.

Madrid: Edição Eletrônica, Universidad Carlos III, 2015.

[13] FALEIROS, A. C. Álgebra Linear e Aplicação. Santo André: Edição Eletrônica

Univ.Federal do ABC, 2009.

[14] SOTO, J. Pseudoinversa de una Matriz. Múrcia: Universidad Católica de Múrcia,

URL: youtube.com/watch?v=g6ED94Wjrlg, 2016.

[15] GRAYBILL, F.A. Introduction to matrices with applications. Belmont, Califórnia,

Wadsworth Pub. Cia, 1969.

105

106

Referências

[16] OSSANI, P.C. Decomposição em Valores Singulares.

URL: youtube.com/watch?v=13CX9YqQ2FQ, 2020.

[17] DUARTE, F. ; MACHADO. J.A.T. . Matrizes Pseudoinversas: Aspectos Matemáti-
cos e Aplicação ao Controlo de Manipuladores Redundantes. Salamanca: 5ª Jornadas
Hispano-Lusas de Ingeniería Eléctrica, 1997.

[18] PELLEGRINI, J. C. . Álgebra Linear. Edição Eletrônica Preliminar, 2015.

[19] MONGE, Juan J.Fallas ; MOLINA, Jeffry Chavarría ; QUIROS, Pablo Soto . Des-
composición en valores singulares de una matriz: un repaso por los fundamentos
teóricos y sus aplicaciones en el procesamiento de imágenes. Costa Rica: Investiga-
ción Operacional, vol.42 nº2 , pág.142-173, Inst.Tec.de Costa Rica, 2021.

[20] RAO, C.Radhakrishna ; MITRA, Sujit Kumar. Generalized Inverse of Matrix and

its Applicátion.. New York: Edição Eletrônica, Wiley & Sons Inc, 1971.

[21] CAMPBELL, S.L. ; MEYER JR, C.D.. Generalized Inverses of Linear Transforma-

tions. New York: Dover Pub.Inc, 1991.

[22] GRUBER, M.H.J. . Matrix Algebra for Linear Models. New York: John Wiley &

Sons Inc , 2014.

[23] RAO, C.R. Calculus of Generalized Inverse of Matrices. Part I: General Theory,

Sankhya Ser. A, vol 29, 1967.

[24] STRANG, G. Álgebra Linear e suas Aplicações: Editora CENGAGE Learning, São

Paulo, tradução da 4ª edição norte-americana, 2012.

[25] PENROSE, R. On best approximate solutions of linear matrix equations. Proc. Cam-

bridge Philos Soc., v.52, p. 17-19, 1956.

