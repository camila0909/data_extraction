Universidade Federal de Goiás
Instituto de Matemática e Estatística
Programa de Mestrado Proﬁssional em
Matemática em Rede Nacional

Transformações lineares, Autovalores e

Autovetores

Marco Aurélio David Ramos

Goiânia

2013

Marco Aurélio David Ramos

Transformações lineares, Autovalores e
Autovetores

Trabalho de Conclusão de Curso apresentado ao Instituto de Matemática e Estatística

da Universidade Federal de Goiás, como parte dos requisitos para obtenção do grau de

Mestre em Matemática.
Área de Concentração: Matemática do Ensino Básico

Orientador: Prof. Dr. Edcarlos Domingos da Silva

Goiânia

2013

                             Dados Internacionais de Catalogação na Publicação (CIP) 

GPT/BC/UFG 

R175t 

Ramos, Marco Aurélio David.  
     Transformações lineares, autovalores e autovetores 
[manuscrito] / Marco Aurélio David Ramos. - 2013. 
     57 f. : il. 

     Orientador: Prof. Dr. Edcarlos Domingos da Silva. 
     Dissertação (Mestrado) – Universidade Federal de Goiás, 
Instituto de Matemática e Estatística, 2013. 
     Bibliografia. 
     Inclui lista de tabelas e figuras. 

     1.  Transformações  lineares.  2.  Equações  diferenciais 
ordinárias. 3. Autovalores. 4. Autovetores. I. Título. 

                                                                     CDU: 517.926                             

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
      
   
 
 
Todos os direitos reservados. É proibida a reprodução total ou parcial deste trabalho

sem a autorização da universidade, do autor e do orientador.

Marco Aurélio David Ramos graduou-se em Matemática pela Universidade Es-

tadual de Goiás. Lecionou em vários colégios da rede privada (Colégio Ateneu Dom

Bosco, Objetivo e outros), também lecionou no Colégio da Polícia Militar: Ayrton
Senna (em Goiânia) e atualmente é professor da rede estadual de ensino em Goiás.

Aos meus ﬁlhos, minha esposa e minha mãe.

Agradecimentos

Agradeço a todos aqueles que me apoiaram nesta fase de crescimento da vida. A

todos aqueles que de forma direta ou indireta me estenderam sua mão, me incentivaram,
perceberam e compreederam este momento, meus agradecimentos.

Um agradeçimento especial à minha esposa amada, Celina, que com muita paciência

caminhou comigo todo este tempo ainda que com pouco tempo para ela. À Éllen Louisy

e ao Marcos Daniel, dois jovens entusiasmados e extraordinários ﬁlhos que Deus me deu.
Agradeço à minha mãe, Dna Diva, que sempre esteve comigo, pela sua preocupação
e apoio a este empreendimento. À minha falecida vó que anteviu tudo isto a muito

tempo e ao meu pai, também falecido, que sempre torceu por mim ..., minha gratidão!

Agradeço aos meus professores pois, sem eles esta caminhada jamais teria se iniciado,
principalmente ao meu orientador Dr. Edcarlos Domingos da Silva, inteligente, bem

humorado e dedicado. Agradeço também à CAPES pelo suporte ﬁnanceiro sem o qual

seria muito difícil a continuação deste estudo. Finalmente meu agradecimento eterno

a Deus pois, Ele já tinha tudo isto preparado para mim.

Resumo
Nesta dissertação estudamos transformações lineares, autovalores e autovetores com

o intuito de resolvermos um sistema de equações diferenciais ordinárias lineares com

coeﬁcientes constantes.

Palavras-chave

Matriz, transformação linear, produto interno, diagonalização, autovalor, autovetor,

vetor, sistema de equações.

6

Abstract
In this thesis we study linear transformations, eigenvalues and eigenvectors with the

objective of solve a system of linear ordinary diﬀerential equations with constant coef-

ﬁcients.

Keywords

Matrix, linear transformations, internal product, diagonalization, eigenvalues, ei-

genvectors, vector, system of equations.

7

Sumário

Resumo

Abstract

Introdução

1 Espaços Vetoriais
1.1 Espaço vetorial

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.2 Subespaço vetorial

. . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.3 Base de um Espaço Vetorial

. . . . . . . . . . . . . . . . . . . . . . . .

1.4 Mudança de Base de um Vetor . . . . . . . . . . . . . . . . . . . . . . .

2 Transformações Lineares e Matriz Associada

2.1 Transformação linear . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.2 Núcleo de uma transformação linear . . . . . . . . . . . . . . . . . . . .

2.3 Composição, Mudança de Base e Semelhança de Matrizes . . . . . . . .

2.4 Gráﬁco da mudança de base de uma transformação linear . . . . . . . .
2.5 Matrizes semelhantes . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3 Produto Interno Euclidiano e a Adjunta de uma Transformação Li-

near

3.1 Produto interno euclidiano . . . . . . . . . . . . . . . . . . . . . . . . .

3.2 Vetores ortogonais
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3 Vetores ortonormais . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.4 Complemento ortogonal

. . . . . . . . . . . . . . . . . . . . . . . . . .

3.5 A adjunta . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.6 Operador auto-adjunto . . . . . . . . . . . . . . . . . . . . . . . . . . .

4 Autovalores e Autovetores
4.1 Autovalores e autovetores

. . . . . . . . . . . . . . . . . . . . . . . . .

4.2 Polinômio característico . . . . . . . . . . . . . . . . . . . . . . . . . .

4.3 Diagonalização de Operadores . . . . . . . . . . . . . . . . . . . . . . .

4.4 Diagonalização de uma matriz quadrada . . . . . . . . . . . . . . . . .

6

7

10

12
12

13

14

15

19
19

21

22

24
27

29

29

29
30

31

33

35

36
36

36

39

39

5 Aplicações

5.1 Sistemas de Equações diferenciais de Primeira Ordem com Coeﬁcientes
Constantes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.2 As 4 possibilidades de um sistema linear 2 × 2. . . . . . . . . . . . . . .

5.2.1

5.2.2
5.2.3

Sistema com dois autovalores reais e distintos.

. . . . . . . . . .

Sistema com dois autovalores complexos.
. . . . . . . . . . . . .
Sistema com um autovalor real repetido e dois autovetores asso-

42

43

44

44

46

ciados.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

48

5.2.4

Sistema com um autovalor real repetido e um único autovetor

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.3 Sistema massa-mola simples . . . . . . . . . . . . . . . . . . . . . . . .

associado.

Conclusão

Referências

49
51

56

57

9

Introdução

Uma da grandes surpresas da Matemática é a capacidade que ela possui de encontrar
mais de um caminho para a solução de um problema. As vezes estes caminhos são até

inesperados. Sem dúvida a Matemática é uma das disciplinas que mais demonstram

o caráter criativo do ser humano. Este é exatamente o caso que vamos tratar aqui.

O problema, que colocaremos em apreço, é o de um sistema massa-mola simples que
vamos resolver via Álgebra Linear.

A primeira seção deﬁne de forma clássica um espaço vetorial bem como o subespaço

vetorial. Ainda aqui é abordada as combinações lineares, a dependência e a indepen-

dência linear, que será útil ao longo de todo este trabalho. A base de um espaço vetorial
e como fazer uma mudança de base de um vetor são apresentadas aqui também.

Na segunda seção introduziremos as transformações lineares, a matriz associada a

uma transformação linear bem como a composição de tais transformações. A deﬁnição
de transformação linear injetora será feita aqui e retomada no início da quarta seção.

A mudança de base de uma matriz é aqui também inserida e generalizada e propicia-

mente neste momento é apresentada a deﬁnição de duas matrizes semelhantes.

Na terceira seção falaremos sobre o produto interno de um espaço vetorial. Deﬁni-

ções como norma de um vetor, vetores ortogonais e vetores ortonormais são utilizados.

Com uma breve introdução da projeção de um vetor sobre outro chegamos ao processo

de ortogonalização de Gram-Schmidt e ainda ao complemento ortogonal de um subes-
Introduzimos também o teorema da representação de Riesz para em seguida
paço.

estudarmos o operador adjunto e consequentemente o operador auto-adjunto.

A quarta seção começa estudando os autovalores, através dos polinômios caracte-
rísticos, e autovetores associados. O teorema espectral é enunciado dando ênfase para

o caso bidimensional e, por último, a diagonalização de uma matriz que utilizaremos

em vários problemas.

Na última seção faremos algumas aplicações aos sistemas de equações lineares com

coeﬁcientes constantes de primeira ordem, esgotando todas as possibilidades para um

sistema 2 × 2. Finalmente resolveremos o problema massa-mola proposto, utilizando

o fato de que toda equação diferencial linear com coeﬁcientes constantes, pode ser

10

transformada num sistema de equações lineares de primeira ordem com coeﬁcientes

constantes.

"Que nenhum desconhecedor da geometria entre aqui."(Inscrição no frontspício da

Academia de Platão).

11

1 Espaços Vetoriais

Na intenção de descrever com rigor os objetos e fenômenos da natureza, podemos
descrever alguns deles através de um número simples, um escalar, enquanto outros com
maior complexidade deverão ser descritos por meio de vetores1.

1.1 Espaço vetorial

Deﬁnição 1.1.1. Um conjunto V não vazio de objetos (vetores) é chamado Espaço

Vetorial Real se estiverem deﬁnidas uma soma u + v (u e v ∈ V) e uma multiplicação
por escalar k.v (k escalar) que satisfazem, quaisquer que sejam u, v e w ∈ V e k e c
escalares2 em R, os axiomas abaixo:

i. u + v = v + u

(comutativa).

ii. (u + v) + w = u + (v + w)

(associativa).

iii. 0 + u = u + 0

(vetor nulo ou zero.)

iv. u + (−u) = (−u) + u = 0

(inverso aditivo ou simétrico de u.)

v. k.(u + v) = k.u + k.v

vi. (k + c).u = k.u + c.u

vii. k.(c.u) = (k.c).u

viii. 1.u = u

Exemplo 1.1.1. Rn é um espaço vetorial pois dados os vetores u = (x1, x2, . . . , xn), v =
(y1, y2, . . . , yn) e w = (z1, z2, . . . , zn), (u, v e w ∈ Rn) e ainda k e c ∈ R, temos:
i. u + v = (x1, x2, . . . , xn) + (y1, y2, . . . , yn) = (x1 + y1, x2 + y2, . . . , xn + yn) = (y1 +
x1, y2 + x2, . . . , yn + xn) = v + u.
ii. (u + v) + w = (y1 + x1, y2 + x2, . . . , yn + xn) + (z1, z2, . . . , zn) = (x1 + y1 + z1, x2 + y2 +
z2, . . . , xn + yn + zn) = (x1, x2, . . . , xn) + (y1 + z1, y2 + z2, . . . , yn + zn) = u + (v + w).

1outros mais complexos ainda serão descritos por tensores. Não é nosso objetivo aqui trabalhar

com os tais objetos.

2Se os escalares estiverem em C então teremos um Espaço Vetorial Complexo.

12

iii. tomando 0 = (0, 0, . . . , 0) com n zeros, temos por i: u + 0 = 0 + u = (x1 + 0, x2 +
0, . . . , xn + 0) = (x1, x2, . . . , xn).
iv. fazendo −u = (−x1, −x2, . . . , −xn), então: u+(−u) = (−u)+u = (x1, x2, . . . , xn)+
(−x1, −x2, . . . , −xn) = (0, 0, . . . , 0)
(cid:125)

(cid:124)

(cid:123)(cid:122)
n−zeros

v. k.(u+v) = k.(x1 +y1, x2 +y2, . . . , xn +yn) = (k.(x1 +y1), k.(x2 +y2), . . . , k.(xn +yn) =
(k.x1 + k.y1, k.x2 + k.y2, . . . , k.xn + k.yn) =
(k.x1, k.x2, . . . , k.xn) + (k.y1, k.y2, . . . , k.yn) = k.(x1, x2, . . . , xn) +
+ k.(y1, y2, . . . , yn) = k.u + k.v
vi. façamos no ítem anterior k = c1 + c2 e u + v = w.
Assim: (c1 + c2).w = k.(u + v) ⇐⇒ k.u + k.v = c1u + c2u + c1v + c2v = c1(u + v) +
c2(u + v) = c1w + c2w.
vii. k.(c.u) = k.(c.(x1, x2, . . . , xn)) = k.(c.x1, c.x2, . . . , c.xn) =
(k.c.x1, k.c.x2, . . . , k.c.xn) = (k.c).(x1, x2, . . . , xn) = (k.c).u.
viii. 1.u = u é óbvio por vii.

Exemplo 1.1.2. Claramente R2 e R3 são espaços vetoriais.

1.2 Subespaço vetorial

Deﬁnição 1.2.1. Um subconjunto W de um espaço vetorial V é chamado de subespaço

de V se W é um espaço vetorial com os mesmos escalares, a mesma adição e a mesma

multiplicação por escalar em V.

Observe que todas as propriedades do espaço vetorial V são herdados por W, então

para que W seja também um espaço vetorial, toda combinação linear de elementos em
W devem pertencer a W, isto é, k1.a + k2.b ∈ W, ∀ a e b ∈ W e k1 e k2 escalares.

Exemplo 1.2.1. O conjunto W das matrizes simétricas n×n é um subespaço de Mn×n
(conjunto das matrizes n × n).
Obviamente W é não vazio. Então dadas as matrizes A e B, temos AT = A e BT = B.
Assim:
(k1.A + k2.B)T = (k1.A)T + (k2.B)T = k1.AT + k2.BT = k1.A + k2.B, quaisquer que
sejam os escalares k1 e k2.
Assim provamos que toda combinação linear de matrizes em W está em W e portanto

W é um subespaço.

13

Exemplo 1.2.2. Seja D o conjunto-solução de todas as funções que satisfazem a equa-
ção diferencial

Então D é um subespaço pois

f (cid:48)(cid:48) + f = 0

(k1.f + k2.g)(cid:48)(cid:48) + (k1.f + k2.g) = (k1.f (cid:48)(cid:48) + k2.g(cid:48)(cid:48)) + (f + g)

= (k1f (cid:48)(cid:48) + k1.f ) + (k2.g(cid:48)(cid:48) + k2.g)
= k1.(f (cid:48)(cid:48) + f ) + k2.(g(cid:48)(cid:48) + g)

= k1.0 + k2.0

= 0 + 0
= 0, ∀ k1 e k2 ∈ R.

E assim D é um subespaço de (cid:61) (Espaço Vetorial de todas as funções diferenciáveis à
valores reais em R).

1.3 Base de um Espaço Vetorial

Deﬁnição 1.3.1. Se S = {v1, v2. . . . , vn} é um conjunto não-vazio de vetores, a equa-
ção vetorial

k1.v1 + k2.v2 + · · · + kn.vn = 0

será linearmente independente se ela admitir uma única solução a saber

k1 = 0 k2 = 0 . . . kn = 0

Se houver outras soluções então S é linearmente dependente.

Exemplo 1.3.1. Os vetores v1 = (1, 2, −1), v2 = (0, −3, 1) e v3 = (−2, 1, 0) são LI
(linearmente independentes) pois k1.v1 + k2.v2 + kn.vn = 0 =⇒ k1 = k2 = k3 = 0.
Alternativamente pode-se veriﬁcar que v1, v2 e v3 são LI observando que o determinante
da matriz, cujas colunas são os vetores v1, v2 e v3 é diferente de zero.

Exemplo 1.3.2. Os vetores v1 = (1, 2, 1), v2 = (2, 9, 0) e v3 = (3, 6, 3) são LD (linear-
mente dependentes) já que:

= 0

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1 2 3

2 9 6

1 0 3

14

Deﬁnição 1.3.2. Um conjunto β ⊂ V gera o espaço V se para todo v ∈ W ⊂ V,
existirem v1, v2, . . . , vn ∈ β e escalares a1, a2, . . . , an tais que:

v = a1.v1 + a2.v2 + · · · + an.vn

Deﬁnição 1.3.3. Um conjunto β ⊂ V é uma base de V se:

• β gera V.

• β é linearmente independente.

Proposição 1.3.1. Em todo espaço vetorial V ﬁnito de base β = {v1, v2, . . .
. . . , vn} um vetor v ∈ V é escrito de maneira única como combinação linear dos vetores
de β.

Demonstração 1.3.1. Suponhamos

v = a1.v1 + a2.v2 + · · · + an.vn
v = b1.v1 + b2.v2 + · · · + bn.vn

Logo,

(a1 − b1).v1 + (a2 − b2).v2 · · · + (an − bn).vn = 0
e a1 = b1, a2 = b2, . . . , an = bn.

Assim (a1, a2, . . . , an) é a coordenada de v em relação à base β.

Exemplo 1.3.3. O conjunto β = {v1, v2, v3} em que v1 = (1, 2, 1), v2 = (2, 9, 0) e
v3 = (3, 3, 4) é uma base de R3 pois todo vetor em R3 é da forma b = (b1, b2, b3) e os
vetores v1, v2, e v3 são LI.

1.4 Mudança de Base de um Vetor

Deﬁnição 1.4.1. Seja α = {a1, a2, . . . , an} e β = {b1, b2, . . . , bn} duas bases ordenadas
de um mesmo espaço vetorial V. Um vetor v pode ser escrito como:

v = x1.a1+ x2.a2+ · · · + xn.an ou

(λ)

v = y1.b1+ y2.b2+ · · · + yn.bn

15












.












x1

x2
...

xn

A coordenada de v em relação à base α é: [v]α =

e em relação à base β é: [v]β =












.












y1

y2
...

yn

Podemos escrever {b1, b2, . . . , bn} em função de {a1, a2, . . . , an} pois aquele é base de
V:






(ρ)

b1 = c11.a1+ c21.a2+ · · · + cn1.an

b2 = c12.a1+ c22.a2+ · · · + cn2.an
...

...

...

...

bn = c1n.a1+ c2n.a2+ · · · + cnn.an

Voltando com estes valores em λ, obtemos:
v = y1.b1 + y2.b2 + · · · + yn.bn
= y1.(c11.a1 + c21.a2 + · · · + cn1.an) + y2.(c12.a1 + c22.a2 + · · · + cn2.an) +
· · · + yn.(c1n.a1 + c2n.a2 + · · · + cnn.an)
= (y1.c11 + y2.c12 + · · · + yn.c1n).a1 + (y1.c21 + y2.c22 + · · · + yn.c2n).a2 + . . .
+ (y1.cn1 + y2.cn2 + · · · + yn.cnn).an
Mas como temos v = x1.a1 + x2.a2 + · · · + xn.an e sendo as coordenadas em uma base
únicas:






x1 = y1.c11+ y2.c12+ · · · + yn.c1n

x2 = y1.c21+ y2.c22+ · · · + yn.c2n
...

...

...

...

xn = y1.cn1+ y2.cn2+ · · · + yn.cnn

16

Matricialmente:



Fazendo:

[P]β

α =












=























x1

x2
...

xn

c11

c12

. . .

c1n

. . .

c21
...

c22
...

c2n
...

cn1

cn2

. . .

cnn












.























y1

y2
...

yn










c11

c12

. . .

c1n

. . .

c21
...

c22
...

c2n
...

cn1

cn2

. . .

cnn












(Esta é a matriz de mudança da

base β para base α.)

E assim: [v]α = [P]β

α.[v]β.

Proposição 1.4.1. A matriz de mudança de base é invertível.

Demonstração 1.4.1. A única solução de [P]β
identicamente nulo dai, [P]β
Segue-se imediatamente que:

α (cid:54)= 0. (cid:4)

α.X = 0, em qualquer base é o vetor

[v]β = ([P]β

α)−1.[v]α

α)−1 é a matriz de mudança da base α para base β,isto é, ([P]β

α)−1 = [P]α

β. Final-

([P]β
mente temos que

[v]β = ([P]α

β).[v]α

Exemplo 1.4.1. Quais são as coordenadas de v = (3, 2, 1) em relação à base β =

{(1, 1, 0), (1, 0, 1), (0, 0, 1)}?

Tomando α como a base canônica, temos

1 1 0

[P]β

α =

1 0 0

0 1 1

















17

Então



1
2
1
−
2
1
2
Assim as coordenadas de v em relação a β são:

1
2
1
2
1
−
2







[P]α

β =









1
−
2
1
2
1
2









1
2
1
2
1
−
2

1
2
1
−
2
1
2









1
−
2
1
2
1
2



3






2




1

.










2



1


0

=

18

2 Transformações Lineares e Matriz Associada

Observe o sistema:






y1 = x1 + 2x2

.

y2 =

−3x2





Ele pode ser escrito na forma Y = A.X em que Y =




y1

y2


, A =


1



2

0 −3








 e X =





 .

x1

x2



−1

2







2

−1

2

Então se X =


, teremos: y1 = −1 + 2.2 = 3 e y2 = −3.2 = −6





ou





 =

3

−6


1



0 −3







x1

x2










 .





. Fica claro assim que o vetor Y =





, foi obtido do

y1

y2








1

2

0 −3


. Em outras palavras houve uma

vetor X =


 através da matriz A =

transformação do vetor X no vetor Y através da matriz A. Isto nós leva a uma deﬁ-

nição:

2.1 Transformação linear

Deﬁnição 2.1.1. Uma tranformação linear de um espaço vetorial V para um espaço

vetorial W é uma aplicação T:V → W tal que para todo u e v ∈ V e para todos os
escalares c ∈ R, temos que
1. T (u + v) = T (u) + T (v)

2. T (c.v) = c.T (v)
Mais especiﬁcamente uma transformação T:Rn → Rm é chamada de transformação
linear se:
1. T (u + v) = T (u) + T (v) para todo u e v em Rn
2. T (c.v) = c.T (v) para todo v em Rn e todos os escalares c.

19

Exemplo 2.1.1. T:R2 → R3 deﬁnida por T




 =






x

y

linear. Veriﬁquemos:

Tomemos u =






x1

y1












 e v =




x2

y2


, então: T (u) = T




x1

y1


 =





T




x2

y2


 =









x2

x2 + y2

x2 − y2

















x

x + y

x − y









é uma transformação









x1

x1 + y1

x1 − y1









e T (v) =

T (u + v) = T






x1 + x2

y1 + y2




 =

x1 + x2

x1 + x2 + y1 + y2

















=









x1

x1 + y1









+









x2

x2 + y2









= T (u) +

x1 + x2 − y1 − y2

x1 − y1

x2 − y2

T (v)

T (c.u) = T





c.x1




c.y1


 =









c.x1

c.x1 + c.y1

c.x1 − c.y1









= c.









x1

x1 + y1

x1 − y1









= c.T (u)

Exemplo 2.1.2. Seja p = p(x) = a0 + a1x + a2x2 + · · · + anxn um polinômio em P e a
função T:Pn → Pn+1, deﬁnida por T (p) = T (p(x)) = x.p(x) = a0x+a1x2+· · ·+anxn+1.
Então T é uma transformação linear.

Com efeito, temos que
T (p1 + p2) = T (p1(x) + P2(x)) = x(p1(x) + p2(x)) = xp1 + xp2 = T (p1) + T (p2). e
T (k.p) = T (k.p(x)) = x.(k.p(x)) = k.(x.p(x)) = k.T (p(x)) = k.T (p)

Teorema 2.1.1. Sendo T:Rn → Rm uma transformação linear, então T é uma tras-
formação por meio de uma matriz, isto é, T = TA, e A é uma matriz m × n tal
que

A = [T (e1); T (e2); . . . ; T (en)].

20

Demonstração 2.1.1. Sejam e1, e2, . . . , en os vetores da base canônica de Rn e v um
vetor de Rn. Podemos escrever:
v = v1e1 + v2e2 + · · · + vnen (em que v1, v2, . . . , vn, são as coordenadas de v). Mas,
T (e1), T (e2), . . . , T (en) são vetores coluna em Rn, então:
T (v) = T (v1e1 + v2e2 + · · · + vnen)
T (v) = v1T (e1) + v2T (e2) + · · · + vnT (en)























v1

v2
...

vn

T (v) = [T (e1); T (e2); . . . ; t(en)].

Logo

T (v) = Av

É claro que T (v) = Av é uma transformação linear pois:

T (u + v) = A(u + v) = Au + Av = T (u) + T (v) e

T (k.v) = A(k.v) = k.(Av) = k.T (v).

Exemplo 2.1.3. A matriz canônica da transformação linear T:R2 → R2, dada por





T (x, y) = (x + y, x − y) é A =




1

1

1 −1




2.2 Núcleo de uma transformação linear

Deﬁnição 2.2.1. Seja T : V −→ W uma transformação linear. O núcleo de T,
denotado por ker(T), é o conjunto de todos os vetores de V que são levados por T em

0 de W, isto é:

ker(T ) = {v ∈ V : T (v) = 0}

Exemplo 2.2.1. O núcleo do operador em R3 dado por

é:

T (x, y, z) = (3x, x − y + z, y − 2x)

a

(cid:122)(cid:125)(cid:124)(cid:123)3x = x − y − z
(cid:125)
(cid:124)

(cid:123)(cid:122)
b

c
(cid:122) (cid:125)(cid:124) (cid:123)
y − 2x = 0

=

21

3x = 0 =⇒ x = 0

de a = c, obtemos 3x = y − 2x =⇒ y = 0

f inalmente de b = 0 temos, x − y + z = 0 =⇒ z = 0

Portanto ker(T ) = {0, 0, 0}

Deﬁnição 2.2.2. T : V −→ W é injetora se. para todo u e v ∈ V ,então u (cid:54)= v implica
que T (u) (cid:54)= T (v).

Deﬁnição 2.2.3. T : V −→ W é sobrejetora se, para todo w ∈ W , existe pelo menos

um v em V tal que w = T (v).

Teorema 2.2.1. Uma transformação linear T : V −→ W é injetiva se e somente se

ker(T ) = 0.

Demonstração 2.2.1. Vamos assumir que T é injetiva. Se v está no núcleo de T,

temos T (v) = 0. Mas T (0) = 0 provando T (v) = T (0). Sendo T injetiva então v=0.

Assim o vetor nulo é o único vetor no núcleo de T.

Assumamos agora que ker(T ) = 0 e dados vetores v e u em V tais que T (v) = T (u).

Então T (v) − T (u) = 0 e T (v − u) = 0. Assim v − u está no núcleo de T e como

ker(T ) = 0, devemos ter v − u = 0. Dai v=u, demonstrando dessa forma que T é

injetiva.

(cid:4)

2.3 Composição, Mudança de Base e Semelhança de Matrizes

Teorema 2.3.1. A composta de transformações lineares T : U −→ V e S : V −→ W

é uma transformação linear S ◦ T : U −→ W .

Demonstração 2.3.1. Sejam v e w ∈ U e k um escalar. Então:

S ◦ T (v + w) = S(T (v + w)) = S(T (v) + T (w)) = S(T (v)) + S(T (w)) =
(S ◦ T )(v) + (S ◦ T )(w)

(S ◦ T )(k.v) = S(T (k.v)) = S(k.T (v)) = k.S(T (v)) = k.(S ◦ T )(v).

e

22

Teorema 2.3.2. Sejam T : Rm −→ Rn, S : Rn −→ Rp transformações lineares e
ainda T ◦ S : Rm −→ Rp. Então suas matrizes canônicas satifazem

Demonstração 2.3.2. Sejam:

[S ◦ T ] = [S].[T ]

[S] = Am×n e [T ] = Bn×p e v um vetor em Rm então:
(S ◦ T )(v) = S(T v)) = S(B.v) = A.B.v





Exemplo 2.3.1. Considere a transformação T : R2 −→ R3 dada por T




x1

x2


 =

e a transformação S : R3 −→ R4 dada por S

x1

x1 + x2

















x1 − x2
tre (S ◦ T ).
Podemos ver que:









. Encon-

2y1

y2 + y3

3.y2 − 2.y3









y1

y2

y3

















=



2 0

0 1









0

1










1



1



0

1







e [T ] =

0 3 −2

1 −1

[S] =

Usando o teorema:

[S ◦ T ] = [S].[T ] =

2 0

0 1









0

1

















.

1

1

0

1









0 3 −2

1 −1

Assim:





(S ◦ T )




x1

x2


 =

23









2 0

2 0

1 5









Teorema 2.3.3. Supondo T = V −→ W, V n-dimensional e W m-dimensional com
bases β e γ, respectivamente e que R([v]β) = (v), deﬁne um isomorﬁsmo R : Rn −→ V
e também S : Rm −→ W é outro isomorﬁsmo em que S(w) = (w). Então:

sendo

[T (v)]γ = A [v]β

A = [[T (v1)]γ, [T (v2)]γ, . . . , [T (vn)]γ)]

2.4 Gráﬁco da mudança de base de uma transformação linear

Observe a ilustração:

(v)

V

T

(cid:47) •T (v) W

R

S

S−1

[v]β Rn

S−1◦T ◦R

(cid:47) •[T (v)]γ Rm

Demonstração 2.4.1. Sabemos que:

mas:

dai:

Finalmente:

(v)β = [R].[v]β
[T v]β = [S].[T (v)]γ

[T v]β = [T ].(v)β

[S].[T (v)]γ = [T ].[R].[v]β
[T v]γ = [S]−1.[T ].[R].[v]β

24

(cid:47)
(cid:125)
(cid:125)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:47)
A é a matriz de T em relação às bases β e γ.

[T v]γ = A[v]β

Exemplo 2.4.1. Seja T:R3 −→ R2 tal que T (x, y, z) = (2x + y − z, 3x − 2y + 4z),
procuremos a Transformação associada às bases β = {(1, 1, 1), (1, 1, 0), (1, 0, 0))} e

γ = {(1, 3), (1, 4)}.
A = [T ]β
γ
A = [γ]−1.[T ].[β]





−1








1 1

3 4




.




2

1 −1

3 −2

4





4 −1


2




 .

−3

1

3 −2

4

1 −1





3

11

5




−1 −8 −3










 .









1 1 1

1 1 0

1 0 0


1 1 1




 .







1 1 0

1 0 0









1 −2

2 −1

1 −1

−1

3








 .




.



1


1





3

3 1

2 1


 .

2 −1

1 −1


 .




1

−1

3

−1










1





11 −13

7 −8

A =

A =

A =







A =

A =

A =












Exemplo 2.4.2. Dada a base α = {(1, −2), (−1, 3)}, determinar a matriz, na base α,
do operador linear F : R2 −→ R2, F(x, y) = (2x − y, x + y).
A = [T ]α


α = TA = [α]−1.[T ].[α]










−1

25

Exemplo 2.4.3. Seja T : R3 −→ R3 a transformação linear representada pela matriz









4 2 2

A =

2 4 2

2 2 4









Encontre T em relação à base β = {(1, −1, 0), (1, 0, −1), (1, 1, 1)}.

Temos:



[β]−1 =

1

−1







1

0

1

1









0 −1 1

Então:



[β] =

−1

1







1

0

1

1

0 −1 1

−1









=









1
3
1
3
1
3

2
−
3
1
3
1
3









1
3
2
−
3
1
3

Logo

[T ]β = [β].A.[β]−1 =









1
3
1
3
1
3

2
−
3
1
3
1
3

[T ]β =









.









1
3
2
−
3
1
3

















2 0 0

0 2 0

0 0 8

.

1

4 2 2

2 4 2

.

−1

















1

0

1

1









2 2 4

0 −1 1

Teorema 2.4.1. Sejam α = {a1, a2, . . . , an} e β = {b1, b2, . . . , bn} duas bases para o
espaço vetorial V. Seja T : V −→ V um operador linear. Então existe uma única

matriz invertível P tal que:

Tβ = P.Tα.P −1 e Tα = P −1.Tβ.P.

As colunas de P são dadas pelas coordenadas dos vetores da base α com relação à base
β, ou seja,

P = [[v1]β, [v2]β, . . . , [vn]β].

26

Demonstração 2.4.2.

[v]α = P −1.[v]β,

[T v]α = P −1.[T v]β

Para todo vetor v ∈ V . Adicionalmente temos:

Logo,

Assim,

Por outro lado temos que

Logo

Provando que

[T v]α = [T ]α.[v]α.

P −1.[T v]β = [T ]α.P −1.[v]β.

[T v]β = P [T ]α.P −1.[v]β.

[T v]β = [T ]β.[v]β.

[T ]β.[v]β = P.[T ]α.P −1.[v]β,

[T ]β = P.[T ]α.P −1.

P é a matriz de mudança da base α para base β. Então P −1 é a matriz de mudança

da base β para base α.
Observe que o exemplo 3.1.4 utilizou especiﬁcamente a relação acima.

A relação acima nos remete a seguinte deﬁnição.

(cid:4)

2.5 Matrizes semelhantes

Deﬁnição 2.5.1. Sejam A e B matrizes n × n. Dizemos que a matriz A é semelhante
a matriz B se existir uma matriz n × n invertível P tal que P −1.A.P = B.

Observe que se B = P −1.A.P , então A = P.B.P −1 ou ainda A.P = P.B.

Exemplo 2.5.1. Seja A =


1



2

0 −1








 e B =




1

0

−2 −1


.

27

Então A ∼ B (A semelhante a B), pois:









 .




















1

2

1 −1

3

1

1 −1

1

0

0 −1

1

1

−1 −1

1

−2 −1


 =



1


 .








 =




Teorema 2.5.1. Sejam A e B matrizes n × n com A ∼ B, então detA = detB.

Demonstração 2.5.1.

detA = det(P.B.P −1)

detA = detP.detB.detP −1

detA = detP.detB.

detA = detB

1
detP

Observe que o fato do determinante de duas matrizes semelhantes serem iguais é
necessário mas, não é suﬁciente. Assim este argumento é utilizado quando queremos

mostrar que duas matrizes n × n não são semelhantes.

(cid:4)


3



1

0 −2







−1 4

−1 3


 e B =





, não são semelhantes pois detA = −6

Exemplo 2.5.2. A =

e detB = 1.

28

3 Produto Interno Euclidiano e a Adjunta de uma

Transformação Linear

3.1 Produto interno euclidiano

Deﬁnição 3.1.1. Chamamos de produto interno euclidiano em Rn uma função que
associa um número real <u,v> a cada par de vetores em Rn. Seu cálculo é dado por:

< u, v >= u.v = u1.v1 + u2.v2 + · · · + un.vn,

com u = (u1, u2, . . . , un) e v = (v1, v2, . . . , vn)

Deﬁnição 3.1.2. A norma ou comprimento de um vetor u = (u1, u2, . . . , un) em Rn
é dada por ||u|| =

u1 + u2 + · · · + un

√

Obs.: Dado u ∈ Rn; u (cid:54)= 0 segue que v =

u
||u||

é unitário.

3.2 Vetores ortogonais

Deﬁnição 3.2.1. Um conjunto de vetores A = {v1, v2, . . . , vn} em Rn é um conjunto
ortogonal se:

Além disso se A é ortogonal então todos estes vetores são LI.

(cid:104)vi, vj(cid:105) = 0, i (cid:54)= j

Demonstração 3.2.1. É óbvio pela deﬁnição de produto interno euclidiano que se
dois vetores são ortogonais então θ =
Por outro lado sabendo que {v1, v2, . . . , vn} é um conjunto ortogonal, então devemos
provar que este conjunto é LI. De fato temos que

= 0 e (cid:104)vi.vj(cid:105) = 0.

e dai ||u||.||v||.cos

π
2

π
2

a1.v1 + a2.v2 + · · · + an.vn = 0; ai ∈ R

implica que

(cid:104)a1.v1 + a2.v2 + · · · + an.vn, vi(cid:105) = ai.(cid:104)vi, vi(cid:105) = ai = 0; ∀i = 1, 2, . . . , n

Logo {v1, v2, . . . , vn} é LI.

29

Exemplo 3.2.1. Os vetores v1 =
são ortogonais entre si. Assim eles s˜ão LI.

, 0,

(cid:18) 1
√
2

(cid:19)

, v2 = (0, 1, 0) e v3 =

1
√
2

(cid:18) 1
√
2

, 0, −

(cid:19)

,

1
√
2

Observe que os vetores do exemplo acima geram R3

3.3 Vetores ortonormais

Deﬁnição 3.3.1. v1, v2, . . . , vn é um conjunto ortonormal se:

vi.vj =






1, se i = j

0, se i (cid:54)= j

Os vetores do exemplo 3.0.11 que geram R3, formam uma base ortonormal.

Nota. A projeção do vetor v sobre o vetor u é dada por:

w = proju(v) =

v.u
||v||2 v

(cid:63)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)

v

w

Basta lembrar que

u.v = ||u||.||v||.cosθ =⇒ cosθ =

u

u.v
||v||.||u||

(cid:19)

w = ||w||. (cid:98)w

(cid:18)

(cid:98)w =

u
||u||

, versor de u

||w|| = ||v||.cosθ

Observe também que um vetor perpendicular a u (ou w) é dado, neste caso, por v − w.

Teorema 3.3.1. (Processo de Ortogonalização de Gram-Schmidt) Todo espaço vetorial

com produto interno possui uma base ortonormal.

30

(cid:63)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
Demonstração 3.3.1. Seja α = {x1, x2, . . . , xn} uma base para um espaço vetorial
V , vamos construir uma base ortogonal β = {v1, v2, . . . , vn} a partir de α utilizando o
Processo de Ortogonalização de Gram-Schmidt.
Tomemos v1 = x1. Agora vamos supor por indução que v1, v2, . . . , vm sejam vetores
ortogonais e formem uma base ortogonal {v1, v2, . . . , vk} para o subespaço gerado pelos
vetores x1, x2, . . . , xk em que 1 ≤ k ≤ m. Construamos o vetor vm+1 através da projeção
ortogonal do vetor xm+1 sobre o subespaço gerado pelos vetores v1, v2, . . . , vk:

Assim:

m
(cid:88)

i=1

(cid:104)xm+1.vi(cid:105)
||vi||2

vi

vm+1 = xm+1 −

m
(cid:88)

i=1

(cid:104)xm+1.vi(cid:105)
||vi||2

vi.

Então xm+1 (cid:54)= 0, se não ele estaria no subespaço gerado por x1, x2, . . . , xm e portanto
seria uma combinação linear dos mesmos. Além disso, para todo 1 ≤ k ≤ m:

(cid:104)vm+1, xk(cid:105) = (cid:104)xm+1 −

m
(cid:88)

i=1

(cid:104)xm+1.vi(cid:105)
||vi||2

vi, xk(cid:105)

= (cid:104)xm+1, xk(cid:105) − (cid:104)

= (cid:104)xm+1, xk(cid:105) −

= (cid:104)xm+1, xk(cid:105) −

vi, xk(cid:105)

(cid:104)vi, vk(cid:105)

m
(cid:88)

i=1
m
(cid:88)

(cid:104)xm+1.vi(cid:105)
||vi||2
(cid:104)xm+1.vi(cid:105)
||vi||2

i=1
(cid:104)xm+1.vk(cid:105)
||vk||2

(cid:104)vk, vk(cid:105) = 0

Logo, {v1, v2, . . . , vm+1} é um conjunto de m+1 vetores ortogonais que geram o
subespaço formado por x1, x2, . . . , xm+1 de dimensão m + 1. Logo é uma base para este
espaço. Se quisermos uma base ortonormal basta tão somente agora normalizar cada

vetor da nova base encontrada.

3.4 Complemento ortogonal

Deﬁnição 3.4.1. Seja V um espaço vetorial com produto interno e W ⊂ V um subes-
paço de V. O complemento ortogonal de W é o subespaço

W ⊥ = {w ∈ V : (cid:104)w, v(cid:105) = 0, ∀ v ∈ V }

31

Exemplo 3.4.1. Seja S = {u = (x, y, 0) ∈ R3|x, y ∈ R}, temos então:

S⊥ = {v = (x, y, z) ∈ R3|(cid:104)v, u(cid:105) = 0}, ∀ u ∈ S

dai

Logo

(cid:104)v, u(cid:105) = (cid:104)(x, y, z).(x, y, 0)(cid:105) = x2 + y2 = 0 =⇒ x = y = 0

S⊥ = {(0, 0, z) ∈ R3|z ∈ R}

Proposição 3.4.1. Para qualquer subespaço W ⊂ V temos

V = W ⊕ W ⊥,

(⊕ representa a soma direta.)

Demonstração 3.4.1. Seja {v1, v2, . . . , vm} uma base ortogonal do subespaço W. Sendo
v ∈ V

w =

m
(cid:88)

i=1

(cid:104)v, vi(cid:105)
||vi||2 vi ∈ W.

Então u = v − w ∈ W ⊥ pois (cid:104)u, vj(cid:105) = 0 ∀j:

(cid:104)u, vj(cid:105) = (cid:104)v − w, vj(cid:105) = (cid:104)v, vj(cid:105) − (cid:104)w, vj(cid:105) = (cid:104)v, vj(cid:105) −

m
(cid:88)

i=1

(cid:104)v, vi(cid:105)
||vi||2 (cid:104)vi, vj(cid:105)

(cid:104)u, vj(cid:105) = (cid:104)v, vj(cid:105) −

(cid:104)v, vj(cid:105)
||vj||2 (cid:104)vj, vj(cid:105) = 0

Então v = w + u em que w ∈ W e u ∈ W ⊥. Observe ainda que se v ∈ W ∩ W ⊥ então
(cid:104)v, v(cid:105) = 0 e portanto v = 0.

Teorema 3.4.1 (Representação de Riesz). Seja V um espaço vetorial de dimensão
ﬁnita, munido de um produto interno e L ∈ V* um funcional linear3. Então existe um
único vetor u ∈ V tal que

L(v) =< v, u > ∀ v ∈ V

Demonstração 3.4.2. Tomemos uma base ortonormal de V {v1, v2, . . . , vn}. Se u ∈
V, então

3transformação linear de V em V*

32

v =

L(v) =

L(v) =

n
(cid:88)

i=1
n
(cid:88)

i=1
n
(cid:88)

i=1

L(v) = (cid:104)v,

< v, ui > ui

< v, ui > L(ui)

< v, ui.L(ui) >

n
(cid:88)

i=1

L(ui).ui(cid:105).

Agora tomando

segue a igualdade

u =

n
(cid:88)

i=1

L(ui).ui

L(v) =< v, u > ∀ v ∈ V

.

Suponha que exista w ∈ V tal que

L(v) =< v, w > .

Então

Logo

< v, u >=< v, w >=⇒< v, u − w >= 0, ∀ v ∈ V.

u − w = 0, ou u = w.

3.5 A adjunta

Deﬁnição 3.5.1. Sejam V e W dois espaços vetoriais com um produto interno e
T : V → W é uma aplicação linear. Então a aplicação T* : W → V é a adjunta de T
se (cid:104)T v, u(cid:105) = (cid:104)v, T ∗u(cid:105); ∀ u, v ∈ V.

Exemplo 3.5.1. Seja T : R2 −→ R2 tal que T (x, y) = (3x, 8x − y). O operador
T ∗ : R2 −→ R2 tal que T ∗(x, y) = (3x + 8y, −y) é o operador adjunto de T.
Vamos veriﬁcar isto tomando dois vetores em R2. Sejam eles v : (x1, y1) e u : (x2, y2),
então:

(cid:104)T v, u(cid:105) = (cid:104)v, T ∗u(cid:105)

33

(cid:104)(3x1, 8x1 − y1)(x2, y2)(cid:105) = (cid:104)(x1, y1)(3x2 + 8y2, −y2)(cid:105)

3x1x2 + 8x1y2 − y1y2 = 3x1x2 + 8x1y2 − y1y2

Teorema 3.5.1. Seja V um espaço vetorial com produto interno e de dimensão ﬁnita
e T : V −→ V um operador linear. Se β = {v1, v2, . . . , vn} é uma base ortonormal de
V e A = [T ]β, então:

aij = (cid:104)T vj, vi(cid:105)

Demonstração 3.5.1. Observe em primeiro lugar que cada coluna de [T ]β, é dada
pelas coordenadas do vetor

n
(cid:88)

T vj =

akjvk.

Dai

k=1

(cid:104)T vj, vi(cid:105) = (cid:104)

n
(cid:88)

akjvk, vi(cid:105) =

k=1

n
(cid:88)

k=1

akj(cid:104)vk, vi(cid:105) = aij

Proposição 3.5.1. Seja V um espaço vetorial com produto interno de dimensão ﬁnita
e T : V −→ V um operador linear. Se β = {v1, v2, . . . , vn} é uma base ortonormal de
V, e A = [T ]β então AT = [T ∗]β

Demonstração 3.5.2. Seja B = [T ∗]β

(cid:4)

aij = (cid:104)T vj, vi(cid:105)
bij = (cid:104)T ∗vj, vi(cid:105)
bij = (cid:104)T ∗vj, vi(cid:105) = (cid:104)vi, T ∗vj(cid:105) = (cid:104)T vi, vj(cid:105) = aji

Assim a matriz do operador adjunto T ∗ é a transposta da matriz do operador T em
uma base ortonormal.

Exemplo 3.5.2. Seja T um operador linear em R2 deﬁnido por:
T (x, y) = (x + y, 2x − 3y), encontremos T ∗.

1



Observe que a matriz associada a T é A =



1


 e a transposta de A é: AT = A∗ =

2 −3



2


1



1 −3


. Assim T ∗(x, y) = (x + 2y, x − 3y)

Observe que no exemplo anterior tomamos como Matriz do operador adjunto T ∗ a

transposta conjugada do operador T .

34

3.6 Operador auto-adjunto

Deﬁnição 3.6.1. Seja V um espaço vetorial com produto interno de dimensão ﬁnita

e T : V −→ V um operador linear. Dizemos que T é um operador auto-adjunto se:

ou

T = T ∗

(cid:104)T v, u(cid:105) = (cid:104)v, T u(cid:105), u, v ∈ V.

Corolário 3.6.1. Seja V um espaço vetorial real, com produto interno, de dimensão
ﬁnita e T : V −→ V um operador linear auto-adjunto. Se β = {v1, v2, . . . , vn} é uma
base ortonormal para V então A = [T ]β é uma matriz simétrica.

Exemplo 3.6.1. Seja T : R2 −→ R2 dado por T (x, y) = (2x + 5y, 5x − 3y) e S :
R3 −→ R3 tal que S(x, y, z) = (x − y, −x + 3y − 2z, −2y). Então T e S são operadores
auto-adjuntos e suas matrizes correspondentes são simétricas.

35

4 Autovalores e Autovetores

4.1 Autovalores e autovetores

Deﬁnição 4.1.1. Seja V um espaço vetorial sobre um corpo K e T : V −→ V um
operador linear. Dizemos que λ é um autovalor de T se existir um vetor v ∈ V , v (cid:54)= 0

tal que:

T v = λv

Se λ é um autovalor de T e v é qualquer vetor, não nulo, tal que T v = λv, dizemos

que v é um autovetor de T associado a λ.

Deﬁnição 4.1.2. O conjunto Vλ = {v ∈ V : T v = λv} dos autovetores associados ao
autovalor λ pelo operador T é um subespaço de V. De fato Vλ é o núcleo do operador
T − λI, como λ é um autovalor de T − λI somente se este for não-injetivo, então

det(T − λI) = 0.

4.2 Polinômio característico

Deﬁnição 4.2.1. O polinômio p(x) = det(xI − T ) é chamado polinômio caracte-
rístico de T. (Observe que as raízes deste polinômio depende diretamente do corpo K
envolvido.)






Exemplo 4.2.1. Seja a matriz A =

tovetores associados.







1 −2

−3

2


, encontremos seus autovalores e au-

det(λI − A) =





 = (λ − 1)(λ − 2) − 6

λ − 1

2

3

λ − 2

p(λ) = λ2 −3λ−4 é o polinômio característico. Encontrando as raízes de p(λ), obtemos
λ1 = −1 e λ2 = 4 como autovalores.
O autovetor associado a λ1 = −1 é:













λ − 1

2

−1 − 1

2

−2

2





 =




3

λ − 2
−2x1 + 2x2 = 0 e 3x1 − 3x2 = 0

3


 =





 =⇒

−1 − 2

3 −3

36

Como essas equações são equivalentes, resolvendo uma delas, encontramos:
x1 − x2 = 0, fazendo x2 = t, segue que x1 = t. Portanto
v1 = (t, t) = t(1, 1) = (1, 1)
Procedendo da mesma forma com λ2 = 4, obtemos:
3x1 + 2x2 = 0, colocando x2 = t, temos x1 = − 2
v2 = (− 2

3t, assim:

3, 1) = (−2, 3)

3t, t) = t(− 2





Exemplo 4.2.2. O polinômio característico da matriz A =

0 −1



1


, é:

0

p(x) = det(xI − T ) =






x

1

−1 x


 = x2 + 1, que não possui raízes em R mas em C. De


fato seus autovalores são
x1 = −i e x2 = i.
O autovetor associado a x1 = −i é:






−ix1 + x2 = 0

−x1 − ix2 = 0

=⇒ −ix1 + x2 = 0

Tomando x1 = 1, temos x2 = i, então v1 = (1, i)

Da mesma forma encontramos para x2 = i, v2 = (i, 1).

Teorema 4.2.1. Todo operador linear auto-adjunto T : V −→ V , possui autovalores

reais e seus autovetores associados são ortogonais.

Demonstração 4.2.1. Seja λ um autovalor de T e v seu autovetor correspondente

(não-nulo) e ainda A uma matriz simétrica associada a T em alguma base β ortonor-
mal.

vtAv = vtλv = λvtv = λ||v||2.

vtAv = vtAtv = (Av)tv = λvtv = λ||v||2.

Além disso, temos que

Portanto

λ = λ

37

Sejam agora λ1 e λ2 autovalores distintos reais de T e v1 e v2 seus autovalores

não-nulos correspondentes. Neste caso segue que

λ1(cid:104)v1, v2(cid:105) = (cid:104)λ1v1, v2(cid:105) = (cid:104)T v1, v2(cid:105)

= (cid:104)v1, T v2(cid:105) = (cid:104)v1, λ2v2(cid:105)

= (cid:104)v1, v2(cid:105)λ2

mas λ1 (cid:54)= λ2, provando que (cid:104)v1, v2(cid:105) = 0

(cid:4)

Observe particularmente que todo operador auto-adjunto possui pelo menos um au-

tovalor real e seu autovetor correspondente

Teorema 4.2.2. (Teorema espectral). Para todo operador auto-adjunto T : V −→ V

de um espaço vetorial de dimensão ﬁnita, munido de produto interno, existe uma base
ortonormal β = {v1, v2, . . . , vn} ∈ V constituida por autovetores de T .

Demonstração 4.2.2. Utilizemos indução sobre n = dimV . Para n=1, o teorema é

óbvio. Suponha que o Teorema é válido para dimV = n − 1. Se tivermos dimV = n

então existe um autovetor v unitário que é um subespaço de dimensão 1. Mas, como
dimV ⊥ = n − 1 a hipótese assegura que há uma base ortonormal {v1, v2, . . . , vn−1} ⊂
V ⊥. Fazendo vn = v
||v|| a base de V será {v1, v2, . . . , vn}. De fato, {v1, v2, . . . , vn} é
ortonormal e gera o subespaço V.

Exemplo 4.2.3. Provemos o Teorema anterior para o caso bidimensional.
Seja u e v uma base ortonormal de V, como a matriz do operador auto-adjunto T é

simétrica, vem:






a b

b

c






A =

O polinômio característico de T é:

p(λ) = λ2 − (a + c)λ + (ac − b2)

38

Temos que:

(a + c)2 − 4(ac − b2) = a2 + 2ac + c2 − 4ac + 4b2

= a2 − 2ac + c2 + 4b2

= (a − c)2 + 4b2 ≥ 0

Logo p(λ) admite apenas raízes reais.
Se p(λ) tiver uma raiz de duplicidade 2 então, u e v servem para comprovar o Teorema.
Se p(λ) tiver duas raízes distintas λ1 e λ2, teremos:

Mas

T v1 = λ1v1 e T v2 = λ2v2.

v2T v1 = v1T v2

v2λ1v1 = v1λ2v2

λ1v1v2 = λ2v1v2

(λ1 − λ2)v1v2 = 0.

Note que λ1 (cid:54)= λ2. Logo temos que

(cid:104)v1.v2(cid:105) = 0.

Assim v1 e v2 são ortogonais. Para obter a base ortonormal basta normalizar cada
vetor da base.

4.3 Diagonalização de Operadores

Sabe-se que dado um operador linear T : V −→ V , a cada base B de V, corresponde
uma matriz TB que representa T na base B. O objetivo aqui é encontrar uma base do
espaço vetorial V de tal modo que a matriz de T seja a mais simple possível, isto é, de

modo que esta matriz seja diagonal.

4.4 Diagonalização de uma matriz quadrada

Deﬁnição 4.4.1. Uma matriz quadrada A é dita diagonalizável se existir uma matriz
invertível P tal que P −1AP é uma matriz diagonal. Dizemos, então, que a matriz P
diagonaliza A.

39

Teorema 4.4.1. Se uma matriz A n×n é diagonalizável. Então, A possui n autovetores

linearmente independentes.

Demonstração 4.4.1. Supondo P = [P1, P2, . . . , Pn] formado por n autovetores LI,
isto é, todos os Pi são autovetores associados, respectivamente, a λ1, λ2, λ3, . . . , λn en-

tão a matriz D =


 é uma matriz diagonal.








λi se i = j

0

se i (cid:54)= j

Logo An×n possui n autovetores LI.

AP1 = λ1P1; AP2 = λ2P2; . . . ; APn = λnPn

(cid:20)

AP1 AP2

. . . APn

(cid:21)

(cid:20)

=

λ1P1 λ2P2

. . . λnPn

(cid:21)

(cid:20)

A

P1 P2

. . . Pn

(cid:21)

(cid:20)

=

P1 P2

. . . Pn

(cid:21)

.












λ1

0
...

0

AP = P D =⇒ D = P −1AP.

Portanto A é diagonalizável.

Reciprocamente se A é diagonalizável temos que

D = P −1AP ou P D = AP.

Então

(cid:20)

A

P1 P2

. . . Pn

(cid:21)

(cid:20)

=

P1 P2

. . . Pn

(cid:21)

.












λ1

0
...

0












0

λ2
...

. . .

. . .

· · ·

0

0

0

· · ·

· · · λn












0

λ2
...

. . .

. . .

· · ·

0

0

0

· · ·

· · · λn

Como P é invertível então as colunas de P =

P1 P2

. . . Pn

são autovetores

(cid:20)

(cid:21)

LI de A.

40

Teorema 4.4.2. Se A é ortogonalmente diagonalizável, isto é, P tAP = D, então A é
simétrica.

Demonstração 4.4.2. Basta usar o corolário 3.6.1.

Exemplo 4.4.1. Vamos diagonalizar a matriz

















2 1 1

1 2 1

1 1 2

Seu polinômio característico é p(r) = r3 − 6r2 + 9r − 4 = (r − 4).(r − 1)2. Dai:

r = 1

v1 =

v2 =





,












−1

0

1

















−1

1

0

r = 4

v3 =










1



1


1

Observe que os autovetores associados a r = 1 são ortogonais ao autovetor associado

a r = 4. Mas, não são ortogonais entre si. Neste caso aplicamos o processo de Gram-

Schmidt obtendo os seguintes vetores:

















−1

0

1

e

















− 1
2

1

− 1
2

Normalizando os vetores obtemos

P =









1√
3

1√
3

1√
3









.

− 1√
2

− 1√
6

0

1√
2

2√
6

− 1√
6

Assim veriﬁcamos diretamente que

















4 0 0

0 1 0

.

0 0 1

D = P tAP =

41

5 Aplicações

Muitos fenômenos que ocorrem na Física, na Química, na Biologia, na Engenharia e
na Economia, podem ser descritas por modelos matemáticos envolvendo equações di-

ferenciais, isto é, equações envolvendo funções e suas derivadas. Vamos aplicar o que

vimos até aqui na resolução de alguns tipos de sistemas de equações diferenciais simples.

Iniciemos lembrando que uma das equações diferenciais mais simples pode ser es-

crita da seguinte forma:

x(cid:48) = a.x

tal que x = f (t) é uma função a ser determinada, x(cid:48) = dx
constante real. As soluções desta equação diferencial são do tipo

dt é a sua derivada e a é uma

x = c.eat, t ∈ R

sendo c é uma constante qualquer. Com efeito, temos que

Logo

x = c.eat

x(cid:48) = a.c.eat

x(cid:48) = a.x; t ∈ R.

Quando o problema for de valor inicial, isto é, quando houver uma condição inicial

o mesmo deverá ser levando em conta na busca da solução preliminar sobre x.

Estamos pressupondo que todas as funções aqui apresentadas são contínuas em todo

intervalo [α, β], α, β ∈ R.

42

5.1 Sistemas de Equações diferenciais de Primeira Ordem com

Coeﬁcientes Constantes

Um sistema de equações diferenciais de primeira ordem com coeﬁcientes constantes é

da forma:






x(cid:48)
1 = a11x1 + a12x2 + · · · + a1nxn

x(cid:48)
2 = a21x1 + a22x2 + · · · + a2nxn
...

...

x(cid:48)
n = an1x1 + an2x2 + · · · + annxn

Note que x1 = f1(t), x2 = f2(t), . . . , xn = fn(t), são funções a determinar e os

coeﬁcientes aij são constantes reais.
Matricialmente podemos escrever












=























x(cid:48)
1

x(cid:48)
2
...

x(cid:48)
n

a11 a12

. . .

a1n

. . .

a21 a22
...
...

a2n
...

an1 an2

. . . ann












.












,












x1

x2
...

xn

ou de forma concisa

X (cid:48) = A.X.

Tomemos uma solução da forma X = v.eλt sendo v um vetor coluna n×1. Derivando

de ambos os lados, obtemos X (cid:48) = v.λ.eλt. Substituindo em X (cid:48) = A.X, vem:

Equivalentemente podemos escrever as seguintes equações:

X (cid:48) = A.X.

v.λ.eλt = A.v.eλt

A.v = λ.v

(A − λ.I).v = 0

Mas este é exatamente o modo de se encontrar os autovalores λ e seus autovetores

associados aos quais vimos na Deﬁnição 4.0.8.

43

5.2 As 4 possibilidades de um sistema linear 2 × 2.

5.2.1 Sistema com dois autovalores reais e distintos.

Exemplo 5.2.1. É o caso do seguinte sistema:






x(cid:48)
1 = x1 + x2

x(cid:48)
2 = 4x1 − 2x2

A matriz dos coeﬁcientes do sistema acima é:






A =




 .

1

1

4 −2

Calculemos seus autovalores:

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

λ − 1 −1

−4

λ + 2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= λ2 + λ − 6 = (λ + 3).(λ − 2).

Dai λ = 2 e λ = −3.

O autovetor associado a λ = 2 é tal que:






λ − 1 −1

−4

λ + 2










 .




x1

x2


 =





0

 .
0

Logo

x1 − x2 = 0 e − 4x1 + 4x2 = 0.

Como ambas são idênticas temos x1 = x2 e assim um autovetor associado será



1


 .

1

v1 =

De forma análoga, temos para λ = −3 o autovetor





v2 =





 .

1

−4

44

Desta forma a matriz P =



1


1



1 −4


, diagonaliza A, isto é:

D = P −1.A.P =


2



0




 .

0 −3

Façamos a seguinte substituição X = P.Y e X (cid:48) = P.Y (cid:48) em X (cid:48) = A.X obtendo as

seguintes igualdades:

ou

X (cid:48) = A.X

P.Y (cid:48) = A.P.Y

Y (cid:48) = P −1.A.P.Y

Y (cid:48) = D.Y






y(cid:48)
1 = 2y1

y(cid:48)
2 = −3y2

.

Mas este último como já vimos é fácil de resolver. As soluções são dadas pelas seguintes
equações:

y1 = c1.e2t e y2 = c2.e−3t, t ∈ R.

Voltando com estes valores em X = P.Y , obtemos








x1

x2


 =


1



1






 .









 =




1 −4

c2.e−3t

c1.e2t − 4.c2.e−3t

c1.e2t

c1.e2t + c2.e−3t




 .

Logo a solução geral é dada por

X = c1.







1
 .e2t + c2

1


 .e−3t; t ∈ R.







1

−4

Aqui c1 e c2 são constantes a serem determinadas com as condições iniciais.

45

5.2.2 Sistema com dois autovalores complexos.

Obviamente estes autovalores são distintos (pois um é o conjugado do outro), quando

as entradas da matriz são reais.





Exemplo 5.2.2. Resolva o sistema X (cid:48) = A.X para A =

Encontremos seus autovalores dados por

3 −2



5


 .

1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

λ − 3

2

−5

λ − 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= λ2 − 4λ + 13 = 0.

Lembrando que se um número complexo é raiz de um polinômio com coeﬁcientes reais
então seu conjugado também é raiz deste polinômio. Então λ1 = 2 + 3i e λ2 = 2 − 3i
são os únicos autovalores para matriz A. O autovetor associado a λ1 = 2 + 3i é:

















2 + 3i − 3

2

−1 + 3i

2





 .





 =




−5

2 + 3i − 1

−5

1 + 3i


 .





 .

x1

x2

Logo (−1 + 3i)x1 + 2x2 = 0 e −5x1 + (1 + 3i)x2 = 0. Então tomando x1 = 2, obtemos
x2 = 1 − 3i

Procedendo da mesma forma com λ2 = 2 − 3i, encontramos:





Observe que quando os autovalores são complexos os autovetores também são comple-

xos.

Um conjunto fundamental de soluções para o sistema inicial é:

x1(t) =






2

1 − 3i


 .e(2+3i)t,







2

1 + 3i


 .e(2−3i)t


x2(t) =

46

x1

x2






2

2




 .

v1 =

1 − 3i

v2 =





 .

1 + 3i

ou em termos gerais:

X(t) = c1.






2

1 − 3i


 .e(2+3i)t + c2.







2


 .e(2−3i)t, t ∈ R.


1 + 3i

Encontremos um conjunto de soluções reais para equação acima lembrando que

eθi = cosθ + isenθ, θ ∈ R. Inicialmente observe que

x1(t) =

=

1 − 3i

2

2











1 − 3i


 .e(2+3i)t =







2


 e2t.e3it


1 − 3i


 .e2t.[cos3t + isen3t]







x2(t) =

2


 .e(2−3i)t =







2


 .e2t.e−3it


1 + 3i

1 + 3i






=

2


 .e2t.(cos3t − isen3t)


1 + 3i

Encontremos a parte real e imaginária de x1 ou de x2:

x1(t) = e2t.















2cos(3t)

2sen(3t)


 + i




cos(3t) + 3sen(3t)

sen(3t) − 3cos(3t)











; t ∈ R.

Logo a solução geral do sistema é:

X(t) = e2t.






c1.










2cos(3t)

2sen(3t)


 + c2.




cos(3t) + 3sen(3t)

sen(3t) − 3cos(3t)











, t ∈ R.

Justiﬁcamos o que foi feito acima voltando à equação X (cid:48) = A.X. Seja os autovalores
complexos conjugados λ1 = α + βi e λ2 = α − βi. Então seus autovetores associados
associados serão, respectivamente, v1 e v2 = v1 satisfazendo

(A − λ1I)v1 = 0

47

e

(A − λ1I)v1 = 0.

Observe que A e I são reais e λ2 = λ1 e v2 = v1.
Podemos encontrar duas soluções reais para o sistema X (cid:48) = A.X correspondente aos
autovalores λ1 e λ2, utilizando as partes real e imaginária de x1(t) ou x2(t) como já
ﬁzemos. Tomemos pois x1(t) = (a + bi)e(α+βi)t. Assim:

x1(t) = (a + bi)eαt.eβit = x1(t) = (a + bi)eαt.(cosβt + isenβt).

Separando as partes real e imaginária, vem:

x1(t) = eαt[(acosβt − bsenβt) + i(asenβt + bcosβt)].

Escrevendo x1(t) = u(t) + iw(t) segue que uma solução do sistema inicial é:

u(t) = eαt(acosβt − bsenβt),

w(t) = eαt(asenβt + bcosβt).

Observe que a é a parte real de v1 e b é a parte imaginária de v1. Além disso não é
difícil provar que u(t) e w(t) são LI.

A solução geral do sistema inicial será:

X(t) = c1.eαt(acosβt − bsenβt) + c2.eαt(asenβt + bcosβt)

Novamente c1 e c2 são constantes a serem determinadas com as condições iniciais.

5.2.3 Sistema com um autovalor real repetido e dois autovetores associa-

dos.

Neste caso não há diﬁculdades pois a solução do problema ﬁca semelhante ao caso 1).

Exemplo 5.2.3. Seja o sistema:






x(cid:48)
1 = 2x1

x(cid:48)
2 = 2x2

A matriz dos coeﬁcientes do sistema acima é:




A =




2 0

0 2


 .

48

Calculemos seus autovalores:

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

λ − 2

0

0

λ − 2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= (λ − 2)2 = 0.

Dai λ = 2 é o único autovalor.

Os autovetores associados a λ = 2 satifazem a seguinte sistema:
















2 0

0 2


 .




x1

x2


 = 2.




x1

x2


 .

Logo






2x1 = 2x1

0x1 = 0

=⇒

2x2 = 2x2

0x2 = 0.

Portanto os autovetores são dados por

v1 = (1, 0)

v2 = (0, 1).

Como não houve vantagem nenhuma (a matriz já estava diagonalizada), não há motivos
para uma troca de variáveis. Assim uma solução do sistema será: x1 = c1.e2t e x2 =
c2.e2t; c1 e c2 ∈ R

ou de forma geral X = c1.



1


 .e2t + c2.

0



0
 .e2t; c1 e c2 ∈ R.



1

5.2.4 Sistema com um autovalor real repetido e um único autovetor asso-

ciado.

Consideremos a equação X (cid:48) = A.X e vamos supor que λ é um autovalor duplo de A,
mas existindo apenas um autovetor v associado. Uma solução é dada por:

tal que v satisfaz

x1(t) = v.eλt, t ∈ R.

(A − λI).v = 0.

49

Considerando dim{v ∈ R2; Av = λv} = 1 uma segunda solução pode ser encontrada
da seguinte forma

x2(t) = v.t.eλt + w.eλt, t ∈ R.

Aqui lembramos que v satisfaz (A − λI).v = 0 e w satisfaz (A − λI).w = v. (Pode-se

demonstrar que esta última equação tem sempre solução para w).
Para veriﬁcar que x2(t) = v.t.eλt + w.eλt é uma solução de X (cid:48) = A.X, façamos a
substituição

Logo obtemos as seguintes equações

X (cid:48) = A.X.

(v.t.eλt + w.eλt)(cid:48) = A.(v.t.eλt + w.eλt)

v.eλt + λ.v.t.eλt + λ.w.eλt = A.v.t.eλt + A.w.eλt

(A.v − λ.v).t.eλt + (A.w − λ.w − v).eλt = 0.

Para que está última equação seja válida devemos ter:

e

(A − λI).v = 0

(A − λ.I).w = v.

Logo basta resolvermos está equação adicional em relação a w para encontrarmos uma

segunda solução.








1

1

−1 3


. Encon-

Exemplo 5.2.4. Seja a equação diferencial X (cid:48) = A.X em que A =

tremos os autovalores e autovetores. Veja que






1 − λ

1

−1

3 − λ




 = λ2 − 4λ + 4 = 0 =⇒ λ1 = λ2 = 2.

Então os autovetores são dados pelo seguinte sistema:






−1 1

−1 1






 .




x1

x2




 = 0.

50

Assim temos −x1 + x2 = 0. Para x1 = 1 temos x2 = 1 e v =



1


 é o único

1

autovetor associado à λ = 2.

Uma solução é dada pela seguinte equação

x1(t) =


 .e2t, t ∈ R.







1

1

Tomando w =






w1

w2




, resolvamos a seguinte equação:









(A − 2.I).w = v =




−1 1

−1 1


 .




w1

w2


 =



1


 .

1

Este sistema pode ser reescrito da seguinte forma:






−w1 + w2 = 1

−w1 + w2 = 1.

Cuja solução é w1 = w2 − 1, fazendo w2 = 1, teremos w1 = 0. Logo a segunda solução
é:



1
 te2t +



1



0
 .e2t, t ∈ R.



1

x2 =

Logo a solução geral é dada por:

X(t) = c1.






1

1


 .e2t + c2.













 .te2t +







0

1

1

1


 .e2t



 , t ∈ R.


5.3 Sistema massa-mola simples

As aplicações são muitas em Física, Engenharia, Economia, Biologia, etc. É possível,

por este método, por exemplo, resolver um sistema com duas massas e três molas:

Se supusermos que não há forças externas, isto é, F1(t) = F2(t) = 0. Então as

coordenadas x1 e x2, serão:

m1

d2x1
dt2 = −(k1 + k2)x1 + k2x2

51

Figura 1: Um sistema massa-mola

d2x2
dt2 = k2x1 − (k2 + k3)x2
Que podem ser transformadas em um sistema de quatro equações de primeira ordem

m2

e resolvidas de acordo com o que já expusemos até aqui.

Façamos uma aplicação mais simples, escolhendo uma que envolve uma mola de

massa m e constante k de acordo com a equação diferencial:

my(cid:48)(cid:48) + ky = 0.

(∗)

Aqui y(t) é o deslocamento da massa no instante t a partir de sua posição de equilíbrio.
a) Sejam x1 = y e x2 = y(cid:48); mostremos que o sistema resultante é dado por:






x(cid:48) =




 x.

0

1

− k

m 0

b) Encontremos os autovalores da matriz para o sistema no ítem (a).
c) Vamos esboçar diversas trajetórias do sistema. Escolhamos uma dessas trajetórias e
construamos os gráﬁcos correspondentes de x1 e de x2 em função de t, em algum caso
particular.

Solução: a) Tomemos y = x1, y(cid:48) = x2. Assim:

1 = y(cid:48) = x2
x(cid:48)

2 = y(cid:48)(cid:48)
x(cid:48)

52

Substituindo na equação my(cid:48)(cid:48) + ky = 0, teremos as seguinte equações:

x(cid:48)
1 = x2

2 = − k
x(cid:48)(cid:48)

m x1.

Matricialmente:






x(cid:48)
1

x(cid:48)
2






 =




0


1

 .











x1

x2

− k

m 0

que é da forma X (cid:48) = A.X.
b) Encontremos os autovalores da matriz A. Inicialmente, o polinômio característico é

dado por








 = λ2 + k
m .

λ −1

k
m

λ

Então os autovalores são dados por

(cid:114)

λ1 = i

k
m

(cid:114)

e λ2 = −i

k
m

.

c) Um autovetor associado a λ1 é:

v1 =




 .



1
(cid:113) k
m



i





(Observe que v2 =




−i

1
(cid:113) k
m


 é o autovetor associado a λ2).

Assim uma solução para x1 é dada por

x1 = v1.ei

√

k

m .t =


 .ei




1
(cid:113) k
m



i

√

k
m .t

x1 =








1
(cid:113) k
m



i

(cid:34)

(cid:32)(cid:114)

cos

k
m

(cid:33)

(cid:32)(cid:114)

.t

+ i.sen

k
m

(cid:33)(cid:35)

.t

, t ∈ R.

53

Deﬁna w =

(cid:113) k
m :

x1 =









cos(w.t)

sen(w.t)





 + i.





 = u(t) + iw(t)

−wsen(w.t)

w.cos(w.t)

Neste caso a solução geral é dada por

X = c1.






cos(w.t)

−w.sen(w.t)






 + c2.




sen(w.t)

w.cos(w.t)


 , t ∈ R.


Observe que

x1(t) = c1.cos(w.t) + c2.sen(w.t), t ∈ R,

é a posição do bloco em relação à sua posição de equilíbrio. Além disso, temos que

x2(t) = −c1.w.sen(w.t) + c2.w.cos(w.t), t ∈ R, ou

x2(t) = c4.cos(w.t) − c3.sen(w.t), t ∈ R,

ao qual representa sua velocidade.

Assim encontramos a solução do problema (*) bem como descrevemos a posição e a
velocidade em cada instante t ∈ R.

54

Figura 2: y = 2cos(t) + 3sen(t)

y = 2cos(2t) − 3sen(2t)

55

Conclusão

Concluimos que é possível um problema, modelado por uma equação diferencial linear
com coeﬁcientes constantes, ser transformanda num sistema de equações diferenciais

lineares de primeira ordem e ser resolvida, utilizando de forma elegante, a álgebra

linear.

56

Referências

[1] Anton Howard e Chris Rorres. Álgebra linear com aplicações. Tradução Claus

Ivo Doering, Porto Alegre, RS, Bookman, 2001.

[2] David Poole. Álgebra linear. Tradutora Martha S. Monteiro · · ·

[et al.],São

Paulo,SP, Pioneira Thomson Learning, 2004.

[3] Rodney Josué Biezuner. Álgebra linear. Notas de aula, UFMG, MG, 2008.

[4] klaus Jänich. Álgebra linear. Rio de Janeiro, RJ, LTC, 1988.

[5] José Luis Boldrini · · · [et al.]. Álgebra linear. São Paulo, SP, Harper & Row

do Brasil, 1980.

[6] Elon Lages Lima. Álgebra linear. Rio de Janeiro, RJ, IMPA, 2011.

[7] Dennis G. Zill e Michael R. Cullen. Equações Diferenciais. tradução Alfredo

Alves de Faria, São Paulo, SP, MAKRON Books, 2001.

[8] William E. Boyce e Richard C. DiPrima. Equações diferenciais elementares
e problemas de valores de contorno. Tradução Valéria de M. Iório, Rio de Janeiro,

RJ, LTC, 2010.

57

