UNIVERSIDADE FEDERAL DE SERGIPE

PRÓ-REITORIA DE PÓS- GRADUAÇÃO E PESQUISA

PROGRAMA DE PÓS-GRADUAÇÃO EM MATEMÁTICA

MESTRADO PROFISSIONAL EM MATEMÁTICA

REDE NACIONAL - PROFMAT

Itálo Guimarães

Diagonalização de operadores com aplicações à sistemas de equações

diferenciais e identiﬁcação de cônicas

São Cristóvão - SE

2018

Itálo Guimarães

Diagonalização de operadores com aplicações à sistemas de equações

diferenciais e identiﬁcação de cônicas

Dissertação apresentada ao Programa de Pós -
Graduação em Matemática da Universidade Fe-
deral de Sergipe, como parte dos requisitos para
obtenção do título de Mestre em Matemática.

Orientador: Prof. Dr. Naldisson dos Santos

São Cristóvão - SE

2018

                                     FICHA CATALOGRÁFICA ELABORADA PELA BIBLIOTECA CENTRAL UNIVERSIDADE FEDERAL DE SERGIPE    G963d  Guimarães, Itálo      Diagonalização de operadores com aplicações a sistemas de equações diferenciais e identificação de cônicas / Itálo Guimarães ; orientador Naldisson dos Santos. - São Cristóvão, 2018.     74 f.; il.       Dissertação (Mestrado em Matemática) - Universidade Federal de Sergipe, 2018.  1.  Curvas.  2. Operadores lineares.  3. Equações diferenciais.    I. Santos, Naldisson dos orient.  lI. Título.  CDU 517.983   Agradecimentos

Ao meu Deus, pelo dom da vida, por me conceder saúde e sabedoria para enfrentar os

desaﬁos dessa caminhada.

Aos meus pais, Itamar e Vera, por tudo que me ensinaram, por todo o amor e cuidado

que sempre dedicaram a mim e por serem os meus exemplos de vida.

À minha esposa Bárbara, por todo amor, carinho, dedicação e por entender a minha

ausência em vários momentos. Sem você nada disso seria possível. Te amo!

Aos meus irmãos, Sávio e Verinha, que sempre manifestaram sua torcida e seu carinho

durante toda essa trajetória. Especialmente, a minha irmã Cláudia, por me receber em sua casa

por todo esse tempo de curso, me tendo como um dos seus ﬁlhos. Obrigado por tudo!

Aos professores do departamento de Matemática da UFS, agradeço a todos que foram

meus professores durante o curso: Prof. Dr. Almir, Prof. Dra. Giovana, Prof. Dr. André,

Prof. Dr. Zaqueu, Prof. Dr. Alysson, Prof. Dr. Bruno, Prof. Dr. Gérson e, especialmente,

ao meu orientador Prof. Dr. Naldisson, por ter acreditado nas minhas ideias, pelas sugestões,

contribuições e paciência durante este árduo período. Não esquecerei seus ensinamentos e sua

conﬁança. Meu muito obrigado a todos vocês!

Aos colegas do PROFMAT, pela amizade, companheirismo, e troca de experiências em

todos os momentos dessa caminhada.

Por ﬁm, agradeço àqueles que passaram por minha vida durante o mestrado e contribuí-

ram na realização de mais um sonho.

Resumo

A presente dissertação tem como objetivo discorrer sobre diagonalização de operadores

lineares, de modo que possamos explorar esses conceitos na solução de sistemas de equações

diferenciais ordinárias e na identiﬁcação de cônicas. Um operador linear em um espaço veto-

rial de dimensão ﬁnita, pode ser representado por uma matriz. Sendo as matrizes diagonais as

mais simples do ponto de vista das operações matriciais, mostraremos sob que condições, dado

um operador linear é possível representá-lo por uma matriz diagonal. Dessa forma, este tra-

balho apresenta o processo de diagonalização de operadores, introduz conceitos básicos sobre

sistemas de equações diferenciais ordinárias e aplicações.

Palavras-chave: Cônicas; Diagonalização de operadores; Sistemas de equações dife-

renciais.

Abstract

The present dissertation aims to discuss diagonalization of linear operators, so that we

can explore these concepts in the solution of systems of ordinary differential equations and in

the identiﬁcation of conics. A linear operator in a vector space of ﬁnite dimension can be re-

presented by a matrix. Since diagonal arrays are the simplest from the point of view of matrix

operations, we will show under what conditions, given a linear operator it is possible to repre-

sent it by a diagonal matrix. Thus, this paper presents the process of operator diagonalization,

introduces basic concepts about systems of ordinary differential equations and applications.

Keywords: Conical; Diagonalization of operators; Systems of differential equations.

Lista de Figuras

3.1 Leonhard Euler

.

. .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

41

4.1 Tanques com galões de salmoura . . . . . . . . . . . . . . . . . . . . . . . . .

4.2 Circuito elétrico .

. .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.3 Elipse .

. .

. .

. .

. .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.4 Parábola .

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

63

65

69

73

Sumário

Introdução

1 Preliminares

1.1 Resultados Básicos de Álgebra Linear

. . . . . . . . . . . . . . . . . . . . . .

1.1.1 Matrizes .

. .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.1.2

Sistemas de Equações Lineares

. . . . . . . . . . . . . . . . . . . . .

1.1.3 Espaços Vetoriais . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.1.4 Transformações Lineares . . . . . . . . . . . . . . . . . . . . . . . . .

1.2 O Teorema do Ponto Fixo de Banach . . . . . . . . . . . . . . . . . . . . . . .

2 Diagonalização de Operadores
2.1 Autovalores e Autovetores

. . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.2 Polinômio Característico . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.3 Processo de Diagonalização . . . . . . . . . . . . . . . . . . . . . . . . . . .

3 Equações Diferenciais

3.1 Um Breve Histórico .

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.2 Equações Diferenciais Ordinárias . . . . . . . . . . . . . . . . . . . . . . . . .

3.3 Sistemas de Equações Diferenciais de Primeira ordem . . . . . . . . . . . . . .

4 Aplicações

4.1 Solução de Sistemas de Equações Diferenciais por Diagonalização . . . . . . .

9

11
11

11

14

16

20

23

27
29

31

33

40
40

41

44

56
56

4.1.1

4.1.2

4.1.3

Sistemas Lineares Homogêneos . . . . . . . . . . . . . . . . . . . . .

Sistemas Lineares Não Homogêneos . . . . . . . . . . . . . . . . . . .

Sistemas de Equações Diferenciais como Modelos Matemáticos . . . .

4.2

Identiﬁcação de Cônicas

. . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Referências

.

. .

. .

. .

. .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

56

61

63

67

73

Introdução

Este trabalho tem como objetivo discorrer sobre Diagonalização de Operadores Linea-

res, assim como sua aplicação em algumas áreas, especialmente na solução de problemas que

envolvem Equações e Sistemas de Equações Diferencias.

Um operador linear em um espaço vetorial de dimensão ﬁnita, pode ser representado

por uma matriz. Sendo as matrizes diagonais as mais simples do ponto de vista das operações

matriciais, nos perguntamos se dado um operador linear, é sempre possível representá-lo por

uma matriz diagonal? Mais precisamente, queremos saber se para todo operador existe uma

base do espaço vetorial tal que a matriz do operador na base seja uma matriz diagonal.

Como todo conhecimento cientíﬁco, as ideias matemáticas passam por um processo

evolutivo incorporando mudanças, sendo tratadas com novas ferramentas e métodos os quais,

muitas vezes, lhes permitem um incremento no seu desenvolvimento. Uma seção cônica é uma

curva cuja equação cartesiana é do segundo grau, e inversamente, toda curva cuja equação é

do segundo grau pode ser obtida a partir da interseção de um cone circular reto de duas folhas

com um plano. Por essa razão, as curvas cujas equações são do segundo grau são chamadas

de seções cônicas, ou simplesmente de cônicas. Exposições gerais sobre as seções cônicas

são conhecidas antes da época de Euclides, e existe uma diversidade de deﬁnições para elas,

cuja equivalência é mostrada na Geometria Elementar. As cônicas desempenham um papel

importante em vários domínios da física, incluindo a astronomia, a economia, a engenharia e

em muitas outras situações, pelo que não é de estranhar que o interesse pelo seu estudo seja tão

antigo. Neste trabalho, veremos como a diagonalização de matrizes simétricas pode ser usada

na identiﬁcação de cônicas.

9

10

O trabalho está dividido em quatro capítulos. No primeiro capítulo, traremos algumas

deﬁnições e resultados necessários para o desenvolvimento do texto, mais claramente, falamos

sobre Matrizes, Sistemas lineares, Espaços Vetoriais, Transformações Lineares o sobre o Teo-

rema do ponto ﬁxo de Banach.

No segundo capítulo, iniciamos com uma breve motivação para o estudo do processo

de diagonalização e, em seguida, tratamos das importantes noções de autovalor e autovetor

de um operador linear, noções estas centrais na teoria das equações diferenciais ordinárias,

de polinômio característico e, encerramos, com uma versão de um teorema chamado Teorema

Espectral para operadores simétricos, que garante que todo operador simétrico é diagonalizável.

No terceiro capítulo, faremos um breve histórico sobre o estudo das equações diferen-

ciais, também apresentaremos algumas deﬁnições sobre as equações diferenciais ordinárias, e

principalmente, falaremos sobre sistemas de equações diferenciais de primeira ordem.

Já no quarto e último capítulo, traremos um método alternativo de resolução de sistemas

de equações diferencias através da diagonalização, também são resolvidos alguns problemas de

aplicação utilizando os sistemas de equações diferenciais como modelos matemáticos e ﬁnali-

zamos falando sobre identiﬁcação de cônicas, também utilizando diagonalização.

CAPÍTULO 1

Preliminares

Apresentaremos neste capítulo algumas deﬁnições e resultados necessários para o de-

senvolvimento deste trabalho.

1.1 Resultados Básicos de Álgebra Linear

1.1.1 Matrizes

As matrizes são ferramentas básicas, pois além de fornecerem meios para a resolução

de sistemas lineares, também representarão as transformações lineares entre espaços vetoriais.

Deﬁnição 1.1.1. Uma matriz m × n A sobre o corpo dos números reais R é uma tabela retan-
gular com m linhas e n colunas da forma



A =



a11

a12

a21
a22
...
...
am1 am2

a1n

···
···
a2n
...
...
··· amn

onde aij,∈ R, i = 1, ..., m e j = 1, ..., n. Usamos, também, a notação A = [aij]m×n.

O símbolo aij signiﬁca o elemento da matriz A que está na i-ésima linha e j-ésima coluna

11

CAPÍTULO 1. PRELIMINARES

12

e será chamado de entrada da matriz A. O conjunto de todas as matrizes A do tipo m × n será
denotado por M (m, n). Uma matriz A ∈ M (m, n) é chamada de matriz quadrada se m = n.

Dizemos que uma matriz quadrada A é uma matriz diagonal se

aij = 0, i (cid:54)= j.

Em particular, dizemos que a matriz diagonal A é uma matriz identidade se

 1 se i = j

0 se i (cid:54)= j

aij =

,

e será denotada por In. A matriz A = [aij] ∈ M (m, n) com aij = 0, 1 ≤ i ≤ m e 1 ≤ j ≤ n,
é chamada de matriz nula e será denotada por 0.

Dada uma matriz A = [aij]m×n, chamamos de transposta de A, e denotamos por At, a

matriz [bij]n×m, onde

para todo 1 ≤ i ≤ n e para todo 1 ≤ j ≤ m.

bij = aji,

Deﬁnição 1.1.2. A soma de duas matrizes A = [aij]m×n e B = [bij]m×n é a matriz C = [cij]m×n
tal que

Observação 1.1.1. A diferença A − B de duas matrizes do tipo m × n é a matriz C tal que

cij = aij + bij.

cij = aij − bij.

Exemplo 1.1.1. Uma matriz quadrada A se diz simétrica se At = A e anti-simétrica se At =
−A. Mostrar que a soma de duas matrizes simétricas é também simétrica. Mostrar que o mesmo
vale para matrizes anti-simétricas.
Solução. Sejam A e B as matrizes. Então (A+B)t = At+Bt = A+B. Logo A+B é simétrica.
Analogamente, se A e B são anti-simétricas, (A + B)t = At + Bt = −A + (−B) = −(A + B).

Deﬁnição 1.1.3. O produto de uma matriz A = [aij]m×n por um número real α, denotado por
αA é a matriz obtida multiplicando cada elemento de A por α, e é,

αA = [αaij]m×n.

CAPÍTULO 1. PRELIMINARES

13

Deﬁnição 1.1.4. O produto de duas matrizes está deﬁnido da seguinte forma. Sejam as matrizes
A = [aij]m×r e B = [bij]r×n. Então AB = C, onde C = [cij]m×n é uma matriz e cada elemento
cij é dado por

n(cid:88)

cij =

aikbkj.

Observação 1.1.2. A multiplicação de matrizes não é comutativa, pois existem matrizes A e B
tais que AB (cid:54)= BA.

k=1

Exemplo 1.1.2. Para cada número real α consideremos a matriz:

Mostrar que TαTβ = Tα+β.
Solução.

TαTβ =

sin α

cos α

Tα =

 cos α − sin α

 .
 ⇒
 cos β − sin β
 cos α − sin α
 ⇒
 cos(α + β) − sin(α + β)

cos α

sin β

cos β

sin α

TαTβ =

sin(α + β)

cos(α + β)

TαTβ = Tα+β.

Em matemática, determinante é uma função matricial que associa a cada matriz qua-

drada um escalar; ela transforma essa matriz em um número real. Esta função permite saber

se a matriz tem ou não inversa. A seguir mostraremos como encontar o determinante de uma

matriz quadrada.

Deﬁnição 1.1.5. Seja A = [aij] uma matriz quadrada:
(i) do tipo 1 × 1. Deﬁnimos o determinante da matriz A, sendo det A = a11.
(ii) do tipo 2 × 2. Deﬁnimos o determinante de A, sendo det A = a11a22 − a12a21.
(iii) do tipo n× n. O ij-ésimo menor de A é o determinante da submatriz Mij do tipo (n− 1)×
(n − 1) obtida quando suprimimos a i-ésima linha e a j-ésima coluna de A. O ij-ésimo
cofator Cij de A é deﬁnido como Cij = (−1)i+j det Mij. Deﬁnimos o determinante da
matriz A, sendo det A = a11C11 + a12C12 + ... + a1nC1n.

CAPÍTULO 1. PRELIMINARES

14

Deﬁnição 1.1.6. Uma matriz quadrada A é dita não-singular ou invertível se existe uma outra
matriz B do mesmo tipo n × n tal que AB = BA = In e denota-se B = A−1. Caso contrário
diz-se que A é uma matriz singular ou não-invertível.

Existem várias maneiras de se encontrar a inversa A−1 de uma matriz A. Supondo que
a matriz A possui inversa, um método que envolve o uso de determinantes é associar a cada
elemento aij com o determinante da matriz Mij que é obtida através da eliminação da linha e
da coluna onde aij se encontra. Além disso, associa-se aij com o cofator Cij.

Se B = [bij], tal que B = A−1, então podemos deﬁnir

bij =

Cji
det A

.

1.1.2 Sistemas de Equações Lineares

Desde a antiguidade, em diversas áreas do conhecimento, muitos problemas são mode-

lados matematicamente por sistemas de equações lineares.

Deﬁnição 1.1.7. Um sistema de equações lineares com m equações e n incognitas é um con-
junto de equações da forma:



a11x1 + a12x2 + ... + a1nxn = b1

a21x1 + a22x2 + ... + a2nxn = b2

...

,

am1x1 + am2x2 + ... + amnxn = bm

onde aij, bi ∈ R, i = 1, ..., m e j = 1, ..., n.

Uma solução do sistema de equações lineares é uma n-upla

Y = (y1, y2, ..., yn)

que satisfaz cada uma das m equações.

Observação 1.1.3. Se b1 = b2 = ... = bm = 0, dizemos que o sistema de equações lineares
é um sistema homogêneo. Note que a n-upla (0, 0, ..., 0) é sempre uma solução do sistema
homogêneo.

CAPÍTULO 1. PRELIMINARES

15

O sistema de equações lineares pode ser escrito sob a forma matricial

AX = B



A =

onde

é a matriz dos coeﬁcientes,

é a matriz das incógnitas e



a11

a12

a21
a22
...
...
am1 am2

a1n

···
···
a2n
...
...
··· amn







x1

x2
...
xn

b1

b2
...
bm

X =

B =

é a matriz dos termos independentes.

Se a matriz A for uma matriz quadrada do tipo n × n e invertível, ou seja, se det A (cid:54)= 0

e assim existe A−1 que é a inversa de A, temos que

AX = B ⇒

A−1AX = A−1B ⇒
InX = A−1B ⇒

X = A−1B.

Isto signiﬁca que se det A (cid:54)= 0 o sistema de equações terá uma única solução. Caso

tenhamos det A = 0 temos que A−1 não existe e assim a igualdade X = A−1B não é válida.

CAPÍTULO 1. PRELIMINARES

16

Exemplo 1.1.3. Resolver o sistema de equações lineares

Solução. Note que a matriz dos coeﬁcientes

possui det A = 3 · (−2) − 1 · 1 = −6 − 1 = −7 (cid:54)= 0. Logo, A possui inversa. Note também,
que sua inversa é dada por

Portanto, usando a equação X = A−1B, onde

A =

1
1 −2

5
x − 2y = −3

 3x + y =

 3
 2
 .
 5

 5

 2

7

A−1 =

1
7
−3
7

−3

B =

7

1
7

é a matriz dos termos independentes, temos que a solução do sistema é dada por

X = A−1B =

1
7
−3
7

1
7

 =

 1

2

 .

−3

1.1.3 Espaços Vetoriais

Os espaços vetoriais são conjuntos com uma estrutura algébrica especíﬁca, seus ele-

mentos podem ser somados e multiplicados por elementos de um corpo, estes elementos serão

chamados de vetores.
Deﬁnição 1.1.8. Um espaço vetorial sobre o corpo R é um conjunto não-vazio V munido com
duas operações: adição

+ : V × V −→ V

e multiplicação por escalar

(u, v)

(cid:55)−→ u + v

· : R × V −→ V
(cid:55)−→ au

(a, u)

CAPÍTULO 1. PRELIMINARES

17

tal que as seguintes propriedades valem:

1. u + (v + w) = (u + v) + w, para todos u, v, w ∈ V .

2. Existe 0 ∈ V tal que u + 0 = u, para todo u ∈ V .

3. Para cada u ∈ V , existe −u ∈ V tal que u + (−u) = 0.

4. u + v = v + u, para todos u, v ∈ V .

5. a(bu) = (ab)u, para todos a, b ∈ R e u ∈ V .

6. (a + b)u = au + bu, para todos a, b ∈ R e u ∈ V .

7. a(u + v) = au + av, para todos u, v ∈ V e a ∈ R.

8. 1 · u = u, para todo u ∈ V .

Deﬁnição 1.1.9. Sejam V um espaço vetorial sobre R e W um subconjunto de V . Dizemos que
W é um subespaço (vetorial) de V se as seguintes condições são satisfeitas:

1. W (cid:54)= ∅.

2. u + v ∈ W , para todos u, v ∈ W .

3. au ∈ W , para todo a ∈ R e u ∈ W .

Exemplo 1.1.4. Sejam V = M (n, n) e

W = {A ∈ V ; At = A}

o conjunto das matrizes simétricas. Então W é um subespaço de V.
Solução. É claro que W (cid:54)= ∅, pois

0t = 0 ⇒ 0 ∈ W.

Dados A, B ∈ W e a ∈ R. Como A, B ∈ W temos que At = A e Bt = B. Logo,

(A + B)t = At + Bt = A + B ⇒ A + B ∈ W

CAPÍTULO 1. PRELIMINARES

18

e

(aA)t = aAt = aA ⇒ aA ∈ W.

Portanto, W é um subespaço de V .

Deﬁnição 1.1.10. Seja V um espaço vetorial sobre R. Um vetor u em V é uma combinação
linear dos vetores u1, u2, ..., un em V se existirem escalares α1, α2, ..., αn ∈ R tais que

u = α1u1 + α2u2 + ... + αnun.

Além disso, sendo V um espaço vetorial sobre R e u1, u2, ..., un ∈ V , dizemos que os
vetores u1, u2, ..., un são linearmente dependentes (LD) se existirem escalares α1, α2, ..., αn ∈
R não todos iguais a 0, tais que

α1u1 + α2u2 + ... + αnun = 0.

Ou, equivalentemente, a equação vetorial acima admite uma solução não-nula. Caso contrário,
dizemos que os vetores u1, u2, ..., un são linearmente independentes (LI), ou, equivalentemente,
a equação vetorial admite apenas a solução nula.

Teorema 1.1.1. Sejam V um espaço vetorial sobre R e u1, u2, ..., un vetores ﬁxados em V .
Então o conjunto

W = {α1u1 + α2u2 + ... + αnun; α1, α2, ..., αn ∈ R}

é um subespaço de V .
Demonstração. Note que W (cid:54)= ∅, pois

0 = 0u1 + 0u2 + ... + 0un ∈ W

Dados u, v ∈ W e a ∈ R. Como u, v ∈ W temos que existem

tais que

x1, ..., xn, y1, ..., yn ∈ R

u = x1u1 + ... + xnun

CAPÍTULO 1. PRELIMINARES

19

e

Logo,

v = y1u1 + ... + ynun.

u + v = (x1u1 + ... + xnun) + (y1u1 + ... + ynun) = (x1 + y1)u1 + ... + (xn + yn)un ∈ W

e

au = a(x1u1 + ... + xnun) = (ax1)u1 + ... + (axn)un ∈ W

Portanto, W é um subespaço de V .

(cid:4)

O subespaço

W = {α1u1 + α2u2 + ... + αnun; α1, α2, ..., αn ∈ R}

de V é chamado o subespaço gerado por u1, u2, ..., un.

Mais geralmente, seja β um subconjunto não-vazio de V . Então

W = { k(cid:88)

αiui; αi ∈ R , ui ∈ β e k ∈ N}

i=1

é o subespaço de V gerado por β, onde β é o conjunto de geradores de V .

Deﬁnição 1.1.11. Um subconjunto β de um espaço vetorial V é uma base de V , se

1. β é um conjunto de geradores de V e

2. β é um conjunto linearmente independente.

Dizemos que um espaço vetorial V tem dimensão ﬁnita se ele tem uma base consistindo
de um número ﬁnito de vetores. O número de elementos de uma de suas bases é chamado de
dimensão de V . Quando um espaço não tem dimensão ﬁnita, dizemos que ele tem dimensão
inﬁnita.
Exemplo 1.1.5. Seja n ∈ N. Para cada 1 ≤ i ≤ n, denotemos por ei o vetor

(δi1, δi2, ..., δin) = (0, ..., 0, 1, 0, ..., 0)

CAPÍTULO 1. PRELIMINARES

20

em Rn, onde a componente 1 encontra-se na i-ésima posição. O conjunto α = {e1, e2, ..., en} é
linearmente independente, pois a equação

k1e1 + k2e2 + ... + knen = 0

é satisfeita somente de k1 = k2 = ... = kn = 0. Além disto, este conjunto também gera Rn,
pois qualquer vetor v = (a1, a2, ..., an) em Rn pode ser escrito como

v = a1e1 + a2e2 + ... + anen.

Assim, α, com a ordenação dada pelos índices dos ei é uma base do Rn, chamada base canônica
de Rn.

1.1.4 Transformações Lineares

Lembramos que uma função f de um conjunto A em um conjunto B, f : A → B, é
uma regra que associa a cada elemento do conjunto A, um único elemento do conjunto B. O
conjunto A é chamado domínio e o conjunto B é chamado contradomínio. O subconjunto de
B formado por elementos b ∈ B tais que f (a) = b para algum a ∈ A é chamado (conjunto)
imagem de f. Para todo elemento a ∈ A, f (a) é chamado a imagem de a por f.

Deﬁnição 1.1.12. Sejam V e W espaços vetoriais. Uma transformação linear de V em W é
uma função T : V → W que possui as seguintes propriedades:

1. T (v1 + v2) = T (v1) + T (v2), para quaisquer v1 e v2 em V ;

2. T (av) = aT (v), para quaisquer v em V e a em R.

Observação 1.1.4. As duas propriedades anteriores são equivalentes à seguinte propriedade:

T (v1 + av2) = T (v1) + aT (v2),

para quaisquer v1 e v2 em V e a em R.

Quando a transformação linear for de um espaço vetorial V nele mesmo, ela será cha-

mada de operador em V .

CAPÍTULO 1. PRELIMINARES

21

Exemplo 1.1.6. Seja f (x) um polinômio arbitrariamente ﬁxado em R[x]. A função T : R[x] →
R[x], dada por T (p(x)) = p(f (x)), é uma transformação linear.

De fato, se p1(x), p2(x) ∈ R[x] e a ∈ R, temos que

T (p1(x) + ap2(x)) = p1(f (x)) + ap2(f (x)) = T (p1(x)) + aT (p2(x)),

mostrando que T é uma transformação linear.

Deﬁnição 1.1.13. Seja T : V → W uma transformação linear. O núcleo de T, denotado por
N (T ), é deﬁnido pelo conjunto

N (T ) = {v ∈ V ; T (v) = 0}.

E, a imagem de T é o conjunto

Im(T ) = T (V ).

O estudo de transformações lineares em espaços vetoriais de dimensão ﬁnita pode ser

reduzido ao estudo de matrizes. Para tal, considere V , W espaços vetoriais de dimensão ﬁnita
sobre R e

α = {u1, ..., un}, β = {w1, ..., wm}

bases de V e W , respectivamente. Seja T : V → W uma transformação linear. Então

T (u1), ..., T (un) ∈ W.

Como β é uma base de W temos que existem únicos aij ∈ R tais que

m(cid:88)

i=1

 a11

...
am1

[T ]α

β =

 .

···
a1n
...
...
··· amn

T (uj) =

aijwi, j = 1, ..., n.

A transposta da matriz dos coeﬁcientes deste sistema será chamada a representação matricial
de T em relação as bases α e β e denotada por

CAPÍTULO 1. PRELIMINARES

22

Agora, deﬁniremos um importante operador em espaços vetoriais com produto interno.

Mais precisamente, mostraremos a existência do operador adjunto de um operador linear e, a

partir deste, a noção de operador simétrico.

Deﬁnição 1.1.14. Um produto interno em um espaço vetorial V é uma função que a cada par
de vetores u e v em V associa o número real, denotado por (cid:104)u, v(cid:105), que satizfaz as seguintes
propriedades:

Para quaisquer vetores u, v e w de V e qualquer número real k,

1. (cid:104)v, v(cid:105) ≥ 0;
2. (cid:104)v, v(cid:105) = 0 se, e somente se v = 0;
3. (cid:104)u, v(cid:105) = (cid:104)v, u(cid:105);
4. (cid:104)u + v, w(cid:105) = (cid:104)u, w(cid:105) + (cid:104)v, w(cid:105);
5. (cid:104)ku, v(cid:105) = k(cid:104)u, v(cid:105).

Observação 1.1.5. A norma do vetor v de V , denotada por (cid:107)v(cid:107), é o número real

(cid:107)v(cid:107) = (cid:104)v, v(cid:105) 1
2 .

Se (cid:107)v(cid:107) = 1, dizemos que v é um vetor unitário.
Deﬁnição 1.1.15. Dois vetores u e v em V são ortogonais quando (cid:104)u, v(cid:105) = 0.

Deﬁnição 1.1.16. Um conjunto de vetores em V é chamado conjunto ortogonal se quaisquer
dois vetores distintos do conjunto são ortogonais. Um conjunto ortogonal no qual cada vetor é

unitário é chamado conjunto ortonormal.
Teorema 1.1.2. Dado um operador linear T em V , existe um único operador linear T ∗ em V
tal que

(cid:104)T (v), w(cid:105) = (cid:104)v, T ∗(w)(cid:105),

para quaisquer v, w ∈ V .
Demonstração. Tome w ∈ V . Como a função deﬁnida por v (cid:55)→ (cid:104)T (v), w(cid:105) é linear em V segue
que existe um único vetor w(cid:48) ∈ V tal que

(cid:104)T (v), w(cid:105) = (cid:104)v, w(cid:48)(cid:105),

CAPÍTULO 1. PRELIMINARES

23

para todo v ∈ V . Basta deﬁnir T ∗(w) = w(cid:48) e sendo {v1, ..., vn} uma base ortonormal de V ,
então

T ∗(w) = w(cid:48) = (cid:104)T (v1), w(cid:105)v1 + ... + (cid:104)T (vn), w(cid:105)vn.

Daí, vê-se que T ∗ é linear.

(cid:4)

O operador T ∗ é chamado de operador adjunto de T . Podemos obter T ∗ a partir de uma
representação matricial de T , tendo em vista que para toda base ortonormal α de V , temos que

[T ∗]α

α = ([T ]α

α)t.

Deﬁnição 1.1.17. Um operador linear T : V → V é dito ser um operador simétrico quando
T ∗ = T .

Assim, T : V → V é simétrico se, e somente se, [T ]α

α é uma matriz simétrica.

Exemplo 1.1.7. Seja T : R3 → R3 o operador linear deﬁnido por T (x, y, z) = (2x − y +
z,−x + y + 3z, x + 3y). Veriﬁcar se T é um operador simétrico.
Solução. Se α é a base canônica de R3, então

 2 −1 1

1

−1
1



[T ]α

α =

3

0

3

é uma matriz simétrica e, portanto, T é um operador simétrico.

1.2 O Teorema do Ponto Fixo de Banach

Deﬁnição 1.2.1. Uma métrica num conjunto M é uma função d : M × M → R, que associa a
cada par ordenado de elementos x, y ∈ M um número real d(x, y), chamado a distância de x
a y, de modo que sejam satisfeitas as seguintes condições para quaisquer x, y, z ∈ M:

1. d(x, x) = 0;

2. se x (cid:54)= y então d(x, y) > 0;

CAPÍTULO 1. PRELIMINARES

24

3. d(x, y) = d(y, x);
4. d(x, z) ≤ d(x, y) + d(y, z).

Deﬁnição 1.2.2. Um espaço métrico é um par (M, d), onde M é um conjunto e d é uma métrica
em M.

Diremos, salvo quando houver possibilidade de dúvida, simplesmente "o espaço métrico

M", deixando subentendida qual a métrica d que está sendo considerada. Os elementos de um

espaço métrico podem ser de natureza bastante arbitrária: números, pontos, vetores, matrizes,

funções, conjuntos, etc. Mas é comum chamarmos sempre os pontos de M.

Deﬁnição 1.2.3. Uma sequência (xn) num espaço métrico M chama-se sequência de Cauchy
quando, para todo ε > 0 dado, existe n0 ∈ N tal que m, n > n0 ⇒ d(xm, xn) < ε.

Deﬁnição 1.2.4. Diz-se que o espaço métrico M é completo quando toda sequência de Cauchy
em M é convergente.
Deﬁnição 1.2.5. Seja (X, d) um espaço métrico. Um ponto ﬁxo de uma aplicação f : X → X
é um ponto x ∈ X tal que f (x) = x.
Deﬁnição 1.2.6. Sejam X, Y espaços métricos. Uma aplicação f : X → Y chama-se uma
contração quando existe uma constante c, com 0 < c < 1, tal que

d(f (x), f (y)) ≤ c.d(x, y),

para quaisquer x, y ∈ X.

Teorema 1.2.1 (Teorema de Banach). Se X é um espaço métrico completo, toda contração
f : X → X possui um único ponto ﬁxo em X. Mais precisamente, se escolhermos um ponto
qualquer x0 ∈ X e pusermos

x1 = f (x0), x2 = f (x1), . . . , xn+1 = f (xn), . . . ,

a sequência (xn) converge em X e a = lim xn é o único ponto ﬁxo de f.

Demonstração. Provemos inicialmente a unicidade. Se f (a) = a e f (b) = b, como f é uma
contração, temos

d(a, b) = d(f (a), f (b)) ≤ c.d(a, b),

CAPÍTULO 1. PRELIMINARES

25

ou seja,

(1 − c)d(a, b) ≤ 0.

Como 1 − c > 0, concluímos que d(a, b) = 0, isto é, a = b. Provemos agora a existência, para
isso, provemos que (xn) é uma sequência de Cauchy em X. Ora

d(x1, x2) = d(f (x0), f (x1)) ≤ c.d(x0, x1),

d(x2, x3) = d(f (x1), f (x2)) ≤ c.d(x1, x2) ≤ c2.d(x0, x1),

e, por recorrência, temos

d(xn, xn+1) ≤ cn.d(x0, x1), ∀n ∈ N.

Então para n, p ∈ N quaisquer, segue que

d(xn, xn+p) ≤ d(xn, xn+1) + d(xn+1, xn+2) + ··· + d(xn+p−1, xn+p)

≤ [cn(1 + c + ··· + cp−1)].d(x0, x1)
≤ cn
1 − c

.d(x0, x1).

Calculando o limite quando n → ∞, obtemos

lim
n→∞ d(xn, xn+p) = 0,

concluindo que (xn) é uma sequência de Cauchy em X. Logo existe a ∈ X tal que lim
Provemos que a é ponto Fixo de f. De fato, como f é contínua, temos

n→∞ xn = a.

f (a) = f ( lim

n→∞ xn)
n→∞ f (xn)
n→∞ xn+1

= lim

= lim

= a,

o que conclui a demonstração.

(cid:4)

CAPÍTULO 1. PRELIMINARES

26

Corolário 1.2.1. Seja X um espaço métrico completo. Se F : X → X é contínua e, para
algum m, F m é uma contração, então existe um único ponto p ﬁxo por F . Mais ainda, p é um
atrator de F , isto é, F m(x) → p quando n → ∞, onde F m(x) é deﬁnido por F (F m−1(x)).

CAPÍTULO 2

Diagonalização de Operadores

Motivação. Vamos considerar o problema de encontrar as funções que dão a evolução das
populações de duas espécies, S1 e S2, convivendo em um mesmo ecossistema no tempo t >
0. Vamos denotar as populações das espécies S1 e S2 em um instante t por x1(t) e x2(t),
respectivamente. Inicialmente vamos supor que a taxa de crescimento da população de uma

espécie não depende do que ocorre com a outra espécie e que esta taxa é proporcional a sua

população existente (ou equivalentemente que a taxa de crescimento relativa é constante), ou

seja, vamos supor que

dx1
dt

dx2
dt

(t) = ax1(t)

(t) = dx2(t)

em que a, d ∈ R. Temos aqui um sistema de equações diferenciais, ou seja, um sistema de
equações que envolvem derivadas das funções que são incógnitas. Neste caso as duas equações

são desacopladas, isto é, podem ser resolvidas independentemente. A solução do sistema é
x1(t) = x1(0)eat e x2(t) = x2(0)edt, para t ≥ 0.

Vamos supor, agora, que as duas populações interagem de forma que a taxa de cresci-

mento da população de uma espécie depende de forma linear não somente da sua população

27

CAPÍTULO 2. DIAGONALIZAÇÃO DE OPERADORES

28

existente, mas também da população existente da outra espécie. Ou seja, vamos supor que

dx1
dt

dx2
dt

(t) = ax1(t) + bx2(t)

(t) = cx1(t) + dx2(t).

Por exemplo, se os individuos de uma espécie competem com os da outra por alimento (a, d >
0) e (b, c < 0), ou os indivíduos da espécie S1 são predadores dos da outra (a, b, d > 0 e c < 0).
Neste caso a solução de uma equação depende da outra. Podemos escrever este sistema na

forma de uma equação diferencial matricial

em que X(cid:48)(t) =

1(t)
x(cid:48)
2(t)
matrizes P e D tais que

c d

X(cid:48)(t) = AX(t),

 x1(t)

x2(t)

 a b

. Vamos supor que existam

 e X(t) =

, A =
 . Substituindo-se A = P DP −1 em X(cid:48)(t) = AX(t) obtemos

A = P DP −1,

 x(cid:48)
 λ1

0

0 λ2

em que D =

X(cid:48)(t) = P DP −1X(t).

Multiplicando-se à esquerda por P −1 e fazendo a mudança de variável Y (t) = P −1X(t), obte-
mos a equação

Y (cid:48)(t) = DY (t),

que pode ser escrita na forma de um sistema de equações desacopladas

y(cid:48)
1(t) = λ1y1(t)

y(cid:48)
2(t) = λ2y2(t)

que tem solução dada por y1(t) = c1eλ1t e y2(t) = c2eλ2t. Assim, da mudança de variáveis

CAPÍTULO 2. DIAGONALIZAÇÃO DE OPERADORES

29

Y (t) = P −1X(t), a solução de X(cid:48)(t) = AX(t) é

X(t) = P Y (t) = P

 c1eλ1t

c2eλ2t

 .

Vamos mostrar como podemos determinar matrizes P e D, quando elas existem, tais
que A = P DP −1, ou multiplicando à esquerda por P −1 e à direita por P , D = P −1AP , com
D sendo uma matriz diagonal. Chamamos diagonalização ao processo de encontrar as matrizes
P e D.

2.1 Autovalores e Autovetores

Deﬁnição 2.1.1. Sejam V um espaço vetorial sobre R e T : V → V um operador linear. Um
número real λ será dito um autovalor de T se existir um vetor não-nulo v em V tal que

T (v) = λv.

O vetor v é chamado de autovetor de T associado a λ.

Observação 2.1.1. Se v é um autovetor de um operador T associado a um autovalor λ, então
todo múltiplo por escalar de v é também um autovetor de T associado a λ. Mais ainda, se
A(λ) = {v ∈ V ; T (v) = λv}, então A(λ) é um subespaço vetorial de V , chamado autoespaço
de T associado a λ.

O seguinte teorema mostra que autovetores associados a autovalores distintos são line-

armente independentes.
Teorema 2.1.1. Sejam T : V → V um operador linear e λ1, λ2, ..., λn autovalores distintos
de T . Se v1, v2, ..., vn são autovetores de T associados aos autovalores λ1, λ2, ..., λn, então o
conjunto {v1, v2, ..., vn} é linearmente independente.
Demonstração. (Indução sobre n). Sejam x1, x2, ..., xn ∈ R, tais que

x1v1 + x2v2 + ... + xnvn = 0.

Se n = 1, então x1v1 = 0. Logo, x1 = 0, pois v1 (cid:54)= 0. Agora, suponhamos que n ≥ 2
e que o resultado seja válido para todo k com 1 ≤ k ≤ n − 1. Aplicando T a equação

CAPÍTULO 2. DIAGONALIZAÇÃO DE OPERADORES

30

x1v1 + x2v2 + ... + xnvn = 0. e usando que T (vi) = λivi, temos que

x1λ1v1 + x2λ2v2 + ... + xnλnvn = 0.

Agora, multiplicando a equação x1v1 + x2v2 + ... + xnvn = 0 por λn e subtraindo da equação
x1λ1v1 + x2λ2v2 + ... + xnλnvn = 0, temos que

(λn − λ1)x1v1 + (λn − λ2)x2v2 + ... + (λn − λn−1)xn−1vn−1 = 0.

Logo, pela hipótese de indução,

(λn − λi)xi = 0, i = 1, ..., n − 1.

Como λn − λi (cid:54)= 0, i = 1, ..., n − 1, temos que xi = 0, i = 1, ..., n − 1. Assim,

xnvn = 0

mas isto implica que xn = 0. Portanto, o conjunto

{v1, v2, ..., vn}

é linearmente independente.

(cid:4)
Corolário 2.1.1. Seja T : V → V um operador linear. se dim V = n e T possui n autovalores
distintos, então V possui uma base formada por autovetores de T .

Demonstração. Pelo teorema anterior, n autovalores distintos implicam na existência de um
conjunto de autovetores {v1, v2, ..., vn} linearmente independente. Considere W um subespaço
gerado por v1, v2, ..., vn. Como W ⊂ V e dim W = n = dim V , temos que W = V , logo
{v1, v2, ..., vn} é uma base de V .

(cid:4)

Veremos que a existência de uma base de V formada por autovetores de um operador
linear T : V → V é equivalente à existência de uma representação deste operador por uma
matriz diagonal.

CAPÍTULO 2. DIAGONALIZAÇÃO DE OPERADORES

31

2.2 Polinômio Característico

Deﬁnição 2.2.1. Seja A uma matriz quadrada de ordem n. A matriz tIn−A, onde t é uma inde-
terminada, é chamada matriz característica de A. O determinante dessa matriz é um polinômio
em t, chamado polinômio característico de A e denotado por PA(t).

Existe uma relação entre os autovalores de um operador e as raízes do polinômio carac-

terístico de alguma matriz associada a ele. Essa relação é dada pelo resultado a seguir.
Teorema 2.2.1. Seja T : V → V um operador linear e seja α = {v1, v2, ..., vn} uma base de
V . Então:

(i) v é um autovetor de T associado a t0 se, e somente se, [v]α é uma solução não trivial do

sistema linear AX = 0, onde A = t0In − [T ]α
α;

(ii) t0 ∈ R é um autovalor de T se, e somente se, t0 é uma raiz do polinômio característico da

matriz [T ]α

α, ou seja, P[T ]α

α(t0) = 0.

Demonstração.

(i) Seja t0 um autovalor de T e v um autovetor de T associado a t0. Como [T (v)]α =

[T ]α

α[v]α e T (v) = t0v, temos

[t0v]α = [T ]α

α[v]α,

ou seja, t0In[v]α = [T ]α

α[v]α. Equivalentemente,

(t0In − [T ]α

α)[v]α = 0.

(ii) Consideremos o sistema linear AX = 0, onde A = t0In − [T ]α

α. De, (t0In −
α)[v]α = 0, segue que AX = 0 tem uma soluçao não trivial, a saber [v]α, já que v não é o
[T ]α
vetor nulo. Como A não é invertível, temos P[T ]α
α(t0) = 0, provando que t0 é uma raiz de P[T ]α
α.
Reciprocamente, se t0 ∈ R é uma raiz de P[T ]α
α(t0) = 0. Portanto, o sistema linear
AX = 0, onde A = t0In − [T ]α
α, tem uma solução X1 = [x1 x2 ... xn] não nula, pois det A = 0.
Vamos provar que t0 é um autovalor de T e que v = x1v1 + x2v2 + ... + xnvn é um autovetor
de T associado a t0. De fato, como X1 é uma solução do sistema AX = 0, temos AX1 = 0.
Equivalentemente,

α, então P[T ]α

(t0In − [T ]α

α)X1 = t0X1 − [T ]α

αX1 = 0,

CAPÍTULO 2. DIAGONALIZAÇÃO DE OPERADORES

32

ou seja,

[t0v]α = t0[v]α = [T ]α

α[v]α = [T (v)]α,

pois, pela construção de v, X1 = [v]α. De [t0v]α = t0[v]α = [T ]α
α[v]α = [T (v)]α, obtemos
que [T (v)]α = [t0v]α, isto é, as coordenadas dos vetores T (v) e t0v na base α são iguais.
Consequentemente, estes vetores são iguais, ou seja, T (v) = t0v. Como por construção v (cid:54)= 0,
segue-se que t0 é um autovalor de T e v é um autovetor de T associado a t0.

(cid:4)

Exemplo 2.2.1. Seja T : R2 → R2 um operador linear cuja representação matricial em relação
à base canônica de R2 é

 4 −1

2

1

 .

A =

Determine os autovalores e autovetores de T .
Solução. O polinômio característico é

PA(t) = det(tI2 − A) = det

 t − 4

−2

1
t − 1

 = t2 − 5t + 6.

Como t2 − 5t + 6 = 0 somente para t1 = 2 e t2 = 3, o teorema anterior nos mostra que
t1 e t2 são os únicos autovalores de T . Para determinarmos os autovetores de T associados a t1,
devemos resolver o sistema linear

 ,

1

t1 − 1

−2

 t1 − 4
 −2 1

−2 1

 x1
 =
 x1
 0
 =

x2

 0
 ,

0

x2

0

ou seja,

que equivale à equação linear −2x1 + x2 = 0. Assim, o autoespaço de T associado a t1 é
{(x, 2x); x ∈ R}. Agora, para determinarmos os autovetores de T associados a t2, devemos
resolver o sistema linear

 t2 − 4

−2

 x1

x2

 =

 0

0

 ,

1

t2 − 1

CAPÍTULO 2. DIAGONALIZAÇÃO DE OPERADORES

33

ou seja,

 −1 1

−2 2

 x1

x2

 =

 0

0

 ,

que equivale à equação linear −x1 + x2 = 0. Assim, o autoespaço de T associado a t2 é
{(x, x); x ∈ R}.

2.3 Processo de Diagonalização

Deﬁnição 2.3.1. Dizemos que uma matriz A, n × n, é semelhante a uma matriz B, n × n, se
existir uma matriz invertível P tal que

A = P BP −1.

A relação de semelhança satisfaz as seguintes propriedades:

• toda matriz quadrada é semelhante a si mesma;

• se a matriz A é semelhante a B, então B é semelhante a A e

• se A é semelhante a B e B é semelhante a C, então A é semelhante a C.

Deﬁnição 2.3.2. Dizemos que uma matriz A, n × n, é diagonalizável, se ela é semelhante a
uma matriz diagonal. Ou seja, se existem matrizes Q e D tais que A = QDQ−1, em que D é
uma matriz diagonal.

Deﬁnição 2.3.3. Dizemos também que uma operador T : V → V de um espaço vetorial de
dimensão ﬁnita V é diagonalizável, se existir uma base β de V tal que [T ]β
β é uma matriz

diagonal.

Exemplo 2.3.1. Toda matriz diagonal A é diagonalizável, pois A = (In)−1AIn.
Teorema 2.3.1. Um operador linear T : V → V admite uma base β em relação à qual a matriz
[T ]β

β é diagonal se, e somente se, essa base β for formada por autovetores de T .

Demonstração. Suponhamos que β = {v1, v2, ..., vn} é uma base de V tal que [T ]β

β é diagonal,

CAPÍTULO 2. DIAGONALIZAÇÃO DE OPERADORES

34

digamos

Como, para cada i ≤ j ≤ n,



a1

0
...
0

0

a2
...
0

0

···
···
0
...
...
··· an

 .

[T ]β

β =

T (vj) = 0v1 + ... + 0vj−1 + ajvj + 0vj+1 + ... + 0vn = ajvj,

segue que aj é um auto valor de T e vj é um auto vetor de T associado a aj. Portanto, β é uma
base formada por autovetores de T .

Suponhamos agora que β = {u1, u2, ..., un} é uma base de V formada por autovetores
de T . Existem, então, números reais b1, b2, ..., bn tais que, para cada i ≤ j ≤ n, T (uj) = bjuj.
Observamos que os bj
β, temos

(cid:48)s não são necessariamente todos distintos. Pela deﬁnição de [T ]β



b1

0
...
0

0

b2
...
0

···
···
...
···

0

0
...
bn

 ,

[T ]β

β =

ou seja, [T ]β

β é uma matriz diagonal.

(cid:4)

Na demonstração do teorema anterior ﬁca claro que, se um operador T tem uma re-
presentação por uma matriz diagonal [T ]β
β são
dadas pelos autovalores de T . Mais ainda, se T é um operador linear em um espaço V de

β, então as entradas da diagonal principal de [T ]β

dimensão n, o teorema anterior nos diz que T é diagonalizável se, e somente se, T tem n auto-

vetores linearmente independentes. Em particular, se T tem n autovalores distintos, então T é

diagonalizável.

Veremos agora que se V é um espaço com produto interno e se T : V → V é um
operador simétrico, então existe uma base ortonormal de V formada por autovetores de T . Em
particular, todo operador simétrico é diagonalizável. Este resultado é conhecido como Teorema
Espectral.

CAPÍTULO 2. DIAGONALIZAÇÃO DE OPERADORES

35

Teorema 2.3.2 (Teorema Espectral). Seja V um espaço vetorial de dimensão ﬁnita sobre R.
Se T : V → V é um operador simétrico, então existe uma base ortonormal β de V tal que [T ]β
é diagonal.

β

[T ]α

Demonstração. A prova será feita por indução sobre a dimensão de V . Chamaremos a matriz
α de A. Se dim V = 1, o resultado é óbvio. Suponhamos que n ≥ 1 e que o resultado é
válido para espaços de dimensão n. Seja V um espaço vetorial tal que dim V = n + 1. Seja α
uma base de V e seja λ uma raiz complexa do polinômio PA. No entanto, sabemos que todas
α em C são números reais. Portanto, λ é um autovalor
as raízes do polinômio característico P[T ]α
de T . Seja v um vetor unitário de T associado a λ. Consideremos o subespaço

W = {w ∈ V ;(cid:104)w, v(cid:105) = 0}.

Note que W = G(v)⊥, onde G(v) é o subespaço gerado por v. Aﬁrmamos que T (W ) ⊂ W .
De fato, seja w ∈ W . Como T é um operador simétrico, temos que

(cid:104)T (w), v(cid:105) = (cid:104)w, T (v)(cid:105) = (cid:104)w, λv(cid:105) = λ(cid:104)w, v(cid:105) = λ0 = 0,

donde T (w) ∈ W . Assim, podemos considerar o operador restrição

S = T|W ,

que é também um operador simétrico. Além disso, como dim G(v) = 1, segue que dim W = n.

Assim, podemos aplicar a hipótese de indução ao operador S para garantir a existência de uma
base ortonormal {v1, v2, ..., vn} de W formada por autovetores de S (logo de T ). Consequente-
mente, β = {v, v1, v2, ..., vn} é uma base ortonormal de V formada por autovetores de T . Daí,
[T ]β

β é diagonal.

(cid:4)

Observação 2.3.1. Uma outra versão (matricial) do Teorema Espectral, nos diz que se A ∈
M (n, n) é uma matriz simétrica, então existe uma matriz ortogonal P ∈ M (n, n) tal que
P −1AP (= P tAP ) é diagonal. Neste caso, dizemos que A é ortogonalmente diagonalizável
e que P diagonaliza A ortogonalmente.

CAPÍTULO 2. DIAGONALIZAÇÃO DE OPERADORES

36

Exemplo 2.3.2. Considere o operador T : R3 → R3 dado por T (x, y, z) = (x + y,−y, z) e a
base canônica α do R3. A matriz que representa T com relação a base α é:

A = [T ]α

1

0
0 −1 0
1
0

0

α =

 1
 t − 1 −1

0

0

0

t + 1

 .
 = (t − 1)(t + 1)(t − 1).

0

0
t − 1

Portanto, o polinômio característico de T é:

PA(t) = det(tI3 − A) = det

As raízes do polinômio característico de T são:

PA(t) = 0 ⇔ (t − 1)(t + 1)(t − 1) = 0 ⇔ t = ±1.

Assim, os autovalores de T são t1 = 1 com multiplicidade 2 e t2 = −1 com multiplicidade 1.
E, para determinar os autovetores de T resolvemos o sistema linear

0

0

t + 1

 t − 1 −1
 0 −1 0

0

0

2

0

0

0

0

0

0
t − 1



 .

z

y

 x
 =


 x

y

z

 =
 0

0

0

0

0

 0
 ,

Para t1 = 1, temos:

que implica em y = 0. Assim o autoespaço de T associado a t1 é {(x, 0, z); x, z ∈ R}. Observe
que {(1, 0, 0), (0, 0, 1)} é uma base para o autoespaço de T associado a t1.

CAPÍTULO 2. DIAGONALIZAÇÃO DE OPERADORES

37

Para t2 = −1, temos: −2 −1

0

0

0
0
0 −2

0



 x

y

z

 =

 0

0

0

 ,

que equivale à equação linear −2x − y = 0 e à z = 0. Assim o autoespaço de T associado a t2
é {(x,−2x, 0); x ∈ R}. Observe que {(1,−2, 0)} é uma base para o autoespaço de T associado
a t2.

Considere então o conjunto β = {(1, 0, 0), (0, 0, 1), (1,−2, 0)}. Esse conjunto é linear-
mente independente, e como dim R3 = 3, temos que β é uma base para R3, formada por auto-
vetores de T . Portanto, T é um operador diagonalizável. Escrevendo as imagens dos elementos

da base β, pela transformação T , como combinações lineares dos elementos de β, temos:

T (1, 0, 0) = (1, 0, 0) = 1(1, 0, 0) + 0(0, 0, 1) + 0(1,−2, 0)

T (0, 0, 1) = (0, 0, 1) = 0(1, 0, 0) + 1(0, 0, 1) + 0(1,−2, 0)

T (1,−2, 0) = (−1, 2, 0) = 0(1, 0, 0) + 0(0, 0, 1) + −1(1,−2, 0)

Portanto, a matriz que representa T com relação a base β de autovetores é:

[T ]β

β =

que é uma matriz diagonal.

Exemplo 2.3.3. Considere a matriz A dada por

A =

O polinômio característico é dado por

PA(t) = det(tI2 − A) = det

 1 0

0

0 1
0
0 0 −1



2
0 −2

 1
 .
 t − 1 −2

0

t + 2

 = (t − 1)(t + 2).

CAPÍTULO 2. DIAGONALIZAÇÃO DE OPERADORES

38

As raízes do polinômio característico são PA(t) = 0 ⇔ (t− 1)(t + 2) = 0 ⇔ t = 1 ou t = −2.
Portanto, A possui dois autovalores t1 = 1 e t2 = −2.

Assim, os autovetores da matriz A associados ao autovalor t1 = 1 são da forma:

Para t1 = 1, temos que:

AX = t1X ⇔

 1

Para t2 = −2, temos que:

AX = t2X ⇔

X1 =

 1

2
0 −2

 ⇔ x2 = 0.

2
0 −2

0

x2

x2

 = 1
 x1
 x1
 = x1
 1
 .
 x1
 x1
 x1
 = −2
 ⇔ x2 =
 .
 = x1
 x1
 1

x2

x2

0

−3
2

x1.

X2 =

−3
2 x1

−3
2

Assim, os autovetores da matriz A associados ao autovalor t2 = −2 são da forma:

Observe que (1, 0) e (1, −3
2 ) são linearmente independentes, portanto A possui 2 autovetores
linearmente independentes, o que implica que a matriz A é diagonalizável. De fato, basta tomar

a matriz diagonalizante

e a matriz diagonal

Q =

D =

 1
 1

1
0 −3

2

0
0 −2


 .

Observe que as colunas de Q são os autovetores de A e a matriz diagonal D foi construída com
os autovalores de A. Temos que A é semelhante a matriz D, ou seja, A = QDQ−1, de fato,
podemos veriﬁcar que:

 1

1
0 −3

2

 1

0
0 −2

 1

2
3

3

0 −2

 =

 1

2
0 −2

 = A.

QDQ−1 =

CAPÍTULO 2. DIAGONALIZAÇÃO DE OPERADORES

39

Assim, A é uma matriz diagonalizável.

CAPÍTULO 3

Equações Diferenciais

3.1 Um Breve Histórico

A teoria das equações diferenciais foi aplicada primeiramente as ciências físicas, pos-

teriormente a outras atividades humanas, envolvendo desde a engenharia e a biologia até a

medicina, os esportes e as artes. Elas associam uma função a uma ou mais de suas derivadas, e

resolvê-las signiﬁca encontrar todas as suas soluções, isto é, todas as funções que satisfazem a

equação.

O estudo das equações diferenciais iniciou-se com o estudo do Cálculo durante o século

XVII, pelos matemáticos Isaac Newton (1642-1727) e Gottfried Wilhelm Leibniz (1646-1716).

No entanto, além de Newton e Leibniz, podemos citar matemáticos de grande relevância para o

desenvolvimento desta teoria, como Bernoulli, Cauchy, Euler, Lagrange, Laplace e Gauss.

O avanço do Cálculo proporcionou a resolução de inúmeros problemas, os quais pude-

ram ser modelados matematicamente na forma de equações diferenciais e vários desses pro-

blemas, como por exemplo, a resolução da braquistócrona, um problema que se destinava a

determinar a forma de uma curva ligando dois pontos distintos sobre um plano vertical, foram

resolvidos explicitamente por grandes matemáticos como os da família Bernoulli e Leonhard

Euler.

Leonhard Euler(1707-1783), o maior matemático do século XVIII, foi um dos matemá-

40

CAPÍTULO 3. EQUAÇÕES DIFERENCIAIS

41

Figura 3.1: Leonhard Euler

ticos que mais contribuiram no desenvolvimento da teoria das equações diferenciais, embora,

seus interesses incluissem todas as áreas da matemática e muitos campos de aplicação. Euler

identiﬁcou a condição para que equações diferenciais de primeira ordem sejam exatas, desenvol-

veu o método de variação de parâmetros, demonstrou a teoria de fatores integrantes, encontrou

a solução geral de equações lineares homogêneas com coeﬁcientes constantes. Nas equações

diferenciais parciais fez, também, importantes contibuições.

No ﬁnal do século XVIII, já haviam sido descobertos diversos métodos elementares

para solucionar equações diferenciais ordinárias. Já no começo do século XIX, Carl Friedrich

Gauss (1777-1855) Augustin-Louis Cauchy (1789-1857) contribuíram no desenvolvimento das

teorias e conceitos de funções de variáveis complexas. O desenvolvimento das soluções de

determinadas equações diferenciais ainda continua como objeto de pesquisa, com problemas

atrativos e importantes ainda não resolvidos.

3.2 Equações Diferenciais Ordinárias

De uma maneira geral, podemos dizer que temos uma equação diferencial se na equação

estão envolvidas funções incógnitas e suas derivadas.

Uma equação diferencial é dita ordinária (EDO) se a função incógnita depender apenas

de uma variável. Se depender de mais de uma variável será denominada equação diferencial

parcial.

A ordem de uma equação diferencial é indicada pela maior ordem de derivação que

aparece na equação. Uma EDO de ordem n tem como expressão geral

F

x, y,

dy
dx

,

d2y

dx2 ,··· ,

dny
dxn

= 0,

(3.2.1)

(cid:16)

(cid:17)

CAPÍTULO 3. EQUAÇÕES DIFERENCIAIS

42

onde F é uma função de n + 1 variáveis.

A equação (3.2.1) representa a relação entre a variável independente x e os valores da
dxn na equação

função incógnita y e suas n primeiras derivadas. Quando pudermos explicitar dny
(3.2.1) teremos uma forma normal da EDO de ordem n, isto é,

(cid:16)

(cid:17)

dny
dxn = f

x, y,

dy
dx

,

d2y

dx2 ,··· ,

dn−1y
dxn−1

.

(3.2.2)

As equações na forma normal podem sempre ser escritas na forma da equação 3.2.1, basta

considerar

(cid:16)

F

x, y,

dy
dx

,

d2y

dx2 ,··· ,

dny
dxn

(cid:17)

=

(cid:16)

(cid:17)

,

d2y

dx2 ,··· ,

dn−1y
dxn−1

= 0.

(3.2.3)

dy
dx

Entretanto, uma equação na forma geral (3.2.1) pode acarretar mais de uma equação na forma

normal (3.2.2). Por exemplo,

dny

x, y,

dxn − f
(cid:16) dy
(cid:17)2 − x = 0

dx

leva às duas equações na forma normal:

√
x e dy
dx

dy
dx

=

= −√

x.

A equação diferencial (3.2.2) é chamada linear se a função f for linear nas variáveis
d2y

dn−1y
dxn−1 . Sendo assim, a forma geral de uma EDO linear de ordem n é

dx2 ,··· ,

y,

dy
dx

,

an(x)

dny
dxn + an−1(x)

dn−1y
dxn−1 + ··· + a1(x)

dy
dx

+ a0(x)y = g(x)

(3.2.4)

onde an não é identicamente nula.

Uma solução de uma EDO, no intervalo I = (a, b), é uma função y = φ(x) que,

juntamente com suas derivadas, satisfaz a equação 3.2.2. Assim, resolver uma EDO (3.2.2) é

encontrar uma função y = φ(x), deﬁnida e derivável até a ordem n no intervalo I, que satisfaz

a equação (3.2.2).

Solução geral de uma EDO é o conjunto de todas as suas soluções. Nas aplicações,

geralmente estamos interessados em soluções particulares que satisfaçam uma dada condição

inicial, ou condições complementares.

CAPÍTULO 3. EQUAÇÕES DIFERENCIAIS

Para uma equação diferencial de orem n, o problema

 Resolver :

(cid:16)

dny
dxn = f

x, y,

dy
dx

,

d2y

dx2 ,··· ,

Sujeito a : y(x0) = y0, y(cid:48)(x0) = y1, . . . , y(n−1)(x0) = yn−1

(cid:17)

dn−1y
dxn−1

43

(3.2.5)

em que y0, y1, . . . , yn−1 são constantes arbitrárias, é chamado de um problema de valor inicial
(PVI) de ordem n. Os valores especíﬁcos y(x0) = y0, y(cid:48)(x0) = y1, . . . , y(n−1)(x0) = yn−1 são
chamados de condições iniciais. Em particular,

 an(x)

dny
dxn + an−1(x)

dn−1y
dxn−1 + ··· + a1(x)

dy
dx

y(x0) = y0, y(cid:48)(x0) = y1, . . . , y(n−1)(x0) = yn−1

+ a0(x)y = g(x)

(3.2.6)

é um PVI linear de ordem n.

Teorema 3.2.1 (Existência e Unicidade). Sejam an(x), an−1(x), . . . , a1(x), a0(x) e g(x) fun-
ções contínuas em um intervalo I com an(x) (cid:54)= 0 para todo x neste intervalo. Se x0 é algum
ponto deste intervalo, então existe uma única solução y(x) para o PVI 4.1.2 neste intervalo.

Para o nosso propósito, precisamos da seguinte proposição.

Proposição 3.2.1. Se f é uma função contínua em um intervalo I, então o conjunto de todas
soluções da EDO linear de primeira ordem

é o conjunto W = {y(x) = eax

= ay + f (x), a ∈ R, a (cid:54)= 0,

(3.2.7)

f (x)e−axdx + ceax : c ∈ R} em I.

Demonstração. Em primeiro lugar, mostraremos que qualquer função de W é uma solução de
3.2.7. De fato, seja y(x) = eax

f (x)e−axdx + ceax, x ∈ I. Então,

dy
dx

(cid:90)
(cid:90)

(cid:90)
(cid:90)

y(cid:48) = aeax

= a(eax

f (x)e−axdx + eaxe−axf (x) + aceax

f (x)e−axdx + ceax) + f (x)

= ay + f (x).

Vamos mostrar que toda solução é desta forma. Seja φ(x) uma solução qualquer de

CAPÍTULO 3. EQUAÇÕES DIFERENCIAIS

44

(3.2.7) em I. Então

= aφ + f (x), x ∈ I.

dφ
dx

Multiplicando esta equação por e−ax, obtemos

ou

e−ax dφ
dx

− ae−axφ = e−axφf (x), x ∈ I,

[e−axφ] = e−axf (x), x ∈ I.

d
dx

Integrando membro a membro, obtemos

e−axφ =

e−axf (x)dx + c, x ∈ I,

(cid:90)

(cid:90)

para alguma constante c ∈ R. Finalmente,

φ = eax

para alguma constante c ∈ R.

e−axf (x)dx + ceax, x ∈ I,

(cid:4)

(cid:90)

Observação 3.2.1. Note que a solução geral da equação (3.2.7 é dada por y = yc + yp, onde
e−axf (x)dx é uma
yc = ceax é a solução geral da equação homogênea y(cid:48) = ay e yp = eax
solução particular da equação não homogênea (3.2.7.

Observação 3.2.2. Segue da proposição anterior juntamente com o Teoreme de existência e
unicidade que y = y0ea(x−x0), x ∈ R, é a única solução do Problema de valor inicial

 y(cid:48)(x) = ay(x)

y(x0) = y0

3.3 Sistemas de Equações Diferenciais de Primeira ordem

Nos concentraremos somente em sistemas de equações diferenciais de primeira ordem

lineares. Desenvolveremos uma teoria para esse tipo de sistema e, no caso de sistemas com

coeﬁcientes constantes, um método de solução que utiliza alguns conceitos básicos de álgebra

matricial.

CAPÍTULO 3. EQUAÇÕES DIFERENCIAIS

45

Deﬁnição 3.3.1. Um sistema de n equações diferenciais de primeira ordem é um conjunto de
equações da forma:



dx1
dt
dx2
dt

dxn
dt

= g1(t, x1, x2, ..., xn)

= g2(t, x1, x2, ..., xn)
...
= gn(t, x1, x2, ..., xn)

(3.3.1)

Quando cada uma das funções g1, g2, ..., gn, do sistema de equações anterior, for linear
nas variáveis dependentes x1, x2, ..., xn, obtemos a forma normal de um sistema de primeira
ordem de equações lineares:



dx1
dt
dx2
dt

dxn
dt

= a11(t)x1 + a12(t)x2 + . . . + a1n(t)xn + f1(t)

= a21(t)x1 + a22(t)x2 + . . . + a2n(t)xn + f2(t)
...
= an1(t)x1 + an2(t)x2 + . . . + ann(t)xn + fn(t)

(3.3.2)

Consideramos que os coeﬁcientes aij(t) bem como as funções fi(t) sejam contínuos em
um intervalo comum I. Quando fi(t) = 0, i = 1, 2, ..., n, o sistema linear é dito ser homogêneo;
caso contrário, ele é não homogêneo.

Se X, A(t) e F (t) representarem as respectivas matrizes

X =



x1(t)

x2(t)
...

xn(t)



d
dt

x1

x2
...
xn



 , A(t) =

 =

...

a11(t) a12(t)

a21(t) a22(t)

...

...

an1(t) an2(t)

a1n(t)

a2n(t)

···
···
...
··· ann(t)

...

a11(t) a12(t)

a21(t) a22(t)

...

an1(t) an2(t)

a2n(t)

a1n(t)

···
···
...
··· ann(t)

...

 , F (t) =

 +

x1

x2
...
xn



f1(t)

f2(t)
...
fn(t)

 ,



f1(t)

f2(t)
...
fn(t)





então o sistema de equações diferenciais de primeira ordem lineares pode ser escrito como

ou simplesmente

X(cid:48) = AX + F.

(3.3.3)

CAPÍTULO 3. EQUAÇÕES DIFERENCIAIS

Se o sistema for homogêneo, sua forma matricial é então

X(cid:48) = AX.

46

(3.3.4)

Uma das motivações para o estudo de sistemas de equações diferenciais lineares de

primeira ordem é que toda equação diferencial linear de ordem n pode ser reduzida a um sistema

linear com a forma normal (3.3.2).

Suponhamos uma equação diferencial linear de ordem n dada por

dny
dtn = an−1(t)

dn−1y
dtn−1 + ··· + a1(t)

dy
dt

+ a0(t)y + f (t).

(3.3.5)

Introduzindo as variáveis

y = x1, y(cid:48) = x2, y(cid:48)(cid:48) = x3, . . . , y(n−1) = xn,

(3.3.6)

decorre que y(cid:48) = x(cid:48)
n. Logo, de
(3.3.5) e (3.3.6), veriﬁcamos que uma equação diferencial linear de ordem n pode ser expressa

2 = x3, . . . , y(n−1) = x(cid:48)

n−1 = xn e y(n) = x(cid:48)

1 = x2, y(cid:48)(cid:48) = x(cid:48)

como um sistema de equações lineares de primeira ordem:



x(cid:48)
1 = x2
x(cid:48)
2 = x3
x(cid:48)
3 = x4
...
x(cid:48)
n−1 = xn
n = an−1(t)xn + ··· + a1(t)x2 + a0(t)x1 + f (t).
x(cid:48)

Exemplo 3.3.1. Se X =

 x

y

z

, então a forma matricial do sistema não homogêneo


=
6x + y + z + t
= 8x + 7y − z + 10t
= 2x + 9y − z + 6t

dx
dt
dy
dt
dz
dt

CAPÍTULO 3. EQUAÇÕES DIFERENCIAIS

é

X(cid:48) =

47

 .

Deﬁnição 3.3.2. Um vetor solução em um intervalo é qualquer matriz coluna

 6 1

1
8 7 −1
2 9 −1

10t

6t

 X +
 t


x2(t)
...

x1(t)

xn(t)



X =

 e−2t

−e−2t

 e X2 =

cujas entradas são funções diferenciáveis que satizfazem o sistema (3.3.3) no intervalo.

Solução. A partir de X(cid:48)

1 =

Exemplo 3.3.2. Veriﬁque que no intervalo (−∞,∞), as matrizes X1 =

 3e6t

5e6t

e

AX1 =

2 =

5 3

30e6t

2e−2t

 X.
 1 3
 são soluções de X(cid:48) =
 18e6t
 e X(cid:48)
 −2e−2t
 e−2t − 3e−2t
 =
 1 3
 e−2t
 3e6t + 15e6t
 3e6t
 =
 1 3


 , X0 =

5e−2t − 3e−2t

15e6t + 15e6t

−e−2t

X(t0) =

AX2 =

x1(t0)

x2(t0)

5 3

5 3

5e6t

...

xn(t0)

1

2e−2t

, temos que
 −2e−2t
 =
 = X(cid:48)
 =
 = X(cid:48)
 18e6t
 ,

30e6t

γ1

2.

γ2
...
γn

Deﬁnição 3.3.3 (Problema de valor inicial). Seja t0 um ponto em um intervalo I e

CAPÍTULO 3. EQUAÇÕES DIFERENCIAIS

onde γi, i = 1, 2, ..., n, são constantes dadas. Assim, o problema

 Resolver : X(cid:48) = A(t)X + F (t)

Sujeito a : X(t0) = X0

48

(3.3.7)

é um problema de valor inicial em I.

Observação 3.3.1. Note que o sistema (3.3.1) pode ser escrito como

onde

X(cid:48) = f (t, X),

 ,



x1

x2
...
xn

X =

e f : R × Rn → Rn.

Esse fato motiva a seguinte deﬁnição.

Deﬁnição 3.3.4. Dada uma aplicação f : U → Rn, deﬁnida em cada ponto (t, X) de um
aberto de U de R × Rn ≡ Rn+1, dizemos que

X(cid:48) = f (t, X)

é a equação diferencial ordinária em Rn deﬁnida por f.

Uma solução dessa equação diferencial ordinária, às vezes denominada curva integral
da equação, é um caminho X : I → Rn deﬁnido e derivável num intervalo I de R, com gráﬁco
inteiramente contido em U e velocidade determinada por f, ou seja, para cada t ∈ I

(t, X(t)) ∈ U e X(cid:48) = f (t, X(t)).

Deﬁnição 3.3.5. O problema  Resolver : X(cid:48) = f (t, X)

Sujeito a : X(t0) = X0

(3.3.8)

CAPÍTULO 3. EQUAÇÕES DIFERENCIAIS

49

é um problema de valor inicial em Rn.

Teorema 3.3.1 (Teorema de Picard). Seja f contínua e lipschitziana em Ω = Ia × Bb, onde
Ia = {t;|t − t0| ≤ a}, Bb = {x;|t − x0| ≤ b}. Se |f| < M em Ω, então existe uma única
solução do PVI (3.3.8) em Iα, onde α = min
Demonstração. Seja X = C(Iα, Bb) o espaço métrico completo das funções contínuas φ :
Iα → Bb, com a métrica da convergência uniforme

(cid:110)

(cid:111)

b
M

a,

.

d(φ1, φ2) = sup
t∈Iα
Para φ ∈ X, seja F (φ) : Iα → Rn deﬁnida por

F (φ)(t) = X0 +

Destacamos as seguintes propriedades de F :

(i) F (X) ⊂ X;

{φ1(t) − φ2(t)}.

(cid:90) t

t0

f (s, φ)(s))ds.

(ii) F n é uma contração, para n suﬁcientemente grande.
De fato, para todo t ∈ Iα,

|F (φ)(t) − x0| =

≤

f (s, φ)(s))ds

|f (s, φ)(s))|ds

(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:90) t
(cid:90) t
(cid:90) t

t0

t0

t0

≤
M ds
≤ M (t − t0)
≤ M α
≤ b.

Logo, F (X) ⊂ X. Quanto a (ii), para todo par φ1, φ2 ∈ X e todo n ∈ N, temos

|F n(φ1)(t) − F n(φ2)(t)| ≤ K n|t − t0|n

n!

.d(φ1, φ2), t ∈ Iα.

(3.3.9)

CAPÍTULO 3. EQUAÇÕES DIFERENCIAIS

50

onde K é a constante de Lipschitz de f. Veriﬁcamos esta desigualdade por indução em n. Para

n = 0 é válida. Suponha que seja válida para n = l, isto é,

|F l(φ1)(t) − F l(φ2)(t)| ≤ K l|t − t0|l

l!

.d(φ1, φ2), t ∈ Iα.

Então,

|F l+1(φ1)(t) − F l+1(φ2)(t)| = F (F l(φ1))(t) − F (F l(φ2))(t)|

≤ (cid:12)(cid:12)(cid:12)(cid:90) t
(cid:90) t

t0

≤

(cid:12)(cid:12)(cid:12)

f (s, F l(φ1)(s)) − f (s, F l(φ2)(s))ds

K|F l(φ1)(s) − F l(φ2)(s)|ds.

t0

Por hipótese de indução, obtemos

|F l+1(φ1)(t) − F l+1(φ2)(t)| ≤ K

(cid:90) t

K l|t − t0|l

l!

t0

K l+1|t − t0|l+1

.d(φ1, φ2)ds.

=

(l + 1)!

.d(φ1, φ2).

Portanto a desigualdade (3.3.9) é válida. Calculando o supremo, segue que

d(F n(φ1) − F n(φ2)) ≤ K nαn
n!

d(φ1, φ2).

E para n grande

K nαn

n!

< 1

pois é o termo geral de uma série cuja soma é eKα. Portanto F n é uma contração em X. Pelo
Corolário (1.2.1) , existe uma única φ ∈ X tal que F (φ) = φ, e isto, prova o teorema.

(cid:4)

Proposição 3.3.1. Seja f contínua e Lipschitziana em Ω = [a, b] × Rn. Então, para todo
(t0, x0) ∈ Ω existe uma única solução do PVI (3.3.8) em I = [a, b].

CAPÍTULO 3. EQUAÇÕES DIFERENCIAIS

51

Demonstração. Considere X = C(I, E) e F : X → X deﬁnida por

(cid:90) t

F (φ)(t) = x0 +

f (s, φ)(s))ds.

Seguindo os passos da demostração do Teorema (3.3.1), obtemos que F tem um único ponto

ﬁxo pois, para n grande, F n é uma contração.

t0

(cid:4)

Corolário 3.3.1 (Equações Lineares). Considere as entradas das matrizes A(t) e F (t) como
sendo funções contínuas em um intervalo comum I que contenha o ponto t0. Logo, existe uma
única solução do problema de valor inicial (3.3.7) neste intervalo.
Demonstração. Seja I = ∪nIn, onde In ⊆ In+1 são intervalos compactos que contém t0.
f (t, x) = A(t)x + F (t) satisfaz as hipóteses da Proposição (3.3.1) em cada intervalo In. Seja
φn a única solução neste intervalo passando por (t0, x0). É claro que φn+1 |In= φn. Logo
φ(t) = φn(t), t ∈ In está bem deﬁnida em I. É claro também que φ é a única solução em I
passando por (t0, x0).

(cid:4)

O seguinte Teorema diz que o conjunto de todas as soluções de um sistema linear ho-

mogêneo é um espaço vetorial sobre os reais.

Teorema 3.3.2 (Princípio da Superposição). Considere X1, X2, ..., Xk um conjunto de vetores
solução do sistema homogêneo X(cid:48) = AX em um intervalo I. Então a combinação linear

X = c1X1 + c2X2 + ... + ckXk,

onde os ci, i = 1, 2, ..., k, são constantes arbitrárias, é também uma solução neste intervalo.

Demonstração.

X(cid:48) = c1X(cid:48)

1 + c2X(cid:48)

2 + . . . + ckX(cid:48)

k

= c1A(t)X1 + c2A(t)X2 + . . . + ckA(t)Xn

= A(t)(c1X1 + c2X2 + . . . + ckXn)

= A(t)X.

CAPÍTULO 3. EQUAÇÕES DIFERENCIAIS

52

(cid:4)

Deﬁnição 3.3.6 (Dependência/independência linear). Considere X1, X2, ..., Xk como sendo um
conjunto de vetores solução do sistema homogêneo X(cid:48) = AX em um intervalo I. Dizemos que
o conjunto é linearmente dependente no intervalo se existirem constantes c1, c2, ..., ck, nem
todas nulas, de modo que

c1X1 + c2X2 + ... + ckXk = 0

para todo t no intervalo. Se o conjunto de vetores não for linearmente dependente no intervalo,

ele será linearmente independente.

Deﬁnição 3.3.7. Qualquer conjunto X1, X2, ..., Xn de n vetores solução linearmente indepen-
dentes do sistema homogêneo X(cid:48) = AX em um intervalo I é dito ser um conjunto fundamental
de soluções no intervalo.

Teorema 3.3.3 (Existência de um conjunto fundamental). Existe um conjunto fundamental de
soluções para o sistema homogêneo (3.3.4) em um intervalo I.

Demonstração. Seja t0 um ponto qualquer de I e seja x1, x2,··· , xn quaisquer n vetores line-
armente independentes em Rn. Pelo Corolário (3.3.1), o sistema (3.3.4) possuem n soluções
φ1, φ2,··· , φn, cada uma deﬁnida no intervalo I e satisfazendo a condição inicial

φj(t0) = xj, j = 1, 2, . . . , n.

(3.3.10)

Mostraremos que as soluções φ1, φ2,··· , φn são linearmente independentes em I. Para isso,
suponha que não são. Então existem escalares a1, a2, . . . , an, nem todos nulos, tais que

a1φ1(t) + a2φ2(t) + ··· + anφn(t) = 0, ∀t ∈ I.

Em particular, pondo t = t0, e usando as condições iniciais (3.3.10), temos

a1x1 + a2x2 + ··· + anxn = 0.

Porém, isto é impossível pois x1, x2, . . . , xn foram escolhidos para serem linearmente indepen-
dentes.

(cid:4)

CAPÍTULO 3. EQUAÇÕES DIFERENCIAIS

53

Teorema 3.3.4. Se A(t) é contínua em algum intervalo I, então o conjunto de todas as soluções
do sistema (3.3.4) forma um espaço vetorial de dimensão n sobre o conjunto dos números reais.
Demonstração. Seja t0 um ponto qualquer de I e seja x1, x2,··· , xn quaisquer n vetores li-
nearmente independentes em Rn. Pelo Teorema (3.3.3) existe um conjunto fundamental de
soluções φ1, φ2,··· , φn satisfazendo

φj(t0) = xj, j = 1, 2, . . . , n.

Seja ψ uma solução qualquer do sistema (3.3.4) em I. Dessa forma existem únicas constantes
c1, c2, . . . , cn tais que

ψ(t0) = c1x1 + c2x2 + ··· + cnxn = δ.

Agora considere

φ(t) = c1φ1(t) + c2φ2(t) + ··· + cnφn(t).

Claramente, φ(t) é uma solução de (3.3.4) em I. Além disso,

φ(t0) = c1φ1(t0) + c2φ2(t0) + ··· + cnφn(0) = c1x1 + c2x2 + ··· + cnxn = ψ(t0).

Portanto, φ(t) e ψ(t) são ambas soluções de (3.3.4) em I com φ(t0) = ψ(t0) = δ. Portanto,
pelo Corolário (3.3.1), φ(t) = ψ(t), ∀t ∈ I.

(cid:4)

Corolário 3.3.2 (Solução geral de sistemas homogêneos). Considere φ1, φ2, . . . , φn como sendo
um conjunto fundamental de soluções do sistema homogêneo (3.3.4) em um intervalo I. Assim,

a solução geral do sistema no intervalo é

ψ = c1φ1 + c2φ2 + ... + cnφn,

onde os ci, i = 1, 2, . . . , n, são constantes arbitrárias.

Demonstração. É uma consequência imediata do Teorema anterior.

Exemplo 3.3.3. A partir do exemplo anterior, sabemos que X1 =

 e−2t

−e−2t

 e X2 =

 3e6t

5e6t

(cid:4)



são soluções linearmente independentes do sistema X(cid:48) =

tanto, X1 e X2 formam um conjunto fundamental de soluções no intervalo. A solução geral do
sistema no intervalo é então

5 3

 1 3
 3
 e−2t + c2

54

 X em (−∞,∞). Por-
 e6t.

 1

−1

CAPÍTULO 3. EQUAÇÕES DIFERENCIAIS

X = c1X1 + c2X2 = c1

5

Deﬁnição 3.3.8. Para sistemas não homogêneos, uma solução particular Xp em um intervalo
I é qualquer vetor, livre de parâmetros arbitrários, cujas entradas são funções que satisfazem

o sistema (3.3.3).

Teorema 3.3.5 (Solução geral de sistemas não homogêneos). Considerando xp uma solução
dada do sistema não homogêneo (3.3.3) em um intervalo I, e sendo xc = c1φ1+c2φ2+...+cnφn
a solução geral no mesmo intervalo do sistema homogêneo associado (3.3.4), temos que a

solução geral do sistema não homogêneo no intervalo é

x = xc + xp.

(3.3.11)

Demonstração. Temos que

x(cid:48) = x(cid:48)

c + x(cid:48)

p

= Axc + Axp + f (t)

= A(xc + xp) + f (t)

= Ax + f (t).

Logo, toda função da forma (3.3.11) é solução do sistema linear não homogêneo. Suponha

agora que ψ é uma solução qualquer do sistema não homogêneo (3.3.3). Então, desde que

(ψ − xp)(cid:48) = ψ(cid:48) − x(cid:48)

p = Aψ + f (t) − Ax(cid:48)

p − f (t) = A(ψ − x(cid:48)

P ),

segue que ψ−xp é solução do sistema linear homogêneo (3.3.4). Dessa forma, existem escalares
c1, c2, . . . , cn tais que ψ − xp = c1φ1 + c2φ2 + ··· + cnφn = xc. Segue daí que ψ = xc + xp.

(cid:4)

CAPÍTULO 3. EQUAÇÕES DIFERENCIAIS

55

Observação 3.3.2. A solução geral xc do sistema homogêneo é chamada de função comple-
mentar do sistema não homogêneo.

CAPÍTULO 4

Aplicações

4.1 Solução de Sistemas de Equações Diferenciais por Diago-

nalização

4.1.1 Sistemas Lineares Homogêneos

Nessa seção, consideraremos um método alternativo para resolver um sistema homogê-

neo de equações diferenciais de primeira ordem lineares. Esse método pode ser aplicado a um
sistema X(cid:48) = AX sempre que a matriz dos coeﬁcientes A for diagonalizável.

Deﬁnição 4.1.1. Um sistema de equações diferenciais linear homogêneo X(cid:48) = AX,

 =





1

2

x(cid:48)
x(cid:48)
...
x(cid:48)

n

a11 a12

a21 a22
...
...
an1 an2

a1n

···
···
a2n
...
...
··· ann





 ,

x1

x2
...
xn

no qual cada x(cid:48)

i é escrito como uma combinação linear de x1, x2, ..., xn é dito ser acoplado.

Observação 4.1.1. Se a matriz dos coeﬁcientes A for diagonalizável, então o sistema pode ser
desacoplado de modo que cada x(cid:48)

i possa ser expresso somente em termos de xi.

56

CAPÍTULO 4. APLICAÇÕES

57

Em um dos exemplos apresentados anteriormente, vimos que a solução geral do sistema

 1 3

5 3

homogêneo X(cid:48) =

vetores da solução têm a forma Xi =

somos solicitados a dizer se podemos sempre obter uma solução da forma

5

−1

 X é X = c1
 k1


 3

 1
 e6t. Como ambos os
 e−2t + c2
 eλit, i = 1, 2, onde k1, k2, λ1 e λ2 são constantes,
 eλt = Keλt

X =

k1

k2

k2
...
kn

k2
...
kn

para o sistema de primeira ordem linear homogêneo X(cid:48) = AX, onde a matriz de coeﬁcientes
A é uma matriz de constantes n × n.

 eλt = Keλt for um vetor solução do sistema , então X(cid:48) = Kλeλt de



Se X =

k1

modo que X(cid:48) = AX se escreve Kλeλt = AKeλt. Após cancelar eλt e rearranjando, obtemos
AK − λK = 0. Como K = IK, a última equação é o mesmo que (A − λI)K = 0. Para
obter uma solução não trivial X de X(cid:48) = AX, temos que calcular um vetor não trivial K que
satisfaça (A − λI)K = 0. Porém, para que (A − λI)K = 0 tenha outras soluções que não
apenas a solução trivial, temos que ter

det(A − λI) = 0.

Essa equação polinomial em λ é chamada de equação característica da matriz A; as soluções
dessa equação são os autovalores de A. Uma solução K (cid:54)= 0 de (A−λI)K = 0 que corresponde
a um autovalor λ é denominada um autovetor de A. Uma solução do sistema homogêneo
X(cid:48) = AX é então X = Keλt.

Quando a matriz A tem autovalores reais e distintos λ1, λ2, ..., λn, então um conjunto de

n autovetores linearmente independentes K1, K2, ..., Kn pode sempre ser obtido e

X1 = K1eλ1t, X2 = K2eλ2t, ..., Xn = Kneλnt

CAPÍTULO 4. APLICAÇÕES

58

é um conjunto fundamental de soluções de X(cid:48) = AX em (−∞,∞). Sendo assim a solução
geral de X(cid:48) = AX no intervalo (−∞,∞) é deﬁnida como

X = c1K1eλ1t + c2K2eλ2t + ... + cnKneλnt.

Se a matriz A tiver n autovetores linearmente independentes, então sabemos que pode-
mos obter uma matriz P tal que P −1AP = D, onde D é uma matriz diagonal. Se ﬁzermos
a substituição X = P Y no sistema X(cid:48) = AX, então P Y (cid:48) = AP Y ou Y (cid:48) = P −1AP Y ou
Y (cid:48) = DY .

A equação Y (cid:48) = DY é igual a



1

2

y(cid:48)
y(cid:48)
...
y(cid:48)

n

 =







 .

y1

y2
...
yn

λ1

0

0 λ2
...
...
0
0

0

···
···
0
...
...
··· λn

Como D é diagonal, a inspeção de Y (cid:48) = DY revela que esse novo sistema é desacoplado; cada
equação diferencial no sistema é da forma y(cid:48)
i = λiyi, i = 1, 2, ..., n. A solução de cada uma
dessas equações lineares é yi = cieλit, i = 1, 2, ..., n. Logo, a solução geral de Y (cid:48) = DY pode
ser escrita como o vetor coluna

Y =


 −2 −1 8

0 −3 8
0 −4 9

cneλnt

c1eλ1t

c2eλ2t

...

 .
 X por diagonalização.

Como agora conhecemos Y e como a matriz P pode ser construída a partir dos autovetores de
A, a solução geral do sistema original X(cid:48) = AX é obtida a partir de X = P Y .

Exemplo 4.1.1. Resolva X(cid:48) =

Solução. Inicialmente calculamos os autovalores e os autovetores correspondentes da matriz

CAPÍTULO 4. APLICAÇÕES

59

dos coeﬁcientes. A partir de

det(λI − A) = det

 λ + 2

0

0

 = (λ + 2)(λ − 1)(λ − 5),

1

−8
λ + 3 −8
λ − 9

4

obtemos λ1 = −2, λ2 = 1 e λ3 = 5. Como os autovalores são distintos, os autovetores são line-
armente independentes. Resolvendo (λiI − A)K = 0 para i = 1, 2, 3, temos, respectivamente,

Portanto, uma matriz que diagonaliza a matriz de coeﬁcientes é

 1

0

0

K1 =

 .

 1

1

1

2

 , K2 =
 2
 , K3 =
 .
 1 2 1

0 2 1

P =

1

0 1 1

As entradas na diagonal principal de D são os autovalores de A que correspondem à ordem na

qual os autovetores aparecem em P :

D =

 −2 0 0

1 0

0

0

0 5

 .

Conforme vimos anteriormente, a substituição X = P Y em X(cid:48) = AX resulta no

sistema desacoplado Y (cid:48) = DY . A solução geral desse último sistema é imediata:

 c1e−2t

c2et

c3e5t

 .

Y =

CAPÍTULO 4. APLICAÇÕES

60

Logo, a solução do sistema dado é

 1 2 1

0 2 1

0 1 1



 c1e−2t

c2et

c3e5t

 =

 c1e−2t + 2c2et + c3e5t

2c2et + c3e5t

c2et + c3e5t

 .

X = P Y =

A solução por diagonalização sempre funcionará desde que possamos determinar n au-
tovetores linearmente independentes de uma matriz A n × n; o método falha quando A tem
autovalores repetidos e n autovetores linearmente independentes não podem ser obtidos. É

claro que nessa última situação A não é diagonalizável.

No entanto, caso a matriz dos coeﬁcientes tenha autovalores repetidos, ainda assim,

podemos solucionar o sistema utilizando conceitos apresentados anteriormente, como veremos

no exemplo seguinte.

Exemplo 4.1.2. Resolver o sistema

x(cid:48)
1(t) = 3x1(t) + x3(t)
x(cid:48)
2(t) = 2x2(t)
x(cid:48)
3(t) = x1(t) + 3x3(t)

satisfazendo a condição inicial x1(0) = 1, x2(0) = 0 e x3(0) = 1.
Solução. Note que o sistema pode ser escrito da forma

 3 0 1

0 2 0

1 0 3

 X.

X(cid:48) =

Inicialmente calculamos os autovalores e os autovetores correspondentes da matriz dos coeﬁci-

entes. A partir de

det(λI − A) = det

 λ − 3

0
−1

0

λ − 2

−1
0

0

λ − 3

 = (λ − 2)(λ − 2)(λ − 4).

Os autovalores são λ1 = 2 (multiplicidade 2) e λ2 = 4 (simples). Sendo assim, o autoespaço
associado a λ1 = 2 é {(x, y,−x); x, y ∈ R} de dimensão 2, do qual {(1, 0,−1); (0, 1, 0)} é

CAPÍTULO 4. APLICAÇÕES

61

base e o autoespaço associado a λ2 = 4 e {(x, 0, x); x ∈ R}, de dimensão 1, cuja base mais
natural é (1, 0, 1). Assim, a matriz A é diagonalizável e, se

 1

P =

0
1 0
−1 0 1

0 1


 2 0 0

0 2 0

0 0 4

 .

então

D = P −1 · A · P =

Desta forma, de X(t) = c1k1e2t + c2k2e2t + c3k3e4t, onde k1, k2 e k3 são os vetores das bases
dos autoespaços, segue que



x1(t) = c1e2t + c3e4t

x2(t) = c2e2t
x3(t) = −c1e2t + c3e4t

A ﬁm de que x1(0) = 1, x2(0) = 0 e x3(0) = 1, deveremos ter

 = P

 1

0

1



 c1

c2

c3

cuja solução é c1 = 0, c2 = 0 e c3 = 1. Logo, a solução que satisfaz a condição inicial dada é
x1(t) = e4t, x2(t) = 0 e x3(t) = e4t.

4.1.2 Sistemas Lineares Não Homogêneos

Seja o sistema não homogêneo

X(cid:48) = AX + f (t),

(4.1.1)

onde A é uma matriz e f (t) um vetor. Pelo Teorema (3.3.5) a solução geral é dada por

X = Xc + Xp,

CAPÍTULO 4. APLICAÇÕES

62

onde Xc é a solução geral do sistema homogêneo e Xp uma solução particular do sistema não
homogêneo. Utilizaremos o método de diagonalização de matrizes para achar uma solução

particular.

Suponhamos que a matriz A seja diagonalizável, isto é, que existe uma matriz invertí-
vel P tal que P −1AP = D com D diagonal. Fazendo a mudança de variáveis X = P Y e
substituindo na equação (4.1.1), temos

P Y (cid:48) = AP Y + f (t) ⇒ Y (cid:48) = P −1AP Y + P −1f (t) ⇒ Y (cid:48) = DY + P −1f (t).

Dessa forma, Y (cid:48) = DY + g(t), onde g(t) = P −1f (t).

Com a diagonalização obtemos n equações lineares de primeira ordem da forma

y(cid:48)
i = λiyi + gi(t)

(cid:90)

(4.1.2)

gi(t)e−λitdt + cieλit é a solução
com i = 1, 2, . . . , n. Pela Proposição (3.2.1), yi(t) = eλit
geral de cada equação em (4.1.2). Finalmente a solução da equação (4.1.1) é obtida através da

equação X = P Y , onde

y1

Y =


 .
 4 2
 3et
 X +
. Assim, obtemos P =
 2
 e K2 =

 por diagonalização.
 1

y2
...
yn

2 1

Exemplo 4.1.3. Resolva X(cid:48) =

λ2 = 5, K1 =

 1

5

2
5

−2
5

1
5

et

−2

 1
. Aplicando-se a substituição X = P Y e
 3et

 1

P −1f (t) =

−2
5

1

5

2
5

1
5

 =

 1

5et
5et

7



et

Solução. Os autovalores e autovetores correspondentes da matriz de coeﬁcientes são λ1 = 0,

 e P −1 =

2
−2 1

CAPÍTULO 4. APLICAÇÕES

o sistema desacoplado é

Y (cid:48) =

 0 0

 Y +

 1

63

 .

0 5
As soluções das duas equações diferenciais y(cid:48)
y1 = 1

5et + c1 e y2 = −7

20 et + c2e5t. Portanto, a solução do sistema original é

1 = 1

2 = 5y2 + 7

5et são, respectivamente,

7

5et
5et
5et e y(cid:48)

X = P Y =

 .

5et + c1

−7
20 et + c2e5t

2 et + c1 + 2c2e5t
4 et − 2c1 + c2e5t
−3

 1

2
−2 1

X = c1

 1
 + c2
 1

−2

 2

1

 =
 −1
 1
 e5t −

2

3
4

 et.

Escrevendo de maneira usual utilizando-se vetores colunas, temos

4.1.3 Sistemas de Equações Diferenciais como Modelos Matemáticos

É natural tentar descrever fenômenos reais através de expressões matemáticas, esses

fenômenos reais podem ser da Física, Química, Biologia, Economia, entre outros. Tais descri-

ções são chamadas de modelos matemáticos.
Problema 1. Considere inicialmente três tanques A, B e C cada um com 100 galões de sal-
moura. Os líquidos bem misturados são bombeados entre os tanques conforme a ﬁgura se-
guinte. Sejam x1(t), x2(t) e x3(t) a quantidade de sal (medida em libras) nos tanques A, B e C

Figura 4.1: Tanques com galões de salmoura

no instante t, respectivamente. A taxa a qual x1(t), x2(t) e x3(t) varia é dada por,

d(xi)

dt

= Te − Ts

para todo i ∈ {1, 2, 3}.

CAPÍTULO 4. APLICAÇÕES

64

A taxa de entrada de sal Te (em libras por min) é igual a taxa de entrada de salmoura de
sal (em galão por min) multiplicado pela concentração de sal no ﬂuxo de entrada (em libras por

galão).

Já a taxa de saída de sal Ts (em libras por min) é igual a taxa de saída de salmoura (em

galão por min) multiplicado pela concentração de sal no ﬂuxo de saída (em libras por galão).

Dos galões A, B e C obtemos as seguintes equações diferenciais:

dx1
dt

= (4 gal/min)(0 lb/gal) + (2 gal/min)(

x2
100

lb/gal) − (6 gal/min)(

x1
100

lb/gal)

dx1
dt

=

1
50

x2 − 3
50

x1;

(4.1.3)

dx2
dt

= (6 gal/min)(

x1
100

lb/gal) + (1 gal/min)(

x3
100

lb/gal) − (7 gal/min)(

x2
100

lb/gal)

dx2
dt

=

3
50

x1 +

1
100

x3 − 7
100

x2;

(4.1.4)

dx3
dt

= (5 gal/min)(

x2
100

lb/gal) − (1 gal/min)(

x3
100

lb/gal) − (4 gal/min)(

x3
100

lb/gal)

dx3
dt

=

1
20

x2 − 1
20

x3.

(4.1.5)

Das equações anteriores temos,

100x2



 x1

x2

x3

 .


 x(cid:48)
 =

1

2

x(cid:48)
x(cid:48)

3



x(cid:48)
1 = 1
x(cid:48)
2 = 3
x(cid:48)
3 = 1

50x2 − 3
50x1 + 1
20x2 − 1

50x1
100x3 − 7
20x3

−3
50

3
50

0

1
50
−7
100

1
20

0

1

100
−1
20

CAPÍTULO 4. APLICAÇÕES

65

Para achar as soluções da equação x(cid:48) = Ax devemos primeiramente encontrar os auto-
50 e, também, os

valores e autovetores. Sendo assim, encontramos λ1 = −1
autovetores associados

20 e λ3 = −1

10 , λ2 = −1

 48

100
−11
10

1

 ; v2 =



 29

57
100

100

1

 .

v1 =

Segue,

1

100
−11
10

−18
100
−6
10

 ; v3 =
 e
 48
 e

 e
 29

−18
100
−6
10

57
100

100

1

1

1

−1
10 t;

−1
20 t;

−1
50 t.

x1 = v1eλ1t =

x2 = v2eλ2t =

x3 = v3eλ3t =

Portanto, a solução geral é dada por x(t) = c1x1 + c2x2 + c3x3.
Problema 2. Considere o circuito mostrado na ﬁgura seguinte contendo um indutor (L), um
resistor (R) e um capacitor (C).

Figura 4.2: Circuito elétrico

Considerando a malha da esquerda, temos pela segunda lei de Kirchhoff que a voltagem

aplicada E(t) em uma malha fechada deve ser igual a soma das quedas de voltagem, sendo

CAPÍTULO 4. APLICAÇÕES

66

assim obtemos a seguinte equação diferencial

E(t) = L

di1
dt

+ i2R.

Agora considere a malha da esquerda, temos a equação

q3 − i2R = 0 ⇒ q3 − CRi2 = 0

1
C

e derivando em função de t, temos

dq3
dt

− CR

di2
dt

= 0.

Como dq

dt = i e i1 = i2 + i3 segue que:

i3 − CR

di2
dt

= 0 ⇒

i1 − i2 − CR

= 0 ⇒

di2
dt

CR

di2
dt

+ i2 − i1 = 0.

Considere E(t) = 0 e obtemos o seguinte sistema de equações diferenciais.

1 + i2R = 0
2 + i2 − i1 = 0

CRi(cid:48)

 Li(cid:48)
 i(cid:48)
 0 − R
 i(cid:48)
 =
 .
 0 − R

1 = − R
L i2
CR i1 − 1
i(cid:48)
2 = 1

CR − 1

i(cid:48)

CR

1

2

L

1

L

1

CR − 1

CR

⇒

CR i2

 i1

i2



Ou seja,

ou ainda i(cid:48) = Ai, onde A =

Calculando os autovalores λ1 e λ2 da matriz A e os autovetores associados v1 e v2
podemos obter i1 = v1eλ1t e i2 = v2eλ2t e, consequentemente, a solução geral do sistema dada

CAPÍTULO 4. APLICAÇÕES

por i(t) = c1i1 + c2i2.

67

4.2

Identiﬁcação de Cônicas

Uma equação quadrática nas variáveis x e y tem a forma

ax2 + bxy + cy2 + dx + ey + f = 0,

onde a, b, c, d, e e f são números reais, com a, b e c não simultaneamente nulos. Esta equação
representa uma (seção) cônica, por poder ser obtida da interseção de um cone circular com
um plano. As cônicas mais importantes são elipses, hipérboles e parábolas, que são chamadas
de cônicas não degeneradas. As outras que incluem um único ponto, um par de retas, são
chamadas cônicas degeneradas.

Nesta seção, não temos o objetivo de introduzir cônicas, apenas veremos como a dia-

gonalização de matrizes simétricas pode ser usada na identiﬁcação das cônicas cujas equações

não estão na forma padrão.

Vejamos o seguinte exemplo.

Exemplo 4.2.1. Considere a cônica C cuja equação é

5x2 − 4xy + 8y2 − 36 = 0.

Esta equação pode ser escrita como,

X tAX − 36 = 0,

onde

e

A =

 5 −2
 .
 x

−2

8



X =

y

CAPÍTULO 4. APLICAÇÕES

68

O polinômio característico de A é dado por

PA(t) = det(tI2 − A) = det

 t − 5

2

2
t − 8

 = t2 − 13t + 36.

Logo, os autovalores de A são t1 = 4 e t2 = 9. Os autovetores associados a t1 = 4 são as

soluções não-nulas do sistema −1

2
2 −4

 x

y

 =

 0

0

 ⇒ x = 2y,

cuja solução é V1 = {(2γ, γ); γ ∈ R}. Assim v1 = (2, 1) é uma base para V1, pois gera V1 é
L.I. e w1 = v1(cid:107)v1(cid:107) = ( 2√
) é uma base ortogonal para V1. Os autovetores associados a t2 = 9
são as soluções não-nulas do sistema

, 1√

5

5

 4 2

2 1

 x

y

 =

 0

0

 ⇒ y = −2x,

cuja solução é V2 = {(−γ, 2γ); γ ∈ R}. Assim v2 = (−1, 2) é uma base para V2, pois gera V2
é L.I. e w2 = v2(cid:107)v2(cid:107) = ( −1√

) é uma base ortogonal para V2. Portanto,

, 2√

5

5

D = P tAP

onde,

e

D =

P = [w1, w2] =



 4 0
 2√

0 9

5
1√
5

Fazendo a mudança de variáveis X = P X(cid:48), onde X(cid:48) =

obtemos

−1√
5
2√
5

 .
 na equação X tAX − 36 = 0,
 x(cid:48)

y(cid:48)

(P X(cid:48))tAP X(cid:48) − 36 = 0 ⇒

(X(cid:48))t(P tAP )X(cid:48) − 36 = 0 ⇒

CAPÍTULO 4. APLICAÇÕES

69

(X(cid:48))tDX(cid:48) − 36 = 0 ⇒

4(x(cid:48))2 + 9(y(cid:48))2 − 36 = 0 ⇒

(x(cid:48))2
9

(y(cid:48))2
4

+

= 1

que é a equação de uma elipse cujo esboço é mostrado na ﬁgura seguinte.

Figura 4.3: Elipse

Generalizaremos, a seguir, o procedimento utilizado no exemplo anterior.

Considere a equação

ax2 + bxy + cy2 + dx + ey + f = 0,

com a, b, c, d, e, f ∈ R, sendo b (cid:54)= 0 e a e c não simultaneamente nulos. Em primeiro lugar,
representando, como já é usual, vetores (x, y) do plano na forma de matriz coluna

 ,

X =

 x

y

CAPÍTULO 4. APLICAÇÕES

70

notamos que qualquer equação do segundo grau ax2 + bxy + cy2 + dx + ey + f = 0 pode ser

reescrita na forma matricial X tAX + BX + f = 0 para alguma matriz simétrica A. De fato,

para isso, note que

ax2 + bxy + cy2 = ax2 +

b
2

xy +

b
2

xy + cy2 ⇒

ax2 + bxy + cy2 = x(ax +

ax2 + bxy + cy2 =

ax2 + bxy + cy2 =

de modo que a equação do segundo grau ﬁca representada em forma matricial por

b
2

b
2

cy + b

2y
2x

x) ⇒

y) + y(cy +

(cid:105) ax + b
 ⇒
 ,
(cid:105) a b
 x
 + f = 0.
(cid:105) x

d e

b
2

2

c

y

y

x y

x y

 +

(cid:104)

(cid:104)
(cid:104)
 x
(cid:105)

y

2

(cid:105) a b
 e B =
(cid:104)

b
2

c

x y

(cid:104)
 a b

2

b
2

c

Denotando, A =

d e

, temos a equação matricial X tAX + BX + f = 0.

A matriz A é uma matriz simétrica, logo pode ser diagonalizada através de uma matriz ortogonal

P , ou seja,

onde D é a matriz diagonal

D = P tAP,

 t1

0

0 t2

 ,

D =

t1 e t2 sendo os autovalores de A. Fazendo a mudança de coordenadas X(cid:48) = P tX, de modo
que X = P X(cid:48), obtemos a equação matricial correspondente no novo sistema de coordenadas:

X tAX + BX + f = 0 ⇒

(P X(cid:48))tAP X(cid:48) + BP X(cid:48) = −f ⇒

(X(cid:48))t(P tAP )X(cid:48) + BP X(cid:48) = −f ⇒

CAPÍTULO 4. APLICAÇÕES

71

(X(cid:48))tDX(cid:48) + (BP )X(cid:48) = −f.

(cid:104)

d(cid:48) e(cid:48) (cid:105)

Chamando a(cid:48) = t1, b(cid:48) = t2 e BP =
(BP )X(cid:48) = −f corresponde a equação algébrica

, segue que a equação matricial (X(cid:48))tDX(cid:48) +

a(cid:48)(x(cid:48))2 + b(cid:48)(y(cid:48))2 + d(cid:48)x(cid:48) + e(cid:48)y(cid:48) = −f.

Portanto, existe um sistema de coordenadas (x(cid:48), y(cid:48)), onde a equação ax2 + bxy + cy2 + dx +
ey + f = 0 tem a forma

a(cid:48)(x(cid:48))2 + b(cid:48)(y(cid:48))2 + d(cid:48)x(cid:48) + e(cid:48)y(cid:48) = −f,

onde a(cid:48) = t1, b(cid:48) = t2 são autovalores de

2

 a b
 .
 x
 , X =

b
2

c

y

A =

 x(cid:48)

y(cid:48)

 e P é uma matriz ortogonal (P −1 =

Mais ainda, X = P X(cid:48), onde X(cid:48) =

P t).

Vejamos mais um exemplo.

Exemplo 4.2.2. Considere a equação

4x2 − 20xy + 25y2 − 15x − 6y = 0,

cuja forma matricial é

onde

e

X tAX + BX = 0,

 4 −10

(cid:105)
(cid:104) −15 −6

−10

25

.

A =

B =

Os autovalores de A são t1 = 0 e t2 = 29. Os autovetores associados a t1 = 0 são as soluções

CAPÍTULO 4. APLICAÇÕES

72

não-nulas do sistema

 −4

10
10 −25

 x

y

 =

 0

0

 ⇒ y =

2
5

x,

cuja solução é V1 = {(γ, 2
é L.I. e w1 = v1(cid:107)v1(cid:107) = ( 5√
t2 = 29 são as soluções não-nulas do sistema

29

5γ); γ ∈ R}. Assim v1 = (1, 2
2√
29

5) é uma base para V1, pois gera V1
) é uma base ortogonal para V1. Os autovetores associados a

,

 25 10

10

4

 x

y

 =

 0

0

 ⇒ 2y = −5x,

cuja solução é V2 = {(−γ, 5
V2 é L.I. e w2 = v2(cid:107)v2(cid:107) = ( −2√
ortonormais correspondentes são, respectivamente

5√
29

2γ); γ ∈ R}. Assim v2 = (−1, 5

2) é uma base para V2, pois gera
) é uma base ortogonal para V2. Portanto, os autovetores

29

,

(cid:18) 5√

,

2√
29

29

(cid:19)

(cid:18) −2√

,

29

(cid:19)

.

,

5√
29

Logo, uma matriz ortogonal que diagonaliza A é

 5√

29
2√
29

 .

−2√
29
5√
29

P =

Substituindo X(cid:48) = P tX e X = P X(cid:48), temos que

X tAX + BX = 0 ⇒

(P X(cid:48))tAP X(cid:48) + BP X(cid:48) = 0 ⇒

(X(cid:48))t(P tAP )X(cid:48) + BP X(cid:48) = 0 ⇒

(X(cid:48))tDX(cid:48) + (BP )X(cid:48) = 0 ⇒

(cid:104)

x(cid:48) y(cid:48) (cid:105) 0

0

0 29

 x(cid:48)

y(cid:48)

 +

(cid:104) −15 −6

(cid:105) 5√

29
2√
29

 x(cid:48)

y(cid:48)

 = 0

−2√
29
5√
29

 =

(cid:105) 5√
(cid:104) −15 −6
 +
0 29y(cid:48) (cid:105) x(cid:48)
(cid:104)

29
2√
29

y(cid:48)

−2√
29
5√
29

(cid:104) −3

29 0

√

(cid:104) −3
(cid:105) x(cid:48)

y(cid:48)

,

(cid:105)
 = 0,

√
29 0

e como

temos

o que implica em

ou

CAPÍTULO 4. APLICAÇÕES

73

√
29(y(cid:48))2 − 3

29x(cid:48) = 0,

√

29
3

(y(cid:48))2,

x(cid:48) =

uma parábola, portanto. Um esboço do gráﬁco desta parábola é feito a seguir.

Figura 4.4: Parábola

Referências

[1] Boldrini, José Luiz. Figueiredo, Vera Lúcia. Wetzler, Henry G. Álgebra Linear. São

Paulo, Editora Harbra.

[2] Boyce, William E. Diprima, Richard C. Equações Diferenciais Elementares e Proble-

mas de valores de contorno. Rio de Janeiro, Editora LTC, 2010.

[3] Brauer, Fred. Nohel, John A. The Qualitative Theory of Ordinary Differential Equations

An Introduction. New York, Dover Publications, 1969.

[4] Figueiredo, Djairo Guedes. Neves, Aloisio Freiria. Equações Diferenciais Aplicadas.

Rio de Janeiro, IMPA, 2009.

[5] Hefez, Abramo. Fernandez, Cecília de Souza. Introdução à Álgebra Linear (Coleção

PROFMAT). Rio de Janeiro: SBM, 2016.

[6] Hoffman, K. Kunze, R. Álgebra Linear. 2a Edição. Editora LTC, Rio de Janeiro, 1979.

[7] Lang, Serge. Introduction to Linear Algebra. Undergraduate Texts in Mathematics,

Springer, 1986.

[8] Lima, Elon Lages. Álgebra Linear. Rio de Janeiro, IMPA, 2009.

[9] Lima, Elon Lages. Espaços métricos. Rio de Janeiro, IMPA, 2005.

[10] Steinbruch, Alfredo. Winterle, Paulo. Álgebra Linear. São Paulo, Editora Mc Grew-Hill,

1987.

74

REFERÊNCIAS

75

[11] Zill, Dennis G. Equações Diferenciais com aplicações em modelagem. São Paulo, Edi-

tora Thomson Learning, 2003.

