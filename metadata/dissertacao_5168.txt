UNIVERSIDADE DO ESTADO DE MATO GROSSO 

CAMPUS DE SINOP 

FACULDADE DE CIÃŠNCIAS EXATAS E TECNOLÃ“GICAS 

MESTRADO PROFISSIONAL EM MATEMÃTICA EM REDE 

NACIONAL - PROFMAT 

RÃ”MULO FANGUEIRO PEREIRA 

METODOLOGIA PARA A FUSÃƒO DE IMAGENS DIGITAIS BASEADA NAS 
EQUAÃ‡Ã•ES DE COLINEARIDADE 

SINOP-MT 
2021 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
RÃ”MULO FANGUEIRO PEREIRA 

METODOLOGIA PARA A FUSÃƒO DE IMAGENS DIGITAIS BASEADA NAS 
EQUAÃ‡Ã•ES DE COLINEARIDADE 

DissertaÃ§Ã£o apresentada ao Programa de 
Mestrado  Profissional  em  MatemÃ¡tica  em 
do 
Rede  Nacional 
departamento 
da 
Universidade  Estadual  do  Mato  Grosso  â€“ 
UNEMAT,  como  requisito  parcial  para 
obtenÃ§Ã£o  do  grau  de  Mestre  em 
MatemÃ¡tica. 

â€“  PROFMAT, 
de  MatemÃ¡tica 

Prof. Dr. Giovane Maia do Vale 
Orientador 

Prof. Dr. JoÃ£o Gabriel Ribeiro 
Coorientador 

Prof. Me. Diogo Albino de Queiroz 
Coorientador 

SINOP 
2021 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
1 

AGRADECIMENTOS 

Em primeiro lugar, eu agradeÃ§o a Deus, o Criador de tudo e de todos, razÃ£o pela qual 

eu existo e estou aqui. 

AgradeÃ§o tambÃ©m especialmente ao meu orientador, professor Doutor Giovane Maia 

do  Vale,  que  representou  para  mim  o  grande  esteio  na  construÃ§Ã£o  deste  trabalho, 

sendo extremamente compreensivo comigo em um momento de dificuldade pessoal 

em minha vida. Foi graÃ§as a ele que este trabalho pÃ´de ser realizado e apresentado. 

TambÃ©m agradeÃ§o Ã  minha famÃ­lia, meu pai, minha mÃ£e, minhas irmÃ£s e meus avÃ³s, 

que  me  deram  todas  as  condiÃ§Ãµes  para  que  eu  pudesse  estudar  e  realizar  este 

trabalho, sendo eles parte fundamental da minha vida. 

AgradeÃ§o tambÃ©m a minha atual namorada LucÃ©lia Santana Fialho, pela ajuda e apoio 

incondicional que ela tem me dado em tudo o que eu venho fazendo, sendo ela um 

porto seguro para mim. 

Por  fim,  agradeÃ§o  tambÃ©m  a  todos  os  professores  integrantes  do  programa  de 

Mestrado Profissional em MatemÃ¡tica em nÃ­vel nacional - PROFMAT, pelo Campus 

de  Sinop  â€“  MT,  bem  como  todos  os  meus  colegas  de  mestrado,  que  foram 

fundamentais no processo de formaÃ§Ã£o e conclusÃ£o deste curso. 

 
 
 
 
 
 
2 

RESUMO 

Este  trabalho  versa  sobre  a  aplicaÃ§Ã£o  da  MatemÃ¡tica  como  a  ferramenta  que 
fundamenta o processamento de imagens capturadas por satÃ©lite. Nestes termos, o 
objetivo deste trabalho foi desenvolver um software livre que efetuasse o processo de 
fusÃ£o de imagens via mÃ©todo de fusÃ£o IHS. Tal software, implementado em C/C++, 
se  baseou  no  algoritmo  de  retificaÃ§Ã£o  de  imagens  que  incorpora  as  equaÃ§Ãµes  de 
colinearidade (Geometria Projetiva). Logo, se efetuou alteraÃ§Ãµes no referido algoritmo 
e nas equaÃ§Ãµes de colinearidade de modo a gerar o referido software. Neste caso, o 
algoritmo de retificaÃ§Ã£o de imagens modificado foi utilizado como uma ferramenta de 
reamostragem  de 
foram 
provenientes  do  satÃ©lite  sino-brasileiro  CBERS  4.  Foram  utilizadas  as  imagens 
multiespectrais (R, G e B) do sensor MUX - CÃ¢mera Multiespectral Regular, com 
de  resoluÃ§Ã£o  espacial,  e  a  imagem  pancromÃ¡tica  do  sensor  PAN  -  CÃ¢mera 
20 ğ‘šğ‘š
PancromÃ¡tica e Multiespectral, com resoluÃ§Ã£o espacial de 
. Por meio do software QGIS 
foram efetuados dois recortes nas imagens CBERS 4. Os recortes foram utilizados nos testes 
e diziam respeito Ã s regiÃµes dos reservatÃ³rios de Ilha Solteira e Engenheiro Souza Dias 
(JupiÃ¡),  localizados  entre  os  estados  de  SÃ£o  Paulo  e  Mato  Grosso  do  Sul.  Os 
resultados  (imagens  fusionadas),  apÃ³s  serem  submetidos  Ã   processo  de  realce  de 
contraste, se mostraram satisfatÃ³rios, indicando o sucesso da pesquisa. 

imagens  multiespectrais.  As 

imagens  de  entrada 

5 ğ‘šğ‘š

Palavras â€“ chave: satÃ©lite, MatemÃ¡tica, imagens, Sensoriamento Remoto, fusÃ£o de 
imagens. 

 
 
 
 
 
 
 
 
 
 
 
3 

ABSTRACT 

This work deals with the application of Mathematics as fundamental tool for processing 
of  images  captured  by  satellite.  In  these  terms,  the  objective  of  this  work  was  to 
develop a free software that could carry out image fusion process via the IHS fusion 
method. Such software, implemented in C / C ++, was based on image rectification 
algorithm  that  incorporates  collinearity  equations  (Projective  Geometry).  Therefore, 
changes were made in the referred algorithm and in collinearity equations in order to 
generate the referred software. In this case, the modified image rectification algorithm 
was used as a multispectral image resampling tool. The input images were from the 
Sino-Brazilian  satellite  CBERS  4.  Multispectral  images  (R,  G  and  B)  from  the  MUX 
sensor - Regular Multispectral Camera, with 20 m spatial resolution, and panchromatic 
image from the PAN sensor - Multispectral and Panchromatic Camera, with a spatial 
resolution  of  5  m,  were  used.  Using  the  QGIS  software, two  cutouts  were made  on 
CBERS 4 images. The cutouts were used in tests and related to the regions of Ilha 
Solteira and Engenheiro Souza Dias (JupiÃ¡) reservoirs, located between the states of 
SÃ£o Paulo and Mato Grosso do Sul. The results (fused images), after being submitted 
through  the  contrast  enhancement  process,  are  defined  as  satisfactory,  indicating 
research success. 

Keywords: satellite, Mathematics, images, Remote Sensing, images fusion. 

 
 
 
 
 
 
4 

ÃNDICE DE FIGURAS 

Figura 1: Espectro visÃ­vel da luz. .............................................................................. 13 

Figura 2: Sistemas de cores primÃ¡rias: RGB e CMYK. ............................................ 14 

Figura 3: Esquema do cubo de cores RGB. ............................................................. 15 

Figura 4: TriÃ¢ngulo de representaÃ§Ã£o matiz/saturaÃ§Ã£o ............................................ 16 

Figura 5: Estrutura piramidal intensidade/matiz/saturaÃ§Ã£o ....................................... 17 

Figura 6: EsquematizaÃ§Ã£o da disposiÃ§Ã£o dos pixels no sistema cartesiano. ........... 20 

Figura 7: RepresentaÃ§Ã£o de uma matriz P de M linhas por N colunas. .................... 21 

Figura 8: Exemplo de representaÃ§Ã£o matricial de pixels em tons de cinza. ............. 21 

Figura 9: Exemplo de composiÃ§Ã£o colorida. ............................................................. 22 

Figura 10: IlustraÃ§Ã£o de resoluÃ§Ãµes espaciais distintas. .......................................... 24 

Figura  11:  IlustraÃ§Ã£o  mostrando  um  esquema  de  resoluÃ§Ã£o  espectral.  Ã€  esquerda, 

resoluÃ§Ã£o multiespectral; Ã  direita, resoluÃ§Ã£o hiper espectral. .................................. 25 

Figura  12:  RepresentaÃ§Ã£o  de  uma  mesma  imagem  em  diferentes  resoluÃ§Ãµes 

radiomÃ©tricas. ............................................................................................................ 26 

Figura 13: Esquema do processo de fusÃ£o de imagens por mÃ©todo IHS. ............... 28 

Figura 14: Exemplo de fusÃ£o IHS: a) Imagem pancromÃ¡tica (tons de cinza) com alta 

resoluÃ§Ã£o;  b)  Imagem  colorida  (RGB)  com  baixa  resoluÃ§Ã£o;  e  c)  Resultado  final  da 

fusÃ£o. ........................................................................................................................ 28 

Figura 15: Esquema de distribuiÃ§Ã£o de angulaÃ§Ãµes e colinearidade em fotogrametria.

 .................................................................................................................................. 29 

Figura  16:  Etapas  do  algoritmo  de  fusÃ£o  de  imagens,  materializadas  no  software 

â€œFusionâ€. .................................................................................................................... 43 

Figura 17: Interface do Fusion: Prompt de Comando de MS-DOS. ......................... 44 

Figura 18: Tabela contendo os dados e especificaÃ§Ãµes da cÃ¢mera MUX, responsÃ¡vel 

pela captaÃ§Ã£o das bandas 

, 

 e 

. ......................................................................... 49 

Figura 19: Tabela contendo os dados e especificaÃ§Ãµes da cÃ¢mera PAN, responsÃ¡vel 

ğ‘…ğ‘…

ğºğº

ğµğµ

pela captaÃ§Ã£o da imagem pancromÃ¡tica. .................................................................. 49 

Figura  20:  AlocaÃ§Ã£o  dos  reservatÃ³rios  hidrelÃ©tricos  -  RetÃ¢ngulo  Superior: 

ReservatÃ³rio de Ilha Solteira; RetÃ¢ngulo Inferior: ReservatÃ³rio Engenheiro Souza Dias 

(JupiÃ¡). ...................................................................................................................... 50 

Figura 21: Usina de Ilha Solteira: (a) ComposiÃ§Ã£o colorida â€“ 

; (b) Componente 

vermelha; (c) Componente verde; (d) Componente azul. .......................................... 51 

20 ğ‘šğ‘š

 
 
5 

Figura  22:  Usina  de  JupiÃ¡:  (a)  ComposiÃ§Ã£o  colorida  â€“ 

;  (b)  Componente 

vermelha; (c) Componente verde; (d) Componente azul. .......................................... 51 

20 ğ‘šğ‘š

Figura 23: Imagens PancromÃ¡ticas: (a) RegiÃ£o da Usina de Ilha Solteira; (b) RegiÃ£o 

da Usina de JupiÃ¡ ...................................................................................................... 52 

Figura 24: ComposiÃ§Ãµes coloridas: (a) RegiÃ£o da Usina de Ilha Solteira; (b) RegiÃ£o 

da Usina de JupiÃ¡. ..................................................................................................... 54 

Figura  25:  Imagens  coloridas  reamostradas  relativas  Ã s  regiÃµes  de  Ilha  Solteira  (Ã  

esquerda) e de JupiÃ¡ (Ã  direita). ................................................................................ 55 

Figura 26: Imagens coloridas reamostradas com realce, relativas Ã s regiÃµes de Ilha 

Solteira (Ã  esquerda) e de JupiÃ¡ (Ã  direita). .............................................................. 55 

Figura 27: Imagens fusionadas: (a) RegiÃ£o da Usina de Ilha Solteira; (b) RegiÃ£o da 

Usina de JupiÃ¡. .......................................................................................................... 56 

Figura 28: Imagens fusionadas resultantes do experimento, com realce  de contraste 

linear, relativas Ã s regiÃµes de Ilha Solteira (Ã  esquerda) e de JupiÃ¡ (Ã  direita). ........ 57 

Figura  29:  Imagem  fusionada  realÃ§ada:  (a)  RegiÃ£o  da  Usina  de  Ilha  Solteira;  (b) 

RegiÃ£o da Usina de JupiÃ¡. ........................................................................................ 58 

Figura  30:  ComparaÃ§Ã£o  de  detalhes  (regiÃ£o  de  Ilha  Solteira):  (a)  Imagem  colorida 

inicial; (b) Imagem fusionada ..................................................................................... 59 

Figura 31: ComparaÃ§Ã£o de detalhes (regiÃ£o de JupiÃ¡): (a) Imagem colorida inicial; (b) 

Imagem fusionada ..................................................................................................... 59 

 
 
 
 
 
 
 
 
6 

SUMÃRIO 

1 

IntroduÃ§Ã£o ........................................................................................................... 8 

1.1  Estado da Arte e Justificativa ......................................................................... 8 

1.2  Objetivos ...................................................................................................... 10 

1.3  Estrutura do Trabalho ................................................................................... 11 

2  FUNDAMENTAÃ‡ÃƒO TEÃ“RICA ......................................................................... 12 

2.1  A Luz e o Olho Humano ............................................................................... 12 

2.2  As Cores PrimÃ¡rias ...................................................................................... 13 

2.3 

Imagens Digitais ........................................................................................... 19 

2.4  ResoluÃ§Ãµes de Imagens ............................................................................... 23 

2.4.1  ResoluÃ§Ã£o Espacial ..................................................................... 23 

2.4.2  ResoluÃ§Ã£o Espectral .................................................................... 24 

2.4.3  ResoluÃ§Ã£o RadiomÃ©trica .............................................................. 25 

2.4.4  ResoluÃ§Ã£o Temporal .................................................................... 27 

2.5  FusÃ£o de Imagens via MÃ©todo IHS .............................................................. 27 

2.6  EquaÃ§Ãµes de Colinearidade ......................................................................... 29 

2.7  RetificaÃ§Ã£o de imagens ................................................................................ 32 

3  Metodologia ....................................................................................................... 36 

3.1  PreÃ¢mbulo .................................................................................................... 36 

3.2  Delineamento MetodolÃ³gico da Pesquisa .................................................... 38 

3.2.1  AlteraÃ§Ãµes Ã s equaÃ§Ãµes de colinearidade ................................... 38 

3.2.2  O algoritmo de fusÃ£o de imagens ................................................ 42 

4  RESULTADOS E ANÃLISES ............................................................................. 48 

4.1  Dados de Entrada ........................................................................................ 48 

4.2  Resultados Experimentais e AnÃ¡lises .......................................................... 53 

4.2.1  Procedimentos preliminares ........................................................ 53 

4.2.2  A fusÃ£o das imagens ................................................................... 55 

 
7 

5  CONCLUSÃ•ES E RECOMENDAÃ‡Ã•ES: ........................................................... 61 

5.1  ConclusÃµes .................................................................................................. 61 

5.2  RecomendaÃ§Ãµes .......................................................................................... 62 

REFERÃŠNCIAS BIBLIOGRÃFICAS ......................................................................... 63 

 
 
 
 
 
 
8 

1 

INTRODUÃ‡ÃƒO 

1.1  Estado da Arte e Justificativa 

No  universo  do  Sensoriamento  Remoto  e  da  Fotogrametria  sÃ£o  aplicadas 

tÃ©cnicas  e  metodologias  intimamente  ligadas  Ã   MatemÃ¡tica  e  que  envolvem,  por 

exemplo,  vetores,  equaÃ§Ãµes,  Geometria,  ajustamento  de  observaÃ§Ãµes  e  conceitos 

espaciais diversos (GONZALEZ e WOODS, 2010). AtÃ© mesmo programas com fins 

cartogrÃ¡ficos, como o Google Maps e o Google Earth, que incorporam tecnologia GPS 

(Global Positioning System) e que fazem uso das metodologias e tÃ©cnicas associadas 

ao  Sensoriamento  Remoto  e  Ã   Fotogrametria,  possuem  em  sua  constituiÃ§Ã£o  uma 

enorme gama de teorias matemÃ¡ticas que os tornam operacionais. 

Ã‰ sabido ainda que, em Sensoriamento Remoto e Fotogrametria os sensores 

possibilitam  a  captaÃ§Ã£o  de  radiaÃ§Ã£o  eletromagnÃ©tica  refletida  pelos  alvos  e  este 

conteÃºdo dÃ¡ origem a dados (imagens digitais) que, quando processados, resultam 

em informaÃ§Ãµes abrangentes sobre as Ã¡reas de interesse (CRÃ“STA, 1992). Assim, 

cabe informar que, as metodologias e tÃ©cnicas ligadas ao Sensoriamento Remoto e Ã  

Fotogrametria  encontram-se  impregnadas  de  elementos  MatemÃ¡ticos,  pois  as 

imagens  digitais,  que  sÃ£o  as  principais  fontes  de  dados  destas  ciÃªncias,  estÃ£o 

intrinsecamente  ligadas  Ã   MatemÃ¡tica  e,  por  este  motivo,  surge  a  demanda  por 

algoritmos e modelos matemÃ¡ticos para o seu processamento e para a subsequente 

geraÃ§Ã£o de informaÃ§Ãµes (CRÃ“STA, 1992). Eis aÃ­ a importÃ¢ncia das imagens digitais, 

as quais se encontram no escopo da pesquisa aqui descrita. 

Neste contexto, ao se buscar por imagens digitais orbitais, por exemplo, no site 

do INPE â€“ Instituto Nacional de Pesquisas Espaciais (http://www.dgi.inpe.br/CDSR/), 

verifica-se  uma  gama  razoÃ¡vel  de  satÃ©lites  e  sensores  disponÃ­veis.  De  modo  mais 

geral,  novos  satÃ©lites  destinados  ao  imageamento  sÃ£o  lanÃ§ados  regularmente, 

aumentando  consideravelmente  a  disponibilidade  de  dados  orbitais.  No  entanto, 

apesar  da  disponibilidade  de  imagens,  o  custo  de  produtos  que  oferecem  alta 

resoluÃ§Ã£o  e  elevado  detalhamento,  por  vezes,  Ã©  proibitivo  para  uma  parcela  da 

comunidade das geociÃªncias. Logo, Ã© comum que se busque uma alternativa viÃ¡vel a 

estes produtos onerosos, conforme o exposto a seguir. 

 
 
 
9 

Ã‰ comum verificar que sensores pancromÃ¡ticos, que imageam o espectro em 

apenas uma banda ou faixa de frequÃªncia (imagens em tons de cinza), dÃ£o origem Ã s 

imagens de mais alta resoluÃ§Ã£o espacial (qualidade do detalhamento da imagem) do 

que os sensores multiespectrais (coloridos) de um mesmo satÃ©lite. Assim, visando um 

melhor  aproveitamento  dos  dados  com  diferentes  resoluÃ§Ãµes  espaciais  e,  muitas 

vezes,  provenientes  de  diferentes  sensores,  alguns  mÃ©todos  de  processamento  de 

imagens  tÃªm  sido  propostos  (VENTURA,  2002).  Estes  mÃ©todos,  genericamente 

conhecidos como â€œmÃ©todos de fusÃ£o de imagensâ€, destinam-se a combinar imagens 

de  diferentes  caracterÃ­sticas  espectrais  e  espaciais  a  fim  de  se  obter  uma  nova 

imagem,  fruto  da  fusÃ£o  de  imagens  pancromÃ¡ticas,  com  boa  resoluÃ§Ã£o  espacial,  e 

imagens multiespectrais que, diferentemente das imagens pancromÃ¡ticas, possuem, 

eventualmente,  pior  resoluÃ§Ã£o  espacial  (LEONARDI  et  al.,  2005).  Ou  seja,  nos 

mÃ©todos de fusÃ£o de imagens, atribui-se cor Ã  uma imagem pancromÃ¡tica de melhor 

resoluÃ§Ã£o  espacial,  utilizando-se  as  cores de  imagens  multiespectrais  de resoluÃ§Ã£o 

espacial inferior (CHAVEZ e BOWELL, 1988). Logo, o resultado Ã© uma imagem com 

boa resoluÃ§Ã£o espacial e informaÃ§Ã£o de cor, a qual era anteriormente inexistente. 

Nestes termos, como a discrepÃ¢ncia de qualidade e de utilidade estabelecida 

entre imagens de satÃ©lite, com e sem coloraÃ§Ã£o, Ã© significativa e considerando a busca 

que  os  interessados  das  Ã¡reas  de  Geoprocessamento,  GeomÃ¡tica  e  Geografia 

(usuÃ¡rios,  pesquisadores  e  professores)  empreendem  por  imagens  com  a  melhor 

qualidade possÃ­vel, verifica-se a importÃ¢ncia da pesquisa teÃ³rica e implementaÃ§Ã£o de 

softwares capazes de gerar imagens digitais que possuam a qualidade almejada por 

tais interessados. 

Cabe informar que jÃ¡ existem algoritmos destinados Ã  fusÃ£o de imagens e que 

estes  sÃ£o  conhecidos,  aprovados  e  consolidados  pela  comunidade  cientÃ­fica 

(LEONARDI et al., 2005). PorÃ©m, segundo Leonardi et al. (2005), dada a diferenÃ§a de 

concepÃ§Ã£o entre tais algoritmos, existe alguma divergÃªncia entre os produtos gerados 

por eles. Logo, Ã© nesse contexto que a pesquisa executada se inseriu. 

Diante  do  exposto  e  considerando  a  importÃ¢ncia  da  MatemÃ¡tica,  bem  como, 

das imagens digitais no Ã¢mbito das GeociÃªncias, buscou-se fazer avanÃ§ar o estado 

da arte no tocante a estes assuntos. Logo, nesta pesquisa efetuou-se alteraÃ§Ãµes a um 

modelo matemÃ¡tico e se criou um novo algoritmo, dando origem, consequentemente, 

a  um  programa  de  computador  destinado  Ã   fusÃ£o  de  imagens  digitais,  baseado  no 

 
10 

processo de retificaÃ§Ã£o de imagens, o qual incorpora em sua concepÃ§Ã£o elementos 

de  Geometria  Projetiva  (equaÃ§Ãµes  de  colinearidade).  Para  se  atingir  tais  metas, 

realizou-se  um  estudo  do  algoritmo  de  retificaÃ§Ã£o  de  imagens  e  das  equaÃ§Ãµes  de 

colinearidade  a  ele  incorporadas.  Diante  do  know-how  adquirido,  efetuou-se 

mudanÃ§as  algorÃ­tmicas  e  matemÃ¡ticas  no  processo  de  modo  a  se  gerar  um 

algoritmo/software de fusÃ£o de imagens baseado na teoria de fusÃ£o de imagens via 

mÃ©todo IHS (Intensity-Hue-Saturation). Cabe especificar que, se optou por utilizar o 

mÃ©todo de fusÃ£o de imagens IHS, devido Ã  qualidade de seus resultados, preconizada 

na literatura, e pelo fato deste ser frequentemente utilizado pela comunidade cientÃ­fica 

(LEONARDI et al., 2005). A anÃ¡lise se deu de forma comparativa entre as imagens 

fusionadas geradas e as respectivas imagens simplesmente reamostradas. 

Por  fim,  especifica-se  que,  em  um  primeiro  momento,  os  resultados  desta 

pesquisa  podem  ser  utilizados  em  aplicaÃ§Ãµes  de  Sensoriamento  Remoto  e 

Fotogrametria, que necessitam de dados confiÃ¡veis e de excelente qualidade espacial 

e espectral. Em adiÃ§Ã£o a isso, por sua facilidade de operaÃ§Ã£o, o sistema criado poderÃ¡ 

ser utilizado tambÃ©m, tanto no Ã¢mbito acadÃªmico, quanto na EducaÃ§Ã£o BÃ¡sica, e.g., 

por professores de Geografia e de HistÃ³ria. 

1.2  Objetivos 

A pesquisa objetivou o estudo e o aprofundamento teÃ³rico relativo Ã s equaÃ§Ãµes 

de colinearidade e ao algoritmo fotogramÃ©trico de retificaÃ§Ã£o de imagens digitais, com 

o  intuito  de  criar  uma  aplicaÃ§Ã£o  computacional  que  efetuasse  a  fusÃ£o  de  imagens 

digitais com base no mÃ©todo IHS de fusÃ£o. A metodologia desenvolvida encontra-se 

no  Ã¢mbito  da  Geometria  Projetiva  e  foi  avaliada  comparativamente  com  relaÃ§Ã£o  Ã s 

imagens reamostradas pelo prÃ³prio sistema implementado. 

A fim de alcanÃ§ar tal objetivo, algumas metas foram estabelecidas e serÃ£o aqui 

detalhadas.  Inicialmente,  procurou-se  estudar  a  teoria  relativa  Ã s  equaÃ§Ãµes  de 

colinearidade,  com  vistas  a  entender  sua  concepÃ§Ã£o  e  parÃ¢metros,  bem  como, 

analisar o algoritmo fotogramÃ©trico de retificaÃ§Ã£o de imagens, objetivando vislumbrar 

sua lÃ³gica de funcionamento e angariar embasamento teÃ³rico para a implementaÃ§Ã£o 

proposta.  Em  seguida,  buscou-se  estudar  a  linguagem  de  programaÃ§Ã£o  C/C++, 

considerando  o  paradigma  procedural  de  implementaÃ§Ã£o  computacional,  para  que, 

 
 
 
11 

com isso, se pudesse projetar e efetuar modificaÃ§Ãµes algorÃ­tmicas e matemÃ¡ticas no 

algoritmo  de  retificaÃ§Ã£o  de  imagens,  objetivando  assim  a  criaÃ§Ã£o  de  um  software 

capaz  de  realizar  a  fusÃ£o  de  imagens.  Por  fim,  a  Ãºltima  meta  foi  efetuar  a 

experimentaÃ§Ã£o  e  posterior  avaliaÃ§Ã£o  comparativa  entre  os  resultados  advindos  do 

novo software proposto. 

1.3  Estrutura do Trabalho 

Este trabalho foi estruturado em 5 capÃ­tulos. ApÃ³s a introduÃ§Ã£o, na qual o tema 

abordado foi contextualizado, chegou-se ao CapÃ­tulo 2, destinado Ã  apresentaÃ§Ã£o dos 

fundamentos  teÃ³ricos  que  embasam  a  metodologia  elaborada  e  executada.  Neste 

capÃ­tulo  se  explana  sobre:  a  luz  e  o  olho  humano,  as  cores  primÃ¡rias,  as  imagens 

digitais, os tipos de resoluÃ§Ãµes de imagens, a fusÃ£o de imagens via mÃ©todo IHS, as 

equaÃ§Ãµes de colinearidade e o algoritmo de retificaÃ§Ã£o de imagens digitais. 

O  CapÃ­tulo  3  destina-se  Ã   apresentaÃ§Ã£o  do  delineamento  metodolÃ³gico 

empreendido e que resultou no software implementado. Neste capÃ­tulo sÃ£o descritos 

os detalhes inerentes Ã  metodologia como, por exemplo, a especificaÃ§Ã£o dos dados 

dos processados (imagens), as alteraÃ§Ãµes matemÃ¡ticas e algorÃ­tmicas para a geraÃ§Ã£o 

do referido programa e os aspectos relativos Ã  implementaÃ§Ã£o do software de fusÃ£o 

de imagens. 

No CapÃ­tulo 4 estÃ£o contidos os resultados obtidos e as anÃ¡lises empreendidas 

sobre tais resultados. 

Ao  final,  tem-se  as  conclusÃµes  advindas  da  pesquisa  realizada.  Ou  seja,  o 

CapÃ­tulo 5 traz as consideraÃ§Ãµes finais e as principais conclusÃµes que se pÃ´de inferir 

partir da realizaÃ§Ã£o do trabalho, bem como, as recomendaÃ§Ãµes relativas a possÃ­veis 

trabalhos futuros. 

 
 
 
 
 
12 

2  FUNDAMENTAÃ‡ÃƒO TEÃ“RICA 

As  seÃ§Ãµes  que  seguem  fundamentam  a  pesquisa  realizada.  Assim,  os 

conteÃºdos  relacionados  Ã   luz  e  ao  olho  humano,  Ã s  cores  primÃ¡rias,  Ã s  imagens 

digitais, aos tipos de resoluÃ§Ãµes de imagens, Ã  fusÃ£o de imagens via mÃ©todo IHS, Ã s 

equaÃ§Ãµes  de  colinearidade  e  ao  algoritmo  de  retificaÃ§Ã£o  sÃ£o  apresentados  e 

brevemente  discutidos  a  fim  de  que  se  tenha  o  arcabouÃ§o  teÃ³rico  necessÃ¡rio  Ã  

execuÃ§Ã£o da pesquisa de acordo com o delineamento metodolÃ³gico previsto. 

2.1  A Luz e o Olho Humano 

Antes de se falar de imagens geradas por sensores embarcados em satÃ©lites, 

faz-se necessÃ¡rio realizar uma abordagem acerca das principais caracterÃ­sticas das 

ondas eletromagnÃ©ticas, alÃ©m do funcionamento do olho humano, com relaÃ§Ã£o a seu 

mecanismo de captaÃ§Ã£o de luz. 

As ondas eletromagnÃ©ticas que compÃµem a luz possuem, como qualquer onda 

de  carÃ¡ter  fÃ­sico,  um  comprimento  de  onda 

  e  uma  frequÃªncia 

,  medida  em 

oscilaÃ§Ãµes  por  segundo,  indicadas  em  hertz  (Hz),  sendo  que  estas  grandezas  sÃ£o 

ğœ†ğœ†

ğ‘“ğ‘“

inversamente  proporcionais  entre  si.  Entre  as  ondas  eletromagnÃ©ticas  de  baixa 

frequÃªncia  (e.  g.,  infravermelho),  podem  ser  citadas  as  ondas  de  rÃ¡dio,  televisÃ£o  e 

micro-ondas.  A  faixa  de  frequÃªncia  correspondente  ao  espectro  eletromagnÃ©tico 

visÃ­vel pelo olho humano varia entre 4.1014 Hz (luz vermelha) e 8.1014 Hz (luz violeta), 

o  que  equivale  a  um 

intervalo  de 

frequÃªncias  relativamente  curto.  Mais 

especificamente, tal faixa corresponde Ã s cores vermelho, alaranjado, amarelo, verde, 

azul,  anil,  violeta  e  suas  variaÃ§Ãµes,  respectivamente,  que  sÃ£o  derivadas  da 

decomposiÃ§Ã£o  da  luz  branca.  Acima  disso  estÃ£o  as  ondas  de  alta  frequÃªncia, 

conhecidas  como  ondas  ultravioleta,  entre  as  quais  se  encontram  o  raio  X  e,  com 

frequÃªncia ainda maior, os raios gama. (GASPAR, 2016).  

Com relaÃ§Ã£o Ã s estruturas do olho humano responsÃ¡veis pela captaÃ§Ã£o de luz, 

estas consistem em pequenos elementos denominados de cones e bastonetes. Os 

cones sÃ£o ativados pela maior presenÃ§a de luz e sÃ£o responsÃ¡veis pela captaÃ§Ã£o das 

cores.  JÃ¡  os  bastonetes,  menos  sensÃ­veis  Ã s  cores,  estÃ£o  presentes  em  uma 

 
 
 
 
13 

quantidade bastante superior Ã  dos cones e possuem a capacidade de captar luz em 

ambientes com baixa luminosidade. Com isso, os bastonetes sÃ£o Ãºteis em ambientes 

de baixa luminosidade, em que o discernimento de formas dos objetos se sobrepÃµe 

ao de cores (GONZALEZ e WOODS, 2010). 

Figura 1: Espectro visÃ­vel da luz. 
Fonte: Gaspar (2016) 

Basicamente,  a  luz  captada  pelo  olho  humano  pode  advir  de  duas  fontes: 

corpos emissores de luz, como o Sol e as lÃ¢mpadas elÃ©tricas, e corpos refletores de 

luz, como a maioria dos objetos visÃ­veis. Considerando esta segunda classe, chega-

se ao Ã¢mbito das imagens digitais produzidas por satÃ©lite que, de modo geral, sÃ£o o 

produto da amostragem e quantizaÃ§Ã£o da radiaÃ§Ã£o eletromagnÃ©tica refletida. 

A Figura 1 ilustra, em destaque, o espectro visÃ­vel que Ã© constituÃ­do por ondas 

com comprimento de onda perceptÃ­veis pelo olho humano. 

2.2  As Cores PrimÃ¡rias 

Quando  se  fala  em  cores  primÃ¡rias,  se  estÃ¡  abordando  basicamente  um 

conjunto de cores determinadas que dÃ¡ origem Ã s demais cores captadas pelo olho 

humano. Neste ponto, cabe especificar que, as cores dizem respeito ao comprimento 

de  onda  da  radiaÃ§Ã£o  eletromagnÃ©tica  refletida  pelos  objetos  e  que  sÃ£o, 

posteriormente, captadas pelo olho humano. Gonzalez e Woods (2010) especificam 

que, para fins de padronizaÃ§Ã£o, a CIE (Commission Internationale de lâ€™Eclairage ou, 

 
 
 
 
em  portuguÃªs,  ComissÃ£o  Internacional  de  IluminaÃ§Ã£o)  determinou,  em  1931,  os 

seguintes valores especÃ­ficos como comprimentos de onda das trÃªs cores primÃ¡rias: 

azul  = 

,  verde  = 

  e  vermelho  = 

.  Assim,  por  exemplo,  um 

14 

objeto  que  reflita  radiaÃ§Ã£o  eletromagnÃ©tica  com  comprimento  de  onda  em  torno  de 

700 ğ‘›ğ‘›ğ‘›ğ‘›

435,8 ğ‘›ğ‘›ğ‘›ğ‘›
546,1 ğ‘›ğ‘›ğ‘›ğ‘›
 serÃ¡ visto como vermelho. 

700 ğ‘›ğ‘›ğ‘›ğ‘›

Complementando  e  corroborando  com  o  acima  descrito,  Gomes  e  Queiroz 

(2001) especificam que as trÃªs cores primÃ¡rias da luz, considerando a percepÃ§Ã£o do 

olho  humano,  sÃ£o  o  vermelho,  o  verde  e  o  azul,  e  que  estas  formam  o  sistema 

conhecido como RGB (Red-Green-Blue, em inglÃªs). Das luzes relativas Ã  estas trÃªs 

cores se originam todas as demais, inclusive a luz branca, quando combinadas (ou 

adicionadas). Por exemplo, o vermelho combinado com o verde, em igual proporÃ§Ã£o, 

forma a cor amarela. De igual modo, o vermelho e o azul formam o magenta e, por 

sua  vez,  o  azul  com  o  verde  resulta  no  ciano.  A  luz  de  cor  branca  Ã©  resultante  da 

combinaÃ§Ã£o, em intensidade mÃ¡xima, das trÃªs cores primÃ¡rias citadas anteriormente. 

Essa combinaÃ§Ã£o vale, tanto para objetos emissores de luz, quanto para os objetos 

refletores. No caso de um objeto refletor, a luz branca (combinaÃ§Ã£o de todas as cores) 

chega  atÃ©  ele  e  as  componentes  da  luz  sÃ£o  absorvidas  por  sua  superfÃ­cie,  com 

exceÃ§Ã£o de uma componente particular que serÃ¡ refletida e que serÃ¡ responsÃ¡vel por 

sua coloraÃ§Ã£o, quando tal objeto for observado pelo olho humano ou imageado. Por 

exemplo, se um objeto Ã© de cor vermelha, significa que ele absorveu as outras faixas 

de cores e refletiu apenas a cor vermelha. 

Figura 2: Sistemas de cores primÃ¡rias: RGB e CMYK. 
Fonte: Confeccionado pelo prÃ³prio autor 

 
 
15 

Outro  sistema  de  cores  primÃ¡rias  muito  utilizado  Ã©  o  sistema  denominado 

CMYK 

(Cyan-Magenta-Yellow-BlacK,  em 

inglÃªs)  correspondente  Ã s  cores 

secundÃ¡rias  do  sistema  anterior,  ciano,  magenta  e  amarelo.  Esse  sistema  Ã© 

normalmente  utilizado  em  impressÃµes  de  imagens  por  impressoras  e  mÃ¡quinas 

grÃ¡ficas  de  modo  geral,  visto  que  as  trÃªs  cores  citadas  dÃ£o  origem,  quando  sÃ£o 

utilizadas tintas de impressÃ£o, a todas as outras, excetuando-se a cor preta, que Ã© 

adicionada  Ã   parte.  DaÃ­  o  nome  CMYK  (GOMES  e  QUEIROZ,  2001).  A  Figura  2 

esquematiza os dois sistemas de cores citados anteriormente. JÃ¡, na Figura 3, tem-

se o cubo de cores RGB, que se baseia em um sistema de coordenadas cartesianas 

tridimensional, no qual cada eixo diz respeito a uma componente ou cor primÃ¡ria. 

Gonzales  e  Woods 

(2010)  explicam  que,  nesta 

representaÃ§Ã£o,  por 

conveniÃªncia, assume-se que todos os valores de cor foram normalizados, de forma 

que o cubo mostrado na Figura 3 Ã© um cubo unitÃ¡rio. Assim, assume-se os valores de 

R, G e B variem no intervalo [0, 1]. Nestes termos, afirma-se que imagens coloridas 

(RGB) sÃ£o constituÃ­das de pixels nos quais comparecem trÃªs componentes de cor. De 

modo geral, assume-se que uma imagem colorida seja o resultado da composiÃ§Ã£o de 

trÃªs imagens, uma para cada cor primÃ¡ria. Ressalta-se que, as componentes ciano, 

magenta e amarelo tambÃ©m comparecem, de forma complementar, no cubo de cores 

RGB. 

Figura 3: Esquema do cubo de cores RGB. 
Fonte: Gonzalez e Woods (2010) 

 
 
16 

AlÃ©m dos sistemas citados, hÃ¡ tambÃ©m outros sistemas de cores, como o IHS 

(Intensity, Hue, Saturation ou, em portuguÃªs, Intensidade-Matiz-SaturaÃ§Ã£o), no qual 

sÃ£o  consideradas,  alÃ©m  do  matiz  da  cor  (hue),  outras  caracterÃ­sticas,  como  a 

intensidade e a saturaÃ§Ã£o. 

O modelo de cores IHS caracteriza-se por utilizar um sistema de coordenadas 

cilÃ­ndricas  polares  que  representa  as  cores,  ao  invÃ©s  de  coordenadas  cartesianas 

como o sistema RGB (CRÃ“STA, 1992). A saturaÃ§Ã£o diz respeito Ã  pureza relativa ou 

quantidade  de  luz  branca  agregada  ao  matiz.  Por  sua  vez,  o  matiz  define  a  cor 

dominante de um objeto. Ou seja, o matiz Ã© o fator que determina o comprimento de 

onda  dominante  que  serÃ¡  refletido  por  um  objeto  e  este  comprimento  de  onda 

determina  sua  cor.  Aliada  a  estas  duas  componentes  tem-se  a  intensidade,  que 

abarca a noÃ§Ã£o de brilho da imagem. 

Conforme o apresentado anteriormente, o modelo RGB Ã© definido tomando-se 

como base um cubo unitÃ¡rio que materializa um sistema cartesiano 3D. Por sua vez, 

as  componentes  do  modelo  IHS  de  cores,  matiz  e  saturaÃ§Ã£o,  podem  ser 

representadas, respectivamente, por um triÃ¢ngulo de cores, de acordo com a Figura 

4. Considerando a Figura 4, Gonzalez e Woods (2010) expressam que o matiz 

 do 

ponto  de  cor 

  Ã©  o  Ã¢ngulo  do  vetor  com  relaÃ§Ã£o  ao  eixo  vermelho.  Assim,  quando 

ğ»ğ»

,  tem-se  a  cor  Ã©  vermelha,  quando 

ğ‘ƒğ‘ƒ

,  a  cor  Ã©  a  amarela,  e  assim  por 

diante (GONZALEZ, 2010). A saturaÃ§Ã£o do ponto 
ğ»ğ» = 0Â°
relaÃ§Ã£o ao centro do triÃ¢ngulo. 

ğ»ğ» = 60Â°

ğ‘ƒğ‘ƒ

 corresponde Ã  sua distÃ¢ncia com 

Figura 4: TriÃ¢ngulo de representaÃ§Ã£o matiz/saturaÃ§Ã£o 
Fonte: Gonzalez (2000) 

 
 
17 

Em adiÃ§Ã£o a isso, Gonzalez e Woods (2010) especificam que, no modelo IHS, 

a  intensidade  Ã©  medida  com  respeito  a  uma  linha  ortogonal  ao  triÃ¢ngulo  (matiz  e 

saturaÃ§Ã£o), passando pelo seu centro. As intensidades de brilho, ao longo desta linha 

variam do preto absoluto, extremo inferior da linha, atÃ© o branco  absoluto, extremo 

superior da linha. A combinaÃ§Ã£o do matiz com a saturaÃ§Ã£o e a intensidade resultarÃ¡ 

no subespaÃ§o de estrutura piramidal de trÃªs lados, conforme mostrado na Figura 5 

(GONZALEZ e WOODS, 2010). 

Figura 5: Estrutura piramidal intensidade/matiz/saturaÃ§Ã£o 
Fonte: adaptado de Gonzalez e Woods (2010) 

Pontos na superfÃ­cie da estrutura apresentada na Figura 5 representam uma 

cor  saturada.  O  matiz  desta  cor  Ã©  definido  por  seu  Ã¢ngulo  com  relaÃ§Ã£o  ao  eixo 

vermelho  e  sua  intensidade  consiste  na  distÃ¢ncia  perpendicular  ao  ponto  preto. 

Considerando uma imagem RGB e presumindo-se que os respectivos valores tenham 

sido  normalizados  no  intervalo  [0,  1]  pode-se  chegar  aos  valores  de  intensidade, 

saturaÃ§Ã£o e matiz por meio das equaÃ§Ãµes abaixo (GONZALEZ e WOODS, 2010): 

                                                        (1) 

ğ‘…ğ‘…+ğºğº+ğµğµ

ğ¼ğ¼ =

3

 
 
18 

3

ğ‘†ğ‘† = 1 âˆ’ ï¿½

ğ‘…ğ‘…+ğºğº+ğµğµï¿½ âˆ™ [ğ‘›ğ‘›Ã­ğ‘›ğ‘›(ğ‘…ğ‘…, ğºğº, ğµğµ)]

                                         (3) 

                                   (2) 

Onde: 

ğ»ğ» = ï¿½

ğœƒğœƒ
360Â° âˆ’ ğœƒğœƒ

    ğ‘ ğ‘ ğ‘ ğ‘  ğµğµ â‰¤ ğºğº
   ğ‘ ğ‘ ğ‘ ğ‘  ğµğµ > ğºğº

â€¢ 

â€¢ 

, 

 e 

 sÃ£o as intensidades de brilho das componentes vermelho, verde e azul, 

normalizadas no intervalo [0, 1]; 
ğ‘…ğ‘…

ğµğµ

ğºğº

1
2[(ğ‘…ğ‘…âˆ’ğºğº)+(ğ‘…ğ‘…âˆ’ğµğµ)]

âˆ’1

ğœƒğœƒ = cos

ï¿½

[(ğ‘…ğ‘…âˆ’ğºğº)
Considerando que os valores 

2

1
2ï¿½
+(ğ‘…ğ‘…âˆ’ğµğµ).(ğºğºâˆ’ğµğµ)]
 e 

, 

 foram, a priori, normalizados no intervalo 

[0,  1],  entÃ£o  os  valores  de  saÃ­da  do  matiz  e  da  intensidade  tambÃ©m  estarÃ£o 
ğµğµ
normalizados no intervalo [0, 1]. PorÃ©m, Gonzalez e Woods (2010) especificam que, 

ğ‘…ğ‘…

ğºğº

para que a saturaÃ§Ã£o esteja normalizada no intervalo [0, 1], deve-se dividir os valores 

decorrentes da equaÃ§Ã£o (3) por 360Âº. 

Cabe esclarecer que, considerando os sistemas RGB e IHS, pode-se ir de um 

a outro sistema via TransformaÃ§Ã£o IHS. Assim, para se ir do sistema IHS para RGB, 

Gonzalez  e  Woods  (2010)  especificam  que  as  equaÃ§Ãµes  aplicÃ¡veis  dependem  dos 

valores  de 

.  Tais  valores  de 

  determinarÃ£o  trÃªs  setores,  correspondentes  a 

intervalos  de  120Âº,  que  separam  as  cores  primÃ¡rias  (ver  Figura  4).  Logo,  apÃ³s 

multiplicar 

ğ»ğ»
 por 360Âº, de modo que os valores de 

ğ»ğ»

 voltem ao seu intervalo original, 

pode-se proceder a transformaÃ§Ã£o como segue (GONZALEZ e WOODS, 2010): 

ğ»ğ»

ğ»ğ»

1)  Se 

, entÃ£o faÃ§a: 

0Â° â‰¤ ğ»ğ» < 120Â°

                                                    (4) 

ğµğµ = ğ¼ğ¼. (1 âˆ’ ğ‘†ğ‘†)

                                            (5) 

ğ‘…ğ‘… = ğ¼ğ¼. ï¿½1 +  

ğ‘†ğ‘†.ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ (ğ»ğ»)
ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ (60Â°âˆ’ğ»ğ»)ï¿½

                                               (6) 

2)  Se 

, entÃ£o subtraia 120Â° de 

ğºğº = 3. ğ¼ğ¼ âˆ’ (ğ‘…ğ‘… + ğµğµ)

 (i. e., 

) e faÃ§a: 

120Â° â‰¤ ğ»ğ» < 240Â°

ğ»ğ»

                                                    (7) 

ğ»ğ» = ğ»ğ» âˆ’ 120Â°

ğ‘…ğ‘… = ğ¼ğ¼. (1 âˆ’ ğ‘†ğ‘†)

                                            (8) 

ğºğº = ğ¼ğ¼. ï¿½1 +  

ğ‘†ğ‘†.ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ (ğ»ğ»)
ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ (60Â°âˆ’ğ»ğ»)ï¿½

 
 
19 

                                               (9) 

3)  Se 

, entÃ£o subtraia 240Â° de 

ğµğµ = 3. ğ¼ğ¼ âˆ’ (ğ‘…ğ‘… + ğºğº)

 (i. e., 

) e faÃ§a: 

240Â° â‰¤ ğ»ğ» < 360Â°

ğ»ğ»
                                                    (10) 

ğ»ğ» = ğ»ğ» âˆ’ 240Â°

ğºğº = ğ¼ğ¼. (1 âˆ’ ğ‘†ğ‘†)

                                            (11) 

ğµğµ = ğ¼ğ¼. ï¿½1 +  

ğ‘†ğ‘†.ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ (ğ»ğ»)
ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ (60Â°âˆ’ğ»ğ»)ï¿½

                                               (12) 

Por fim, ainda segundo Gonzalez e Woods (2010), para se retornar aos valores 

ğ‘…ğ‘… = 3. ğ¼ğ¼ âˆ’ (ğºğº + ğµğµ)

originais de 

, 

 e 

, deve-se normalizar os valores obtidos no intervalo original. 

ğ‘…ğ‘…

ğºğº

ğµğµ

2.3 

Imagens Digitais  

Uma  imagem  digital  Ã©  composta,  basicamente,  por  unidades  denominadas 

pixels  (contraÃ§Ã£o  da  expressÃ£o  inglesa  â€œpicture  elementâ€),  as  quais  sÃ£o  pequenos 

quadrados, coloridos (nas imagens coloridas) ou em tons de cinza (nas imagens em 

tons de cinza e pancromÃ¡ticas, popularmente conhecidas como imagens â€œem preto e 

brancoâ€)  (CRÃ“STA,  1992).  Essas  pequenas  unidades,  quando  em  conjunto  e 

organizadas em um grid (formato de disposiÃ§Ã£o quadriculado, em linhas e colunas, 

conforme  figura  6),  formam  a  imagem  toda.  A  qualidade  de  uma  imagem  de  uma 

determinada  cena  dependerÃ¡  entÃ£o  da  quantidade  de  pixels  que  a  compÃµe.  Logo, 

quanto  mais  pixels  a  imagem  da  referida  cena  contiver,  melhor  a  sua  qualidade 

pictÃ³rica. Esses pixels, normalmente, sÃ£o dispostos em um sistema referencial (grid 

retangular) baseado no sistema referencial cartesiano plano, de modo a se facilitar a 

manipulaÃ§Ã£o da imagem em questÃ£o. 

A Figura 6 apresenta o sistema referencial da imagem com origem no canto 

superior esquerdo. Neste sistema, a contagem de colunas (

) ocorre com relaÃ§Ã£o ao 

eixo 

, da esquerda para a direita e se inicia em 0 (zero). JÃ¡ a contagem de linhas (

ğ‘ğ‘

) 

da imagem, ocorre de cima para baixo e tambÃ©m se inicia em 0 (zero). Dessa forma, 

ğ‘¥ğ‘¥

ğ‘™ğ‘™

a  posiÃ§Ã£o  de  um  pixel  qualquer  da  imagem  pode  ser  dada  por  meio  de  um  par 

ordenado (

), ou (

). 

ğ‘¥ğ‘¥, ğ‘¦ğ‘¦

ğ‘ğ‘, ğ‘™ğ‘™

 
 
 
20 

Cabe  informar  que,  para  fins  de  manipulaÃ§Ã£o  e  no  intuito  de  se  gerar  uma 

analogia com o primeiro quadrante do plano cartesiano, muitos autores se referem Ã  

posiÃ§Ã£o dos pixels das imagens considerando pares ordenados no formato (

). 

ğ‘™ğ‘™, ğ‘ğ‘

Figura 6: EsquematizaÃ§Ã£o da disposiÃ§Ã£o dos pixels no sistema cartesiano. 
Fonte: Elaborado pelo autor. 

CrÃ³sta (1992) especifica ainda que, cada pixel Ã© um quadrado que recobre uma 

determinada Ã¡rea de uma cena do mundo real, representando-a pictoricamente. No 

caso de imagens de satÃ©lite, a medida de Ã¡rea do mundo real coberta por cada pixel 

serÃ¡ a mesma de qualquer outro pixel pertencente Ã  mesma imagem. Ainda segundo 

o autor, a cada pixel Ã© atribuÃ­do, no sistema referencial da imagem, um valor de â€œ

â€ e 

um  valor  de  â€œ

â€,  que  correspondem  Ã s  posiÃ§Ãµes  de  linha  e  coluna  do  elemento, 

ğ‘¥ğ‘¥

tomadas com relaÃ§Ã£o ao seu centro. Por convenÃ§Ã£o, a origem do sistema referencial 

ğ‘¦ğ‘¦

da imagem se localiza no ponto superior esquerdo da imagem. AlÃ©m disso, um terceiro 

valor, na variÃ¡vel â€œ

â€, Ã© atribuÃ­do ao pixel, e se refere Ã  tonalidade de cinza desse pixel, 

a qual pode variar do branco ao preto, passando por tonalidades de cinza. Se forem 

ğ‘§ğ‘§

consideradas as imagens coloridas, entÃ£o, costumeiramente, se terÃ¡, para cada pixel, 

trÃªs valores de intensidade associados, respectivamente, Ã s componentes R, G e B, 

a fim de que a cor do pixel seja gerada. Na prÃ¡tica, uma imagem colorida Ã© formada 

por  trÃªs  imagens  em  tons  de  cinza  que  contÃªm  as  contribuiÃ§Ãµes  de  bandas 

especÃ­ficas, R, G e B, do espectro eletromagnÃ©tico. 

 
 
21 

Uma imagem pode ser representada matricialmente. Segundo Marques Filho e 

Vieira Neto (1999), os pixels se distribuem em uma matriz de ordem M x N (M linhas 

por N colunas), de modo que a cada pixel estÃ¡ associado um nÃºmero inteiro (

), que 

varia de 0 a 2n -1, e que indica a tonalidade deste pixel. Sendo assim, quanto maior o 

ğ‘§ğ‘§

valor de â€œ

â€, maior a quantidade de tons de cinza que poderÃ¡ ser utilizada na geraÃ§Ã£o 

de uma imagem. 

ğ‘›ğ‘›

Figura 7: RepresentaÃ§Ã£o de uma matriz P de M linhas por N colunas. 
Fonte: Confeccionado pelo prÃ³prio autor. 

De  modo  mais  didÃ¡tico,  tem-se  na  Figura  7  uma  ilustraÃ§Ã£o  do  conceito  de 

distribuiÃ§Ã£o  matricial  de  tons  de  cinza  em  cada  pixel.  Neste  casso, 

,  com  

 e 

, representa a intensidade de brilho (ou nÃ­vel de cinza) na 

ğ‘ƒğ‘ƒ(ğ‘¥ğ‘¥, ğ‘¦ğ‘¦)

posiÃ§Ã£o (
ğ‘¥ğ‘¥ = 1, â€¦ , ğ‘€ğ‘€

). 

ğ‘¦ğ‘¦ = 1, â€¦ , ğ‘ğ‘

ğ‘¥ğ‘¥, ğ‘¦ğ‘¦

Figura 8: Exemplo de representaÃ§Ã£o matricial de pixels em tons de cinza. 
Fonte: AlcÃ¢ntara (2016) 

 
 
 
22 

Na Figura 8 Ã© apresentado um exemplo esquemÃ¡tico de imagem digital. Neste 

exemplo,  efetuou-se  a  ampliaÃ§Ã£o  de  um  pequeno  recorte  de  uma  imagem  e  se 

apresenta,  ao  lado  deste  recorte,  uma  grade  com  os  nÃºmeros  que  definem  as 

intensidades de brilho dos pixels presentes no recorte. 

Conforme o citado anteriormente, as imagens coloridas sÃ£o, conforme o prÃ³prio 

nome diz, imagens que apresentam cor. Essas imagens, em meio digital, podem se 

apresentar em diferentes formatos de representaÃ§Ã£o, tais como o formato RGB (do 

inglÃªs: Red-Green-Blue), que apresenta este nome devido Ã s cores primÃ¡rias da luz, 

ou o formato IHS (Intensity-Hue-Saturation) (MENESES e ALMEIDA, 2012). O formato 

RGB Ã© o mais usual, mas para a manipulaÃ§Ã£o de imagens de satÃ©lite serÃ£o utilizados 

tambÃ©m outros formatos, como o citado IHS, que Ã© menos conhecido pelo pÃºblico de 

modo  geral.  Ressalta-se  que,  as  imagens  coloridas  sÃ£o  formadas  a  partir  da 

justaposiÃ§Ã£o das componentes de cor constituintes em cada pixel. No caso de uma 

imagem  RGB,  cada  pixel  recebe  uma  contribuiÃ§Ã£o  das  tonalidades  verde,  azul  e 

vermelho,  com  intensidades  particulares,  de  modo  a  formar  a  coloraÃ§Ã£o  final  dos 

pixels e, consequentemente, da imagem como um todo. 

Na  imagem  apresentada  na  Figura  9,  pode-se  verificar  uma  composiÃ§Ã£o 

colorida Ã  direita e suas componentes de cor, separadas Ã  esquerda. Nota-se que, 

cada  imagem  componente  constitui  uma  imagem  em  tons  de  cinza  e  que  a  sua 

composiÃ§Ã£o dÃ¡ origem Ã  imagem colorida. 

Figura 9: Exemplo de composiÃ§Ã£o colorida. 
Fonte: Gomes e Queiroz (2001). 

 
 
23 

2.4  ResoluÃ§Ãµes de Imagens 

O termo resoluÃ§Ã£o, no Ã¢mbito do Sensoriamento Remoto, diz respeito a quatro 

parÃ¢metros  relacionados  Ã   imagem,  os  quais  sÃ£o:  resoluÃ§Ã£o  espacial,  resoluÃ§Ã£o 

espectral,  resoluÃ§Ã£o  radiomÃ©trica  e  resoluÃ§Ã£o  temporal.  Estes  parÃ¢metros  estÃ£o 

intrinsecamente  associados  Ã   imagem,  definindo-a  e  determinando  sua  possÃ­vel 

utilidade. Nas subseÃ§Ãµes que seguem, as definiÃ§Ãµes destes parÃ¢metros sÃ£o postas. 

2.4.1  ResoluÃ§Ã£o Espacial 

A resoluÃ§Ã£o espacial, tambÃ©m chamada de resoluÃ§Ã£o geomÃ©trica, estÃ¡ ligada 

ao detalhamento do espaÃ§o imageado pelo satÃ©lite. Nas palavras de CrÃ³sta (1992): 

A  resoluÃ§Ã£o  espacial  Ã©  definida  pela  capacidade  do  sistema  sensor  em 
"enxergar" objetos na superfÃ­cie terrestre; quanto menor o objeto possÃ­vel de 
ser  visto,  maior  a  resoluÃ§Ã£o  espacial.  A  maneira  mais  comum  de  se 
determinar a resoluÃ§Ã£o espacial de um sensor Ã© pelo seu campo instantÃ¢neo 
de  visada  ou  IFOV.  Este  campo  Ã©  determinado  pelas  propriedades 
geomÃ©tricas do sistema sensor e define a Ã¡rea do terreno imageado que Ã© 
"vista" pelo instrumento sensor de uma dada altitude e a um dado momento. 
O IFOV Ã© medido pelas dimensÃµes da Ã¡rea vista no terreno e, de uma forma 
simplificada, ele representa o tamanho do pixel [...] (CRÃ“STA, 1992, p. 25) 

Em  outras  palavras,  a  resoluÃ§Ã£o  espacial  Ã©,  basicamente,  a  Ã¡rea  do  terreno 

recoberta por um pixel da imagem. Normalmente, se considera um pixel quadrado e, 

por este motivo, a resoluÃ§Ã£o espacial Ã© dada pelo comprimento do lado do quadrado, 

no  terreno,  recoberto  por  este  pixel.  Por  exemplo,  uma  imagem  com  resoluÃ§Ã£o 

espacial de 1 metro terÃ¡ cada pixel cobrindo uma Ã¡rea equivalente a 1 metro por um 

metro do terreno (

), assim como, uma imagem de resoluÃ§Ã£o espacial de 0,1 metro 

terÃ¡ cada pixel cobrindo uma Ã¡rea de 0,1 metro por 0,1 metro (

2

1 ğ‘›ğ‘›

). Deste modo, 

quanto  numericamente  menor  for  a  resoluÃ§Ã£o  espacial  de  uma  imagem,  mais 

2

0,01 ğ‘›ğ‘›

detalhada  ela  serÃ¡  e,  consequentemente,  maior  serÃ¡  a  quantidade  de  pixels  que  a 

comporÃ¡. A Figura 10 fornece um exemplo no qual uma mesma cena Ã© imageada com 

imagens de diferentes resoluÃ§Ãµes espaciais. Pode-se perceber que, quanto menor a 

Ã¡rea do terreno abarcada por um pixel, mais detalhada serÃ¡ a imagem, pois pequenos 

detalhes poderÃ£o ser observados no plano da imagem. 

 
 
 
 
 
24 

Figura 10: IlustraÃ§Ã£o de resoluÃ§Ãµes espaciais distintas. 
Fonte: Jensen (2007) apud AlcÃ¢ntara (2016). 

Por vezes, alguns autores se referem Ã  resoluÃ§Ã£o espacial por meio da sigla 

GSD que, em inglÃªs, significa Ground Sample Distance (em portuguÃªs: distÃ¢ncia de 

amostra do terreno). 

2.4.2  ResoluÃ§Ã£o Espectral 

A  resoluÃ§Ã£o  espectral  estÃ¡  relacionada  com  as 

faixas  do  espectro 

eletromagnÃ©tico que o sensor pode captar. CrÃ³sta (1992) explica: 

A resoluÃ§Ã£o espectral Ã© um conceito inerente Ã s imagens multiespectrais de 
Sensoriamento Remoto. Ela Ã© definida pelo nÃºmero de bandas espectrais de 
um  sistema  sensor  e  pela  largura  do  intervalo  de  comprimento  de  onda 
coberto por cada banda. Quanto maior o nÃºmero de bandas e menor a largura 
do  intervalo,  maior  Ã©  a  resoluÃ§Ã£o  espectral  de  um  sensor.  O  conceito  de 
banda,  pode  ser  exemplificado  no  caso  de  duas  fotografias  tiradas  de  um 
mesmo objeto, uma em branco-e-preto e a outra colorida; a foto branco-e-

 
 
 
 
25 

preto representa o objeto em apenas uma banda espectral, enquanto a foto 
colorida  representa  o  mesmo  objeto  em  trÃªs  bandas  espectrais,  vermelha, 
azul e verde que, quando combinadas por superposiÃ§Ã£o, mostram o objeto 
em cores. (CRÃ“STA, 1992, p. 25) 

A  resoluÃ§Ã£o  espectral  Ã©  um  parÃ¢metro  que depende  do  sensor  de  captaÃ§Ã£o. 

Em  termos  gerais,  ela  estÃ¡  relacionada  Ã   quantidade  de  intervalos  do  espectro 

eletromagnÃ©tico que o sensor Ã© capaz de captar. Por exemplo, em se considerando a 

faixa  de  luz  visÃ­vel  do  espectro  eletromagnÃ©tico,  que  vai  do  vermelho  ao  violeta, 

quanto mais subdivisÃµes esse sensor dispuser para captar essa faixa eletromagnÃ©tica, 

menor  serÃ¡  o  intervalo  de  comprimento  de  onda  captado  por  cada  subdivisÃ£o,  e 

melhor serÃ¡ a resoluÃ§Ã£o espectral do sensor. Vale frisar que os satÃ©lites normalmente 

captam tambÃ©m faixas alÃ©m do espectro eletromagnÃ©tico visÃ­vel.  

Figura 11: IlustraÃ§Ã£o mostrando um esquema de resoluÃ§Ã£o espectral. Ã€ esquerda, resoluÃ§Ã£o 
multiespectral; Ã  direita, resoluÃ§Ã£o hiper espectral. 
Fonte: Lu e Fei (2014) apud AlcÃ¢ntara (2016) 

A  Figura  11  acima  ilustra  o  esquema  de  imageamento  feito  por  sensores 

espectrais,  podendo  ser  multiespectral  (menor  resoluÃ§Ã£o)  ou  hiper  espectral  (maior 

resoluÃ§Ã£o),  e  que  se  encontram  embarcados  em  satÃ©lites.  A  ilustraÃ§Ã£o  mostra  as 

bandas  espectrais  passÃ­veis  de  serem  capturadas  pelos  respectivos  tipos  de 

sensores. 

2.4.3  ResoluÃ§Ã£o RadiomÃ©trica 

A resoluÃ§Ã£o radiomÃ©trica, tambÃ©m conhecida como resoluÃ§Ã£o de intensidade, 

estÃ¡ ligada ao nÃºmero de tons de cinza que a imagem dispÃµe para ser formada. Nas 

palavras de CrÃ³sta (1992): 

A  resoluÃ§Ã£o  radiomÃ©trica  Ã©  dada  pelo  nÃºmero  de  nÃ­veis  digitais, 
representando  nÃ­veis  de  cinza,  usados  para  expressar  os  dados  coletados 

 
 
 
 
26 

pelo  sensor.  Quanto  maior  o  nÃºmero  de  nÃ­veis,  maior  Ã©  a  resoluÃ§Ã£o 
radiomÃ©trica. Para entender melhor esse conceito, pensemos numa imagem 
com apenas 2 nÃ­veis (branco e preto) em comparaÃ§Ã£o com uma imagem com 
32  nÃ­veis  de  cinza  entre  o  branco  e  o  preto;  obviamente  a  quantidade  de 
detalhes perceptÃ­veis na segunda serÃ¡ maior do que na primeira e, portanto, 
a segunda imagem terÃ¡ uma melhor resoluÃ§Ã£o radiomÃ©trica. (CRÃ“STA, 1992, 
p. 26) 

Basicamente, quanto maior a quantidade de nÃ­veis de cinza disponÃ­veis para 

compor a imagem, melhor serÃ¡ sua qualidade grÃ¡fica. Esses tons de cinza sÃ£o dados 

como uma potÃªncia de 2. Segundo Gonzalez e Woods (2010): 

Com base em algumas consideraÃ§Ãµes relativas ao hardware, o nÃºmero de 
nÃ­veis de intensidade normalmente Ã© igual a 2k, sendo k um nÃºmero inteiro 
[...] O nÃºmero mais comum Ã© k=8 (8 bits), com 16 bits sendo utilizados em 
algumas  aplicaÃ§Ãµes  nas  quais  o  realce  em  determinadas  faixas  de 
intensidade Ã© necessÃ¡rio. A quantizaÃ§Ã£o de intensidade utilizando 32 bits Ã© 
rara. (GONZALEZ e WOODS, 2010, p. 39) 

Sendo  assim,  o  nÃºmero  de  bits  Ã©  o  expoente  da  potÃªncia  2k  e  indica, 

indiretamente, o nÃºmero de tons de cinza que compÃµem a imagem. 

Figura 12: RepresentaÃ§Ã£o de uma mesma imagem em diferentes resoluÃ§Ãµes radiomÃ©tricas. 
Fonte: AlcÃ¢ntara (2016) 

Na Figura 12 Ã© apresentado um exemplo no qual uma mesma cena Ã© imageada 

com  diferentes  quantidades  de  nÃ­veis  de  brilho.  Assim,  pode-se  perceber  que  a 

imagem de maior resoluÃ§Ã£o radiomÃ©trica (11 bits) Ã© mais detalhada, ou nÃ­tida, e que 

apresenta a melhor qualidade visual. 

 
 
27 

2.4.4  ResoluÃ§Ã£o Temporal 

De acordo com AlcÃ¢ntara (2016), a resoluÃ§Ã£o temporal estÃ¡ ligada ao intervalo 

de tempo com que as imagens ou informaÃ§Ãµes sÃ£o captadas. Este intervalo de tempo 

correspondente ao tempo que o satÃ©lite leva para voltar a cobrir a mesma Ã¡rea. Por 

exemplo,  um  imageamento  de  uma  mesma  regiÃ£o,  realizado  quinzenalmente,  terÃ¡ 

uma resoluÃ§Ã£o temporal melhor  do que um imageamento efetuado a cada 26 dias. 

Sendo assim, a resoluÃ§Ã£o temporal, tambÃ©m chamada de tempo de revisita, Ã© muito 

Ãºtil  no  acompanhamento  da  evoluÃ§Ã£o  geogrÃ¡fica  de  diferentes  localidades  e  seus 

contextos,  como  o  desmatamento  de  florestas.  Cabe  especificar  que,  a  resoluÃ§Ã£o 

temporal estÃ¡ correlacionada com a altitude em que o satÃ©lite orbita e com o tipo de 

Ã³rbita em que ele estÃ¡. 

2.5  FusÃ£o de Imagens via MÃ©todo IHS 

As imagens de satÃ©lite comumente utilizadas possuem caracterÃ­sticas diversas, 

principalmente  se  forem  consideradas,  por  exemplo,  as  resoluÃ§Ãµes  vistas  nas 

subseÃ§Ãµes anteriores. Isso ocorre devido Ã s caracterÃ­sticas dos sensores de captaÃ§Ã£o 

de imagens. 

Tendo  em  vista  a  resoluÃ§Ã£o  espectral,  verifica-se  que  existem  dois  tipos 

principais de imagens captadas por satÃ©lites: as imagens pancromÃ¡ticas, em tons de 

cinza, e as imagens multiespectrais, que podem dar origem Ã s composiÃ§Ãµes coloridas. 

Se  forem  considerados  os  sensores  de  um  mesmo  satÃ©lite,  constata-se  que, 

frequentemente,  as  imagens  pancromÃ¡ticas  apresentam  melhor  resoluÃ§Ã£o  espacial 

que as respectivas imagens multiespectrais. PorÃ©m, se a resoluÃ§Ã£o espacial Ã© uma 

desvantagem  para  as  imagens  multiespectrais,  os  dados  relativos  Ã   cor  sÃ£o  uma 

vantagem desejÃ¡vel para os usuÃ¡rios. Deste modo, para que se obtenha dados com 

o melhor das caracterÃ­sticas de ambos os tipos de imagens, pode-se efetuar a fusÃ£o 

entre imagens RGB e pancromÃ¡tica, relativas a uma mesma Ã¡rea do terreno, e assim 

se obter um produto em cores e com resoluÃ§Ã£o espacial satisfatÃ³ria. 

Segundo  CrÃ³sta  (1992),  um  mÃ©todo  comumente  utilizado  para  isso  Ã© 

denominado  mÃ©todo  de  fusÃ£o  IHS.  Neste  mÃ©todo  considera-se  o  fato  de  que  a 

intensidade  (

)  estÃ¡  correlacionada  Ã   imagem  pancromÃ¡tica.  De  acordo  com  Al-

Wassai et al. (2011), este mÃ©todo consiste basicamente em: 

ğ¼ğ¼

 
28 

1)  Registrar as imagens multiespectrais R, G, B e pancromÃ¡tica de uma mesma Ã¡rea 

em  um  mesmo  sistema  referencial  cartogrÃ¡fico  e  efetuar  a  reamostragem  das 

componentes R, G e B para a dimensÃ£o da imagem pancromÃ¡tica; 

2)  Efetuar a transformaÃ§Ã£o das imagens das bandas R, G e B reamostradas para o 

sistema IHS. Nesta transformaÃ§Ã£o sÃ£o utilizadas as equaÃ§Ãµes de (1) a (3). 

3)  Considerando-se que 

 (intensidade) estÃ¡ correlacionada Ã  imagem pancromÃ¡tica, 

substituir 

 pela imagem pancromÃ¡tica; 

ğ¼ğ¼

4)  Converter a imagem, do sistema IHS para o sistema RGB, obtendo o produto final. 

ğ¼ğ¼

Para esta transformaÃ§Ã£o sÃ£o utilizadas as equaÃ§Ãµes de (4) a (12). 

Segundo  Gonzalez  e  Woods  (2010),  na  fusÃ£o  IHS  a  substituiÃ§Ã£o  da 

componente  de  intensidade  pela  banda  pancromÃ¡tica  produz  imagens  com  melhor 

definiÃ§Ã£o espacial. Assim, problemas relativos ao sharpening, associados Ã  simples 

reamostragem das bandas R, G e B, sÃ£o quase que totalmente eliminados. 

A Figura 13 apresenta o esquema dos passos descritos anteriormente. 

Figura 13: Esquema do processo de fusÃ£o de imagens por mÃ©todo IHS. 
Fonte: Confeccionado pelo prÃ³prio autor. 

A seguir, Ã© apresentado um exemplo do processo de fusÃ£o (INPE, 2021): 

Figura 14: Exemplo de fusÃ£o IHS: a) Imagem pancromÃ¡tica (tons de cinza) com alta resoluÃ§Ã£o; b) 
Imagem colorida (RGB) com baixa resoluÃ§Ã£o; e c) Resultado final da fusÃ£o. 
Fonte: Acervo do Instituto Nacional de Pesquisas Espaciais 

 
 
 
29 

Ao se comparar a imagem das Figuras 14(b) e 14(c), pode-se notar um ganho 

em qualidade pictorial com relaÃ§Ã£o ao resultado da fusÃ£o. 

2.6  EquaÃ§Ãµes de Colinearidade  

De modo simplificado, as equaÃ§Ãµes se baseiam na suposta colinearidade entre 

trÃªs  elementos  relacionados  Ã   tomada  das  imagens:  o  centro  Ã³ptico  do  sistema  de 

lentes de captaÃ§Ã£o, um ponto qualquer na imagem e seu ponto homÃ³logo no espaÃ§o 

fÃ­sico  imageado  (espaÃ§o  objeto).  Assim,  as  equaÃ§Ãµes  de  colinearidade  sÃ£o 

basicamente materializadas tomando-se por base as relaÃ§Ãµes de proporÃ§Ã£o entre as 

distÃ¢ncias  referentes  a  esses  pontos,  podendo-se  tambÃ©m  se  chegar  ao  mesmo 

resultado pelo uso da semelhanÃ§a de triÃ¢ngulos. 

Figura 15: Esquema de distribuiÃ§Ã£o de angulaÃ§Ãµes e colinearidade em fotogrametria. 
Fonte: Universidade Federal de ViÃ§osa (adaptado) 

Do esquema da figura acima pode-se depreender as seguintes relaÃ§Ãµes: 

 
 
 
 
30 

Onde: 

                                                     (13) 

ğ‘¥ğ‘¥ğ‘¥ğ‘¥
ğ‘§ğ‘§ğ‘¥ğ‘¥ =
ğ‘¦ğ‘¦ğ‘¥ğ‘¥
ğ‘§ğ‘§ğ‘¥ğ‘¥ =

ğ‘¥ğ‘¥ğ‘¥ğ‘¥

ğ‘§ğ‘§ğ‘¥ğ‘¥
ğ‘¦ğ‘¦ğ‘¥ğ‘¥

ğ‘§ğ‘§ğ‘¥ğ‘¥

â€¢ 

â€¢ 

, 

 e 

 sÃ£o coordenadas do ponto 

 contido na imagem, tomando-se por base 

ğ‘¦ğ‘¦ğ‘¥ğ‘¥

o  sistema  cartesiano 
ğ‘¥ğ‘¥ğ‘¥ğ‘¥
fotogramÃ©trico); 

ğ‘§ğ‘§ğ‘¥ğ‘¥

tridimensional  da 

ğ‘¥ğ‘¥

imagem 

(sistema 

referencial 

, 

 e 

 sÃ£o as coordenadas do ponto P contido no terreno, tomando-se por 

base o sistema cartesiano da imagem (sistema referencial fotogramÃ©trico); 
ğ‘¥ğ‘¥ğ‘¥ğ‘¥

ğ‘¦ğ‘¦ğ‘¥ğ‘¥

ğ‘§ğ‘§ğ‘¥ğ‘¥

Considerando que a coordenada 

 corresponde Ã  distÃ¢ncia focal (

) no sensor 

de captaÃ§Ã£o, pode-se substituir 

 por 

, de modo que as equaÃ§Ãµes acima passam a 
ğ‘§ğ‘§ğ‘¥ğ‘¥

ğ‘“ğ‘“

ser escritas como: 

ğ‘§ğ‘§ğ‘¥ğ‘¥

ğ‘“ğ‘“

ğ‘¥ğ‘¥ğ‘¥ğ‘¥ = âˆ’ğ‘“ğ‘“.

ğ‘¥ğ‘¥ğ‘¥ğ‘¥
ğ‘§ğ‘§ğ‘¥ğ‘¥

(14) 

Segundo Silva (2020), levando-se em conta que tanto as coordenadas do ponto 

ğ‘¦ğ‘¦ğ‘¥ğ‘¥ = âˆ’ğ‘“ğ‘“.

ğ‘¦ğ‘¦ğ‘¥ğ‘¥
ğ‘§ğ‘§ğ‘¥ğ‘¥

na  imagem  quanto  do  ponto  no  terreno  estÃ£o  contidas  em  sistemas  cartesianos 

tridimensionais  distintos,  faz-se  necessÃ¡ria  a  conversÃ£o  entre  estes  sistemas,  de 

modo a se obter uma padronizaÃ§Ã£o entre pontos no terreno e na imagem. O sistema 

cartesiano tomado como padrÃ£o serÃ¡ o do terreno. Para a conversÃ£o entre os dois 

sistemas cartesianos tridimensionais, utiliza-se a seguinte relaÃ§Ã£o: 

Onde: 

ï¿½

ğ‘¥ğ‘¥ğ‘¥ğ‘¥
ğ‘¦ğ‘¦ğ‘¥ğ‘¥
ğ‘§ğ‘§ğ‘¥ğ‘¥

ï¿½ = ğœ†ğœ† . ğ‘…ğ‘…ğœ”ğœ”ğœ”ğœ”ğœ”ğœ” ï¿½

ğ‘‹ğ‘‹ âˆ’ ğ‘‹ğ‘‹ğ‘‹ğ‘‹
ğ‘Œğ‘Œ âˆ’ ğ‘Œğ‘Œğ‘‹ğ‘‹
ğ‘ğ‘ âˆ’ ğ‘ğ‘ğ‘‹ğ‘‹

ï¿½

                                      (15) 

â€¢ 

â€¢ 

, 

  e 

  sÃ£o  coordenadas  de  pontos,  considerando  o  sistema  cartesiano 

tridimensional padrÃ£o do terreno; 
ğ‘‹ğ‘‹

ğ‘Œğ‘Œ
, 

ğ‘ğ‘
  e 

  sÃ£o  coordenadas  do  ponto  P  no  terreno,  considerando  o  sistema 

cartesiano tridimensional da imagem; 
ğ‘¥ğ‘¥ğ‘¥ğ‘¥

ğ‘¦ğ‘¦ğ‘¥ğ‘¥

ğ‘§ğ‘§ğ‘¥ğ‘¥

 
 
 
â€¢  Xo,  Yo  e  Zo  sÃ£o  coordenadas  do  centro  perspectivo  do  sensor  no  sistema 

31 

 Ã© a matriz de rotaÃ§Ã£o entre os sistemas tridimensionais. Ela pode ser definida 

tridimensional padrÃ£o do terreno; 

 Ã© um dado fator de escala; 

 Ã© a distÃ¢ncia focal da cÃ¢mera; 

ğœ†ğœ†

â€¢ 

â€¢ 

â€¢ 

ğ‘“ğ‘“
como o produto 
ğ‘…ğ‘…ğœ”ğœ”ğœ”ğœ”ğœ”ğœ”

â€¢ 

â€¢ 

â€¢ 

ğ‘€ğ‘€ğœ”ğœ”  =   ï¿½

ğ‘€ğ‘€ğœ”ğœ”  =   ï¿½

0
ğ‘ğ‘ğ‘‹ğ‘‹ğ‘ğ‘ ğœ”ğœ”

0
1
0
ğ‘ğ‘ğ‘ ğ‘ ğ‘ ğ‘  ğœ”ğœ”
0 âˆ’ğ‘ğ‘ğ‘ ğ‘ ğ‘ ğ‘  ğœ”ğœ” ğ‘ğ‘ğ‘‹ğ‘‹ğ‘ğ‘ ğœ”ğœ”
ğ‘ğ‘ğ‘‹ğ‘‹ğ‘ğ‘ ğœ”ğœ” 0 âˆ’ğ‘ğ‘ğ‘ ğ‘ ğ‘ ğ‘  ğœ”ğœ”
1
ğ‘ğ‘ğ‘ ğ‘ ğ‘ ğ‘  ğœ”ğœ” 0

0
ğ‘ğ‘ğ‘‹ğ‘‹ğ‘ğ‘ ğœ”ğœ”

0

ï¿½

ï¿½ 

, onde: 

ğ‘…ğ‘…ğœ”ğœ”ğœ”ğœ”ğœ”ğœ”  =  ğ‘€ğ‘€ğœ”ğœ”. ğ‘€ğ‘€ğœ”ğœ”. ğ‘€ğ‘€ğœ”ğœ”

 Ã© a matriz de rotaÃ§Ã£o ao redor do eixo 

Ã© a matriz de rotaÃ§Ã£o ao redor do eixo 

; 

; 

ğ‘¥ğ‘¥

ğ‘¦ğ‘¦

. 

ğ‘§ğ‘§

 Ã© a matriz de rotaÃ§Ã£o ao redor do eixo 

ğ‘ğ‘ğ‘‹ğ‘‹ğ‘ğ‘ ğœ”ğœ”
ğ‘€ğ‘€ğœ”ğœ”  =   ï¿½
âˆ’ğ‘ğ‘ğ‘ ğ‘ ğ‘ ğ‘  ğœ”ğœ”
Logo, a matriz 
0

ğ‘ğ‘ğ‘ ğ‘ ğ‘ ğ‘  ğœ”ğœ” 0
ğ‘ğ‘ğ‘‹ğ‘‹ğ‘ğ‘ ğœ”ğœ” 0
1

ï¿½

 Ã© dada pela seguinte fÃ³rmula: 
0

ğ‘…ğ‘…ğœ”ğœ”ğœ”ğœ”ğœ”ğœ”

ğ‘…ğ‘…ğœ”ğœ”ğœ”ğœ”ğœ”ğœ” =   ï¿½

cos ğ‘˜ğ‘˜ . cos ğœ”ğœ”

ğ‘ğ‘ğ‘ ğ‘ ğ‘ ğ‘  ğœ”ğœ”. cos ğœ”ğœ” + cos ğœ”ğœ”. ğ‘ğ‘ğ‘ ğ‘ ğ‘ ğ‘  ğœ”ğœ”. ğ‘ğ‘ğ‘ ğ‘ ğ‘ ğ‘  ğœ”ğœ” ğ‘ğ‘ğ‘ ğ‘ ğ‘ ğ‘  ğœ”ğœ”. ğ‘ğ‘ğ‘ ğ‘ ğ‘ ğ‘  ğœ”ğœ” âˆ’ cos ğœ”ğœ”. ğ‘ğ‘ğ‘ ğ‘ ğ‘ ğ‘  ğœ”ğœ”. cos ğœ”ğœ”
âˆ’ cos ğœ”ğœ” . ğ‘ğ‘ğ‘ ğ‘ ğ‘ ğ‘  ğœ”ğœ” cos ğœ”ğœ”. cos ğœ”ğœ” âˆ’ ğ‘ğ‘ğ‘ ğ‘ ğ‘ ğ‘  ğœ”ğœ”. ğ‘ğ‘ğ‘ ğ‘ ğ‘ ğ‘  ğœ”ğœ”. ğ‘ğ‘ğ‘ ğ‘ ğ‘ ğ‘  ğœ”ğœ” cos ğœ”ğœ”. ğ‘ğ‘ğ‘ ğ‘ ğ‘ ğ‘  ğœ”ğœ” + ğ‘ğ‘ğ‘ ğ‘ ğ‘ ğ‘  ğœ”ğœ”. ğ‘ğ‘ğ‘ ğ‘ ğ‘ ğ‘  ğœ”ğœ”. cos ğœ”ğœ”

ğ‘ğ‘ğ‘ ğ‘ ğ‘ ğ‘  ğœ”ğœ”

âˆ’ cos ğœ”ğœ” . ğ‘ğ‘ğ‘ ğ‘ ğ‘ ğ‘  ğœ”ğœ”

cos ğœ”ğœ”. cos ğœ”ğœ”

ï¿½
(16) 

Ao se substituir as equaÃ§Ãµes (15) e (16) na equaÃ§Ã£o (14), obtÃ©m-se as equaÃ§Ãµes de 

colinearidade, explicitadas na seguinte relaÃ§Ã£o (ANDRADE, 1998 apud SILVA, 2020): 

xp = âˆ’ğ‘“ğ‘“

ğ‘Ÿğ‘Ÿ11(ğ‘‹ğ‘‹ âˆ’ Xo) + ğ‘Ÿğ‘Ÿ12(ğ‘Œğ‘Œ âˆ’ Yo) + ğ‘Ÿğ‘Ÿ13(ğ‘ğ‘ âˆ’ Zo)
ğ‘Ÿğ‘Ÿ31(ğ‘‹ğ‘‹ âˆ’ Xo) + ğ‘Ÿğ‘Ÿ32(ğ‘Œğ‘Œ âˆ’ Yo) + ğ‘Ÿğ‘Ÿ33(ğ‘ğ‘ âˆ’ Zo)

(17) 

Sendo que: 

yp = âˆ’ğ‘“ğ‘“

ğ‘Ÿğ‘Ÿ21(ğ‘‹ğ‘‹ âˆ’ Xo) + ğ‘Ÿğ‘Ÿ22(ğ‘Œğ‘Œ âˆ’ Yo) + ğ‘Ÿğ‘Ÿ23(ğ‘ğ‘ âˆ’ Zo)
ğ‘Ÿğ‘Ÿ31(ğ‘‹ğ‘‹ âˆ’ Xo) + ğ‘Ÿğ‘Ÿ32(ğ‘Œğ‘Œ âˆ’ Yo) + ğ‘Ÿğ‘Ÿ33(ğ‘ğ‘ âˆ’ Zo)

Onde: 

xp = xâ€²p âˆ’ xf
ï¿½
yp = yâ€²p âˆ’ xf

â€¢ 

, com 

, denotam os elementos da matriz 

; 

â€¢  xâ€™p e yâ€™p sÃ£o coordenadas do ponto p contido na imagem, tomando-se por base o 

ğ‘…ğ‘…ğœ”ğœ”ğœ”ğœ”ğœ”ğœ”

ğ‘–ğ‘–, ğ‘—ğ‘— = 1, â€¦ ,3

ğ‘Ÿğ‘Ÿğ‘–ğ‘–ğ‘–ğ‘–
sistema referencial fiducial; 

â€¢  xf  e yf Ã© a coordenada x do centro do plano da imagem; 

 
 
 
 
 
 
32 

Deste  modo,  tomando-se  os  elementos  de  rotaÃ§Ã£o  (Ï‰,  Ï†,  Îº)  e  os  elementos  de 

translaÃ§Ã£o  (Xo,  Yo,  Zo),  pode-se  projetar  as  coordenadas  do  sistema  cartesiano 

tridimensional  da  imagem  para  o  sistema  tridimensional  padrÃ£o  do  terreno, 

considerando a coordenada Z conhecida, utilizando-se as equaÃ§Ãµes de colinearidade 

na forma inversa. Segundo Andrade (1998), tais equaÃ§Ãµes sÃ£o: 

ğ‘‹ğ‘‹ = ğ‘‹ğ‘‹ğ‘‹ğ‘‹ + (ğ‘ğ‘ âˆ’ ğ‘ğ‘ğ‘‹ğ‘‹). ( 

ğ‘Ÿğ‘Ÿ11. xp + ğ‘Ÿğ‘Ÿ21. yp âˆ’ ğ‘Ÿğ‘Ÿ31ğ‘“ğ‘“
ğ‘Ÿğ‘Ÿ13. xp + ğ‘Ÿğ‘Ÿ23. yp âˆ’ ğ‘Ÿğ‘Ÿ33ğ‘“ğ‘“

 )

(18) 

Por fim, Andrade (1998) especifica que as equaÃ§Ãµes de colinearidade, tanto na 

ğ‘Œğ‘Œ = ğ‘Œğ‘Œğ‘‹ğ‘‹ + (ğ‘ğ‘ âˆ’ ğ‘ğ‘ğ‘‹ğ‘‹). (

ğ‘Ÿğ‘Ÿ12. xp + ğ‘Ÿğ‘Ÿ22. yp âˆ’ ğ‘Ÿğ‘Ÿ32ğ‘“ğ‘“
ğ‘Ÿğ‘Ÿ13. xp + ğ‘Ÿğ‘Ÿ23. yp âˆ’ ğ‘Ÿğ‘Ÿ33ğ‘“ğ‘“

)

sua forma direta, quanto na sua forma inversa, envolvem seis parÃ¢metros de cunho 

geomÃ©trico que sÃ£o: duas translaÃ§Ãµes, uma em 

 e outra em 

, trÃªs rotaÃ§Ãµes em torno, 

respectivamente, dos eixos 

, 

 e 

, e um fator de escala, associado Ã  distÃ¢ncia focal 
ğ‘¦ğ‘¦

ğ‘¥ğ‘¥

. 

ğ‘“ğ‘“

ğ‘¥ğ‘¥

ğ‘¦ğ‘¦

ğ‘§ğ‘§

2.7  RetificaÃ§Ã£o de imagens 

A retificaÃ§Ã£o de imagens, nas palavras de Silva (2020), â€œÃ© o processo pelo qual 

ocorre a eliminaÃ§Ã£o das distorÃ§Ãµes causadas pelos Ã¢ngulos de atitude da cÃ¢mera que 

fez a tomada de uma imagemâ€, ou seja, Ã© a correÃ§Ã£o da imagem original, a qual sofre 

distorÃ§Ãµes  devido  aos  efeitos  de  possÃ­veis  inclinaÃ§Ãµes  do  sensor  de  captaÃ§Ã£o  em 

relaÃ§Ã£o ao terreno ou objeto imageado. 

Segundo  CrÃ³sta  (1992),  as  principais  fontes  de  distorÃ§Ã£o  das  imagens  de 

satÃ©lite sÃ£o a rotaÃ§Ã£o da Terra e a instabilidade da plataforma, a qual sofre variaÃ§Ãµes 

na altitude, na velocidade do satÃ©lite em relaÃ§Ã£o ao terreno e nos eixos de rotaÃ§Ã£o do 

satÃ©lite. Sendo assim, o objetivo deste processo Ã© fazer com que a imagem fique em 

uma  perspectiva,  apesar  de  cÃ´nica,  perfeitamente  vertical,  como  se  o  sensor  de 

captaÃ§Ã£o estivesse posicionado ortogonalmente ao objeto, algo que Ã© praticamente 

impossÃ­vel  durante  o  processo  de  imageamento.  A  precisÃ£o  desejada  deve  ser 

comparÃ¡vel  Ã   de  um  mapa,  ou  seja,  o  processo  deve  apresentar  uma  exatidÃ£o 

rigorosa em relaÃ§Ã£o ao objeto imageado. Segundo Cerqueira (2004), neste processo 

 
 
 
 
 
33 

sÃ£o  utilizados  modelos  matemÃ¡ticos,  normalmente  baseados  nas  equaÃ§Ãµes  de 

colinearidade (vistas anteriormente). 

De  modo  bem  simplificado,  pode-se  dizer  que  o  processo  de  retificaÃ§Ã£o  de 

imagens, realizado pelos softwares especializados, consiste no  estabelecimento de 

uma relaÃ§Ã£o entre as coordenadas da imagem original e da imagem retificada, com a 

realocaÃ§Ã£o  de  pixels,  de  modo  a  se  corrigir  as  distorÃ§Ãµes  da  imagem  original 

(CRÃ“STA,1992). 

Segundo  Andrade  (1998),  existe  mais  de  uma  forma  de  se  implementar  o 

algoritmo  de  retificaÃ§Ã£o  de  imagens:  o  mÃ©todo  direto  e  o  mÃ©todo  indireto.  Por  uma 

questÃ£o  de  eficiÃªncia,  o  algoritmo  descrito  a  seguir  diz  respeito  ao  mÃ©todo  de 

retificaÃ§Ã£o  digital  indireto,  que  utiliza  as  equaÃ§Ãµes  de  colinearidade  na  sua  forma 

direta. Neste caso, as equaÃ§Ãµes de colinearidade, na sua forma direta, sÃ£o utilizadas 

para 

relacionar 

fotocoordenadas  da 

imagem 

retificada, 

reamostrada,  com 

fotocoordenadas da imagem original, obtendo assim o tom de cinza que preencherÃ¡ 

a  respectiva  posiÃ§Ã£o  do  grid  da  imagem  retificada.  Para  que  a  retificaÃ§Ã£o  ocorra  a 

contento, duas outras transformaÃ§Ãµes sÃ£o utilizadas para relacionar os referenciais de 

imagem e o fotogramÃ©trico. Tais transformaÃ§Ãµes sÃ£o dadas em momento oportuno. 

Os passos que compÃµem o algoritmo de retificaÃ§Ã£o sÃ£o os seguintes: 

1)  Leitura  da  imagem  de  entrada  (imagem  original):  no  processo  de  leitura, 

alÃ©m  das  intensidades  de  brilho  da  imagem  de  entrada,  sÃ£o  carregados  e 

armazenados os valores relativos Ã  largura e altura da imagem original. 

2)  Leitura  dos  parÃ¢metros  de  transformaÃ§Ã£o:  alÃ©m  dos  valores  de  brilho  da 

imagem de entrada e de suas dimensÃµes, sÃ£o lidos em arquivo ou introduzidos 

via  interface  os  valores  dos  Ã¢ngulos  de  orientaÃ§Ã£o  exterior  (

, 

  e 

),  que 

fornecem  a  atitude  da  cÃ¢mera  no  momento  da  tomada  da  imagem,  alÃ©m  da 

ğœ”ğœ”

ğœ”ğœ”

ğœ”ğœ”

distÃ¢ncia focal da referida cÃ¢mera (

). 

3)  CÃ¡lculo  da  matriz  de  rotaÃ§Ã£o:  para  o  cÃ¡lculo  da  matriz  de  rotaÃ§Ã£o  sÃ£o 

ğ‘“ğ‘“

necessÃ¡rios os Ã¢ngulos 

, 

 e 

 (passo 2) e Ã© utilizada a fÃ³rmula prevista na 

equaÃ§Ã£o 16. 

ğœ”ğœ”

ğœ”ğœ”

ğœ”ğœ”

4)  CÃ¡lculo das dimensÃµes da imagem retificada (reamostrada): para se ter as 

dimensÃµes  (largura  e  altura)  da  imagem  reamostrada,  inicialmente  sÃ£o 

tomadas  as  coordenadas  dos  vÃ©rtices  (cantos)  da  imagem  de  entrada.  Em 

 
34 

seguida, estas coordenadas sÃ£o transformadas do  sistema referencial  digital 

para o sistema referencial com origem no centro da imagem. Assim, a partir do 

sistema referencial digital (

), pode-se obter as coordenadas 

 de pontos 

no sistema referencial com origem no centro da imagem. Essa transformaÃ§Ã£o 

(ğ‘¥ğ‘¥, ğ‘¦ğ‘¦)

ğ‘ğ‘, ğ‘™ğ‘™

Ã© dada por: 

Onde: 

ğ‘Šğ‘Šâˆ’1

                                        (19) 

ğ‘¥ğ‘¥
ğ‘¦ğ‘¦ï¿½ = ï¿½

ğ‘†ğ‘†ğ‘¥ğ‘¥
0
0 âˆ’ğ‘†ğ‘†ğ‘¦ğ‘¦ï¿½ ï¿½

ï¿½

ğ‘ğ‘ âˆ’

2
ğ»ğ»âˆ’1

ğ‘™ğ‘™ âˆ’

2

ï¿½

â€¢ 

â€¢ 

â€¢ 

â€¢ 

 Ã© a dimensÃ£o do pixel na direÃ§Ã£o 

 Ã© a dimensÃ£o do pixel na direÃ§Ã£o 

ğ‘¥ğ‘¥

; 

; 

 representa o nÃºmero de colunas da imagem; e 

ğ‘¦ğ‘¦

ğ‘†ğ‘†ğ‘¥ğ‘¥
ğ‘†ğ‘†ğ‘¦ğ‘¦

 o nÃºmero de linhas da imagem. 

ğ‘Šğ‘Š

De posse das coordenadas expressas no sistema referencial com origem no 

ğ»ğ»

centro  da  imagem  e  do  valor  negativo  da  distÃ¢ncia  focal  (

),  como  terceira 

coordenada,  sÃ£o  utilizadas  as  equaÃ§Ãµes  de  colinearidade  inversa  (equaÃ§Ã£o 

âˆ’ğ‘“ğ‘“

(18)) a fim de se ter as coordenadas dos vÃ©rtices da imagem retificada, dadas 

no sistema referencial com origem no centro da imagem, com relaÃ§Ã£o Ã  imagem 

retificada.  Por  fim,  as  coordenadas  dos  vÃ©rtices  da  imagem  retificada  sÃ£o 

transformadas para o sistema referencial digital por meio da equaÃ§Ã£o: 

ï¿½ğ‘ğ‘
ğ‘™ğ‘™

ï¿½ = ï¿½

âˆ’1
ğ‘†ğ‘†ğ‘¥ğ‘¥
0
âˆ’1ï¿½ ï¿½
0 âˆ’ğ‘†ğ‘†ğ‘¦ğ‘¦

ğ‘¥ğ‘¥
ğ‘¦ğ‘¦ï¿½ + ï¿½

ğ‘Šğ‘Šâˆ’1
2
ğ»ğ»âˆ’1
2

                                            (20) 

ï¿½

Silva (2020) explica que, em alguns casos como, por exemplo, na retificaÃ§Ã£o 

de imagens, 

 e 

 podem ser unitÃ¡rios, visto que, o que se tem Ã© um rearranjo 

dos  pixels  da  imagem  e  nÃ£o  uma  relaÃ§Ã£o  dos  pixels  com  coordenadas  de 

ğ‘†ğ‘†ğ‘¥ğ‘¥

ğ‘†ğ‘†ğ‘¦ğ‘¦

terreno. A partir das coordenadas dos cantos da imagem retificada, pode-se ter 

acesso Ã s suas dimensÃµes. Em relaÃ§Ã£o a isso, explica Silva (2020) ainda que, 

considerando  que  a  imagem  retificada  poderÃ¡  nÃ£o  ser  retangular,  com  base 

nas  novas  coordenadas  dos  cantos  calculadas,  cria-se  um  retÃ¢ngulo 

envolvente que possa abarcar a imagem retificada, de modo a permitir que ela 

seja  visualizada  ao  final  do  processo.  Assim,  cria-se  uma  matriz,  com  as 

 
35 

dimensÃµes  do  retÃ¢ngulo  envolvente,  na  qual  serÃ£o  alocados  os  valores  de 

brilho relativos Ã  imagem retificada. 

5)  Varredura e preenchimento da imagem retificada: neste passo, toma-se a 

matriz destinada Ã  alocaÃ§Ã£o dos valores de brilho relativos Ã  imagem retificada 

e efetua-se a sua varredura, tomando-se, paulatinamente, as coordenadas de 

seus  pixels  (

),  transformando-os  para  coordenadas  (

),  por  meio  da 

equaÃ§Ã£o 19, e, por meio das equaÃ§Ãµes de colinearidade direta (equaÃ§Ã£o 17) 

ğ‘¥ğ‘¥, ğ‘¦ğ‘¦

ğ‘ğ‘, ğ‘™ğ‘™

chega-se  Ã s  coordenadas  (

),  relativas  Ã   imagem  de  entrada.  Neste  caso, 

para se ter acesso Ã s coordenadas (

ğ‘¥ğ‘¥, ğ‘¦ğ‘¦

 dos pixels da imagem original, deve-

se  ainda  utilizar  a  equaÃ§Ã£o  20.  Silva  (2020)  especifica  que,  visto  que  as 
ğ‘ğ‘, ğ‘™ğ‘™)

coordenadas  (

)  geradas  geralmente  nÃ£o  coincidem  com  coordenadas  de 

pixel  (valores  inteiros)  da  imagem  original,  utiliza-se  algum  mÃ©todo  de 

ğ‘ğ‘, ğ‘™ğ‘™

interpolaÃ§Ã£o para se obter o tom de cinza que preencha o respectivo pixel da 

matriz relativa Ã  imagem retificada (reamostrada). 

O resultado final Ã© a imagem retificada, que Ã© salva em arquivo. 

 
 
 
 
36 

3  METODOLOGIA 

Este  capÃ­tulo  destina-se  Ã   apresentaÃ§Ã£o  dos  procedimentos  metodolÃ³gicos 

efetivamente  desenvolvidos  e  realizados  na  pesquisa.  Cabe  especificar  que,  a 

metodologia executada possuiu carÃ¡ter teÃ³rico e experimental. Ou seja, ela consistiu, 

inicialmente,  na  sondagem  do  algoritmo  de  retificaÃ§Ã£o  de  imagens  digitais  e  das 

equaÃ§Ãµes de colinearidade que o integram, a fim de se angariar conhecimento teÃ³rico 

e  algorÃ­tmico  com  o  objetivo  de  projetar  e  implementar  mudanÃ§as  algorÃ­tmicas  e 

matemÃ¡ticas em tais modelos de modo a gerar um novo algoritmo capaz de efetuar a 

fusÃ£o de imagens digitais, com base no mÃ©todo IHS. 

Logo,  buscando  detalhar  o  trabalho  realizado  tem-se,  na  seÃ§Ã£o  3.1,  um 

pequeno preÃ¢mbulo, destinado Ã  descriÃ§Ã£o dos principais recursos computacionais e 

dos  dados  de  entrada,  utilizados  na  pesquisa.  Na  seÃ§Ã£o  3.2  encontra-se  a 

pormenorizaÃ§Ã£o do delineamento metodolÃ³gico executado para a geraÃ§Ã£o do produto 

final. 

3.1  PreÃ¢mbulo 

Considerando que a proposta da pesquisa envolve o uso de imagens, foram 

coletadas  e  utilizadas  imagens  provenientes  do  satÃ©lite  CBERS  4,  disponibilizadas 

gratuitamente no site do INPE (http://www.dgi.inpe.br/CDSR/). A escolha do produto 

se  deu  por  conta  de  sua  Ã³tima  qualidade  e  pelo  fato  de  estarem  disponÃ­veis  as 

imagens multiespectrais e a pancromÃ¡tica, necessÃ¡rias Ã  execuÃ§Ã£o da metodologia. 

As ortoimagens multiespectrais, possuem resoluÃ§Ã£o espacial de 

 e foram geradas 

pelo  sensor  MUX  -  CÃ¢mera  Multiespectral  Regular.  Por  sua  vez,  a  imagem 

20ğ‘šğ‘š
, Ã© proveniente do sensor PAN - CÃ¢mera 

pancromÃ¡tica, com resoluÃ§Ã£o espacial de 

PancromÃ¡tica  e  Multiespectral.  Todas  as  imagens  sÃ£o  do  formato  GEOTIFF e  vÃªm 
5ğ‘šğ‘š
georreferenciadas  no  Sistema  Referencial  GeodÃ©sico  WGS84,  considerando  o 

sistema  de  projeÃ§Ã£o  UTM  (zona  22).  O  cÃ³digo  EPSG  (European  Petroleum  Survey 

Group)  que  identifica  estes  elementos  geodÃ©sicos  Ã©  o  32722,  o  qual  foi  usado  no 

sistema de informaÃ§Ãµes geogrÃ¡ficas QGIS. As imagens do sensor MUX e PAN sÃ£o do 

dia 29/04/2020 e correspondem Ã  orbita 160 e ao ponto 123. Estas imagens dizem 

 
 
 
 
37 

respeito ao rio ParanÃ¡ e imediaÃ§Ãµes, na regiÃ£o compreendida entre TrÃªs Lagoas â€“ MS 

e Ilha Solteira - SP. SÃ£o elas: 

â€¢  Banda 1 = 0,51 - 0,85 Âµm (PAN): imagem pancromÃ¡tica; 

â€¢  Banda 5 = 0,45 - 0,52 Î¼m (B): componente azul; 

â€¢  Banda 6 = 0,52 - 0,59 Î¼m (G): componente verde; 

â€¢  Banda 7 = 0,63 - 0,69 Î¼m (R): componente vermelha. 

Cabe  especificar  que,  as  imagens  CBERS  nÃ£o  foram  processadas  em  sua 

totalidade,  uma  vez  que,  a  largura  da  imagem  pancromÃ¡tica  (60  km)  nÃ£o  se 

equiparava  Ã   largura  das  imagens  multiespectrais  (120  km),  alÃ©m  do  que,  o 

processamento integral das imagens fugiria do objetivo do trabalho. Nestes termos, 

foram feitos dois recortes, relativos aos reservatÃ³rios das usinas hidrelÃ©tricas de Ilha 

Solteira e Engenheiro Souza Dias (JupiÃ¡), os quais sÃ£o apresentados no capÃ­tulo que 

segue. 

Neste caso, os recortes foram feitos por meio do software QGIS. O QGIS Ã© um 

Sistema  de  InformaÃ§Ãµes  GeogrÃ¡ficas  (SIG)  gratuito,  criado  por  Gary  Sherman  em 

2002. No entanto, sua versÃ£o inicial foi lanÃ§ada em somente em 2009 (MARQUES, 

2021). 

Segundo  a  pÃ¡gina  do  prÃ³prio  SIG,  o  QGIS  Ã©  um  Sistema  de  InformaÃ§Ã£o 

GeogrÃ¡fica (SIG) de CÃ³digo Aberto, licenciado segundo a LicenÃ§a PÃºblica Geral GNU, 

multiplataforma,  que  permite  a  visualizaÃ§Ã£o,  ediÃ§Ã£o  e  anÃ¡lise  de  dados 

georreferenciados (QGIS, 2021). Ele suporta inÃºmeros formatos de vetores, rasters, 

bases de dados e funcionalidades. AlÃ©m das funcionalidades nativas do SIG, ele ainda 

conta  com  complementos 

(plugins),  desenvolvidos  e  disponibilizados  por 

colaboradores de todo o mundo no site do QGIS1. A versÃ£o utilizada nesta pesquisa 

foi a 3.12.2 BucureÈ™ti. 

Para a geraÃ§Ã£o dos recortes, inicialmente se ajustou o projeto inicial do QGIS 

ao  sistema  referencial  cartogrÃ¡fico  e  projeÃ§Ã£o  das 

imagens  do  CBERS  4 

(WGS84/UTM zona 22), adotando no menu â€œPropriedadesâ€ o respectivo cÃ³digo EPSG 

32722,  relativo  aos  ditos  parÃ¢metros  geodÃ©sicos.  ApÃ³s  se  carregar  as  imagens 

(camadas raster), se utilizou a ferramenta raster â€œRecortar raster pela extensÃ£oâ€ para 

1 DisponÃ­vel em:< https://plugins.qgis.org/>. Acesso: 01/05/2021 

 
                                            
38 

se gerar os recortes das imagens de entrada. Os locais dos recortes foram escolhidos 

por  possuÃ­rem  feiÃ§Ãµes  significativas  e  nÃ£o  possuÃ­rem  cobertura  de  nuvens.  Cabe 

informar que estas razÃµes tambÃ©m justificaram a escolha das imagens como um todo. 

Sendo  assim,  de  posse  dos  recortes,  utilizou-se  o  QGIS,  uma  vez  mais,  para  a 

geraÃ§Ã£o de composiÃ§Ãµes coloridas. 

3.2  Delineamento MetodolÃ³gico da Pesquisa 

O delineamento metodolÃ³gico Ã© descrito considerando-se dois focos: 

â€¢  As alteraÃ§Ãµes matemÃ¡ticas e algorÃ­tmicas associadas ao processo de retificaÃ§Ã£o 

de imagens digitais; e 

â€¢  A geraÃ§Ã£o do algoritmo de fusÃ£o de imagens. 

Logo, as subseÃ§Ãµes que seguem destinam-se Ã  pormenorizaÃ§Ã£o dos referidos 

focos e expressam o cerne da pesquisa empreendida. 

3.2.1  AlteraÃ§Ãµes Ã s equaÃ§Ãµes de colinearidade 

Cabe inicialmente lembrar que, considerando uma imagem aÃ©rea, a retificaÃ§Ã£o 

de  imagens  digitais  Ã©  o  processo  destinado,  principalmente,  Ã   eliminaÃ§Ã£o  das 

distorÃ§Ãµes presentes em tal imagem, decorrentes dos Ã¢ngulos de atitude da cÃ¢mera 

que fez a tomada. Estes Ã¢ngulos de rotaÃ§Ã£o podem ocorrer em torno do eixo 

 e/ou 

do eixo 

 e/ou do eixo 

. No Ã¢mbito da Fotogrametria, estes Ã¢ngulos de atitude sÃ£o 

ğ‘¥ğ‘¥

denotados, respectivamente, por Ï‰, Ï† e Îº (Ã¢ngulos de orientaÃ§Ã£o exterior). AlÃ©m, dos 

ğ‘¦ğ‘¦

ğ‘§ğ‘§

parÃ¢metros  relativos  Ã   atitude  do  sensor  de  imageamento,  o  modelo  possui  um 

parÃ¢metro associado Ã  escala da imagem e que tambÃ©m estÃ¡ associado Ã  geometria 

da cÃ¢mera. Este parÃ¢metro Ã© a distÃ¢ncia focal 

 que, em conjunto com a altitude de 

voo,  estabelece  um  fator  de  escala  para  a  imagem  gerada.  Por  fim,  pode-se  ainda 

ğ‘“ğ‘“

considerar a possibilidade de se efetuar duas translaÃ§Ãµes, uma na direÃ§Ã£o 

 e outra 

na direÃ§Ã£o 

. Estas translaÃ§Ãµes podem se dar por meio das coordenadas do centro 

ğ‘¥ğ‘¥

perspectivo, que estÃ£o incorporadas Ã s equaÃ§Ãµes de colinearidade. Nestes termos, 

ğ‘¦ğ‘¦

 
 
 
 
 
os parÃ¢metros considerados na retificaÃ§Ã£o sÃ£o: trÃªs rotaÃ§Ãµes (em torno dos eixos 

, 

 e 

), um fator de escala e duas translaÃ§Ãµes (nas direÃ§Ãµes dos eixos 

 e 

). 

ğ‘¥ğ‘¥

39 

ğ‘¦ğ‘¦

ğ‘§ğ‘§

Cabe lembrar que, o algoritmo de retificaÃ§Ã£o, da forma como foi detalhado na 

ğ‘¦ğ‘¦

ğ‘¥ğ‘¥

seÃ§Ã£o 2.7, destina-se a efetuar rotaÃ§Ãµes em imagens, possivelmente, em torno dos 

eixos 

, 

  e 

.  Assim,  uma  imagem  de  entrada,  tomada  com  cÃ¢mera  afetada  de 

variaÃ§Ãµes angulares, possivelmente nos trÃªs eixos, pode ser corrigida de distorÃ§Ãµes 

ğ‘¥ğ‘¥

ğ‘¦ğ‘¦

ğ‘§ğ‘§

decorrentes de tais variaÃ§Ãµes. Logo, uma imagem afetada pela atitude da cÃ¢mera no 

momento da tomada passaria, quando processada, a ser uma imagem de perspectiva 

central. 

Nestes  termos,  cabe informar  que  os  parÃ¢metros,  relativos  Ã s  tais  correÃ§Ãµes 

angulares  e  que  integram  as  equaÃ§Ãµes  de  colinearidade,  nÃ£o  foram  alterados. 

Buscou-se deixar essa potencialidade no modelo a fim de que ela estivesse disponÃ­vel 

e pudesse ser utilizada se necessÃ¡rio. Segundo Al-Wassai et al. (2011), as imagens 

a  serem  fusionadas  devem  ser  registradas  num  mesmo  sistema  referencial 

geodÃ©sico,  a  fim  de  que  a  superposiÃ§Ã£o  estabelecida  entre  as  imagens 

multiespectrais  e  a  imagem  pancromÃ¡tica  seja  efetiva.  Logo,  deixar  tais 

parÃ¢metros  Ã   disposiÃ§Ã£o,  implica  em  poder  se  efetuar  ajustes  angulares  que 

possam  corrigir,  a  posteriori,  possÃ­veis  discrepÃ¢ncias  angulares  associadas  Ã  

superposiÃ§Ã£o. 

Andrade (1998), e Wolf e Dewitt (2000) afirmam que a distÃ¢ncia focal (

) estÃ¡ 

relacionada  Ã   escala  da  imagem  capturada.  Tal  informaÃ§Ã£o  foi  bastante  Ãºtil  na 

ğ‘“ğ‘“

pesquisa empreendida, pois Al-Wassai  et  al.  (2011)  especificam  que  as  imagens 

multiespectrais  devem  ser  reamostradas  para  as  dimensÃµes  da 

imagem 

pancromÃ¡tica. Consequentemente, esta caracterÃ­stica associada Ã  distÃ¢ncia focal 

, 

que estÃ¡ presente nas equaÃ§Ãµes de colinearidade direta e inversa, pode ser utilizada 

ğ‘“ğ‘“

para que a reamostragem preconizada por Al-Wassai et al. (2011) ocorra. 

Nestes termos, buscou-se efetuar alteraÃ§Ãµes no dito parÃ¢metro de modo que 

ele efetuasse o controle da escala. ApÃ³s estudos, experimentaÃ§Ãµes, e observaÃ§Ãµes 

de efeitos associados Ã  distÃ¢ncia focal e suas variaÃ§Ãµes, a exemplo do proposto em 

Silva  (2020),  decidiu-se  pelo  uso  de  dois  parÃ¢metros,  de  modo  que,  a  proporÃ§Ã£o 

estabelecida entre eles expressasse um fator de escala. Assim, os dois parÃ¢metros, 

 e 

, passaram a substituir o valor da distÃ¢ncia focal nas equaÃ§Ãµes de colinearidade 

ğ‘“ğ‘“â€²

â„

 
direta e inversa. Corroborando com Silva (2020), as equaÃ§Ãµes de colinearidade direta 

e inversa ficaram, respectivamente: 

40 

ğ‘¥ğ‘¥ğ‘ğ‘ = âˆ’â„

ğ‘Ÿğ‘Ÿ11(ğ‘‹ğ‘‹ âˆ’ ğ‘‹ğ‘‹ğ¶ğ¶ğ¶ğ¶) + ğ‘Ÿğ‘Ÿ12(ğ‘Œğ‘Œ âˆ’ ğ‘Œğ‘Œğ¶ğ¶ğ¶ğ¶) + ğ‘Ÿğ‘Ÿ13(âˆ’ğ‘“ğ‘“â€²)
ğ‘Ÿğ‘Ÿ31(ğ‘‹ğ‘‹ âˆ’ ğ‘‹ğ‘‹ğ¶ğ¶ğ¶ğ¶) + ğ‘Ÿğ‘Ÿ32(ğ‘Œğ‘Œ âˆ’ ğ‘Œğ‘Œğ¶ğ¶ğ¶ğ¶) + ğ‘Ÿğ‘Ÿ33(âˆ’ğ‘“ğ‘“â€²)

ğ‘¦ğ‘¦ğ‘ğ‘ = âˆ’â„

ğ‘Ÿğ‘Ÿ21(ğ‘‹ğ‘‹ âˆ’ ğ‘‹ğ‘‹ğ¶ğ¶ğ¶ğ¶) + ğ‘Ÿğ‘Ÿ22(ğ‘Œğ‘Œ âˆ’ ğ‘Œğ‘Œğ¶ğ¶ğ¶ğ¶) + ğ‘Ÿğ‘Ÿ23(âˆ’ğ‘“ğ‘“â€²)
ğ‘Ÿğ‘Ÿ31(ğ‘‹ğ‘‹ âˆ’ ğ‘‹ğ‘‹ğ¶ğ¶ğ¶ğ¶) + ğ‘Ÿğ‘Ÿ32(ğ‘Œğ‘Œ âˆ’ ğ‘Œğ‘Œğ¶ğ¶ğ¶ğ¶) + ğ‘Ÿğ‘Ÿ33(âˆ’ğ‘“ğ‘“â€²)

ğ‘‹ğ‘‹ = âˆ’ğ‘“ğ‘“â€²

ğ‘Ÿğ‘Ÿ11ğ‘¥ğ‘¥ğ‘ğ‘ + ğ‘Ÿğ‘Ÿ21ğ‘¦ğ‘¦ğ‘ğ‘ + ğ‘Ÿğ‘Ÿ31(âˆ’â„)
ğ‘Ÿğ‘Ÿ13ğ‘¥ğ‘¥ğ‘ğ‘ + ğ‘Ÿğ‘Ÿ23ğ‘¦ğ‘¦ğ‘ğ‘ + ğ‘Ÿğ‘Ÿ33(âˆ’â„)

(21) 

(22) 

Onde: 

ğ‘Œğ‘Œ = âˆ’ğ‘“ğ‘“â€²

ğ‘Ÿğ‘Ÿ12ğ‘¥ğ‘¥ğ‘ğ‘ + ğ‘Ÿğ‘Ÿ22ğ‘¦ğ‘¦ğ‘ğ‘ + ğ‘Ÿğ‘Ÿ32(âˆ’â„)
ğ‘Ÿğ‘Ÿ13ğ‘¥ğ‘¥ğ‘ğ‘ + ğ‘Ÿğ‘Ÿ23ğ‘¦ğ‘¦ğ‘ğ‘ âˆ’ ğ‘Ÿğ‘Ÿ33(âˆ’â„)

  e 

  sÃ£o  coordenadas  do  ponto  na  imagem  dadas  no  sistema  referencial 

ğ‘¦ğ‘¦ğ‘ğ‘

fotogramÃ©trico da imagem original; 
ğ‘¥ğ‘¥ğ‘ğ‘
  Ã©  o  primeiro  dos  valores  associados  Ã   escala  e  que  substituiu  parcialmente  a 

distÃ¢ncia focal; 
â„

 Ã© o segundo dos valores associados Ã  escala e que substituiu parcialmente a 

distÃ¢ncia focal; 
ğ‘“ğ‘“â€²

 sÃ£o os elementos da matriz de rotaÃ§Ã£o; 

  sÃ£o  coordenadas  de  pontos  no  sistema  referencial  fotogramÃ©trico  da 

  e 

ğ‘Ÿğ‘Ÿğ‘–ğ‘–ğ‘–ğ‘–
, 
imagem transformada; 
ğ‘‹ğ‘‹

ğ‘Œğ‘Œ
, 

ğ‘ğ‘
 e 

 sÃ£o coordenadas do CP no sistema referencial fotogramÃ©trico da 

imagem transformada; 
ğ‘‹ğ‘‹ğ¶ğ¶ğ¶ğ¶
ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘

ğ‘Œğ‘Œğ¶ğ¶ğ¶ğ¶

Cabe explicar que, na prÃ¡tica, ao se adotar dois valores, 

 e 

, em lugar da 

distÃ¢ncia focal 

, significa que agora hÃ¡ dois parÃ¢metros corresponsÃ¡veis pela escala. 

ğ‘“ğ‘“â€²

â„

Assim, 

 estÃ¡ associado Ã  imagem de entrada e 

ğ‘“ğ‘“

 diz respeito Ã  imagem reamostrada 

(de saÃ­da). Dessa forma, a proporÃ§Ã£o entre estes novos parÃ¢metros implicarÃ¡ em uma 

ğ‘“ğ‘“â€²

â„

mudanÃ§a  de  escala  quando,  por  meio  das  equaÃ§Ãµes  de  colinearidade  direta  ou 

â€¢ 

â€¢ 

â€¢ 

â€¢ 

â€¢ 

â€¢ 

 
 
 
 
 
 
 
 
41 

inversa, se for de um sistema referencial fotogramÃ©trico para outro. Nestes termos, 

pode-se dizer que o fator de escala 

, matematicamente, Ã© expresso por (Silva, 2020): 

ğ¸ğ¸

ğ‘“ğ‘“â€²

                                                          (23) 

Objetivando  trabalhar  com 

ğ¸ğ¸ =

â„

  de  modo  percentual,  fixou-se 

  =  100.  Nestes 

termos, cabe ao usuÃ¡rio arbitrar o valor de 

ğ¸ğ¸
percentual de ampliaÃ§Ã£o/reduÃ§Ã£o da imagem de saÃ­da. Por exemplo, se 

ğ‘“ğ‘“â€²
entÃ£o as dimensÃµes da imagem de saÃ­da (largura e altura) serÃ£o iguais Ã s da imagem 

 estabelecerÃ¡ entÃ£o um 

. A escolha de 

ğ‘“ğ‘“â€²

â„

, 

â€²

ğ‘“ğ‘“

= â„ = 100

de entrada, considerando que nenhuma rotaÃ§Ã£o foi empreendida. E mais, se forem 

adotados valores de 

 maiores que 100, entÃ£o ocorrerÃ¡ a ampliaÃ§Ã£o percentual da 

imagem de saÃ­da. Por outro lado, adotar valores de 

ğ‘“ğ‘“â€²

 menores que 100 implicarÃ¡ na 

reduÃ§Ã£o da imagem de saÃ­da. 

ğ‘“ğ‘“â€²

Em  adiÃ§Ã£o  ao  que  foi  expresso  sobre  as  alteraÃ§Ãµes  relativas  Ã   escala, 

automatizou-se a determinaÃ§Ã£o de 

. Para tanto, foram considerados os valores de 

resoluÃ§Ã£o espacial das imagens a serem processas. Ou seja, uma vez que o algoritmo 

ğ‘“ğ‘“â€²

expresso  por  Al-Wassai  et  al.  (2011)  previa  que  as  imagens  multiespectrais 

deveriam ser reamostradas para as dimensÃµes da imagem pancromÃ¡tica, entÃ£o o 

parÃ¢metro 

 ficou: 

fâ€²

Onde: 

                                                (24) 

â€²

ğ‘“ğ‘“

=

ğ‘…ğ‘…ğ‘…ğ‘…ğ‘€ğ‘€ğ‘€ğ‘€
ğ‘…ğ‘…ğ‘…ğ‘…ğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒ Ã— 100

 Ã© a resoluÃ§Ã£o espacial das imagens multiespectrais; e 

 Ã© a resoluÃ§Ã£o espacial da imagem pancromÃ¡tica. 

â€¢ 

â€¢ 

ğ‘…ğ‘…ğ¸ğ¸ğ‘€ğ‘€ğ‘€ğ‘€
ğ‘…ğ‘…ğ¸ğ¸ğ¶ğ¶ğ‘ƒğ‘ƒğ‘ƒğ‘ƒ

A fim de materializar o expresso acima e considerando o exposto na seÃ§Ã£o 3.1, 

pode-se dizer que, como a resoluÃ§Ã£o espacial das imagens multiespectrais Ã© de 

e a resoluÃ§Ã£o espacial da imagem pancromÃ¡tica Ã© de 

, entÃ£o tem-se: 

20 ğ‘šğ‘š

5 ğ‘šğ‘š

Assim,  considerando 

â€²

ğ‘“ğ‘“

=

20
,  chega-se  Ã   conclusÃ£o  de  que  a  imagem 
5

Ã— 100 = 400

multiespectral resultante do processamento passarÃ¡ a ter 4 vezes as dimensÃµes da 

â€²

ğ‘“ğ‘“

= 400

imagem de entrada e uma quantidade de pixels de 16 vezes a quantidade da imagem 

original. 

 
 
 
Apesar das translaÃ§Ãµes, nas direÃ§Ãµes 

 e 

, estarem disponÃ­veis para alteraÃ§Ã£o 

42 

nas equaÃ§Ãµes de colinearidade, os parÃ¢metros relativos Ã  elas nÃ£o foram alterados e 

ğ‘¥ğ‘¥ 

ğ‘¦ğ‘¦

permaneceram  fixos.  PorÃ©m,  verificou-se  que  se  forem  efetuadas  translaÃ§Ãµes 

planimÃ©tricas no sistema referencial fotogramÃ©trico das imagens, entÃ£o poder-se-Ã¡ ter 

imagens  resultantes  transladadas  nas  direÃ§Ãµes 

  e/ou 

.  PorÃ©m,  alteraÃ§Ãµes 

algorÃ­tmicas deverÃ£o ocorrer a fim de se abarcar o processo de translaÃ§Ã£o. 

ğ‘¥ğ‘¥

ğ‘¦ğ‘¦

3.2.2  O algoritmo de fusÃ£o de imagens 

Apesar do carÃ¡ter restrito do algoritmo de retificaÃ§Ã£o de imagens, ele Ã© capaz 

de  correlacionar  duas  imagens  digitais  quaisquer  (imagem  original  e  imagem 

reamostrada). Considerando as alteraÃ§Ãµes descritas na subseÃ§Ã£o 3.2.1 e o software 

que as incorpora, pode-se afirmar que a imagem reamostrada, alÃ©m de poder sofrer 

rotaÃ§Ãµes, pode agora tambÃ©m possuir dimensÃ£o distinta Ã  da imagem de entrada. 

Cabe especificar que, no software de fusÃ£o de imagens construÃ­do, o algoritmo 

de retificaÃ§Ã£o foi definido como uma funÃ§Ã£o destinada Ã  reamostragem das imagens 

multiespectrais, conforme o estabelecido por Al-Wassai et al. (2011). Ou seja, uma 

vez que a imagem pancromÃ¡tica Ã© de melhor resoluÃ§Ã£o espacial (

), entÃ£o ela 

possui  uma  quantidade  maior  de  linhas  e  de  colunas  que  as  imagens 

5 m

multiespectrais relativas Ã  mesma Ã¡rea do terreno. Assim, para que o processo de 

fusÃ£o  de  imagens  ocorra  a  contento,  as  imagens  multiespectrais  devem  ser 

reamostradas de modo a ficarem com as mesmas quantidades de linhas e colunas 

da imagem pancromÃ¡tica. 

Para  fins  de  explanaÃ§Ã£o,  serÃ£o  considerados  os  passos  apresentados  no 

esquema  da  Figura  16.  Os  referidos  passos  estÃ£o  materializados  no  software 

â€œFusionâ€, que foi implementado em C/C++ (aplicaÃ§Ã£o console). 

â€¢  DefiniÃ§Ã£o das Imagens 

Inicialmente, ao se executar o software, uma janela do prompt de comando 

do MS-DOS (Microsoft Disk Operating System) se abre e Ã© apresentada uma lista 

de  imagens  de  formato  PGM,  as  quais  se  encontram  no  mesmo  diretÃ³rio  do 

programa.  Assim,  o  usuÃ¡rio  pode  ter  uma  visÃ£o  geral  das  possibilidades  de 

imagens  a  serem  utilizadas  para  processamento.  Cabe  esclarecer  que,  o 

 
 
 
programa nÃ£o foi construÃ­do com interface no padrÃ£o â€œFor Windowsâ€, por se tratar 

de uma versÃ£o preliminar, destinada Ã  pesquisa somente. 

43 

Figura 16: Etapas do algoritmo de fusÃ£o de imagens, materializadas no software â€œFusionâ€. 
Fonte: Elaborado pelo autor. 

Na Figura 17 Ã© apresentada a interface do Fusion, na qual se pode ver, ao 

final, a solicitaÃ§Ã£o da primeira imagem de entrada (Componente R). Ao introduzir 

o nome da componente, deve-se pressionar â€œEnterâ€ e o programa, paulatinamente, 

solicitarÃ¡  a  inserÃ§Ã£o  das  demais  imagens  (Componente  G,  Componente  B  e 

imagem  pancromÃ¡tica).  O  programa  possui  um  temporizador  destinado  Ã  

contagem  do  tempo  de  execuÃ§Ã£o.  AlÃ©m  do  processamento  propriamente  dito,  o 

temporizador computa o tempo de inserÃ§Ã£o dos nomes das imagens. 

 
 
44 

Figura 17: Interface do Fusion: Prompt de Comando de MS-DOS. 
Fonte: Elaborado pelo autor. 

â€¢  Leitura das Imagens 

ApÃ³s  a  introduÃ§Ã£o  do  nome  e  do  formato  da  Ãºltima  imagem,  o  programa 

efetua a leitura das imagens e armazena as suas intensidades de brilho em vetores 

do  tipo 

  (inteiro  sem  sinal  no  intervalo  [0,255]).  As  dimensÃµes  das 

imagens  tambÃ©m  sÃ£o  armazenadas  e  sÃ£o  alocadas  em  variÃ¡veis  do  tipo 

unsigned char

(inteiro). 

â€¢  Leitura de ParÃ¢metros 

int

Considerando  que  a  rotina  relativa  Ã   retificaÃ§Ã£o  modificada  incorpora  o 

programa Fusion, os parÃ¢metros relativos Ã s rotaÃ§Ãµes, 

, permaneceram na 

implementaÃ§Ã£o  e  sÃ£o  lidos  a  partir  de  arquivo  (setup.dat),  juntamente  com  os 

Îº, Ï† e Ï‰

valores das resoluÃ§Ãµes espaciais das imagens. 

â€¢  CÃ¡lculo de 

Uma  vez  que,  as  dimensÃµes  da  imagem  tenham  sido  lidas  quando  do 

fâ€²

carregamento das imagens, efetua-se, de acordo com a equaÃ§Ã£o 28, o cÃ¡lculo do 

parÃ¢metro 

. 

â€¢  Reamostragem das Imagens Multiespectrais 

fâ€²

Considerando que as imagens multiespectrais possuem dimensÃµes (largura 

e altura) menores que as das respectivas imagens pancromÃ¡ticas utilizadas e que 

Al-Wassai et al. (2011) preconizam a reamostragem das imagens multiespectrais, 

sÃ£o enviadas Ã  funÃ§Ã£o de retificaÃ§Ã£o modificada: 

1)  Um Ãºnico vetor de imagem multiespectral, a ser reamostrado; 

 
 
 
 
45 

2)  DimensÃµes da imagem multiespectral (largura e altura); 

3)  Os valores dos Ã¢ngulos de rotaÃ§Ã£o, 

, que, neste caso particular 

foram tomados nulos; 

Îº, Ï† e Ï‰

4)  O valor de 

, calculado anteriormente. 

Cabe  especificar  que,  a  referida  funÃ§Ã£o  Ã©  utilizada  (chamada)  trÃªs  vezes, 

fâ€²

uma para cada vetor de imagem multiespectral. 

Como saÃ­da, a funÃ§Ã£o de retificaÃ§Ã£o modificada retorna o vetor da imagem 

reamostrada  e  as  suas  dimensÃµes.  Ao  se  analisar  e  constatar  a  correÃ§Ã£o  dos 

resultados  parciais,  verificou-se  que  a  rotina  efetuou  de  modo  exÃ­mio  a 

reamostragem. Assim, a ampliaÃ§Ã£o almejada ocorreu a contento. PorÃ©m, verificou-

se tambÃ©m que, as dimensÃµes da imagem pancromÃ¡tica excediam as dimensÃµes 

da imagem reamostrada, fato que dificultaria a geraÃ§Ã£o da imagem fusionada. 

Diante  deste  fato,  buscou-se  efetuar  novos  e  sucessivos  recortes  das 

imagens do CBERS 4 a fim de se verificar, sistematicamente, as suas dimensÃµes 

e  se efetuar  cÃ¡lculos  comparativos. Nestes  termos,  foi  possÃ­vel  constatar  que  a 

referida  inconsistÃªncia  era  proveniente  do  algoritmo  de  recortes  do  QGIS  e  que 

este efeito indesejÃ¡vel, quando da geraÃ§Ã£o de imagens fusionadas por meio deste 

SIG,  poderia  ser  tratado  por  meio  do  recurso  â€œ

â€,  que  equiparava 

coerentemente as referidas dimensÃµes. 

Superimpose

O  aspecto  favorÃ¡vel  descoberto  foi  a  presenÃ§a  de  uma  certa  simetria 

sistemÃ¡tica.  Ou  seja,  como  as  imagens  eram  registradas  no  mesmo  sistema 

referencial  geodÃ©sico,  verificou-se  que  as  quantidades  de  linhas  e  colunas 

excedentes  da  imagem  pancromÃ¡tica  diziam  respeito  Ã s  bordas  (ou  quadro)  da 

imagem pancromÃ¡tica. Assim, optou-se por descartar, simetricamente, tal quadro 

da imagem pancromÃ¡tica, de modo que se chegasse Ã  igualdade das dimensÃµes 

para todas as imagens necessÃ¡rias ao processamento (pancromÃ¡tica e imagens 

multiespectrais reamostradas). 

Algo digno de nota Ã© que, este passo, relacionado Ã  reamostragem, Ã© o que 

torna  a  pesquisa  Ãºnica.  Ou  seja,  o  processo  de  reamostragem  baseado  no 

algoritmo de retificaÃ§Ã£o de imagens digitais e, consequentemente, nas equaÃ§Ãµes 

de colinearidade, Ã© algo que nÃ£o se encontra na literatura. 

 
 
46 

â€¢  NormalizaÃ§Ã£o das Imagens no Intervalo [0,1] 

Seguindo  o  preconizado  na  seÃ§Ã£o  2.2,  os  valores  de  brilho  das  imagens 

reamostradas  foram,  linearmente,  normalizados  no  intervalo  [0,1].  Para  tanto, 

foram criados vetores do tipo 

 (nÃºmeros reais, ou de ponto flutuante, com 

dupla  precisÃ£o)  que  receberam  os  valores  de  brilho  dos  vetores  reamostrados 

double

originais  e,  a  partir  destes  novos  vetores,  se  efetuou  a  normalizaÃ§Ã£o.  Cabe 

esclarecer  que,  a  normalizaÃ§Ã£o  foi  implementada  como  funÃ§Ã£o  e  foi  utilizada 

(chamada) trÃªs vezes, uma vez para cada vetor de imagem. 

â€¢  GeraÃ§Ã£o das Componentes 

 (Matiz) e 

 (SaturaÃ§Ã£o) 

Tomando  os  vetores 

H

,  cujos  tons  de  cinza  foram  normalizados  no 

S

intervalo [0,1], e utilizando-se as equaÃ§Ãµes 2 e 3, gerou-se vetores 

double

 relativos 

Ã s  componentes 

  (Matiz)  e 

  (SaturaÃ§Ã£o).  Cabe  esclarecer  que,  como  no 

double

processo  de  fusÃ£o  de  imagens  IHS  a  componente 

  (Intensidade)  Ã©  substituÃ­da 

H

S

pela imagem pancromÃ¡tica, nÃ£o foi necessÃ¡rio gerar um vetor relativo Ã  

I

â€¢  TransformaÃ§Ã£o do Sistema IHS para RGB 

. 

I

Tomando-se os vetores 

, relativos Ã s componentes 

 e 

, e Ã  imagem 

pancromÃ¡tica,  utilizou-se  as  equaÃ§Ãµes  de  4  a  12  para  se  gerar  as  novas 

double

H

S

componentes 

, 

 e 

, que comporÃ£o a imagem fusionada. 

â€¢  NormalizaÃ§Ã£o das Imagens 
G

R

B

, 

 e 

 para o Intervalo [0,255] 

Segundo o descrito na seÃ§Ã£o 2.2, os vetores 

R

G

B

, 

 e 

, resultantes do passo 

anterior,  estÃ£o  normalizados  no  intervalo  [0,1].  Assim,  a  fim  de  que  o  resultado 

R

G

B

possa ser adequadamente visualizado a posteriori, efetua-se a normalizaÃ§Ã£o dos 

referidos vetores no intervalo [0,255]. 

â€¢  Realce Linear e GeraÃ§Ã£o da ComposiÃ§Ã£o Colorida Fusionada 

Cabe esclarecer que, alguns trabalhos encontrados na literatura, bem como, 

os autores Gonzalez e Woods (2010), preconizam que algum prÃ©-processamento 

baseado  em  histograma  pode  ser  utilizado  na  imagem  pancromÃ¡tica  de  modo  a 

melhorar  o  aspecto  pictÃ³rico  da  imagem  fusionada.  PorÃ©m,  neste  trabalho  este 

tipo  de  prÃ©-processamento  nÃ£o 

foi  utilizado.  Com 

isso,  buscou-se  um 

conhecimento  epistemolÃ³gico  acerca  do  processo  de  fusÃ£o  em  si,  sem  que 

 
47 

nenhum processo acessÃ³rio modificasse as caracterÃ­sticas das imagens a priori e 

viesse a se propagar, gerando alteraÃ§Ãµes a posteriori. 

Assim,  gerou-se,  em  cada  experimento,  uma  imagem  fusionada  de  baixo 

contraste,  sem  qualquer  tipo  de  prÃ©  ou  pÃ³s-processamento,  e  tambÃ©m  uma 

imagem  afetada  de  realce  de  contraste  linear  (pÃ³s-processamento).  Tais 

resultados sÃ£o apresentados no capÃ­tulo que segue. 

Informa-se ainda que, as imagens, fusionada â€œin naturaâ€  e aquela  na qual 

foi aplicado o realce de contraste linear, foram ambas geradas pelo mesmo laÃ§o 

,  responsÃ¡vel  pela  composiÃ§Ã£o  da  imagem  de  saÃ­da.  Para  tanto,  foram  feitas 

alteraÃ§Ãµes  diretamente  no  prÃ³prio  cÃ³digo,  a  fim  de  que  as  imagens  fossem 
for
geradas, uma apÃ³s outra. 

â€¢  Salvamento da Imagem Fusionada 

ApÃ³s  o  processamento,  o  programa  salva  automaticamente  a  imagem  de 

saÃ­da com o nome â€œFusion.rawâ€. Cabe explicar que, o formato da imagem da saÃ­da 

nÃ£o Ã© o mesmo daquele requerido para a imagem de entrada (PGM). Isso se deu, 

por conta de a imagem de saÃ­da ser colorida e as imagens de entrada serem em 

tons  de  cinza.  Logo,  haveria  a  necessidade  de  adequaÃ§Ã£o  das  rotinas  de 

salvamento. PorÃ©m, foi priorizada a obtenÃ§Ã£o de resultados para fins de anÃ¡lise 

de  consistÃªncia  do  algoritmo  e  da  implementaÃ§Ã£o  propostos,  de  modo  que  se 

considerou os formatos das imagens de entrada e de saÃ­da irrelevantes. Por outro 

lado, fixou-se como marco a necessidade de que, para ambos os casos (input e 

output), as imagens fossem matriciais (raster). 

â€¢  Encerramento do Programa 

Ao 

final  do  processamento,  o  programa 

indica  o 

tempo 

total  de 

processamento  e  informa  ao  usuÃ¡rio  que  a  imagem  de  saÃ­da  foi  salva.  Para 

encerrar o processamento, basta pressionar qualquer tecla que a janela do prompt 

de comando do MS-DOS se fecharÃ¡. 

 
 
 
 
48 

4  RESULTADOS E ANÃLISES  

Considerando  a  necessidade  de  se  averiguar  a  eficiÃªncia  da  metodologia 

elaborada  e  executada,  foram  feitos  experimentos  e  a  posterior  anÃ¡lise  dos 

resultados. Nestes termos, na seÃ§Ã£o 4.1 sÃ£o fornecidas algumas informaÃ§Ãµes sobre 

as imagens digitais, que sÃ£o a fonte de dados para os processamentos realizados. 

AlÃ©m disso, na seÃ§Ã£o 4.2 sÃ£o apresentados os principais resultados, provenientes dos 

experimentos, com as devidas discussÃµes e as conclusÃµes que se pÃ´de depreender 

destas. 

4.1  Dados de Entrada 

Conforme o relatado na seÃ§Ã£o 3.1, as imagens adquiridas sÃ£o provenientes do 

satÃ©lite  CBERS  4.  Estas 

imagens  ortorretificadas  sÃ£o  georreferenciadas, 

considerando o Sistema Referencial GeodÃ©sico WGS84 e o sistema de projeÃ§Ã£o UTM 

(zona 22). As imagens sÃ£o relativas Ã  regiÃ£o de fronteira entre os estados de Mato 

Grosso do Sul e SÃ£o Paulo, onde se encontra o Rio ParanÃ¡.  

Apesar das imagens serem provenientes de um mesmo satÃ©lite, o sensor de 

captaÃ§Ã£o  das  imagens  das  bandas 

, 

  e 

  foi  a  CÃ¢mera  Multiespectral  Regular 

(MUX)  e  o  sensor  que  realizou  a  captura  da  imagem  pancromÃ¡tica  foi  a  CÃ¢mera 

ğ‘…ğ‘…

ğºğº

ğµğµ

PancromÃ¡tica Multiespectral (PAN). Os dados relativos aos sensores MUX e PAN sÃ£o 

disponibilizados  pelo  site  do  INPE  e  apresentados  a  seguir,  respectivamente,  nas 

Figuras 18 e 19. 

Apesar da grande Ã¡rea abarcada pelas imagens, para fins de experimentaÃ§Ã£o 

se efetuou dois recortes nas respectivas imagens e com estes recortes foram feitos 

os  experimentos.  No  sentido  de  posicionar  as  Ã¡reas  de  estudo,  logo  abaixo, 

encontram-se as coordenadas referentes Ã s hidrelÃ©tricas abarcadas em cada um dos 

recortes, considerando a ordem em que aparecem na imagem da Figura 20: 

â€¢  Ao  Norte  (mais  acima  na  imagem):  Usina  HidrelÃ©trica  de  Ilha  Solteira  - 

coordenadas 20.3828Â° S, 51.3622Â° W â€“ MunicÃ­pio de Ilha Solteira â€“ SP. 

â€¢  Ao  Sul  (mais  abaixo  na  imagem):  Usina  HidrelÃ©trica  Engenheiro  Souza  Dias  - 

Coordenadas 20Â°46'45"S, 51Â°37'45"W â€“ MunicÃ­pio de TrÃªs Lagoas â€“ MS. 

 
 
 
 
49 

Figura 18: Tabela contendo os dados e especificaÃ§Ãµes da cÃ¢mera MUX, responsÃ¡vel pela captaÃ§Ã£o 

das bandas 

, 

 e 

. 

Fonte: Instituto Nacional De Pesquisas Espaciais 

ğ‘…ğ‘…

ğºğº

ğµğµ

Figura 19: Tabela contendo os dados e especificaÃ§Ãµes da cÃ¢mera PAN, responsÃ¡vel pela captaÃ§Ã£o 
da imagem pancromÃ¡tica. 
Fonte: Instituto Nacional de Pesquisas Espaciais 

Na  imagem  da  Figura  20  encontram-se  os  dois  retÃ¢ngulos  que  delimitam  os 

recortes  de  imagem  inerentes  Ã s  Ã¡reas  escolhidas.  Conforme  o  especificado,  o 

retÃ¢ngulo superior demarca o reservatÃ³rio da HidrelÃ©trica de Ilha Solteira e o retÃ¢ngulo 

inferior delimita o reservatÃ³rio da hidrelÃ©trica Engenheiro Souza Dias (JupiÃ¡). 

 
 
 
50 

Figura 20: AlocaÃ§Ã£o dos reservatÃ³rios hidrelÃ©tricos - RetÃ¢ngulo Superior: ReservatÃ³rio de Ilha 
Solteira; RetÃ¢ngulo Inferior: ReservatÃ³rio Engenheiro Souza Dias (JupiÃ¡). 
Fonte: Acervo do autor 

Ainda  no  sentido  de  demarcar  as  Ã¡reas  de  estudo,  tem-se  abaixo  as 

coordenadas  relativas  aos  cantos  superior  esquerdo  e  inferior  direito  de  cada 

retÃ¢ngulo que abarca os reservatÃ³rios em questÃ£o. Assim, tem-se: 

â€¢  Usina de Ilha Solteira: 

Canto superior esquerdo: zona 22, E 454596,2444, N 7756314,6773. 

Canto inferior direito: zona 22, E 476420,7575, N 7742748,0881. 

â€¢  Usina de JupiÃ¡: 

Canto superior esquerdo: zona 22, E 425103,6593, N 7711780,8737. 

Canto inferior direito: zona 22, E 443978,9138, N 7699099,0621. 

Cada  uma  das  imagens  multiespectrais  (

, 

  e 

)  possui  uma  resoluÃ§Ã£o 

espacial de 

, ou seja, cada pixel da imagem recobre uma Ã¡rea de 20 metros por 
ğ‘…ğ‘…

ğµğµ

ğºğº

20 metros. A resoluÃ§Ã£o espacial da imagem pancromÃ¡tica Ã© de 

20 ğ‘šğ‘š

, o que evidÃªncia 

uma riqueza de detalhes bastante superior. As composiÃ§Ãµes coloridas e respectivas 

5 ğ‘šğ‘š

componentes podem ser vistas nas Figuras 21 e 22. 

 
 
51 

(a)                                                               (b) 

(c)                                                              (d) 

Figura 21: Usina de Ilha Solteira: (a) ComposiÃ§Ã£o colorida â€“ 

; (b) Componente vermelha; 

(c) Componente verde; (d) Componente azul. 
Fonte: Acervo do autor 

20 ğ‘šğ‘š

(a)                                                               (b) 

Figura 22: Usina de JupiÃ¡: (a) ComposiÃ§Ã£o colorida â€“ 

; (b) Componente vermelha; 

(c)                                                              (d) 

(c) Componente verde; (d) Componente azul. 
Fonte: Acervo do autor 

20 ğ‘šğ‘š

 
  
 
  
 
 
  
 
  
 
52 

As  imagens  da  Figura  21  e  22  possuem  uma  tonalidade  escura  devido  a 

caracterÃ­sticas  do  sensor  de  captaÃ§Ã£o.  Assim,  por  vezes  faz-se  necessÃ¡rio  algum 

processamento relativo ao realce do contraste a fim de melhorar seu aspecto pictorial. 

A  seguir,  estÃ£o  expostas  as  imagens  pancromÃ¡ticas  (recortes)  de  alta 

resoluÃ§Ã£o, relativas a cada regiÃ£o de hidrelÃ©trica. Para a geraÃ§Ã£o destes recortes foi 

utilizada a banda 01 da CÃ¢mera PancromÃ¡tica Multiespectral (PAN). 

(a) 

(b) 
Figura 23: Imagens PancromÃ¡ticas: (a) RegiÃ£o da Usina de Ilha Solteira; (b) RegiÃ£o da Usina de 
JupiÃ¡  
Fonte: Acervo do autor 

 
 
 
53 

4.2  Resultados Experimentais e AnÃ¡lises  

Nas subseÃ§Ãµes que seguem sÃ£o apresentados os resultados da pesquisa, bem 

como, as explanaÃ§Ãµes, elucubraÃ§Ãµes e conclusÃµes expressas sobre tais resultados. 

4.2.1  Procedimentos preliminares 

Primeiramente,  foi  realizada,  via  software  QGIS,  a  composiÃ§Ã£o  entre  as  trÃªs 

imagens  (recortes)  relativas  Ã s  bandas 

, 

  e 

,  obtendo-se  as  imagens  coloridas 

com  resoluÃ§Ã£o  espacial  de  20  metros  para  cada  uma  das  duas  regiÃµes.  Tais 

ğ‘…ğ‘…

ğºğº

ğµğµ

composiÃ§Ãµes sÃ£o apresentadas na Figura 24. Cada imagem colorida possui a mesma 

quantidade de linhas e de colunas que cada uma das trÃªs imagens originÃ¡rias, mais 

especificamente, 944X634 pixels para a imagem da regiÃ£o de JupiÃ¡ e 1091X678 pixels 

para a imagem que abarca a regiÃ£o de ilha Solteira. 

Como  as  imagens  pancromÃ¡ticas  possuem  dimensÃµes  (largura  e  altura) 

maiores que as das imagens relativas Ã s bandas 

, 

 e 

, para que se realizasse a 

fusÃ£o  entre  a  imagem  colorida  e  a  pancromÃ¡tica,  foi  necessÃ¡rio  fazer  uma 

ğ‘…ğ‘…

ğºğº

ğµğµ

reamostragem  das  imagens  multiespectrais,  de  modo  que  estas  ficassem  com  as 

mesmas  dimensÃµes  das  imagens  pancromÃ¡ticas  e,  assim,  se  pudesse  realizar  o 

processo de fusÃ£o a contento. O processo de reamostragem, que estÃ¡ incorporado ao 

software  Fusion,  desenvolvido  no  Ã¢mbito  deste  trabalho,  implicou  na  geraÃ§Ã£o  de 

imagens (recortes) com maior quantidade de linhas e colunas. PorÃ©m, como era de 

se esperar, a reamostragem das componentes nÃ£o implicou na melhoria da qualidade 

pictorial  das  imagens.  Cabe  especificar  ainda  que,  a  dimensÃ£o  da  imagem 

pancromÃ¡tica  relativa  Ã   regiÃ£o  de  Ilha  Solteira  Ã©  de  4361X2709,  ao  passo  que  a 

pancromÃ¡tica  da  regiÃ£o  de  JupiÃ¡  apresenta  dimensÃ£o  de  3773X2533.  As  imagens 

coloridas simplesmente reamostradas sÃ£o apresentadas na Figura 25. 

 
 
 
 
 
54 

(a) 

(b) 

Figura 24: ComposiÃ§Ãµes coloridas: (a) RegiÃ£o da Usina de Ilha Solteira; (b) RegiÃ£o da Usina de 
JupiÃ¡. 
Fonte: Acervo do autor 

 
 
 
55 

Figura 25: Imagens coloridas reamostradas relativas Ã s regiÃµes de Ilha Solteira (Ã  esquerda) e de 
JupiÃ¡ (Ã  direita). 
Fonte: Acervo do autor  

Como a tonalidade das imagens resultantes do processo de reamostragem se 

mostrou escura, visto que nÃ£o se submeteu as componentes a prÃ©-processamento, 

foi realizado, via software IrfanView, um realce de contraste de cores, de modo que, 

se  melhorasse  a  qualidade  visual  das  imagens.  Os  resultados  sÃ£o  apresentados  a 

seguir. 

Figura 26: Imagens coloridas reamostradas com realce, relativas Ã s regiÃµes de Ilha Solteira (Ã  
esquerda) e de JupiÃ¡ (Ã  direita). 
Fonte: Acervo do autor  

Ã‰  importante  frisar  que,  apesar  do  aumento  da  resoluÃ§Ã£o  em  pixels  das 

imagens coloridas no processo de reamostragem e do realce de contraste efetuado 

via IrfanView, a qualidade pictorial da imagem se manteve baixa, o que implica em 

uma ausÃªncia de melhora na qualidade quando a simples reamostragem Ã© efetuada. 

4.2.2  A fusÃ£o das imagens 

Tendo  jÃ¡  disponÃ­veis  a  imagem  pancromÃ¡tica  de  alta  resoluÃ§Ã£o  e  a  imagem 

colorida reamostrada de cada regiÃ£o (recortes), reuniu-se as condiÃ§Ãµes necessÃ¡rias 

para  a  fusÃ£o  de  imagens.  O  procedimento  se  baseou  no  mÃ©todo  IHS,  no  qual  se 

 
  
 
   
 
 
 
substitui a componente "Intensidade" da imagem colorida pela imagem pancromÃ¡tica, 

conforme explicado na seÃ§Ã£o 2.5. Na figura a seguir sÃ£o mostrados os resultados dos 

experimentos. 

56 

(a) 

(b) 
Figura 27: Imagens fusionadas: (a) RegiÃ£o da Usina de Ilha Solteira; (b) RegiÃ£o da Usina de JupiÃ¡. 
Fonte: Acervo do autor 

 
 
 
57 

Como se pode notar, novamente a tonalidade das imagens se mostrou escura, 

sendo necessÃ¡rio a aplicaÃ§Ã£o de um processo de realce de contraste. Tal processo 

foi  realizado  por  dois  caminhos  distintos.  Primeiramente  foi  realizado  um  realce  de 

contraste linear, incorporado ao cÃ³digo do prÃ³prio programa Fusion, e se obteve uma 

boa melhora no aspecto visual das imagens. Seguem os resultados na Figura 28. 

Figura 28: Imagens fusionadas resultantes do experimento, com realce  de contraste linear, relativas 
Ã s regiÃµes de Ilha Solteira (Ã  esquerda) e de JupiÃ¡ (Ã  direita). 
Fonte: Acervo do autor 

Pode-se  verificar  que,  nos  resultados  apresentados  nas  Figuras  27,  28  e  29 

que,  apesar  das  discrepÃ¢ncias  relativas  ao  contraste,  a  geometria  das  imagens 

fusionadas  se  mostrou  superior  Ã   das 

imagens  que 

foram  simplesmente 

reamostradas.  Segundo  Gonzalez  e  Woods  (2010)  o  acrÃ©scimo  na  qualidade 

geomÃ©trica e na nitidez da imagem Ã© proveniente da substituiÃ§Ã£o da componente 

pelos nÃ­veis de brilho provenientes da imagem pancromÃ¡tica. Em adiÃ§Ã£o a isso, pode-
ğ¼ğ¼
se  concluir  que  o  software  Fusion  foi  eficaz  em  gerar  imagens  fusionadas  de  alta 

qualidade, a menos do contraste final. PorÃ©m, como fora dito anteriormente, em um 

primeiro  momento  se  buscou  efetuar  mudanÃ§as  no  algoritmo  de  retificaÃ§Ã£o  de 

imagens  digitais  e  nas  equaÃ§Ãµes  de  colinearidade,  de  modo  que,  estes  elementos 

fossem  agregados  ao  mÃ©todo  de  fusÃ£o  IHS  de  imagens.  Assim,  partindo-se  do 

pressuposto de que qualquer prÃ©-processamento poderia falsear os resultados, optou-

se por se processar dados â€œin naturaâ€. 

Para  fins  de  comparaÃ§Ã£o,  foi  realizado  o  processo  de  realce  automÃ¡tico  das 

imagens  fusionadas  no  software  IrfanView  (ver  Figura  29).  Decorre  de  tal 

processamento  uma  melhora  do  aspecto  visual  das  imagens.  Deste  modo,  essas 

imagens sÃ£o consideradas o produto final do experimento e expressam um resultado 

bastante satisfatÃ³rio. 

 
   
 
 
58 

(a) 

(b) 
Figura 29: Imagem fusionada realÃ§ada: (a) RegiÃ£o da Usina de Ilha Solteira; (b) RegiÃ£o da Usina de 
JupiÃ¡. 
Fonte: Acervo do autor  

Cabe ressaltar que, devido Ã  ausÃªncia de prÃ©-processamento baseado em 

histograma, indicado para a imagem pancromÃ¡tica (GONZALEZ e WOODS, 2010), as 

 
 
 
59 

cores  passaram  a  ter  alguma  discrepÃ¢ncia  com  relaÃ§Ã£o,  por  exemplo,  Ã s  imagens 

simplesmente reamostradas. No entanto, este resultado se assemelha sobremaneira 

com o obtido em outros trabalhos anÃ¡logos e que usaram softwares comerciais, como 

Ã© o caso de Pisani et al. (2019) e Oliveira (2018). Assim, pode-se dizer que se chegou 

a resultados totalmente passÃ­veis de serem utilizados no Ã¢mbito das GeociÃªncias. 

(a)                                                                              (b) 
Figura 30: ComparaÃ§Ã£o de detalhes (regiÃ£o de Ilha Solteira): (a) Imagem colorida inicial; 
(b) Imagem fusionada 
Fonte: Acervo do autor 

(a)                                                                              (b) 
Figura 31: ComparaÃ§Ã£o de detalhes (regiÃ£o de JupiÃ¡): (a) Imagem colorida inicial; 
(b) Imagem fusionada 
Fonte: Acervo do autor 

 
 
 
60 

Para uma melhor compreensÃ£o acerca dos resultados obtidos, as Figuras 30 e 

31 trazem em destaque detalhes com Ãªnfase na malha urbana, visto que este tipo de 

Ã¡rea apresenta riqueza de detalhes radiomÃ©tricos e geomÃ©tricos que podem auxiliar 

na comparaÃ§Ã£o visual do grau de qualidade das imagens. A comparaÃ§Ã£o se dÃ¡ entre 

a  imagem  gerada  inicialmente  pela  simples  composiÃ§Ã£o  das  bandas 

, 

  e 

,  e  a 

imagem final fusionada, resultante do experimento. 

ğ‘…ğ‘…

ğºğº

ğµğµ

Analisando  comparativamente  as 

regiÃµes  homÃ³logas,  presentes  na 

composiÃ§Ã£o  colorida  inicial,  de  baixa  resoluÃ§Ã£o  espacial,  e  na  imagem  fusionada, 

pode-se verificar que, tanto no par de imagens apresentadas na Figura 30, quanto no 

par de imagens presentes na Figura 31, a nitidez e a qualidade geomÃ©trica da imagem 

fusionada  sÃ£o  bastante  superiores.  Ou  seja,  como  se  pode  notar,  para  ambas  as 

regiÃµes, houve uma diferenÃ§a significativa na qualidade dos detalhes mostrados em 

cada imagem, com a melhora na resoluÃ§Ã£o espacial, que passou de 20 metros para 5 

metros.  

Destas  observaÃ§Ãµes,  conclui-se  que,  com    relaÃ§Ã£o  Ã s  imagens  fusionadas 

resultantes,  houve  um  ganho  em  qualidade  geomÃ©trica,  visto  que  os  contornos  se 

apresentam  bem  definidos,  e  radiomÃ©trica,  considerando  que  as  cores  propiciam  a 

identificaÃ§Ã£o precisa dos alvos. Sendo assim, pode-se depreender que o objetivo do 

trabalho  foi  alcanÃ§ado,  uma  vez  que,  as  alteraÃ§Ãµes  e  modificaÃ§Ãµes  empreendidas 

propiciaram a geraÃ§Ã£o de resultados satisfatÃ³rios. 

 
 
 
61 

5  CONCLUSÃ•ES E RECOMENDAÃ‡Ã•ES: 

A  seguir,  sÃ£o  feitas  consideraÃ§Ãµes  acerca  da  pesquisa  realizada  e  dos 

experimentos  efetuados,  com  Ãªnfase  nos  resultados  e  nas  conclusÃµes  alcanÃ§adas. 

TambÃ©m  sÃ£o  feitas  algumas  recomendaÃ§Ãµes  no  sentido  de  contribuir  para  com  os 

possÃ­veis  trabalhos  relacionados  ao  software  desenvolvido,  objetivando  o  seu 

aproveitamento na construÃ§Ã£o de futuros trabalhos acadÃªmicos. 

5.1  ConclusÃµes 

A pesquisa envolveu conceitos matemÃ¡ticos aliados Ã s noÃ§Ãµes das Ã¡reas de 

GeodÃ©sia,  Fotogrametria,  Sensoriamento  Remoto  e  InformÃ¡tica.  Ela  consistiu  no 

estudo e posteriores alteraÃ§Ãµes do algoritmo de retificaÃ§Ã£o de imagens digitais, bem 

como, das equaÃ§Ãµes de colinearidade, com vistas Ã  construÃ§Ã£o de um novo software 

capaz de  efetuar a fusÃ£o  de imagens via mÃ©todo IHS. O software, implementado 

em  um  contexto  experimental  acadÃªmico,  apresentou 

resultados  bastante 

satisfatÃ³rios. 

Assim,  da  execuÃ§Ã£o  da  pesquisa,  pÃ´de-se  tirar  algumas  conclusÃµes  Ãºteis  e 

robustas. Primeiramente, ficou evidente que as alteraÃ§Ãµes efetuadas nas equaÃ§Ãµes 

de colinearidade foram condizentes e geraram os resultados satisfatÃ³rios esperados, 

e, deste modo, as alteraÃ§Ãµes algorÃ­tmicas relacionadas ao algoritmo de retificaÃ§Ã£o de 

imagens  digitais  foram  efetivas  em  gerar  as  imagens  multiespectrais  reamostradas 

necessÃ¡rias Ã  fusÃ£o de imagem via mÃ©todo IHS. AlÃ©m disso, o descarte do quadro da 

imagem  pancromÃ¡tica,  que  objetivava  parear  as  dimensÃµes  desta  imagem  com  as 

dimensÃµes relativas Ã s imagens multiespectrais, funcionou a contento. Vale ressaltar 

tambÃ©m  que  a  implementaÃ§Ã£o  do  software  Fusion  gerou  os  resultados  esperados, 

sendo que da anÃ¡lise visual depreende-se a boa qualidade destes resultados, os quais 

estÃ£o  em  acordo  com  trabalhos  de  mesmo  cerne.  As  manipulaÃ§Ãµes  e  dispositivos 

digitais  utilizados  atuaram  dentro  do  previsto.  Por  fim,  ressalta-se  que  o  software 

QGIS foi de grande valia ao processo de fusÃ£o de imagens, pois foi eficaz no registro 

das imagens e na geraÃ§Ã£o de recortes. 

Pelas  conclusÃµes  expostas  acima,  conclui-se  que  houve  um  ganho  cientÃ­fico 

relacionado aos resultados obtidos e ao produto de software gerado. PorÃ©m, o ganho 

maior ficou por conta do know-how adquirido pela equipe executora do projeto. 

 
 
 
 
62 

5.2  RecomendaÃ§Ãµes 

Entende-se que o produto de software, obtido como resultado deste trabalho, 

poderÃ¡ ser utilizado futuramente em outros trabalhos cujas temÃ¡ticas se assemelhem 

Ã quela trabalhada no Ã¢mbito da pesquisa aqui descrita. E mais, entende-se que os 

resultados  alcanÃ§ados  (imagens  fusionadas)  sÃ£o  passÃ­veis  de  serem  utilizados  em 

aplicaÃ§Ãµes de geoprocessamento. 

AlÃ©m  disso,  o  software  aqui  implementado  pode  ser  tambÃ©m  utilizado  no 

ambiente  universitÃ¡rio  ou  em  ambientes  educacionais  de  modo  geral,  como 

ferramenta  didÃ¡tica  no  aprendizado  de  disciplinas  relacionadas  ao  Sensoriamento 

Remoto e Ã  Geografia, alÃ©m da prÃ³pria MatemÃ¡tica, servido de exemplo de aplicaÃ§Ã£o 

prÃ¡tica de conceitos matemÃ¡ticos. 

Buscando  aprimorar  o  produto  cientÃ­fico  e  tecnolÃ³gico  gerado,  apresenta-se 

algumas  recomendaÃ§Ãµes  para  trabalhos  futuros.  Entre  elas,  sugere-se  efetuar  a 

comparaÃ§Ã£o estatÃ­stica dos resultados gerados pelo Fusion com resultados gerados 

por outros softwares como, por exemplo, o QGIS. Ã‰ interessante tambÃ©m que seja 

considerada  possibilidade  de  se  introduzir,  no  software  obtido  neste  trabalho,  as 

potencialidades relativas Ã  manipulaÃ§Ã£o de sistemas referenciais geodÃ©sicos, a fim de 

que o registro das imagens possa se tornar um processo interno a ele. AlÃ©m disso, ao 

Fusion,  Ã©  possÃ­vel  acrescentar  mÃ©todos  eficazes  de  realce  de  contraste,  alÃ©m  de 

acrescentar  e  automatizar  o  prÃ©-processamento  baseado  em  histograma,  relativo  Ã  

imagem pancromÃ¡tica, preconizado por Gonzalez e Woods (2010). Em adiÃ§Ã£o a isso, 

Ã©  viÃ¡vel  e  desejÃ¡vel  efetuar  a  substituiÃ§Ã£o  da  interpolaÃ§Ã£o  bilinear,  presente  no 

software  Fusion,  por  algum  outro  mÃ©todo  de  interpolaÃ§Ã£o,  buscando  obter, 

possivelmente, um resultado distinto ou talvez melhor do que o jÃ¡ alcanÃ§ado. Por fim, 

entende-se ser interessante a recomendaÃ§Ã£o de se implementar para o Fusion uma 

interface amigÃ¡vel (â€œFor Windowsâ€), a fim de tornar a sua utilizaÃ§Ã£o mais agradÃ¡vel e 

intuitiva aos possÃ­veis usuÃ¡rios. 

Por  fim,  conclui-se  que  este  trabalho  tenha  gerado  resultados  cientÃ­ficos 

relevantes e uma ferramenta Ãºtil ao ambiente acadÃªmico e a quem dela quiser fazer 

uso. 

 
 
 
 
63 

REFERÃŠNCIAS BIBLIOGRÃFICAS 

AL-WASSAI, F.A.; KALYANKAR, N.V.; AL-ZUKY, A.A. The IHS transformations-
based image fusion. J. Glob. Res. Comp. Sci., 2 (5), 2011. 

ANDRADE, J. B.; Fotogrametria. Curitiba: SBEE, 1998. 

CERQUEIRA,  Jorge  Dirceu  Melo  de.  OrtorretificaÃ§Ã£o  Digital  de  Imagens  de 
SatÃ©lites  de  Alta  ResoluÃ§Ã£o  Espacial.  Recife,  2004.  DissertaÃ§Ã£o  (Mestrado)  â€“ 
Centro de Tecnologia e GeociÃªncias, Universidade Federal de Pernambuco. 

CHAVEZ, P. S. Jr.; BOWELL, J.A.. Comparison of spectral information content 
of  Landsat  thematic  mapper  and  SPOT  for  three  different  sites  in  Phoenix. 
Photogrammetric Enginnering and Remote Sensing, 54(12):1699-1708. 1988. 

CRÃ“STA,  Alvaro  Penteado.  Processamento  digital  de 
sensoriamento remoto. Ed. rev. Campinas, SP: IG/UNICAMP, 1992. 

imagens  de 

GASPAR, Alberto. Compreendendo a fÃ­sica - Ondas, Ã³ptica e TermodinÃ¢mica. 
Manual do professor, Ensino MÃ©dio. 3. ed. SÃ£o Paulo: Ãtica, 2016. 

GOMES,  Herman  M.;  QUEIROZ,  JosÃ©  EustÃ¡quio  R. 
IntroduÃ§Ã£o  ao 
Processamento Digital de Imagens. Universidade Federal de Campina Grande. 
em: 
e 
Departamento 
<http://www.dsc.ufcg.edu.br/~hmg/disciplinas/graduacao/vc-2016.2/Rita-Tutorial-
PDI.pdf>, acesso em 29 mar. 2021. 

ComputaÃ§Ã£o. 

DisponÃ­vel 

Sistemas 

de 

GONZALEZ,  Rafael  C.;  WOODS,  Richard  C..  Processamento  digital  de 
imagens. 3. ed. SÃ£o Paulo : Pearson Prentice Hall, 2010. 

INSTITUTO  NACIONAL  DE  PESQUISAS  ESPACIAIS.  CÃ¢meras  Imageadoras 
CBERS-3  e  4.  DisponÃ­vel  em:  <http://www.cbers.inpe.br/sobre/cameras/cbers3-
4.php>, acesso em 12 ago. 2021. 

INSTITUTO  NACIONAL  DE  PESQUISAS  ESPACIAIS.  FusÃ£o  de  Imagens  do 
SatÃ©lite  CBERS-2B  no  SPRING.  Por  LaÃ©rcio  M.  Namikawa.  DisponÃ­vel  em:  
<http://wiki.dpi.inpe.br/doku.php?id=fusaohrcccdcbers2b:exemplo>, acesso em 28 
mai. 2021. 

LEONARDI,  Silvia  Shizue;  ORTIZ,  Jussara  de  Oliveira;  FONSECA,  Leila  Maria 
Garcia.  ComparaÃ§Ã£o  de  tÃ©cnicas  de  fusÃ£o  de  imagens  para  diferentes 
sensores  orbitais.  Anais  XII  SimpÃ³sio  Brasileiro  de  Sensoriamento  Remoto, 
GoiÃ¢nia, Brasil, 16-21 abril 2005, INPE, p. 4111-4113. 2005. 

MARQUES  FILHO,  OgÃª;  VIEIRA  NETO,  Hugo.  Processamento  Digital  de 
Imagens. Rio de Janeiro: Brasport, 1999. 

MENESES,  P.R.,  ALMEIDA,  T.,  IntroduÃ§Ã£o  ao  Processamento  de  Imagens  de 
Sensoriamento Remoto. 3. ed., BrasÃ­lia-DF, 2012. 

 
 
64 

OLIVEIRA,  Claudianne  Brainer  De  Souza.  AnÃ¡lise  da  integraÃ§Ã£o  espacial  de 
mÃºltiplos  sensores.  DissertaÃ§Ã£o  de  Mestrado.  Programa  de  PÃ³s-GraduaÃ§Ã£o  em 
CiÃªncias. 2018. 

PISANI,  Rodrigo,  BUENO,  Viviane,  FIUZA,  Joice,  ESTELA,  Paulo.  (2019). 
AVALIAÃ‡ÃƒO DE TÃ‰CNICAS DE FUSÃƒO DE IMAGENS ORBITAIS UTILIZANDO 
PRODUTOS  DO  SATÃ‰LITE  CBERS  4  PARA  A  APA  DO  RIO  MACHADO-MG. 
Caderno de Geografia. 29. 58-71. 10.5752/p.2318-2962.2019v29nespp58. 

SILVA, E. G. Estudo das transformaÃ§Ãµes planas: uma aplicaÃ§Ã£o baseada nas 
equaÃ§Ãµes  de  colinearidade.  2020.  DissertaÃ§Ã£o  (Mestrado  Profissional  em 
MatemÃ¡tica  â€“  Profmat).  UNEMAT  -  Universidade  do  Estado  de  Mato  Grosso. 
Sinop-MT. 

UNIVERSIDADE  FEDERAL  DE  VIÃ‡OSA.  Fotogrametria  digital.  Departamento  de 
Solos.  DisponÃ­vel  em:  <https://pt.slideshare.net/guest72086/fotogrametria-digital>, 
acesso em 29 abr. 2021. 

VENTURA,  F.  N.  FusÃ£o  de  imagens  de  sensores  remotos  utilizando  a 
transformada de wavelet. 2002-08-12. (INPE -TDI/). DissertaÃ§Ã£o (Mestrado em 
ComputaÃ§Ã£o Aplicada) - Instituto Nacional de Pesquisas Espaciais, SÃ£o JosÃ© dos 
Campos. 2002. 

WOLF,  P.;  DEWITT,  B.  Elements  of  photogrammetry  â€“  with  applications  in 
GIS. 3.ed. United States of America: Mc Graw Hill, 2000. 

 
 
