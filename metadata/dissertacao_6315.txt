Universidade Federal de Goiás
Instituto de Matemática e Estatística
Programa de Mestrado Proﬁssional em
Matemática em Rede Nacional

Transformações lineares no plano e
aplicações

LEONARDO BERNARDES NOGUEIRA

Goiânia
2013

LEONARDO BERNARDES NOGUEIRA

Transformações lineares no plano e
aplicações

Trabalho de Conclusão de Curso apresentado ao Programa
de Pós–Graduação do Instituto de Matemática e Estatística
da Universidade Federal de Goiás, como requisito parcial
para obtenção do título de Mestre em matemática

Área de concentração: Matemática de Ensino Básico.

Orientador: Prof. Dr. Maurílio Márcio Melo

Goiânia
2013

Todos os direitos reservados. É proibida a reprodução total ou parcial do
trabalho sem autorização da universidade, do autor e do orientador(a).

Leonardo Bernardes Nogueira

Licenciado em Matemática e especialista em Educação Matemática pela UnB.
Professor da Secretaria de Educação do Distrito Federal.

À todos os meus alunos que já foram e aos que estão por vir, é por eles que quero

ser um professor melhor.

Agradecimentos

Primeiro às pessoas que amo, por sempre estarem presente em minha vida, não
apenas nos momentos felizes, mas principalmente nos mais difíceis. À Universidade
Federal de Goiás (UFG) e ao PROFMAT, que proporcionam não só a mim, mas a vários
estudantes, a oportunidade de sermos pessoas melhores. Ao professor Maurilio Márcio
Melo, pelo apoio e incentivo para a conclusão deste trabalho. Aos demais professores que
contribuíram com a formação de proﬁssionais melhores. Aos meus pais, Guilherme Rocha
Nogueira e Joana D’Arc Arantes Bernardes Nogueira e namorada, Halinna Dornelles
Wawruk, pelo incentivo, paciência, carinho e apoio que me foram dados ao decorrer de
todo o curso. À CAPES pelo suporte ﬁnanceiro. Aos meus colegas do Mestrado, pelo
agradável convívio, amizade e ajuda. Muito obrigado a todos!

Se vi mais longe foi por estar de pé sobre ombros de gigantes [...]

Isaac Newton,
Carta para Robert Hooke (15 de fevereiro de 1676).

Resumo

Nogueira, Leonardo Bernardes. Transformações lineares no plano e aplica-
ções. Goiânia, 2013. 63p. Trabalho de conclusão de curso. Instituto de Matemá-
tica e Estatística, Universidade Federal de Goiás.

Este trabalho inicia-se com um breve embasamento histórico sobre o desenvolvimento
de espaços vetoriais e transformações lineares. Em seguida, apresenta conceitos funda-
mentais básicos, que formam uma linguagem mínima necessária para falar sobre Álgebra
Linear, com enfoque maior nos operadores lineares do plano R2. Através de exemplos,
explora-se um vasto conjunto de transformações no plano a ﬁm de mostrar outras apli-
cações de matrizes no ensino médio e prepara o terreno para a apresentação do Teorema
Espectral para operadores auto-adjuntos de R2. Este Teorema diz que para todo opera-
dor auto-adjunto T : E → E, num espaço vetorial de dimensão ﬁnita, munido de produto
interno, existe uma base ortonormal {u1, . . . , un} ⊂ E formada por autovetores de T . O tra-
balho culmina com aplicações sobre o estudo das secções cônicas, formas quadráticas e
equações do segundo grau em x e y, no qual o Teorema Espectral se traduz como Teorema
dos Eixos Principais, embora essa nomenclatura não seja usada nesse trabalho (para um
estudo mais aprofundado neste tema ver [3], [4], [5], [7]). Retomando assim um estudo
feito por Joseph Louis Lagrange em "Recherche d’Arithmétique", entre 1773 e 1775, no
qual estudou a propriedade de números que são a soma de dois quadrados. Assim, foi
levado a estudar os efeitos das transformações lineares com coeﬁcientes inteiros numa
forma quadrática de duas variáveis.

Palavras–chave

Álgebra Linear, Teorema Espectral, Secções Cônicas

Abstract

Nogueira, Leonardo Bernardes. Linear transformations on the plane and
applications. Goiânia, 2013. 63p. Completion of course work. Instituto de
Matemática e Estatística, Universidade Federal de Goiás.

This paper begins with a brief history about the development of vector spaces and linear
transformations, then presents fundamental concepts for the study of Linear Algebra, with
greater focus on linear operators in the R2 space. Through examples it explores a wide
range of operators in R2 in order to show other applications of matrices in high school
and prepares the ground for the presentation a version of Spectral Theorem for self-
adjoint operators in R2, which says that for every operator self-adjoint T : E → E in ﬁnite
dimensional vector space with inner product, exists an orthonormal basis {u1, . . . , un} ⊂ E
formed by eigenvectors of T , and culminates with their applications on the study of conic
sections, quadratic forms and equations of second degree in x and y; on the study of
operators associated to quadratic forms, a version of Spectral Theorem could be called
as The Main Axis Theorem albeit this nomenclature is not used in this paper. Thereby
summarizing a study made by Lagrange in "Recherche d’arithmétique ", between 1773
and 1775, which he studied the property of numbers that are the sum of two squares.
Thus he was led to study the effects of linear transformation with integer coefﬁcients in a
quadratic form in two variables.

Keywords

Linear Algebra, Spectral Theorem, Conic Section

Sumário

Lista de Figuras

1 Introdução

2 Espaços Vetoriais

3 Subespaços

4 Bases

5 Transformações Lineares

6 Núcleo e Imagem

7 Produto Interno

8 Subespaços Invariantes, Operadores Auto-Adjuntos, Um Caso Particular do

Teorema Espectral

9 Seções Cônicas e Formas Quadráticas

10 Conclusão

Referências Bibliográﬁcas

11

12

15

18

24

27

36

42

46

52

62

63

Lista de Figuras

2.1 Soma de vetores.

3.1 Combinação linear de vetores em R2
3.2 Variedade Aﬁm, translação por um vetor.

16

20
23

5.1 Rotação de Vetores.
5.2 Rotação de ângulo θ.
5.3 Projeção ortogonal sobre uma reta.
5.4 Reﬂexão em torno de uma reta.
5.5

31
31
32
33
(a) e (b) representam as reﬂexão com relação ao eixo x e y respectivamente. 34
Reﬂexão sobre o eixo x.
34
(a)
(b) Reﬂexão sobre o eixo y.
34

7.1 Teorema de Pitágoras em sua forma vetorial.
7.2 Decomposição do vetor v em uma base ortogonal.
7.3 Projeção ortogonal.

9.1 Cone e suas secções.
9.2 Hipérbole Transladada.
9.3 Base ortonormal a partir de uma rotação anti-horária de um ângulo θ da

base canônica.
9.4 Parábola após rotação.
9.5 Elipse após rotação seguida de uma translação.

43
44
45

53
55

56
60
61

CAPÍTULO 1

Introdução

Aspectos Históricos

A área de álgebra linear ﬁcou adormecida na matemática, não apresentando
nada de substancial até a metade do século XVIII. Um assunto relevante cujas questões
levaram ao desenvolvimento da teoria de sistemas lineares que, por sua vez, levaram ao
desenvolvimento da teoria de espaços vetoriais é o estudo das curvas algébricas.

Em 1770, no entanto, o matemático Euler conseguiu caracterizar as transforma-
ções ortogonais para n = 2 e 3, quando estudava quadrados de números similares aos
quadrados mágicos. Devido ao seu raciocínio puramente algébrico, ele também conse-
guiu generalizar as soluções para qualquer valor de n, não se restringindo somente a 3.
Um processo semelhante não poderia ocorrer na geometria, pois, nesse caso, era preciso
imaginar um espaço com dimensões maiores que 3.

Partindo dos estudos feitos por Euler, Joseph Louis Lagrange publicou o "Re-
cherche d?Arithmétique", entre 1773 e 1775, no qual estudou a propriedade de números
que são a soma de dois quadrados. Assim, foi levado a estudar os efeitos das transforma-
ções lineares com coeﬁcientes inteiros numa forma quadrática de duas variáveis. A partir
desse estudo, ele instituiu que o discriminante da nova forma quadrática é o produto do
antigo discriminante pelo quadrado de uma quantidade (determinante da transformação
linear).

Johann Carl Friedrich Gauss, por sua vez, também estudou a questão com
duas e três variáveis. Ele apresentou uma notação similar a da matriz que caracteriza a
transformação linear. Além disso, Gauss estabeleceu a fórmula e uma notação simbólica
para a composição de duas transformações lineares e também para o produto, o que marca
um passo fundamental em direção ao conceito de matriz.

Hermann Günter Grassmann publicou em 1844 a primeira versão de "Lineale
Ausdehnungslehre". Nesse trabalho discutiu e obteve uma boa parte dos resultados
elementares da teoria atual de espaços vetoriais e de álgebra linear, além de ter conseguido
algo bem próximo de uma formalização axiomática, mas devido a sua forma obscura de

13

apresentação, seus resultados não inﬂuenciaram seus contemporâneos e a maior parte de
seus resultados foi redescoberto independentemente de seu trabalho.

Em 1846, Arthur Cayley publicou o tratado "Sur Quelques Résultats de Géome-
trié de Position". Esse tratado surgiu como um passo decisivo na direção de generalizar os
espaços de dimensão maior que três, pois nesse trabalho ele mostrou que se podem obter
resultados em geometria tridimensional trabalhando-se com espaços de dimensão maior
que três. Esse resultado poderia ter sido obtido por Möbius, mas ele adotou uma postura
comum à sua época e descartou essa possibilidade.

Giuseppe Peano (1858-1952) publicou em 1888 sua própria leitura do "Ausdeh-
nungslehre", com o titulo de "Calcolo Geométrico"no qual escreveu uma deﬁnição axio-
mática do que ele chamou de "sistema linear", que foi considerada a primeira deﬁnição
axiomática de um espaço vetorial, mas a teoria de espaços vetoriais não foi desenvolvida
antes de 1920.

Situação Atual

A análise de livros didáticos de Matemática constitui um parâmetro indicador
do estado atual em que se encontra o ensino da Álgebra Linear. Especiﬁcamente no
ensino médio, pode-se constatar pela leitura de [6] que o conteúdo de Álgebra linear
é apresentado simplesmente para a resolução de sistemas de equações, onde a única
aplicação de matrizes se restringe apenas na resolução de sistemas.

Este trabalho mostra outros aspectos da aplicabilidade de conceitos de Álgebra
Linear, como de espaços vetoriais, subespaços, bases, transformações lineares, produto
interno e formas quadráticas, tendo sempre o foco no plano R2, ou seja em espaços de
dimensão 2, visando um aprofundamento dos conceitos e mantendo a simplicidade para a
aplicabilidade no ensino médio. Em outras palavras este trabalho mostra uma outra área
de atuação para Álgebra Linear, que é retratada nos livros de ensino médio apenas pelo
uso de matrizes e sistemas lineares, mostrando sua aplicabilidade no estudo das secções
cônicas (parábolas, hipérboles e elipses), que são abordadas ao ﬁnal do ensino médio.

Apresentação dos Capítulos

Dividimos o conteúdo deste trabalho em 9 Capítulos: No Capítulo 1, que se
desenvolve no momento, contêm alguns aspectos históricos que motivaram o desenvolvi-
mento do tema e uma abordagem sobre a situação atual do ensino de Álgebra linear no
ensino médio. Os Capítulos 2 ao 5 desenvolvem os conceitos fundamentais e as proposi-
ções básicas, constituídas de vários exemplos geométricos para que se tenha a assimilação
dos conceitos mais abstratos fundamentados nas ideias intuitivas de geometria. O Capítulo

14

6, aparentemente muito abstrato, contêm o conceito de isomorﬁsmo, que confere sentido
preciso à aﬁrmação de que dois espaços vetoriais de mesma dimensão são algebricamente
indistinguíveis, o que dá uma amplitude maior ao estudo aparentemente restrito ao plano,
que tem dimensão 2. Os Capítulos 7 e 8 são uma preparação para a conclusão que aparece
no Capítulo 9 com o estudo das cônicas, como sendo uma área bastante próspera para
aplicações de matrizes e Álgebra Linear. A importância do Capítulo 8, não reside apenas
como preparação para o Capítulo 9, nele se encontra uma demonstração do teorema Es-
pectral para operadores auto-adjunto de R2, ponto alto do trabalho (para um estudo mais
aprofundado ver [3], [4], [5], [7]).

Espaços Vetoriais

CAPÍTULO 2

A noção de espaço vetorial é a base do estudo que faremos, é o terreno onde
desenvolve toda a Álgebra Linear. Este Capítulo apresenta os axiomas de espaços
vetorial, deduz suas consequências mais imediatas e exibe os exemplos mais importantes
dessa noção.

Um espaço vetorial E é um conjunto, cujos elementos são chamados vetores,
no qual estão deﬁnidas duas operações: a adição, que a cada par de vetores u, v ∈ E faz
corresponder um novo vetor u + v ∈ E, chamado a soma de u e v, e a multiplicação por um
número real, que a cada número α ∈ R e cada vetor v ∈ E faz corresponder um vetor α · v,
ou αv , chamado o produto de α por v. Essas operações devem satisfazer, para quaisquer
α, β ∈ R e u, v, w ∈ E, as condições abaixo, chamadas os axiomas de espaço vetorial:
comutativa: u + v = v + u;
associativa: w + (u + v) = (w + u) + v e (αβ)v = α(βv);
vetor nulo: existe um vetor 0 ∈ E, chamado vetor nulo, ou vetor zero,
v + 0 = 0 + v = v para todo v ∈ E
inverso aditivo: para cada vetor v ∈ E existe um vetor −v ∈ E, chamado o inverso aditivo,
ou simétrico de v, tal que −v + v = v + (−v) = 0;
distributividade: (α + β)v = αv + βv e α(u + v) = αu + αv;
multiplicação por 1: 1 · v = v.

tal que

Observação: O mesmo símbolo 0 representa o vetor nulo e o número zero.

Exemplo 2.1 Para todo número natural n, o símbolo Rn, representa o espaço vetorial eu-
clidiano n-dimensional. Os elementos de Rn são as listas ordenadas u = (u1, . . . , un), v =
(β1, . . . , βn) de números reais.

Por deﬁnição, a igualdade vetorial u = v signiﬁca as n igualdades numéricas

u1 = v1, . . . , un = vn.

Os números u1, . . . , un são chamados as coordenadas do vetor u. As operações

do espaço vetorial Rn são deﬁnidas por

u + v = (u1 + v1, . . . , un + vn)

(2-1)

α · u = (αu1, . . . , αun)

16

(2-2)

O vetor nulo é, por deﬁnição, aquele cujas coordenadas são todas iguais a zero: 0 =
(0, 0, . . . , 0).

O inverso aditivo de u = (u1, . . . , un) é −u = (−u1, . . . , −un). Temos então que
estas deﬁnições fazem de Rn um espaço vetorial. Com efeito temos que mostrar que todos
os axiomas de espaço vetorial valem para Rn, vide demonstração em [4]. Para n = 1,
tem-se R1 = R = reta numérica, R2 é o plano euclidiano e R3 é o espaço euclidiano
tridimensional da nossa experiência cotidiana.

Para ajudar a compreensão, os vetores de R2 e R3 podem ser representados por
ﬂechas com origem no mesmo ponto O. A soma u + v é a ﬂecha que liga a origem O ao
vértice que lhe é oposto no paralelogramo que tem u e v como lados (Veja Figura).

Figura 2.1: Soma de vetores.

Por sua vez, o produto αu é a ﬂecha colinear a u, de comprimento α vezes o

comprimento de u, com o mesmo sentido de u se α > 0 e com sentido oposto se α < 0.

Valem num espaço vetorial, como consequências dos axiomas, as regras opera-

cionais habitualmente usadas nas manipulações numéricas. Vejamos algumas delas:

1. Se w + u = w + v então u = v. Em particular, w + u = w implica u = 0 e w + u = 0

implica u = −w. Com efeito, da igualdade w + u = w + v segue-se que

u = 0+u = (−w+w)+u = −w+(w+u) = −w+(w+v) = (−w+w)+v = 0+v = v

Em particular, w + u = w implica w + u = w + 0, logo u = 0. E se w + u = 0 então
w + u = w + (−w) logo u = −w.

17

2. Dados 0 ∈ R e v ∈ E tem-se 0 · v = 0 ∈ E. Analogamente, dados α ∈ R e 0 ∈ E, vale

α · 0 = 0.
Com efeito, v + 0 · v = 1 · v + 0 · v = (1 + 0) · v = 1 · v = v, logo 0 · v = 0 como vimos
acima. De modo análogo, como α · 0 + α · 0 = α · (0 + 0) = α · 0, segue de 1. que
α · 0 = 0.

3. Se α (cid:54)= 0 e v (cid:54)= 0 então α · v (cid:54)= 0.

Com efeito, se fosse α · v = 0 então v = 1 · v = (α−1 · α)v = α−1 · (αv) = α−1 · 0 = 0.

4. (−1) · v = −v.
Com efeito,
v + (−1) · v = 1 · v + (−1) · v = [1 + (−1)] · v = 0 · v = 0,
logo (−1)v = −v, pela regra 1.

No que se segue, escreveremos u − v para signiﬁcar u + (−v). Evidentemente,

u − v = w ⇔ u = v + w.

Exemplo 2.2 Sejam u = (a, b) e v = (c, d) vetores de R2 com u (cid:54)= 0, isto é a (cid:54)= 0 ou
b (cid:54)= 0. A ﬁm de que v seja múltiplo de u, isto é, v = αu para algum α ∈ R, é necessário
e suﬁciente que se tenha ad − bc = 0. A necessidade é imediata pois v = αu signiﬁca
c = αa e d = αb. multiplicando a primeira destas igualdades por b e a segunda por a
obtemos bc = αab e ad = αab, logo ad = bc, ou seja, ad − bc = 0. Reciprocamente, se
(cid:1) a.
ad = bc então, supondo a (cid:54)= 0, obtemos d = (cid:0) c
a
logo, pondo α = c
a, vem d = αb e c = αa, isto é v = αu. se for b (cid:54)= 0, tomaremos α = d
b
para ter v = αu.

(cid:1) b. Além disso, é claro que c = (cid:0) c
a

Subespaços

CAPÍTULO 3

Um subespaço vetorial do espaço vetorial E é um subconjunto F ⊂ E que,
relativamente às operações de E, é ainda um espaço vetorial. Os subespaços vetoriais
constituem uma rica fonte de exemplos de espaços vetoriais.

Seja E um espaço vetorial. Um subespaço vetorial (ou simplesmente um subes-

paço) de E é um subconjunto F ⊂ E com as seguintes propriedades:

1. 0 ∈ F;
2. Se u, v ∈ F então u + v ∈ F
3. se v ∈ F então, para todo α ∈ R, αv ∈ F

Para mostrar que um subespaço vetorial é um espaço vetorial, temos que mostrar
que um subconjunto F de um espaço vetorial E, com as propriedades acima é uma espaço
vetorial, ou seja, satisfaz os axiomas de espaço vetorial, vide demonstração em [4].

Segue-se que se u e v pertencem ao subespaço F e α, β são números reais
quaisquer então αu + βv ∈ F, mais geralmente, dados v1, . . . , vm ∈ F e α1, . . . , αm ∈ R,
tem-se v = α1v1 + . . . + αmvm ∈ F.

O conjunto {0}, com o único elemento 0, e o espaço inteiro E são exemplos

triviais de subespaços de E. Todo espaço vetorial é, em si mesmo, um subespaço.

Exemplo 3.1 Seja v ∈ E um vetor não-nulo. O conjunto F = {αv; α ∈ R} de todos os
múltiplos de v é um subespaço vetorial de E, chamado de reta que passa pela origem na
direção de v.

Exemplo 3.2 Sejam a1, . . . , an números reais. O conjunto H de todos os vetores v =
(x1, . . . , xn) ∈ Rn tais que

a1x1 + . . . + anxn = 0

é um subespaço vetorial de Rn. No caso desinteressante em que a1 = . . . = an = 0, o
subespaço H é todo o Rn. Se, ao contrário, pelo menos um dos ai é (cid:54)= 0, H chama-se uma
hiperplano de Rn que passa pela origem.

19

Exemplo 3.3 Sejam E um espaço vetorial e L um conjunto de índices. Se, para cada
λ ∈ L, Fλ é um subespaço vetorial de E, então a interseção

F = (cid:92)
λ∈L

Fλ

é ainda um subespaço vetorial de E. Segue-se então do Exemplo 3.2 que o conjunto dos
vetores v = (x1, . . . , xn) ∈ R cujas coordenadas satisfazem as m condições abaixo

a11x1 + a12x2 + . . . + a1nxn = 0
a21x1 + a22x2 + . . . + a2nxn = 0
...
am1x1 + am2x2 + . . . + amnxn = 0

é um subespaço vetorial de Rn, que é a interseção F = F1 ∩ . . . ∩ Fm dos hiperplanos Fi
deﬁnidos, segundo o Exemplo 3.2, por cada uma das equações acima.

Seja X um subconjunto do espaço vetorial E. O subespaço vetorial de E gerado

por X é, por deﬁnição, o conjunto do todas as combinações lineares

α1v1 + α2v2 + . . . + αmvm

de vetores v1, . . . , vm ∈ X.

Pode-se mostrar que o conjunto de todas as combinações lineares que se podem
formar com vetores retirados dos conjunto X é, de fato, um subespaço vetorial, que
indicaremos pelo símbolo G(X).

O subespaço G(X), gerado pelo subconjunto X ⊂ E, contém o conjunto X e, além
disso, é o menor subespaço de E que contém X. Noutras palavras, se F é um subespaço
vetorial de E e X ⊂ F, então G(X) ⊂ F. Evidentemente, se X já é um subespaço vetorial,
então G(X) = X. Quando o subespaço G(X) coincide com E, diz-se que X é um conjunto
de geradores de E.

Explicitamente: um conjunto X é um conjunto de geradores do espaço vetorial E

quando todo vetor w ∈ E pode exprimir-se como combinação linear

w = α1v1 + . . . + αmvm

de vetores v1, . . . , vm pertencentes a X

Exemplo 3.4 Se v ∈ E é um vetor não-nulo, o subespaço gerado por v é a reta que passa
pela origem e contém v.

20

Exemplo 3.5 Sejam u = (a, b) e v = (c, d) vetores de R2 tais que nenhum deles é múltiplo
do outro, então u (cid:54)= 0, v (cid:54)= 0 e, pelo Exemplo 2.2, aﬁrmamos que X = {u, v} é um conjunto
de geradores de R2, ou seja, que qualquer vetor w = (r, s) ∈ R2 pode exprimir-se como
uma combinação linear w = xu + yv. De fato esta igualdade vetorial em R2 equivale às
duas igualdades numéricas

ax + cy = r
bx + dy = s.

Como ad − bc (cid:54)= 0, o sistema de equações acima possui uma solução (x, y), logo existem
x, y ∈ R, tais que xu + yv = w. Esta mesma conclusão pode também ser obtida geometrica-
mente, conforme mostra a ﬁgura 3.1. A partir da ponta de w, traçam-se paralelas às retas
que contêm u e v, determinando assim os múltiplos xu, yv, que somados dão w.

Figura 3.1: Combinação linear de vetores em R2

Exemplo 3.6 Os chamados vetores canônicos.

e1 = (1, 0, 0, . . . , 0),
e2 = (0, 1, 0, . . . .0),
...
en = (0, 0, . . . , 0, 1)

constituem um conjunto de geradores do espaço Rn. Com efeito, dado v = (α1, . . . , αn) ∈
Rn, tem-se v = αe1 + . . . + αen.

Resulta do Exemplo 3.5, que os únicos subespaços vetoriais de R2 são {0}, as
retas que passam pela origem e o próprio R2. Com efeito, seja F ⊂ R2 um subespaço
vetorial. Se F contém apenas o vetor nulo, então F = {0}. Se F contém algum vetor u (cid:54)= 0
então há duas possibilidades: ou todos os demais vetores de F são múltiplos de u, ou então
F contém, além de u, um outro vetor v que não é múltiplo de u. Neste caso, F contém todas
as combinações lineares xu + yv, logo F = R2, pelo Exemplo 3.5.

Exemplo 3.7 O sistema Linear de m equações a n incógnitas

21

a11x1 + a12x2 + . . . + a1nxn = b1
a21x1 + a22x2 + . . . + a2nxn = b2
...
am1x1 + am2x2 + . . . + amnxn = bm

possui uma solução (x1, . . . , xn) se, e somente se, o vetor b = (b1, . . . , bn) é combinação
linear dos vetores-coluna

v1 = (a11, a21, . . . , am1)
...
vn = (a1n, a2n, . . . , amn)

da matriz A = [ai j]. Com efeito, estas equações signiﬁcam que

b = x1v1 + x2v2 + . . . + xnvn.

Em particular, se os vetores-colunas v1, . . . , vn gerarem Rn, o sistema possui solução, seja
qual for o segundo membro b.

Sejam F1 e F2 subespaços vetoriais de E. O subespaço vetorial de E gerado pela
reunião F1 ∪ F2 é, como se vê facilmente, o conjunto de todas as somas v1 + v2, onde
v1 ∈ F1 e v2 ∈ F2. Ele é representado pelo símbolo F1 + F2.

Mais geralmente, dados os subconjuntos X,Y ⊂ E, indica-se com X + Y o
conjunto cujos elementos são as somas u + v, onde u ∈ X e v ∈ V . Quando X = {u}
reduz-se a um único elemento u, escreve-se u + Y em vez de {u} + Y . Diz-se então que
u +Y resulta de Y pela translação de u.

Quando os subespaços F1, F2 ⊂ E têm em comum apenas o elemento {0},
escreve-se F1 ⊕ F2 em vez de F1 + F2 e diz-se que F = F1 ⊕ F2 é soma direta de F1 e
F2.

Teorema 3.1 Sejam F, F1, F2 subespaços vetoriais de E, com F1 ⊂ F e F2 ⊂ F. As seguin-
tes aﬁrmações são equivalentes:

1. F = F1 ⊕ F2
2. todo elemento w ∈ F se escreve, de modo único, como a soma w = v1 + v2 onde

v1 ∈ F1 e v2 ∈ F2

Prova. Provaremos que (1) ⇒ (2). Para isso, suponhamos que F1 ∩ F2 = {0} e que se
tenha u1 + u2 = v1 + v2, com u1, v1 ∈ F1 e u2, v2 ∈ F2. Então u1 − v1 = v2 − u2. Como
u1 − v1 ∈ F1 e v2 − u2 ∈ F2, segue-se que u1 − v1 e v2 − u2 pertencem ambos a F1 e a F2.
Mas F1 ∩ F2 = {0}. Logo u1 − v1 = v2 − u2 = 0, ou seja, u1 = v1 e u2 = v2. Para provar
que (2) ⇒ (1), seja v ∈ F1 ∩ F2. Então 0 + v = v + 0 com 0, v ∈ F1 e v, 0 ∈ F2. Pela hipótese

(2), isto implica 0 = v e portanto F1 ∩ F2 = {0}.

22

(cid:3)

Exemplo 3.8 Em R2, sejam F1 o subespaço gerado pelo vetor e1 = (1, 0) e F2 o subes-
paço gerado pelo vetor e2 = (0, 1). Então F1 é a reta que passa pela origem na direção de
e1, ou seja, é o conjunto de vetores da forma (α1, 0), enquanto que F2 tem a forma (0, α2).
É claro que R2 = F1 ⊕ F2.

A noção de subespaço vetorial abrange as retas, planos e seus análogos mul-
tidimensionais apenas nos casos em que esses conjuntos contém a origem. Para incluir
retas, planos, etc. que não passam pela origem, tem-se a noção de variedade aﬁm, que
discutiremos agora.

Seja E um espaço vetorial. Se x, y ∈ E e x (cid:54)= y, a reta que une os pontos x, y é por

deﬁnição o conjunto

r = {(1 − t)x + ty;t ∈ R}

Pondo v = y − x, podemos ver que r = {x + tv;t ∈ R}.

Um subconjunto V ⊂ E chama-se uma variedade aﬁm quando a reta que une
dois pontos quaisquer de V está contida em V. Assim, V ⊂ E é uma variedade aﬁm se, e
somente se, cumpre a seguinte condição:

x, y ∈ V,t ∈ R ⇒ (1 − t)x + ty ∈ V

Exemplo 3.9 Um exemplo óbvio de variedade aﬁm é um subespaço vetorial. Ao contrá-
rio dos subespaços vetoriais, que nunca são vazios, pois devem conter o zero, a deﬁnição
acima é formulada de tal modo que o conjunto vazio a cumpre, logo é uma variedade
aﬁm.

Teorema 3.2 Seja V uma variedade aﬁm não-nula no espaço vetorial E. Existe um único
subespaço vetorial F ⊂ E tal que, para todo x ∈ V tem-se

V = x + F = {x + v; v ∈ F}.

Vide demonstração em [4].

Exemplo 3.10 Vimos no exemplo 3.7 que o conjunto V das soluções de um sistema linear
de m equações e n incógnitas é uma variedade aﬁm. Supondo V (cid:54)= ∅, tomemos x0 ∈ V e
chamemos de F o subespaço vetorial de Rn formado pelas soluções do sistema homogêneo

23

Figura 3.2: Variedade Aﬁm, translação por um vetor.

correspondente (descrito no Exemplo 3.3). Tem-se V = x0 + F. Diz-se então que "todas
as soluções do sistema se obtêm somando uma solução particular com a solução geral do
sistema homogêneo associado".

Bases

CAPÍTULO 4

Os espaços vetoriais de dimensão ﬁnita possuem um estrutura algébrica extre-
mamente simples, evidenciada pelas ideias de base e dimensão. Uma vez ﬁxada uma base
num espaço vetorial de dimensão n, seus elementos são meramente combinações lineares
dos n vetores básicos, com coeﬁcientes univocamente determinados.

Seja E um espaço vetorial. Diz-se que um conjunto X ⊂ E é linearmente
independente (abreviadamente, L.I.) quando nenhum vetor v ∈ X é combinação linear
de outros elementos de X. Para evitar ambiguidade, no caso em que X = {v} consta de
um único elemento v, diz-se que X é L.I., por deﬁnição, quando v (cid:54)= 0. Quando X é L.I.,
diz-se também que os seus elementos são linearmente independentes.

Quando o conjunto X é L.I. seus elementos são todos (cid:54)= 0, pois o vetor nulo
é combinação linear de quaisquer outros: 0 = 0 · v1 + . . . + 0 · vm (se não há "outros",
X = {v}, v (cid:54)= 0).

Um critério extremamente útil para veriﬁcar a independência linear de um

conjunto é dado pelo teorema abaixo.

Teorema 4.1 Seja X um conjunto L.I. no espaço vetorial E. Se α1v1 + . . . + αmvm = 0
com v1, . . . , vm ∈ X, então α1 = . . . = αm = 0. Reciprocamente, se a única combinação
linear nula de vetores de X é aquela cujos coeﬁcientes são todos iguais a zero, então X é
um conjunto L.I..

Pode-se obter uma demonstração do teorema acima em [4], mas para um abordagem mais
ampla do assunto vide [5] e [7].

Corolário 4.1.1 Se v = α1v1 + . . . + αmvm = β1v1 + . . . + βmvm e os vetores v1, . . . , vm são
L.I. então α1 = β1, . . . , αm = βm.

Com efeito, tem-se neste caso (α1 − β1)v1 + . . . + (αm − βm)vm = 0 logo (α1 −

β1) = . . . = (αm − βm) = 0.(cid:3)

Exemplo 4.1 Os vetores canônicos e1 = (1, 0, . . . , 0), . . . , en = (0, . . . , 0, 1) em Rn são L.I..
Com efeito, α1e1 + . . . + αnen = 0 signiﬁca (α1, . . . , αn) = 0, logo α1 = . . . = α =0.

25

Teorema 4.2 Sejam v1, . . . , vm vetores não-nulos do espaço vetorial E. Se nenhum deles
é combinação linear dos anteriores então o conjunto X = {v1, . . . , vm} é L.I..

Prova. Suponhamos, por absurdo, que uma combinação linear dos vetores dados, com
coeﬁcientes não todos nulos, fosse igual a zero. Se αrvr fosse a última parcela não-nula
dessa combinação, teríamos então

α1v1 + . . . + αrvr = 0 = 0

v1 − . . . − αr−1
com αr (cid:54)= 0. Daí viria vr = − α1
αr
αr
elementos anteriores a ele na lista v1, . . . , vm.(Observe que r > 1 pois v1 (cid:54)= 0)

vr−1, logo vr seria combinação linear dos
(cid:3)

Observação: Vale um resultado análogo, com "subsequentes"em vez de "anteriores"no
enunciado.

Um conjunto X ⊂ E diz-se linearmente dependente (abreviadamente, L.D.)

quando não é L.I.

Isto signiﬁca que algum dos vetores v ∈ X é combinação linear de outros
elementos de X, ou então X = {0}. A ﬁm de que X seja L.D. é necessário e suﬁciente
que exista uma combinação linear nula α1v1 + . . . + αmvm = 0 de vetores v1, . . . , vm ∈ X
com algum coeﬁciente αi (cid:54)= 0. Se X ⊂ Y e X é L.D., então Y também é L.D.. Se 0 ∈ X
então o conjunto X é L.D..

Exemplo 4.2 Os vetores u = (1, 2, 3), v = (4, 5, 6) e w = (7, 8, 9) em R3 são L.D. pois
w = 2v − u.

Exemplo 4.3 Quando os vetores v1, . . . , vm são L.D., isto não signiﬁca que qualquer um
deles seja combinação linear dos demais. Por exemplo, se u = (1, 2), v = (3, 4), w = (4, 8)
então {u, v, w} ⊂ R2 é um conjunto L.D. pois w = 4u + 0 · v porém v não é combinação
linear de u e w.

Uma base de um espaço vetorial E é um conjunto B ⊂ E linearmente inde-
pendente que gera E. Isto signiﬁca que todo vetor v ∈ E se exprime, de modo único,
como combinação linear v = α1v1 + . . . + αmvm de elementos v1, . . . , vm da base B se
B = {v1, . . . , vm} é uma base de E e v = α1v1 + . . . + αmvm, então os números α1, . . . , αm
chamam-se as coordenadas do vetor v na base B.

Teorema 4.3 Se os vetores v1, . . . , vm geram o espaço vetorial E então qualquer conjunto
com mais de m vetores é L.D..

26

Corolário 4.3.1 Se os vetores v1, . . . , vm geram o espaço vetorial E e os vetores u1, . . . , un
são L.I., então n (cid:54) m.

Corolário 4.3.2 Se o espaço vetorial E admite uma base B = {u1, . . . , un} com n elemen-
tos, qualquer outra base de E possui também n elementos.

Diz-se que o espaço vetorial E tem dimensão ﬁnita quando admite uma base B =
{v1, . . . , vn} com um números ﬁnito n de elementos. Este número, que é o mesmo para
todas as bases de E, chama-se a dimensão do espaço vetorial E: n= dim E. Por extensão,
diz-se que o espaço vetorial E = {0} tem dimensão zero.

Corolário 4.3.3 Se a dimensão de E é n, um conjunto com n vetores gera E se, e somente
se, é L.I..

Para uma demonstração do teorema acima e de seus corolários vide [2], [4] e [5].

Teorema 4.4 Seja E um espaço vetorial de dimensão ﬁnita n. Então:

(a) Todo conjunto X de geradores de E contém uma base,
(b) Todo conjunto L.I. {v1, . . . , vm} ⊂ E está contido numa base,
(c) Todo subespaço vetorial F ⊂ E tem dimensão ﬁnita, a qual é (cid:54) n,
(d) Se a dimensão do subespaço F ⊂ E é igual a n, então F = E.

Vide demonstração do teorema acima em [2] e [4].

Exemplo 4.4 O espaço vetorial M(m × n), das matrizes m × n, tem dimensão ﬁnita, igual
a m · n. uma base para M(m × n) é formada pelas matrizes ei j, cujo ij-ésimo elemento (na
interseção da i-ésima linha com a j-ésima coluna) é igual a 1 e os demais elementos são
iguais a zero.

Diz-se que a variedade aﬁm V ⊂ E tem dimensão r quando V = x + F, onde o

subespaço vetorial F ⊂ E tem dimensão r.

Transformações Lineares

CAPÍTULO 5

Sejam E, F espaços vetoriais. Uma transformação linear T : E → F é uma
correspondência que associa a cada vetor v ∈ E um vetor T (v) ∈ F de modo que valham,
para quaisquer u, v ∈ E e α ∈ R as relações

T (u + v) = T (u) + T (v)

e

T (α · v) = α · T (v).

O vetor T (v) chama-se a imagem (ou o transformado) de v pela transformação T .

Se T : E → F é uma transformação então T (0) = 0. Com efeito, T (0) = T (0 +
0) = T (0) + T (0). Além disso, dados u, v ∈ E e α, β ∈ R, tem-se T (αu + βv) = T (αu) +
T (βv) = α · T (u) + β · T (v). Mais geralmente, dados v1, . . . , vm em E e α1, . . . , αm ∈ R,
vale a igualdade

T (α1v1 + . . . + αmvm) = α · T (v1) + . . . + αm · T (vm).

Daí resultam T (−v) = −T (v) e T (u − v) = T (u) − T (v).

A soma de duas transformações lineares T, G : E → F e o produto de uma
transformação linear T : E → F por um número α ∈ R são as transformações lineares
T + G : E → F e αT : E → F, deﬁnidas respectivamente por (T + G)v = T (v) + G(v)
e (αT )v = α · T (v), para todo v ∈ E. O símbolo 0 indica a transformação linear nula
0 : E → F deﬁnida por 0 · v = 0 e, deﬁnindo −T : E → F por (−T )(v) = −A(v), vê-se que
(−T ) + T = T + (−T ) = 0.

Dadas as transformações lineares R : E → F, S : F → G, onde o domínio de
S coincide com a imagem de R, deﬁne-se o produto SR : E → G pondo, para cada
v ∈ E, (SR)(v) = S(R(v)).

Vemos que SR é uma transformação linear. Observamos também que SR nada
mais é do que a composta S ◦ R das funções S e R. Segue-se então dos princípios gerais
que se T : G → H é outra transformação linear, valem as propriedades

Associatividade: (T S)R = T (SR)

28

De fato, (T S)R = (T S) ◦ R, portanto (T S) ◦ R(v) = (T S)(R(v)) = T (S(R(v))) =
T (SR(v)) = T ◦ SR(v) para todo v, logo (T S)R = T (SR).

A linearidade tampouco é necessária para mostrar que, dadas RE → F e S, T :

F → G, tem-se a

Distributividade à esquerda: (S + T )R = SR + T R, que decorre simplesmente da
deﬁnição de S + T .

Usando a linearidade de T : F → G, vemos que, dadas, R, S : E → F, vale a

propriedade

Distributividade à direita: T (R + S) = T R + T S.

Com efeito, para todo v ∈ E, tem-se

[T (R + S)](v) = C[(R + S)(v)] = C[(R(v) + S(v)] = T (R(v)) + T (S(v)) =
= (T R)(v) + (T S)(v) = (T R + T S)(v)

Homogeneidade: S(αR) = α(SR), válida para α ∈ R, R : E → F e S : F → G quaisquer.

As transformações lineares T : E → E do espaço vetorial E em si mesmo são
chamados operadores lineares em E. Um operador linear especial é o operador identidade
I : E → E deﬁnido por I(v) = v para todo v ∈ E.

Uma transformação linear T : E → F é um tipo particular de função que tem o
espaço vetorial E como domínio e o espaço F como contra-domínio. Em geral, para se
deﬁnir uma função f : X → Y é necessário especiﬁcar o valor de f (x) para cada elemento
x no seu domínio X. O que torna as transformações lineares tão manejáveis é que para se
conhecer uma transformação T , basta que se saibam os valores T (v) que T assume nos
vetores v ∈ B, onde B é uma base de E. Isto é particularmente útil quando E tem dimensão
ﬁnita. Neste caso, um número ﬁnito de valores T (v1), . . . , T (vn) (onde {v1, . . . , vn} ⊂ E
é uma base) atribuídos arbitrariamente, deﬁnem inteiramente uma transformação linear
T : E → F. Mais precisamente, vale o seguinte Teorema:

Teorema 5.1 Sejam E, F espaços vetoriais e B uma base de E. A cada vetor u ∈ B,
façamos corresponder (de maneira arbitrária) um vetor u(cid:48) ∈ F. Então existe uma única
transformação linear T : E → F tal que T (u) = u(cid:48).

Prova. Todo vetor v ∈ E se exprime, de modo único, como uma combinação linear
v = α1u1 + . . . + αmum de elementos u1, . . . , um da base B. Deﬁnimos T : E → F pondo

Dados v, w ∈ E temos

T (v) = α1u(cid:48)

1 + . . . + αmu(cid:48)
m

v = α1u1 + . . . + αmum

29

e

então

logo

w = β1u1 + . . . + βmum

v + w =

m
∑
i=1

(αi + βi)ui

T (v + w) = ∑(αi + βi)u(cid:48)

i = ∑ αiu(cid:48)

i +∑ βiu(cid:48)

i = T (v) + T (w).

De maneira análoga se vê que T (αv) = α · T (v), portanto T : E → F, assim deﬁnida, é
uma transformação linear, tal que T (u) = u(cid:48), para todo u ∈ B. Quanto à unicidade, seja
G : E → F outra transformação linear tal que G(u) = u(cid:48) para todo u ∈ B. Então, para cada
v = ∑ αiui ∈ E tem-se

G(v) = G (cid:0)∑ αiui

(cid:1) = ∑ αi · G(ui) = ∑ αi · u(cid:48)

i = T (v)

portanto G = T . Isto completa a demonstração.

(cid:3)

Em virtude do Teorema 5.1, se quisermos deﬁnir uma transformação linear
T : Rn → Rm basta escolher, para cada j = 1, . . . , n, um vetor v j = (a1 j, a2 j, . . . , am j) ∈ Rm
e dizer que v j = A · e j é a imagem do j-ésimo vetor da base canônica, e j = (0, . . . , 1, . . . , 0),
pela transformação linear T . A partir daí, ﬁca determinada a imagem T (v) de qualquer
vetor v = (x1, . . . , xn) ∈ Rn. Com efeito, tem-se v = x1e1 + . . . + xnen, logo

T (v) = T

(cid:33)

x je j

=

(cid:32) n
∑
j=1

n
∑
j=1

x jT (e j) =

n
∑
j=1

(a1 jx j, a2 jx j, . . . , am jx j)

=

(cid:32) n
∑
j=1

a1 jx j,

n
∑
j=1

a2 jx j, . . . ,

(cid:33)

am jx j

,

n
∑
j=1

ou seja,

onde

T (x1, x2, . . . , xn) = (y1, y2, . . . , ym),

y1 = a11x1 + a12x2 + . . . + a1nxn

y2 = a21x1 + a22x2 + . . . + a2nxn

...

ym = am1x1 + am2x2 + . . . + amnxn.

(5-1)

Resumindo: uma transformação linear T : Rn → Rm ﬁca inteiramente determi-

30

nada por uma matriz A = [ai j] ∈ M(m × n). Os vetores-colunas dessa matriz são as ima-
gens T (e j) dos vetores da base canônica de Rn. A imagem de T (v) de um vetor arbitrário
v = (x1, . . . , xn) ∈ Rn é o vetor w = (y1, . . . , ym) ∈ Rm cujas coordenadas são dadas pelas
equações 5-1 acima, nas quais ocorrem os vetores-linha da matriz A. Diz-se que A é a
matriz da transformação de T relativa às bases canônicas de Rn e Rm.

Dessa maneira temos que o espaço vetorial M(m×n), das matrizes m×n munido
das operações usuais de soma, multiplicação por escalar e produto, satisfazem todas as
condições de transformações lineares, ou seja, para R e S, transformações lineares cujas
matrizes associadas sejam A e B respectivamente, temos que as matriz de R + S, αR e R ◦ S
(onde a soma, multiplicação por escalar e composição estejam deﬁnidas) são A + B, αA
e A × B respectivamente (para um estudo mais aprofundado desses resultados vide [1],
[2] e [8]). A partir de agora faremos uso abertamente de todas as propriedades matriciais,
como determinantes, transposta, ortogonalidade, semelhança, adjunta e etc.

Exemplo 5.1 Seja v = (x, y, z) um vetor genérico do R3. Então, a fórmula

T (v) = (2x − y + z, x + 3y − 2z)

(5-2)

Deﬁne uma transformação T : R3 → R2, é fácil ver que T assim deﬁnida é linear. Por
exemplo, a imagem do vetor canônico e1 = (1, 0, 0) é o vetor w = (2, 1). Observe que, se

(cid:32)

A =

(cid:33)

2 −1
1

1
3 −2

, então

(cid:32)

Avt =

2 −1
1

1
3 −2

(cid:32)




 =

(cid:33)






x
y
z

2x − y + z
x + 3y − 2z

(cid:33)

.

Comparando 5-2 e 5-4, vemos que a transformação T : R3 → R2 é dada por

T (v) = (Avt)t.

(5-3)

(5-4)

(5-5)

Observe que no exemplo acima, utilizamos a noção de matriz transposta, indi-
cada pelo expoente t, isso se faz necessário uma vez que a imagem de uma transformação
linear é um vetor linha e o produto da matriz A pelo vetor coluna v é um vetor coluna.

Exemplo 5.2 (Rotação de ângulo θ em torno da origem em R2) Trata-se do operador
R : R2 → R2, que leva cada vetor v no vetor R(v) que dele resulta pela rotação de ângulo
θ em torno da origem. A ﬁgura 5.1 deixa claro que R(u + v) = R(u) + R(v). É bem mais
claro ainda que R(αv) = αRv para v ∈ R2 e α ∈ R, logo R é uma transformação linear.

31

Figura 5.1: Rotação de Vetores.

Para um vetor v = (x, y) ∈ R2 arbitrário, seja R(v) = (x(cid:48), y(cid:48)). Sabemos que existe uma
matriz A associada a transformação R, tal que R(v) = (Avt)t, temos que A ∈ M(2 × 2).
Queremos então determinar a matriz

(cid:32)

A =

(cid:33)

a b
c d

onde R(e1) = (a, c) e R(e2) = (b, d), com e1 = (1, 0) e e2 = (0, 1).

Ora, pelas deﬁnições de seno e cosseno, o vetor unitário R(e1), que forma com

e1 um ângulo θ, tem coordenadas
cosθ e sen θ, ou seja, R(e1) = (cos θ, sen θ). Além disso, como e2 forma com e1 um ângulo
reto, R(e2) também forma com R(e1) um ângulo reto. Logo R(e2) = (−sen θ, cos θ)

Figura 5.2: Rotação de ângulo θ.

Portanto, a rotação R : R2 → R2 leva um vetor v = (x, y) no vetor R(v) = (Avt)t =

(x(cid:48), y(cid:48)), onde

portanto

(cid:32)

A =

(cid:33)

cos θ −sen θ
cos θ
sen θ

(cid:32)

Avt =

cos θ −sen θ
cos θ
sen θ

(cid:33) (cid:32)

(cid:33)

(cid:32)

=

x
y

x cos θ − ysen θ
xsen θ + y cos θ

(cid:33)

obtemos assim:

x(cid:48) = x cos θ − ysen θ
y(cid:48) = xsen θ + y cos θ

32

A matriz de R relativa à base canônica de R2 é portanto a matriz A.

Exemplo 5.3 (Projeção ortogonal sobre uma reta) A reta y = ax é o conjunto dos pon-
tos (x, ax) ∈ R2, onde x varia em R. Ela é o subespaço vetorial de R2 gerado por (1, a).
Consideremos o operador P : R2 → R2 que faz corresponder a cada v = (x, y) ∈ R2 o vetor
P(v) = (x(cid:48), ax(cid:48)), cuja extremidade é o pé da perpendicular baixada de v sobre a reta y = ax
como é mostrado na ﬁgura 5.3.

Figura 5.3: Projeção ortogonal sobre uma reta.

Queremos determinar x(cid:48) em função de x e y, o que nos dará as coordenadas
(x(cid:48), ax(cid:48)) de P(v) em função das coordenadas de v. No caso particular em que a = 0, a reta
y = ax é o eixo das abscissas e a projeção P(v) é simplesmente a (x, 0). As equações da
projeção P sobre o eixo horizontal são portanto x(cid:48) = x e y(cid:48) = 0. A matriz de P na base

canônica de R2 é

. No caso geral, a extremidade do vetor P(v) é o vértice do

(cid:32)

(cid:33)

1 0
0 0

ângulo reto num triângulo retângulo cujos demais vértices são a origem e a extremidade
do vetor v. Pelo teorema de Pitágoras, temos

dist(v, 0)2 = dist(P(v), 0)2 + dist(v, P(v))2,

ou seja,

x2 + y2 = (x(cid:48))2 + a2(x(cid:48))2 + (x − x(cid:48))2 + (y − ax(cid:48))2.

Supondo x(cid:48) (cid:54)= 0, desenvolvendo, simpliﬁcando e dividindo ambos os membros por x(cid:48),
obtemos (1 + a2)x(cid:48) = x + ay, donde

x(cid:48) =

x + ay
1 + a2 ,

ou seja

x(cid:48) =

1
1 + a2 x +

a
1 + a2 y.

O caso x(cid:48) = 0 signiﬁca que v(x, y) está sobre a perpendicular à reta y = ax passando pela
origem. Ora, a equação dessa perpendicular é x + ay = 0, logo a expressão x(cid:48) = (x+ay)
(1+a2)
fornece x(cid:48) em função de x e y em quase todos os casos, falta analisarmos quando a reta se

33

trata do eixo y, cuja equação nesse caso é x + ay = 0 com a = 0, assim a projeção P(v) é
simplesmente igual a (0, y). As equações da projeção P sobre o eixo vertical são portanto

x(cid:48) = 0, y(cid:48) = y. A matriz de P na base canônica de R2 é

. Vemos em particular,

(cid:32)

(cid:33)

0 0
0 1

que a projeção P : R2 → R2 é um operador linear, cujas matrizes na base canônica de R2
são:

caso a reta seja y = ax

(cid:33)

(cid:32) 1

1+a2
a
1+a2

a
1+a2
a2
1+a2

;

caso a reta seja o eixo y

(cid:32)

(cid:33)

.

0 0
0 1

Observe que a segunda matriz é o lim
a→∞
nesse tema.

da primeira matriz, mas não vamos nos aprofundar

Exemplo 5.4 (Reﬂexão em torno de uma reta.) Seja S : R2 → R2 a reﬂexão em torno
da reta y = ax. Para todo v = (x, y) ∈ R2, a reta y = ax é a bissetriz do ângulo entre v
e S(v) e é perpendicular à reta que liga v a S(v). Seja P : R2 → R2 a projeção ortogonal
sobre a reta y = ax. A ﬁgura 5.4 mostra que, para todo v ∈ R2, tem-se v + S(v) = 2P(v), ou
seja, I + S = 2P, onde I : R2 → R2 é o operador identidade. Daí vem S = 2P − I. Usando
o exemplo anterior, concluímos que, para todo v = (x, y), tem-se S(v) = (x(cid:48), y(cid:48)), onde

x(cid:48) =

1 − a2
1 + a2 x +

2a
1 + a2 y,

y(cid:48) =

2a
1 + a2 x −

1 − a2
1 + a2 y.

Obtemos assim a matriz A associada a transformação S é:

Figura 5.4: Reﬂexão em torno de uma reta.

A =

(cid:32) 1−a2
2a
1+a2
1+a2
1+a2 − 1−a
2a
1+a2

(cid:33)

.

Observamos que a matriz A, a princípio, não contempla o caso em que a reta se trata do
eixo y, isso se dá porque o parâmetro a na matriz A é o coeﬁciente angular da reta, ou
seja, a tan(θ), onde θ é o ângulo que a reta faz com o eixo positivo x, de fato o eixo y é

34

descrito pela equação y = ax quando a tende ao inﬁnito e portanto a matriz A retrata, de
fato, a reﬂexão com relação a uma reta que passa pela origem.

Mostraremos agora uma outra maneira de se obter a reﬂexão sobre uma reta, só
que nesse caso vamos enfatizar a argumentação sobre o ângulo θ que a reta faz com o
eixo positivo x.

Exemplo 5.5 (Reﬂexão em torno de uma reta L que passa pela origem.) É uma trans-
formação que leva cada ponto sobre sua imagem especular com relação a à reta L. A ﬁgura
5.5 ilustra a reﬂexão T com relação ao eixo x, para qual T (x, y) = (x, −y). Obviamente,

T (e1) = e1 = (1, 0)

e T (e2) = (0, −1)

de modo que a matriz de T é

(cid:32)

A =

(cid:33)

.

0
1
0 −1

Como uma veriﬁcação, notamos que

T (x, y) =

(cid:34)(cid:32)

1
0
0 −1

(cid:33) (cid:32)

(cid:33)(cid:35)t

x
y

= (x, −y).

De maneira análoga, obtemos que a matriz de reﬂexão com relação ao eixo y é

(cid:32)

(cid:33)

.

−1 0
1
0

(a) Reﬂexão sobre o eixo x.

(b) Reﬂexão sobre o eixo y.

Figura 5.5: (a) e (b) representam as reﬂexão com relação ao eixo

x e y respectivamente.

Seja L a reta que passa através da origem do R2 e forma um ângulo θ com o
eixo x positivo. Então, pode-se conseguir a reﬂexão T com relação a L através de uma
rotação de um ângulo −θ para mover L sobre o eixo x, em seguida uma reﬂexão com
relação ao eixo x e, ﬁnalmente, uma rotação de um ângulo θ para mover L de volta à sua

35

posição original. Temos então três operadores lineares envolvidos Rθ, Tx e R−θ que fazem
respectivamente as movimentações citadas acima. Para cada um dos operadores citados
temos uma matriz associada, logo a matriz A associada a transformação Rθ ◦ Tx ◦ R−θ é o
produto das suas matrizes individuais, portanto

(cid:32)

A =

cos(θ) −sen (θ)
cos(θ)
sen (θ)

(cid:33)

(cid:32)

·

0
1
0 −1

(cid:33)

(cid:32)

·

cos(−θ) −sen (−θ)
cos(−θ)
sen (−θ)

(cid:33)

(cid:32)

A =

cos(θ) −sen (θ)
cos(θ)
sen (θ)

(cid:33)

(cid:32)

·

0
1
0 −1

(cid:33)

(cid:32)

·

cos(θ)
−sen (θ)

sen (θ)
cos(θ)

(cid:33)

,

portanto

(cid:32)

A =

cos2(θ) − sen 2(θ)

2sen (θ) cos(θ)

2sen (θ) cos(θ) −(cos2(θ) − sen 2(θ))

(cid:33)

.

Usando as igualdades trigonométricas

obtemos a matriz

cos(2θ) = cos2(θ) − sen 2(θ)

sen (2θ) = 2sen (θ) cos(θ),

(cid:32)

A =

sen (2θ)
cos(2θ)
sen (2θ) − cos(2θ)

(cid:33)

.

Para mais exemplos de operadores lineares vide [1], [2], [3], [7] e [8].

Núcleo e Imagem

CAPÍTULO 6

Nesta seção, será examinada com cuidado a possibilidade de uma transforma-
ção linear admitir ou não uma inversa. Veremos que isto está associado à existência e à
unicidade da solução de um sistema de equações lineares. Será introduzido o conceito
de isomorﬁsmo, que dará um sentido preciso à aﬁrmação de que dois espaços vetoriais
de mesma dimensão são algebricamente indistinguíveis. Tudo começa com o núcleo e a
imagem de uma transformação.

À toda transformação linear T : E → F estão associados dois subespaços vetoriais
indispensáveis para estudar o comportamento de T : o núcleo de T , que é o subespaço de
E, e a imagem de T , que é um subespaço de F, vide demonstração desse fato em [14].

A imagem de T é o subconjunto Im(T ) ⊂ F, formado por todos os vetores

w = T (v) ∈ E que são imagens de elementos de E pela transformação T .

A noção de imagem tem sentido seja qual for a função T : E → F, seja linear ou
não. Quando T é linear, então Im(T ) é um subespaço vetorial de F, como se vê facilmente.
Se Im(T ) = F, dizemos que a transformação T é sobrejetiva. Isto signiﬁca que,

para qualquer w ∈ F dado, pode-se achar v ∈ E tal que T (v) = w.

Seja X ⊂ E um conjunto de geradores do espaço vetorial E. A imagem da
transformação linear T : E → F é o subespaço vetorial de F gerado pelos vetores T (v), v ∈
X. Em particular, T é sobrejetiva se, e somente se, transforma X num conjunto de
geradores de F. Se v1, . . . , vn geram E os vetores T (v1), . . . , T (vn) geram Im(T ). Segue-se
que a dimensão de Im(T ) é menor do que ou igual à dimensão do domínio de T .

Uma transformação linear R : F → E chama-se uma inversa à direita da transfor-

mação T : E → F quando se tem T (R(w)) = w para todo w ∈ F.

Teorema 6.1 A ﬁm de que uma transformação linear T : E → F, entre espaços vetoriais
de dimensão ﬁnita, possua uma inversa à direita R é necessário e suﬁciente que T seja
sobrejetiva.

Prova. Se T admite uma inversa à direita R : F → E então para todo w ∈ F tem-se
T (R(w)) = w, logo w = T (v), onde v = R(w), e T é sobrejetiva. Suponhamos, em
seguida, que T seja sobrejetiva. A ﬁm de deﬁnir uma transformação linear R : F → E com

37

T (R(w)) = w, ∀w ∈ F, tomamos uma base B = {w1, . . . , wm} ⊂ F. Como T é sobrejetiva,
podemos escolher vetores v1, . . . , vm ∈ E tais que T (v1) = w1, . . . , T (vm) = wm. Pelo
Teorema 5.1, existe uma transformação linear R : F → E tal que R(w1) = v1, . . . , R(wm) =
vm. Aﬁrmamos que, para todo w ∈ F, tem-se T (R(w)) = w, de fato, sendo B uma base,
podemos escrever w = β1w1 + . . . + βmwm, portanto

T (R(w)) = T (β1R(w1) + . . . + βmR(wm))

= T (β1v1 + . . . + βmvm)

= β1T (v1) + . . . + βmT (vm)

= β1w1 + . . . + βmwm = w.

(cid:3)

O núcleo da transformação linear T : E → F é o conjunto dos vetores v ∈ E tais
que T (v) = 0. Usaremos a notação N (T ) para representar o núcleo de T . É fácil ver
que N (T ) é um subespaço vetorial de E. Uma transformação linear T : E → F chama-se
injetiva quando v (cid:54)= v(cid:48) em E ⇒ T (v) (cid:54)= T (v(cid:48)) em F. Equivalentemente: T (v) = T (v(cid:48)) ⇒
v = v(cid:48). Esta noção tem sentido para qualquer função T : E → F, seja ela linear ou não. No
caso linear, porém, o teorema abaixo simpliﬁca a veriﬁcação da injetividade.

Teorema 6.2 A ﬁm de que uma transformação linear T : E → F seja injetiva é necessário
e suﬁciente que seu núcleo N (T ) contenha apenas o vetor nulo.

Prova. Seja T injetiva. Então v ∈ N (T ) = {0}. Reciprocamente, seja N (T ) = {0}. Então
T (v) = v(cid:48) ⇒ T (v − v(cid:48)) = T (v) − T (v(cid:48)) = 0 ⇒ v − v(cid:48) = 0 ⇒ v − v(cid:48) ∈ N (T ) ⇒ v − v(cid:48) = 0 ⇒
(cid:3)
v = v(cid:48).

Teorema 6.3 Uma transformação linear é injetiva se, e somente se, leva vetores L.I. em
vetores L.I..

Prova. Seja T : E → F uma transformação linear injetiva. Se os vetores v1, . . . , vn ∈ E
são L.I., vamos provar que T (v1), . . . , T (vn) são L.I. em F. Com efeito, se
α1T (v1) + . . . + αnT (vn) = 0, então T (α1v1 + . . . + αnvn) = 0, logo α1v1 + . . . + αnvn = 0
pois T é injetiva. Como v1, . . . , vn são L.I., segue-se que α1 = . . . = αn = 0, portanto
T (v1), . . . , T (vn) são L.I.. Reciprocamente se a transformação linear T : E → F leva
vetores L.I. em vetores L.I., então v (cid:54)= 0 em E ⇒ {v} L.I. ⇒ {T (v)} L.I. ⇒ T (v) (cid:54)= 0,
(cid:3)
portanto N (T ) = {0} e T é injetiva.

38

Segue-se deste teorema que se E tem dimensão ﬁnita n e T : E → F é uma
transformação linear injetiva, então dimF (cid:62) n. Assim, por exemplo, não existe uma
transformação linear injetiva de R3 em R2.

Teorema 6.4 Seja T : E → F uma transformação linear. Para todo b ∈ Im(T ), o conjunto
V = {x ∈ E; T (x) = b}, formado pelas soluções do sistema linear T (x) = b, é uma
variedade aﬁm em E, paralela ao N (T ).

Observe que o sistema T (x) = b é o usualmente conhecido sistema matricial
AX = B, das aulas do ensino médio, onde A é a matriz associada à transformação T ,
enquanto que X e B são os vetores colunas dos vetores x e b, ou seja, X t = x e Bt = b.
Prova. Fixemos x0 ∈ V, isto é, com T (x0) = b. Aﬁrmamos que V = x0 + N (T ). Com
efeito, v ∈ N (T ) ⇒ T (x0 + v) = T (x0) + T (v) = b + 0 = b ⇒ x0 + x ∈ V. Logo x0 +
N (T ) ⊂ V. Reciprocamente,

x ∈ V ⇒ x = x0 + (x − x0) = x0 + v
⇓
b = T (x) = T (x0 + v) = t(x0) + t(v) = b + T (v)
⇓
b = b + T (v) ⇒ T (v) = 0 ⇒ x = x0 + v ∈ x0 + N (T ).

Logo V ⊂ x0 + N (T ).

(cid:3)

Teorema 6.5 Sejam E e F espaços vetoriais de dimensão ﬁnita e T transformação linear,
então T : E → F possui inversa à esquerda se, e somente se, é injetiva.

Prova. Seja R : F → E inversa à esquerda de T . Então T (u) = T (v) ⇒ u = R(T (u)) =
R(T (v)) = v, logo T é injetiva. Reciprocamente, suponha que T seja injetiva. A ﬁm
de obter uma inversa à esquerda R para T , tomemos {v1, . . . , vn} ⊂ E, uma base. Pelo
Teorema 6.3, os vetores T (v1), . . . , T (vn) ∈ F são L.I., logo podemos achar vetores
w1, . . . , wk ∈ F tais que

{T (v1, . . . , T (vn), w1, . . . , wk)} ⊂ F

seja uma base. Pelo Teorema 5.1, a ﬁm de se deﬁnir a transformação linear R :
F → E, basta especiﬁcar seus valores nos elementos desta base. Poremos R(T (v1)) =
v1, . . . , R(T (vn)) = vn, b(w1) = 0, . . . , b(wk) = 0. Dado qualquer v ∈ E, tem-se v = α1v1 +
. . . + αnvn, logo

R(T (v)) = R(α1T (v1) + . . . + αnT (vn))

= α1R(T (v1) + . . . + αnR(T (vn)

= α1v1 + . . . + αnvn,

portanto R é uma inversa à esquerda de T .

39

(cid:3)

Uma transformação T : E → F chama-se invertível quando existe R : F → R,
transformação linear, tal que RT = IE e T R = IF, ou seja, quando R é, ao mesmo tempo,
inversa à esquerda e à direita de T , em outras palavras quando a matriz A associada a T
for quadrada, ou seja, A ∈ M(n × n), e o determinante de A for diferente de zero.

Neste caso, dizemos que R é a inversa de T e escrevemos R = T −1, o que signiﬁca

que a matriz B associada a R é a inversa da matriz A associada a T .

A ﬁm de que a transformação linear T , seja invertível, é necessário e suﬁciente
que ela seja injetiva e sobrejetiva. Diz-se, então que T é uma bijeção linear entre E e F
ou, mais apropriadamente que T : E → F é um isomorﬁsmo e que os espaços vetoriais E
e F são isomorfos.

Se T : E → F e R : F → G são isomorﬁsmos, então T −1 : F → E e RT : E → G
α · T −1.

também são isomorﬁsmos. Tem-se (RT )−1 = T −1R−1 e para α (cid:54)= 0, (αT )−1 = 1
Observe que as mesmas relações valem para as suas matrizes associadas.

Um isomorﬁsmo T : E → F entre espaços vetoriais transforma toda base de E
numa base de F. Reciprocamente, se uma transformação linear T : E → F leva alguma
base de E numa base F então T é um isomorﬁsmo.

Do que foi dito acima resulta, em particular, que dois espaços vetoriais de
dimensão ﬁnita isomorfos têm a mesma dimensão. A recíproca é verdadeira, como
veremos agora.

Com efeito, seja E um espaço vetorial de dimensão ﬁnita n. Fixando uma base
{v1, . . . , vn} ⊂ E, podemos deﬁnir uma transformação linear T : Rn → E pondo, para cada
v = (α1, . . . , αn) ∈ Rn, T (v) = α1v1 + . . . + αnvn. Temos que T (e1) = v1, . . . , T (en) = vn.
Assim, T transforma a base canônica {e1, . . . , en} na base {v1, . . . , vn} ⊂ E, logo é um
isomorﬁsmo entre Rn e E.

Noutras palavras, todo espaço de dimensão n é isomorfo a Rn.
Como vimos T −1 : E → Rn e o produto RT −1 : E → F de T −1 por outro
isomorﬁsmo R : Rn → F são isomorﬁsmos, segue-se que dois espaços vetoriais E e F,
ambos de dimensão n, são isomorfos.

Isso justiﬁca que a restrição desse trabalho ao espaço R2 é apenas aparente, uma

vez que todos os espaços de dimensão 2 são isomorfos a R2.

Teorema 6.6 (Teorema do Núcleo e da Imagem) Sejam E, F espaços vetoriais de di-
mensão ﬁnita. Para toda transformação linear T : E → F tem-se que dim E = dim N (T )+
dim Im(T ).

40

Prova. O teorema resulta imediatamente da seguinte aﬁrmação mais precisa, que prova-
remos a seguir: se {T (u1), . . . , T (up)} é uma base de Im(T ) e {v1, . . . , vq} é uma base de
N (T ) então {u1, . . . , up, v1, . . . , vq} é uma base de E.
Com efeito, em primeiro lugar, se tivermos

α1u1 + . . . + αp + βv1 + . . . + βvq = 0,

(6-1)

então, aplicando a transformação T a ambos os membros desta igualdade e lembrando
que v1, . . . , vq pertencem ao núcleo de T , obtemos a igualdade

α1T (u1) + . . . + αpT (up) = 0.

Como os vetores T (u1), . . . , T (up) são L.I., resulta daí que α1 = . . . = αp = 0. Portanto a
igualdade 6-1 se reduz a igualdade

β1v1 + . . . + βqvq = 0.

Como v1, . . . , vq são L.I., concluímos que β1 = . . . = βq = 0. Isto mostra que os vetores
u1, . . . , up, v1, . . . , vq são L.I..

Em seguida, consideramos um vetor arbitrário w ∈ E. Como T (w) ∈ Im(T ),

podemos escrever

T (w) = α1T (u1) + . . . + αpT (up),

pois {T (u1), . . . , T (up)} é uma base da imagem de T . A igualdade acima pode ser reescrita
como

T [w − (α1u1 + . . . + αpup)] = 0.

Assim, o vetor w − (α1u1 + . . . + αpup) pertence ao núcleo de T , logo pode ser expresso
como combinação linear dos elementos da base {v1, . . . , vq}. Temos então

w − (α1u1 + . . . + αpup) = β1v1 + . . . + βqvq,

ou seja, w = α1u1 + . . . + αpup + β1v1 + . . . + βqvq.
{u1, . . . , up, v1, . . . , vq} geram E e portanto constituem uma base.

isto mostra que os vetores
(cid:3)

Corolário 6.6.1 Sejam E, F espaços vetoriais de mesma dimensão ﬁnita n. Uma trans-
formação linear T : E → F é injetiva se, e somente se, é sobrejetiva e portanto é um
isomorﬁsmo.

Com efeito, temos n = dim N (T ) + dim Im(T ). logo N (T ) = {0} se, e somente

se dim Im(T ) = n, ou seja, Im(T ) = F.

41

Teorema 6.7 Se uma transformação linear T : E → F tem uma inversa à esquerda
R : F → E e uma inversa à direita S : F → E, então R = S e T é um isomorﬁsmo, com
T −1 = R = S.

Prova. Tem-se RT = IE e T S = IF. Portanto R = RIF = R(T S) = (RT )S = IES = S.

(cid:3)

Corolário 6.7.1 Seja dim E = dim F. Se as transformações lineares T : E → F, R : F → E
são tais que RT = IE, então T R = IF e R = T −1.

Com efeito, RT = IE ⇒ T injetiva ⇒ T sobrejetiva ⇒ T S = IF para algum S ⇒

S = R ⇒ T R = IF.

Exemplo 6.1 (Projeção sobre o eixo das abscissas) Considere o operador linear T :
R2 → R2, tal que T (x, y) = (0, y), nestas condições N (T ) é o subespaço gerado pelo
vetor e1 e Im(T ) é o subespaço gerado pelo vetor e1, então como no exemplo 3.8 temos
que R2 = N (T ) ⊕ Im(T )

Exemplo 6.2 Para todo operador linear T : E → E num espaço vetorial de dimensão
ﬁnita vale a relação dim E = dim N (T ) + dim Im(T ). Isto porém não implica que
se tenha sempre E = N (T ) ⊕ Im(T ). Por exemplo, se T : R2 → R2 é deﬁnido por
T (x, y) = (x − y, x − y) então, tomando w = (1, 1), temos w = T (v), com v = (2, 1) e
T (w) = 0, logo N (T ) ∩ Im(T ) contém o vetor não nulo w

Produto Interno

CAPÍTULO 7

Os axiomas de espaço vetorial não são suﬁcientes para abordar certas noções
geométricas como ângulo, perpendicularismo, comprimento, distância, etc. Isto se torna
possível com a introdução de um produto interno.

Um produto interno num espaço vetorial E é um funcional bilinear simétrico
e positivo em E. Mais precisamente, um produto interno é uma função E × E → R,
que associa a cada par de vetores u, v ∈ E um número real (cid:104)u, v(cid:105), chamado de produto
interno de u por v, de modo que sejam válidas as seguintes propriedades, para quaisquer
u, u(cid:48), v, v(cid:48) ∈ E e α ∈ R:

Bilinearidade: (cid:104)u + u(cid:48), v(cid:105) = (cid:104)u, v(cid:105) + (cid:104)u(cid:48), v(cid:105), (cid:104)αu, v(cid:105) = α(cid:104)u, v(cid:105), (cid:104)u, v + v(cid:48)(cid:105) = (cid:104)u, v(cid:105) + (cid:104)u, v(cid:48)(cid:105),
(cid:104)u, αv(cid:105) = α(cid:104)u, v(cid:105);

Comutatividade (simetria): (cid:104)u, v(cid:105) = (cid:104)v, u(cid:105);

Positividade: (cid:104)u, u(cid:105) > 0 se u (cid:54)= 0.

Como (cid:104)0, v(cid:105) = (cid:104)0 + 0, v(cid:105) = (cid:104)0, v(cid:105) + (cid:104)0, v(cid:105), segue-se que (cid:104)0, v(cid:105) = (cid:104)v, 0(cid:105) = 0 para

todo v ∈ E.

Resulta da positividade que se (cid:104)u, v(cid:105) = 0, para todo v ∈ E, então u = 0. Com

efeito, se u (cid:54)= 0 teríamos (cid:104)u, v(cid:105) (cid:54)= 0 pelo menos quando v = u.

Segue-se desta observação que se u, u(cid:48) ∈ E são vetores tais que (cid:104)u, v(cid:105) = (cid:104)u(cid:48), v(cid:105)
para todo v ∈ E, então u = u(cid:48). Com efeito, isto implica que (cid:104)u − u(cid:48)(cid:105) = 0 para todo v ∈ E,
logo u − u(cid:48) = 0 e u = u(cid:48).

O número não negativo |u| = (cid:112)(cid:104)u, u(cid:105) chama-se a norma ou o comprimento do

vetor u. Com esta notação, tem-se |u|2 = (cid:104)u, u(cid:105) e a igualdade

(cid:104)u + v(cid:105) = (cid:104)u, u(cid:105) + (cid:104)u, v(cid:105) + (cid:104)v, u(cid:105) + (cid:104)v, v(cid:105),

lê-se: |u + v|2 = |u|2 + |v|2 + 2(cid:104)u, v(cid:105).

Quando |u| = 1 diz-se que u ∈ E é um vetor unitário. Todo vetor u (cid:54)= 0 se escreve

como u = |u| · u(cid:48), onde u(cid:48) é um vetor unitário. Basta pôr u(cid:48) = |u|−1 · u.

43

Exemplo 7.1 No espaço euclidiano Rn, produto interno canônico dos vetores u =
(α1, . . . , αn) e v = (β1, . . . , βn) é deﬁnido por (cid:104)u, v(cid:105) = α1β1 + . . . + αnβn. Este é o pro-
duto interno que consideraremos em Rn.

Exemplo 7.2 Consideremos R2 como modelo aritmético do plano euclidiano, no qual se
introduziu um sistema de coordenadas cartesianas. Dados u = (α1, α2) e v = (β1, β2), os
números

(cid:113)

|u| =

1 + α2
α2
2

e

|v| =

(cid:113)

1 + β2
β2
2

medem realmente os comprimentos das ﬂechas que representam esses vetores. Supo-
nhamos u (cid:54)= 0, v (cid:54)= 0 e chamemos de θ o ângulo formado por essas ﬂechas. Aﬁrma-
mos que o produto interno (cid:104)u, v(cid:105) = α1β1 + α2β2 acima deﬁnido é igual a |u||v| cos θ.
Isto será provado em três passos: 1o) Se os vetores u e v são perpendiculares, então
(cid:104)u, v(cid:105) = 0 = |u||v| cos 90o. Com efeito, por um lado,

|u + v|2 = (cid:104)u + v, u + v(cid:105) = |u|2 + |v|2 + 2(cid:104)u, v(cid:105)

e por outro lado, pelo Teorema de Pitágoras,

|u + v|2 = |u|2 + |v|2,

logo (cid:104)u, v(cid:105) = 0. 2o) Se |u| = |v| = 1, então (cid:104)u, v(cid:105) = cos θ. Com efeito, tomando o vetor

Figura 7.1: Teorema de Pitágoras em sua forma vetorial.

unitário u∗ perpendicular a u temos, pela deﬁnição de seno e cosseno, v = cos θ · u +
senθ · u∗. Tomando o produto interno de ambos os membros desta igualdade por u,
vem (cid:104)u, v(cid:105) = cos θ · (cid:104)u, u(cid:105) + sen θ · (cid:104)u, u∗(cid:105). Como (cid:104)u, u(cid:105) = 1 e (cid:104)u, u∗(cid:105) = 0, pelo primeiro
passo, temos (cid:104)u, v(cid:105) = cos θ. 3o) Caso geral: pomos u = |u| · u(cid:48) e v = |v| · v(cid:48), onde u(cid:48) e
v(cid:48) são vetores unitários na mesma direção e sentido de u e v respectivamente. Então
(cid:104)u, v(cid:105) = |u||v|(cid:104)u(cid:48), v(cid:48)(cid:105) = |u||v| cos θ. Vemos, em particular, que o vetores u,v formam um

44

Figura 7.2: Decomposição do vetor v em uma base ortogonal.

ângulo agudo quando (cid:104)u, v(cid:105) > 0, um ângulo obtuso quando (cid:104)u, v(cid:105) < 0 e um ângulo reto
quando (cid:104)u, v(cid:105) = 0.

Seja E um espaço com produto interno. Dois vetores u, v ∈ E chamam-se orto-
gonais (ou perpendiculares) quando (cid:104)u, v(cid:105) = 0. Escreve-se, então u⊥v. Em particular, 0 é
ortogonal a qualquer vetor de E. Um conjunto X ⊂ E diz-se ortogonal quando todos os ve-
tores de X são ortogonais dois a dois. Se, além disso, todos os vetores de X são unitários,
então X chama-se um conjunto ortonormal. Portanto, o conjunto X ⊂ E é ortonormal se,
e somente se, dados u, v ∈ X tem-se (cid:104)u, v(cid:105) = 0 se u (cid:54)= v e (cid:104)u, v(cid:105) = 1 se v = u. Uma base
ortonormal é uma base de E que é um conjunto ortonormal.

Teorema 7.1 Num espaço vetorial E com produto interno, todo conjunto ortonormal X
de vetores não nulo é L.I..

Prova. Sejam v1, . . . , vn ∈ X. Temos (cid:104)vi, v j(cid:105) = 0 se i (cid:54)= j. Se α1v1 + . . . + αnvn = 0 é uma
combinação linear nula desses vetores então, para cada i = 1, 2, . . . , n, tomamos os produto
interno de ambos os membros desta igualdade por vi e temos

α1(cid:104)v1, vi(cid:105) + . . . + αn(cid:104)vn, vi(cid:105) = 0.

Logo αi(cid:104)vi, vi(cid:105) = αi|vi|2 = 0, pois todos os produtos internos (cid:104)v j, vi(cid:105), com j (cid:54)= i, são
nulos em virtude da ortogonalidade de X. Além disso, como os vetores pertencentes ao
conjunto X são todos não nulos, resulta de αi|vi|2 = 0 que αi = 0. Assim, os coeﬁcientes
da combinação linear ∑ αivi = 0 são todos iguais a zero e os vetores do conjunto X são,
(cid:3)
portanto, linearmente independentes.

Exemplo 7.3 A base canônica {e1, . . . , en} ⊂ Rn é ortonormal: tem-se (cid:104)ei, e j(cid:105) = 0 se i (cid:54)= j
e (cid:104)ei, e j(cid:105) = 1 se i = j. No plano R2 os vetores u = (1, 1) e v = (−1, 1) são ortonormais.
Pondo

u(cid:48) =

(cid:33)

(cid:32)√
2
2

,

√
2
2

e

v(cid:48) =

−

(cid:32)

(cid:33)

√
2
2

,

√
2
2

45

o conjunto {u(cid:48), v(cid:48)} ⊂ R2 é uma base ortonormal.

Num espaço vetorial E com produto interno, seja u um vetor unitário. Dado
qualquer v ∈ E, o vetor (cid:104)u, v(cid:105) · u chama-se a projeção ortogonal de v sobre o eixo
que contém u. A justiﬁcativa para esta denominação está no fato de que, escrevendo
w = v − (cid:104)u, v(cid:105)u, tem-se v = (cid:104)u, v(cid:105)u + w, onde w é perpendicular a u. Com efeito, tomando
o produto interno de u por ambos os membros da igualdade w = v − (cid:104)u, v(cid:105)u tem-se

(cid:104)u, w(cid:105) = (cid:104)u, v(cid:105) − (cid:104)u, v(cid:105)(cid:104)u, u(cid:105) = (cid:104)u, v(cid:105) − (cid:104)u, v(cid:105) = 0,

pois (cid:104)u, u(cid:105) = 1.

Figura 7.3: Projeção ortogonal.

Quando se tem apenas u (cid:54)= 0, o eixo que contém u é o mesmo que contém o vetor
|u|(= |u|−1 · u). A projeção ortogonal de v sobre este eixo é, portanto, igual

unitário u(cid:48) = u
a (cid:104)u(cid:48), v(cid:105)u(cid:48), ou seja, (cid:104)u,v(cid:105)

(cid:104)u,u(cid:105) · u. Usaremos a notação

pru(v) =

(cid:104)u, v(cid:105)
(cid:104)u, u(cid:105)

· u

para indicar a projeção ortogonal do vetor v sobre o eixo que contém o vetor não-nulo u.
Se z = pru(v), tem-se v = z + w, com w⊥z. Pelo Teorema de Pitágoras, |v|2 =
|z|2 + |w|2. Em particular vemos que |z| (cid:54) |v|, isto é, o comprimento da projeção pru(v) é
menor do que ou igual ao comprimento v.

Ora, a norma de vetor pru(v) é igual a |(cid:104)u,v(cid:105)|
|u|

. Segue-se então que, para quaisquer

u, v ∈ E tem-se |(cid:104)u,v(cid:105)|
|u|

(cid:54) |v|, ou seja

|(cid:104)u, v(cid:105)| (cid:54) |u| · |v|

(desigualdade de Schwarz).

A rigor, o argumento acima prova a desigualdade de Schwarz apenas no caso
em u (cid:54)= 0, mas ela é óbvia no caso em que u, v é múltiplo um do outro. Isto resulta do
raciocínio acima pois, no Teorema de Pitágoras |v|2 = |z|2 + |w|2, dizer |v| = |z| signiﬁca
que w = 0, isto é, que v é múltiplo do u.

CAPÍTULO 8

Subespaços Invariantes, Operadores
Auto-Adjuntos, Um Caso Particular do
Teorema Espectral

A Adjunta de uma transformação Linear T : E → F é uma transformação linear

T ∗ : F → E tal que, para v ∈ E e w ∈ F quaisquer se tenha:

(cid:104)T (v), w(cid:105) = (cid:104)v, T ∗(w)(cid:105).

A transposta de uma matriz A = [ai j] ∈ M(m×n) é a matriz At = [a ji] ∈ M(n×m)

que tem como linhas as colunas de A e como colunas as linhas de A, na mesma ordem.

Teorema 8.1 Sejam U = {u1, . . . , un} ⊂ E e V = {v1, . . . , vm} ⊂ F bases ortonormais. Se
A = [ai j] ∈ M(m × n) é a matriz da transformação linear T : E → F nas bases U,V então
a matriz da adjunta T ∗ : F → E nas bases V ,U é a transposta At = [a ji] ∈ M(n × m) de
A.

Prova. Por deﬁnição de um transformação linear, temos

e

T (u j) =

m
∑
i=1

ai jvi

( j = 1, . . . , n)

T ∗(vi) =

n
∑
r=1

briur,

onde B = [bri] ∈ M(n × m) é a matriz de T ∗ nas bases V ,U, a ser determinada. Como
ambas as bases são ortonormais, temos, para cada i = 1, . . . , m e cada j = 1, . . . , n:

b ji = (cid:104)u j, T ∗(vi)(cid:105) = (cid:104)T (u j), vi(cid:105) = ai j,

portanto, B = At, transposta de A.

(cid:3)

47

É apresentada a seguir uma lista de propriedades operacionais da adjunta de um
transformação linear, as quais se traduzem em propriedades de transposta de matriz. A
validez dessas propriedades decorre da observação de que duas transformações lineares
T, R : E → F são iguais quando se tem (cid:104)T (u), v(cid:105) = (cid:104)R(u), v(cid:105) para quaisquer u ∈ E e v ∈ F

I∗ = I
(T + R)∗ = T ∗ + R∗
(αT )∗ = αt∗
(RT )∗ = T ∗R∗
T ∗∗ = T

n = In)

(It
(A + B)t = At + Bt
(αA)t = αAt
(BA)t = AtBt
(At)t = A

onde A e B são as matrizes associadas às transformações lineares T e R respectivamente.
Diz-se que um subespaço vetorial F ⊂ E é invariante pelo operador linear
T : E → E quando T (F) ⊂ F, isto é, quando a imagem de T (v) de qualquer vetor v ∈ F é
ainda um vetor de F.

Exemplo 8.1 Os subespaços {0} e E são invariantes por qualquer operador linear T :
E → E. O núcleo N (T ) e a imagem Im(T ) são também exemplos óbvios de subespaços
invariantes. Um subespaço F de dimensão 1 (reta passando pela origem) é invariante
por T se, e somente se, existe um número λ tal que T (v) = λv para todo v ∈ F. [Com
efeito, ﬁxando um vetor u (cid:54)= 0 em F, todos os demais elementos de F são da forma
αu, α ∈ R. Como T (u) ∈ F, tem-se T (u) = λu. Para qualquer outro v ∈ F, vale v = αu
logo T (v) = αT (u) = αλu = λαu, logo T (v) = λv, como o mesmo λ.]

Um vetor v (cid:54)= 0 em E chama-se um autovetor do operador linear T : E → E

quando existe λ ∈ R tal que

T (v) = λv.

O número λ ∈ R, por sua vez, chama-se autovalor do operador linear T quando existe um
vetor não-nulo v ∈ E tal que T (v) = λv. Diz-se então que o autovalor λ corresponde ao
autovetor v e , vice-versa, que o autovetor v também corresponde ao autovalor λ. Então,
para todo w = αv, tem-se T (w) = λw.

Achar um autovetor (ou, o que é equivalente, um autovalor) do operador T é,

portanto, o mesmo que achar um subespaço de dimensão 1 invariante por T .

Analogamente, diz-se que o número real λ é um autovalor da matriz A ∈
M(n×n) quando λ é um autovalor do operador T : Rn → Rn, cuja matriz na base canônica
é A. Isto signiﬁca que existe um vetor x (cid:54)= 0 em Rn tal que T (x) = λx ou, o que é o mesmo,
uma matriz não-nula x ∈ M(n × 1) tal que Ax = λx.

Exemplo 8.2 Uma rotação R : R2 → R2 em torno da origem, de ângulo diferente de 0◦ou
180◦, não admite outros subespaços invariantes além de {0} e R2.

48

Exemplo 8.3 O operador T : R2 → R2, deﬁnido por T (x, y) = (x + αy, y), chama-se
cisalhamento. Se α (cid:54)= 0, os únicos subespaços invariantes por T são {0}, R2 e o eixo
das abscissas. Com efeito, qualquer outro subespaço de R2 é uma reta F, formada pelos
múltiplos kv = (ka, kb) de um vetor v = (a, b), com b (cid:54)= 0. Se k (cid:54)= 0 tem-se kv ∈ F mas
T (kv) = (ka + αkb, kb) = kv + (αkb, 0) /∈ F logo F não é invariante por T .

Teorema 8.2 A autovalores diferentes do mesmo operador correspondem autovetores
linearmente independentes.

Prova. Dado o operador linear T : E → E, sejam v1, . . . , vm vetores não-nulos em E tais
que T (v1) = λ1v1, . . . , T (vn) = λmvm, onde os números reais λ1, . . . , λm são dois a dois
diferentes. Provaremos, por indução que esses são L.I.. A aﬁrmação é óbvia quando m = 1.
Suponha-a verdadeira para m − 1 vetores, concluiremos daí sua validez para m. Dada a
combinação linear nula

α1v1 + . . . + αmvm = 0,

(8-1)

aplicamos o operador T a ambos os membros desta igualdade, levando em conta que
T (vi) = λivi. Resulta então que

λ1α1v1 + . . . + λmαmvm = 0.

(8-2)

Multiplicamos a igualdade 8-1 por λm e subtraindo de 8-2 vem:

(λ1λ2)α1v1 + . . . + (λm−1 − λm)αm−1vm−1 = 0.

Pela hipótese de indução, os vetores v1, . . . , vm−1 são L.I.. Logo

(λ1λ2)α1 = . . . = (λm−1 − λm)αm−1 = 0.

Como os autovetores são todos diferentes, as m − 1 diferenças nos parênteses acima são
(cid:54)= 0, logo α1 = . . . = αm−1 = 0. Isto reduz a igualdade 8-1 a αmvm = 0. Como vm (cid:54)= 0,
segue-se que αm = 0. Assim, a igualdade 8-1 só pode ocorrer quando todos os coeﬁcientes
(cid:3)
αi são nulos, o que prova o teorema.

Corolário 8.2.1 Sejam dim E = n. Se um operador linear T : E → E possui n autovalores
diferentes então existe uma base {v1, . . . , vn} ⊂ E em relação à qual a matriz de T é
diagonal (isto é, tem a forma [ai j] com ai j = 0 se i (cid:54)= j).

Com efeito, se T (v1) = λ1v1, . . . , T (vn) = λnvn com os vi não nulos e os λi dois
a dois distintos então {v1, . . . , vn} é, em virtude do Teorema 8.2, uma base de E. A matriz

49

de T nesta base é

λ1









λ2

. . .









λn

na qual os termos que não aparecem são iguais a zero.

A igualdade T (v) = λv equivale a (T − λI)v = 0, logo v é um autovetor do
operador T : E → E se, e somente se, o operador T − λI : E → E não possui inversa,
ou seja, se o determinante da sua matriz associada for igual a zero. Para a validação dos
resultado de matrizes e um estudo mais aprofundado vide [3] e [7].

Exemplo 8.4 Um caso particular importante ocorre quando dim E = 2. Vimos no Exem-
plo 2.2 que se {u, v} ⊂ E é uma base então os vetores αu + βv e γu + δv são L.D. se,
e somente se, αδ − βγ = 0. Dados o operador T : E → E e a base {u, v} ⊂ E, sejam
T (u) = au + cv e T (v) = bu + dv. Noutras palavras, a matriz do operador T na base {u, v}
é

A =

(cid:32)

(cid:33)

.

a b
c d

Então (T − λI)(u) = (a − λ)u + cv e (T − λI)(v) = bu + (d − λ)v. A ﬁm de que T − λI não
possua inversa é necessário e suﬁciente que det(A−λI) = 0, ou seja, (a−λ)(d −λ)−bc =
0, ou ainda, que λ seja raiz do polinômio

p(λ) = λ2 − (a + d)λ + ad − bc,

chamado polinômio característico do operador T .

Portanto, o número real λ é um autovalor do operador T : E → E onde dim E = 2,
se, e somente se, é uma raiz do polinômio característico do operador T , o qual, por
deﬁnição é p(λ) = λ2 − (a + d)λ + ad − bc. Observe que essa deﬁnição se justiﬁca pela
teoria das matrizes onde raiz desse polinômio representa o valor para o qual o operador
não é invertível. Os coeﬁcientes de p(λ) são tirados da matriz A em relação a uma base
qualquer de E.

Observação. A matriz A do operador T muda quando se passa de uma base para outra.
Mas o polinômio p(λ) (isto é, as expressões a + d e ad − bc, que são seus coeﬁcientes)
permanece sem alteração. Vide [5] e [7].

Exemplo 8.5 No caso da rotação R : R2 → R2, R(x, y) = (x cosθ − y senθ, x senθ +
y cosθ), temos a = cosθ, b = −senθ, c = senθ, d = cosθ, logo o polinômio característico
de R é

p(λ) = λ2 − (2cosθ)λ + 1.

50

Se θ (cid:54)= 0◦ e θ (cid:54)= 180◦, o trinômio p(λ) não possui raiz real pois nesse caso seu discri-
minante ∆ = 4(cos2θ − 1) é negativo. Consequentemente R só possui autovalor (reais) se
θ = 0◦ ou θ = 180◦.

Exemplo 8.6 Deﬁnamos o operador T : R2 → R2 pondo T (x, y) = (4x + 3y, x + 2y). Seu
polinômio característico é p(λ) = λ2 − 6λ + 5, cujas raízes são λ1 = 1 e λ2 = 5. Estes
números são autovalores de T . Existem, portanto, vetores não-nulos v1 e v2 em R2, tais
que T (v1) = v1 e T (v2) = 5v2. Pelo Teorema 8.2, v1 e v2 formam uma base R2, em relação
à qual a matriz do operador T tem a forma diagonal:

A =

(cid:32)

(cid:33)

.

1 0
0 5

A ﬁm de determinar v1 = (x, y) e v2 = (r, s) exprimimos as igualdades T (v1) = v1 e
t(v2) = 5v2 em termos de coordenadas, obtemos os sistemas lineares

(cid:40)

4x + 3y = x,
x + 2y = y

(cid:40)

e

4r + 3s = 5r
r + 2s = 5s

.

Ambos os sistemas acima são indeterminados, e tinham que ser assim pois se v é autovetor
de T , todo múltiplo αv também é. Tomando uma solução não-nula de cada um desses
sistemas obtemos v1 = (1, −1), v2 = (3, 1) tais que {v1, v2} ⊂ R2 é uma base formada por
autovetores de T .

Um operador linear T : E → E, num espaço vetorial munido de produto interno,
chama-se auto-adjunto quando T = T ∗, ou seja, quando (cid:104)T (u), v(cid:105) = (cid:104)u, T (v)(cid:105) para
quaisquer u, v ∈ E.

Teorema 8.3 T : E → E é auto-adjunto se, e somente se, sua matriz A = [ai j] relativa-
mente a uma (e portanto a qualquer) base ortonormal U = {u1, . . . , un} ⊂ E é uma matriz
simétrica.

Prova. Sabemos que (cid:104)ui, T ((u j))(cid:105)=[i-ésima coordenada do vetor T (u j) na base U]=[i-
ésimo elemento da j-ésima coluna de A]=ai j. Portanto a matriz A é simétrica se, e somente
se, (cid:104)ui, T (u j)(cid:105) = (cid:104)T (ui), u j(cid:105) para quaisquer i, j = 1, . . . , n. Devido à linearidade de T e
à bilinearidade do produto interno, isto equivale a dizer que (cid:104)u, T (v)(cid:105) = (cid:104)T (u), v(cid:105) para
(cid:3)
quaisquer u, v ∈ E, ou seja, que T é auto-adjunta.

Teorema 8.4 Se λ1, . . . , λm são autovalores dois a dois diferentes do operador auto-
adjunto T : E → E, os autovetores correspondentes v1, . . . , vm são dois a dois ortogonais.

Prova. Para i (cid:54)= j quaisquer:

(λi − λ j)(cid:104)vi − v j(cid:105) = (cid:104)λivi, v j(cid:105) − (cid:104)vi, λ jv j(cid:105) = (cid:104)T (vi), v j(cid:105) − (cid:104)vi, T (v j)(cid:105)

= (cid:104)T (vi), v j(cid:105) − (cid:104)T (vi), v j(cid:105) = 0 pois T é auto-adjunto.

51

(cid:3)

Observação: Se T (v) = λv então, para todo múltiplo w = αv, tem-se ainda T (w) = λw.
Logo, na situação do Teorema 8.4, os vetores v1, . . . , vm podem ser tomados unitários,
caso haja conveniência.

Um problema importante sobre operadores num espaço vetorial de dimensão
ﬁnita é o de encontrar uma base em relação à qual a matriz desse operador seja a mais
simples possível. Mostraremos que, se T : E → E e um operador auto-adjunto (associado
a uma matriz simétrica, pois T = T ∗ ⇒ A = At) num espaço vetorial de dimensão ﬁnita
com produto interno, existe uma base ortonormal em E, relativamente à qual a matriz de
T é uma matriz diagonal A = [ai j], isto é, ai j = 0 se i (cid:54)= j.

Quando se diz que a matriz do operador T : E → E na base {u1, . . . , un} ⊂ E é
uma matriz diagonal, isto signiﬁca que, para todo j = i, . . . , n, tem-se T (u j) = λ ju j, ou
seja, os vetores da base dada são todos eles autovetores de T .

Vamos mostrar um caso particular do Teorema Espectral para operadores auto-

adjuntos em que o espaço tem dimensão 2.

Teorema 8.5 (Teorema Espectral para Operadores Auto-Adjuntos de R2) Seja
T : E → E um operador auto-adjunto num espaço vetorial de dimensão 2, munido
de produto interno. Existe uma base ortonormal {u1, u2} ⊂ E formada por autovetores
de T .

Prova. Seja {v, w} ⊂ E uma base ortonormal arbitrária. Em virtude do Teorema 8.3,
temos T (v) = av + bw, T (w) = bv + cw. Como vimos no Exemplo 8.4, os autovalores
de T são as raízes reais do polinômio característico p(λ) = λ2 − (a + c)λ + ac − b2. O
discriminante deste trinômio é ∆ = (a + c)2 − 4(ac − b2) = (a − c)2 + 4b2 (cid:62) 0. Se ∆ = 0,
então b = 0, a = c e T = aI, logo todo vetor não-nulo em E é um autovetor. Se ∆ > 0, então
o trinômio p(λ) possui 2 raízes reais distintas λ1, λ2. Isto, como sabemos, quer dizer que
os operadores T − λ1I e T − λ2I são ambos não invertíveis, logo existem vetores não-
nulos (que podemos supor unitário) u1, u2 ∈ E tais que T (u1) = λ1u1 e T (u2) = λ2u2.
(cid:3)
Pelo Teorema 8.4 {u1, u2} ⊂ E é uma base ortonormal de autovetores de T .
Para uma abordagem mais completa sobre o Teorema Espectral vide [3], [4], [5] e [7].

Seções Cônicas e Formas Quadráticas

CAPÍTULO 9

Aplicaremos agora todo nosso estudo de Álgebra Linear ao problema geométrico

de se determinar o gráﬁco, no plano xy, de um equação da forma

ax2 + 2bxy + cy2 + dx + ey + f = 0

(9-1)

onde os coeﬁcientes a, b, . . . , f são constantes reais, com a, b e c não todos nulos. Tal
equação é chamada de equação de segundo grau em x e y.

A razão pela qual escrevemos 2b em lugar de b para o coeﬁciente do termo
xy é que a natureza do gráﬁco é determinada, em grande parte, pela forma quadrática
associada

ax2 + 2bxy + cy2 = (cid:104)T (x, y), (x, y)(cid:105)

(9-2)

em x e y, que corresponde ao operador auto-adjunto T : R2 → R2 associado à matriz
simétrica 2 × 2

A =

(cid:32)

(cid:33)

a b
b c

.

(9-3)

Em geral, a forma quadrática q nas variáveis x1, x2, . . . , xn que corresponde ao

operador auto-adjunto T : Rn → Rn é a função q : Rn → R deﬁnida por

q(x) = (cid:104)x, T (x)(cid:105).

A chave para a análise da equação de segundo grau em 9-1 é o fato de que a
matriz simétrica A associada a transformação T em 9-3 é ortogonalmente diagonalizável,
ou seja, de acordo com o Teorema 8.5, existe uma base B ortonormal tal que A nessa base
B é uma matriz diagonal. Usando esse fato, demonstraremos que, exceto por alguns casos
degenerados, o gráﬁco de toda equação de segundo grau em x e y é uma seção cônica. A
expressão seção cônica vem do fato destas serem as curvas em que um plano intercepta
um cone. O cone utilizado é um cone circular reto com duas folhas que se estendem ao
inﬁnito em ambos os sentidos, como mostra a ﬁgura 9.1. Há três tipo de seções cônicas,
conforme ilustrado na ﬁgura 9.1. Se o plano secante é paralelo a alguma geratiz do cone,

então a curva de interseção é uma parábola. De outro modo, ou ela é uma única curva
fechada - elipse -, ou é uma hipérbole com dois ramos.

53

Figura 9.1: Cone e suas secções.

Pode-se veriﬁcar que, para um sistema apropriado de coordenadas xy nos planos
secantes da ﬁgura 9.1, as equações dos três tipos de seções cônicas tomam as seguintes
formas:

Parábola:

Elipse:

Hipérbole:

ou x2 = ky

y2 = kx
y2
x2
b2 = 1
a2 +
y2
x2
b2 = 1
a2 −

(9-4)

ou

y2
a2 −

x2
b2 = 1

Seções cônicas com essas equações estão ilustradas nas ﬁguras 9.1. Diz-se que uma seção
cônica está em posição canônica em relação aos eixos coordenados se sua equação tomar
uma das formas listadas nas Equações 9-4, para uma demonstração dessas equações vide
[1], [3], [5], [7] e [8].

Exemplo 9.1 A equação 4x2 + 9y2 − 36 = 0 pode ser escrita na forma

x2
9

+

y2
4

= 1;

e em consequência seu gráﬁco é uma elipse com a = 3 e b = 2. Observe a singular de a
e b: elipse tem interseções (a, 0) e (−a, 0) com eixo x e interseções (0, b) e (0, −b) com

54

o eixo y. Também é claro que o gráﬁco é simétrico em relação ao eixo y, bem como em
relação ao eixo x (substitua x por −x ou y por −y).

A equação 9x2 − 4y2 + 36 = 0 pode ser escrita na forma

y2
9

−

x2
4

= 1;

assim, seu gráﬁco é uma hipérbole cujos ramos interceptam o eixo y nos pontos (0, −3) e
(0, 3). Seu gráﬁco também é simétrico em relação a cada eixo coordenado.

Finalmente, a equação y2 + 4x = 0 pode ser escrita na forma

y2 = −4x,

e, consequentemente, seu gráﬁco é uma parábola que se abre ao longo do eixo x negativo.
A parábola tem a interseção única (0, 0) e é simétrica em relação ao eixo x (a substituição
de y por −y não altera a equação), apesar de não o ser em relação ao eixo y.

Além de parábola, elipse ou hipérbole, o gráﬁco de uma equação de segundo grau
em x e y pode ser uma reta, um par de retas, um único ponto ou o conjunto vazio. Estes
casos especiais, ilustrados no exemplo a seguir, são referidos como cônicas degeneradas.

Exemplo 9.2 O gráﬁco da equação x2 = 0 é o eixo y. O gráﬁco da equação y2 − 1 = 0 é
o par de retas paralelas y = 1 e y = −1. O gráﬁco da equação x2 − y2 = 0 é o par de retas
concorrentes y = x e y = −x. O gráﬁco da equação x2 + y2 = 0 consiste apenas no ponto
(0, 0). E o gráﬁco da equação x2 + y2 + 1 = 0 é o conjunto vazio.

Observe que nenhuma das equações em 9-4 contém simultaneamente termo em
x2 e termo em x, nem há alguma que contenha simultaneamente termo em y2 e termo em
y. A presença de qualquer um desses pares em uma equação de segundo grau indica que o
gráﬁco é uma seção cônica que sofreu translação a partir de sua posição canônica. Se não
há presente um termo em xy, então se podem remover tais pares de termos da equação a
ﬁm de se identiﬁcar seu gráﬁco - através de um processo de completamento de quadrados
e de translação de coordenadas.

Exemplo 9.3 Para se identiﬁcar o gráﬁco da equação de segundo grau

3x2 − 2y2 − 18x + 8y + 13 = 0,

reunimos os termos em x e os termos em y e completamos o quadrado em cada variável:

(3x2 − 18x) − (2y2 − 8y) + 13 = 0

55

3(x2 − 6x) − 2(y2 − 4y) = −13
3(x2 − 6x + 9) − 2(y2 − 4y + 4) = −13 + 27 − 8

3(x − 3)2 − 2(y − 2)2 = 6.

Agora, fazemos a substituição

x(cid:48) = x − 3,

y(cid:48) = y − 2,

que corresponde à escolha de um novo sistema de coordenadas transladado x(cid:48)y(cid:48) cuja
origem é o antigo ponto x = 3, y = 2. O resultado é a equação

ou seja,

3(x(cid:48))2 − 2(y(cid:48))2 = 6,

(x(cid:48))2
2

−

(y(cid:48))2
3

= 1.

Essa última equação tem a forma da primeira equação em 9-4, com a =
3.
Portanto, o gráﬁco da equação de segundo grau em questão é uma hipérbole transladada e
está em posição canônica no sistema x(cid:48)y(cid:48), conforme mostra a ﬁgura 9.2. Observamos que

2 e b =

√

√

Figura 9.2: Hipérbole Transladada.

os eixos x(cid:48) e y(cid:48) são variedades aﬁns, cujos subespaços vetoriais associados são os eixos x
e y, e que a matriz A do operador auto-adjunto associado à forma quadrática é

(cid:32)

A =

(cid:33)

3
0
0 −2

e seus autovalores são λ1 = a = 3 e λ2 = c = −2.

Consideremos agora a presença de um termo xy em uma equação de segundo

grau, que é chamada de termo de produto cruzado das variáveis.

56

A forma quadrática q : R2 → R2 deﬁnida como na Equação 9-2 por q(v) =
(cid:104)T (v), v(cid:105) (nesse caso temos v = (x, y) ∈ R2), só contém o termo de produto cruzado caso
b (cid:54)= 0, nessas condições, pelo Teorema 8.5, temos que existem {u1, u2} ⊂ R2, uma base
ortonormal de autovetores de T correspondente aos autovalores λ1, λ2. Nessas condições
a matriz A do operador T na nova base {u1, u2} é

(cid:32)

A =

(cid:33)

0
λ1
0 λ2

e portanto

q(v(cid:48)) = (cid:104)T (v(cid:48)), v(cid:48)(cid:105) = λ1(x(cid:48))2 + λ2(y(cid:48))2,

onde (x(cid:48), y(cid:48)) são as coordenadas do vetor v ∈ R2 de acordo com a nova base {u1, u2}.

Reordenando os autovalores λ1 e λ2 ou simplesmente trocando o sinal ou de
u1 ou de u2 caso seja mais conveniente, vide ﬁgura 9.3, podemos supor que u2 é
uma rotação 90◦ no sentido horário do vetor u1, onde u1 = (cos θ, sen θ) e portanto
u2 = (−sen θ, cos θ). Podemos então concluir que as coordenadas de v = (x, y) na base
canônica de R2, se relaciona com coordenadas de v(cid:48) = (x(cid:48), y(cid:48)) na base ortonormal {u1, u2}
de R2 de acordo com a seguinte equação:

Figura 9.3: Base ortonormal a partir de uma rotação anti-horária

de um ângulo θ da base canônica.

xe1 + ye2 = x(cid:48)u1 + y(cid:48)u2.

Aplicando o produto interno em ambos os lados pelo vetor e1 e em seguida pela vetor e2
obtemos

(cid:40)

x = x(cid:48)(cid:104)u1, e1(cid:105) + y(cid:48)(cid:104)u2, e1(cid:105)
y = x(cid:48)(cid:104)u1, e2(cid:105) + y(cid:48)(cid:104)u2, e2(cid:105)

(cid:40)

⇐⇒

x = x(cid:48) cos θ − y(cid:48)sen θ
y = x(cid:48)sen θ + y(cid:48) cos θ

57

(cid:32)

x
y

(cid:32)

onde B =

(cid:33)

(cid:32)

=

cos θ −sen θ
cos θ
sen θ

(cid:33) (cid:32)

(cid:33)

x(cid:48)
y(cid:48)

ou

(cid:32)

x
y

(cid:33)

(cid:32)

= B

(cid:33)

x(cid:48)
y(cid:48)

(cid:33)

cos θ −sen θ
cos θ
sen θ

é a matriz de passagem das coordenadas (x(cid:48), y(cid:48)) para as

coordenadas (x, y), para uma maior explanação sobre mudança de base vide [1], [3] e [8].
Deﬁnamos então um operador linear R : R2 → R2 tal que R(x(cid:48), y(cid:48)) = (x(cid:48) cos θ −
y(cid:48)sen θ, x(cid:48)sen θ + y(cid:48) cos θ) = (x, y) que tem B como sua matriz associada. R é uma rotação,
como já vimos no Capítulo 5, entretanto nesse caso, R retrata uma rotação dos eixos
coordenados deixando o ponto em questão ﬁxado.

Consideremos agora o caso de uma equação completa de segundo grau como

ax2 + 2bxy + cy2 + dx + ey + f = 0.

Considerando a base ortonormal {u1, u2} de R2 deﬁnida acima, podemos reescrever a
equação acima em função dos operadores lineares T e R do seguinte modo

(cid:104)T (R(x(cid:48), y(cid:48))), R((x(cid:48), y(cid:48)))(cid:105) + (cid:104)(d, e), R(x(cid:48), y(cid:48))(cid:105) + f = 0,

usando a adjunta R∗ de R obtemos

(cid:104)R∗(T (R(x(cid:48), y(cid:48)))), (x(cid:48), y(cid:48))(cid:105) + (cid:104)R∗(d, e), (x(cid:48), y(cid:48))(cid:105) + f = 0,

que por sua vez é igual a

λ1(x(cid:48))2 + λ2(y(cid:48))2 + d(cid:48)x(cid:48) + e(cid:48)y(cid:48) + f = 0,

onde d(cid:48) = (cid:104)(d, e), u1(cid:105) e e(cid:48) = (cid:104)(d, e), u2(cid:105).

Observe que o termo constante na equação não se altera por essa mudança de
coordenadas, ao passo que, os termos x(cid:48) e y(cid:48) podem ser eliminados por uma translação
como foi feito no Exemplo 9.3. Temos também que a matriz C associada à transformação
R∗ ◦ T ◦ R é o resultado da multiplicação das matrizes

(cid:32)

cos θ
sen θ
−sen θ cos θ

(cid:33) (cid:32)

a b
b c

(cid:33) (cid:32)

cos θ −sen θ
senθ
cos θ

(cid:33)

,

uma vez que as colunas de B são autovetores de A, obtemos então

(cid:32)

sen θ
cos θ
−sen θ cos θ

(cid:33) (cid:32)

λ1 cos θ −λ2sen θ
λ2 cos θ
λ1sen θ

(cid:33)

(cid:32)

=

(cid:33)

.

0
λ1
0 λ2

Por deﬁnição de determinante, a equação acima nos mostra que det A = detC = λ1 · λ2,
uma vez que a matriz de R∗ é a inversa da matriz de R. Com efeito

58

detC = det(BtAB) = det Bt det A det B.

Como Bt = B−1, então

detC = det(BtAB) = det(B−1AB) = det B−1 det A det B

= (det B)−1 det A det B

= det A.

Portanto a cônica deﬁnida em R2 pela equação

ax2 + 2bxy + cy2 + dx + ey + f = 0

pode, numa nova base ortonormal de R2, ser representada pela equação

λ1(x(cid:48))2 + λ2(y(cid:48))2 + d(cid:48)x(cid:48) + e(cid:48)y(cid:48) + f = 0,

onde d(cid:48) = (cid:104)(d, e), u1(cid:105) e e(cid:48) = (cid:104)(d, e), u2(cid:105), com u1, u2 autovetores associados a λ1, λ2

autovalores da matriz A =

(cid:32)

(cid:33)
.

a b
b c

Segundo os sinais desses autovalores, as seguintes possibilidades podem ocorrer:

1◦) Se λ1λ2 > 0 esta equação representa uma elipse, ou suas degenerações (um ponto

ou o vazio).

2◦) Se λ1λ2 < 0 esta equação representa uma hipérbole ou sua degeneração (par de

retas concorrentes).

3◦) Se λ1λ2 = 0 esta equação representa uma parábola ou suas degenerações (par de

retas paralelas, um reta ou o vazio).

Como o determinante de A é igual ao produto de seus autovalores λ1 · λ2, então
o sinal de λ1 · λ2 é o mesmo que −(b2 − ac). Podemos assim, reescrever as possibilidades
em função −(b2 − ac);

1◦) uma elipse ou suas degenerações, se b2 − ac < 0,
2◦) uma parábola ou suas degenerações, se b2 − ac = 0,
3◦) uma hipérbole ou suas degenerações, se b2 − ac > 0.

Para a análise mais detalhada das possibilidades de gráﬁcos segundo os autovalores e os
determinantes vide [1], [2], [5], [7] e [8].

59

Exemplo 9.4 Considere a seguinte equação do segundo grau

16x2 + 24xy + 9y2 + 15x − 20y = 0.

(9-5)

Temos que a matriz A, associada ao operador linear T da forma quadrática, da equação
9-5 é

A =

e o polinômio característico de T é

(cid:32)

(cid:33)

16 12
9
12

p(λ) = λ2 − 25λ

cujas raízes, e portanto os autovalores de T , são λ1 = 25 e λ2 = 0. Os autovetores
correspondentes a λ1 = 25 são as soluções não triviais do sistema

(cid:32)

−9
12
12 −16

(cid:33) (cid:32)

(cid:33)

(cid:32)

=

(cid:33)

.

0
0

x
y

Portanto, segue-se que (4, 3) é um autovetor associado a λ1 = 25, de modo que v1 =
1
5(4, 3) é um outro vetor unitário associado a λ1. Sendo v1 um dos vetores que formam a
nova base ortonormal B de R2, então podemos determinar o outro vetor v2 por uma mera
rotação de 90◦ do vetor v1, assim obtemos v2 = 1
4 (−3, 4) associado a λ2, dessa maneira a
equação 9-5 reescreve-se em termos das nova base B = { 1

5(−3, 4)} como

5 (4, 3), 1

25(x(cid:48))2 − 25(y(cid:48)) = 0;

ou seja

y(cid:48) = (x(cid:48))2.

Haja vista que, d(cid:48) = (cid:104)(15, −20), 1
5 (−3, 4)(cid:105) = −25. Dessa
forma, vemos que o gráﬁco da equação 9-5 é uma parábola que sofreu rotação e é exibida
pela ﬁgura a seguir. Os eixos x(cid:48) e y(cid:48) são obtidos a partir dos eixos x e y por uma rotação
anti-horária de um ângulo

5(4, 3)(cid:105) = 0 e e(cid:48) = (cid:104)(15, −20), 1

θ = arctan

(cid:19)

(cid:18)yv1
xv1

= arctan

(cid:19)

(cid:18)3
4

≈ 36, 87◦.

Observe que mesmo analisando as possibilidades de seu gráﬁco utilizando os coeﬁcientes
não é suﬁciente para determinar seu gráﬁco, uma vez que b2 − ac = 144 − 144 = 0 nos diz
que pode ser tanto uma parábola quanto suas degenerações, entretanto, uma vez feito o
cálculo, a informação da característica serve para uma rápida conﬁrmação dos resultados.

Exemplo 9.5 Com relação ao gráﬁco da seguinte equação do segundo grau

34x2 − 24xy + 41y2 − 40x − 30y − 25 = 0.

(9-6)

60

Figura 9.4: Parábola após rotação.

Temos que a matriz A, associada ao operador linear T da forma quadrática, da equação
9-6 é

A =

e o polinômio característico de T é

(cid:32)

(cid:33)

34 −12
−12
41

p(λ) = λ2 − 75λ + 1250

cujas raízes, e portanto os autovalores de T , são λ1 = 25 e λ2 = 50.

Os autovetores correspondentes a λ1 = 25 são as soluções não triviais do sistema

(cid:32)

9 −12
16

−12

(cid:33) (cid:32)

(cid:33)

(cid:32)

=

(cid:33)

.

0
0

x
y

Portanto, segue-se que (4, 3) é um autovetor associado a λ1 = 25, de modo que v1 =
1
5(4, 3) é um outro vetor unitário associado a λ1. Sendo v1 um dos vetores que formam a
nova base ortonormal B de R2, então podemos determinar o outro vetor v2 por uma mera
rotação de 90◦ do vetor v1, assim obtemos v2 = 1
4 (−3, 4) associado a λ2, dessa maneira a
equação 9-6 reescreve-se em termos da nova base B = { 1

5 (−3, 4)} como

5 (4, 3), 1

25(x(cid:48))2 + 50(y(cid:48))2 − 50x(cid:48) − 25 = 0;

ou seja

y(cid:48) = (x(cid:48))2

Haja vista que, d(cid:48) = (cid:104)(−40, −30), 1
5(−3, 4)(cid:105) = 0. Para
identiﬁcar o gráﬁco dessa equação transformada, completamos o quadrado como feito no
Exemplo 9.3:

5(4, 3)(cid:105) = −50 e e(cid:48) = (cid:104)(−40, −30), 1

25[(x(cid:48))2 − 2x(cid:48)] + 50(y(cid:48))2 − 25 = 0
25[(x(cid:48))2 − 2x(cid:48) + 1] + 50(y(cid:48))2 − 25 + 25 = 0

25(x(cid:48) − 1)2 + 50(y(cid:48))2 = 50

61

(x(cid:48) − 1)2
2

+

(y(cid:48))2
1

= 1.

Dessa forma, vemos que o gráﬁco da equação 9-6 é uma elipse que sofreu rotação seguida
de uma translação por um vetor v = (1, 0)B . Observe que as coordenadas de v estão na base
B, então teríamos a princípio que determinar v na base canônica e para isso deveríamos
usar a matriz de passagem das coordenadas (x(cid:48), y(cid:48)) para as coordenadas (x, y), entretanto
basta observar que v retrata simplesmente o vetor v1 da base B, então a translação se dá
de fato pelo vetor v = v1 = 1
5 (4, 3) e é exibida pela ﬁgura a seguir 9.5. Os eixos x(cid:48) e y(cid:48) são
obtidos a partir dos eixos x e y por uma rotação anti-horária de um ângulo

θ = arctan

(cid:19)

(cid:18)yv1
xv1

= arctan

(cid:19)

(cid:18)3
4

≈ 36, 87◦.

A característica b2 − ac = 144 − 1394 = −1250 < 0 serve para conﬁrmar nossas expec-

Figura 9.5: Elipse após rotação seguida de uma translação.

tativa de que era uma elipse.

Conclusão

CAPÍTULO 10

Nos Capítulos anteriores desenvolvemos conceitos fundamentais, nem todos
básicos, sobre Álgebra Linear, cujo o foco foi os operadores lineares de R2. Através de
vários exemplos, exploramos um vasto conjunto de transformações geométricas no plano.
Desta maneira, explicitamos, a nível de ensino médio, uma outra abordagem das matrizes
sem ﬁcar restrito à resolução de sistemas lineares.

Através do Teorema Espectral para operadores auto-adjuntos do espaços de
dimensão 2, este trabalho apresenta, nos Capítulos 8 e 9, uma justiﬁcativa algébrica para
os cálculos que aparecem no estudo das cônicas. Poder-se-ia dizer que tal justiﬁcativa não
cabe à nível de ensino médio, uma vez que, no espaço R2 a noção geométrica intuitiva dos
alunos é suﬁciente para que aceitem os processo aritmético que aparecem nos estudos das
cônicas. Entretanto, a compreensão do Teorema Espectral, caso particular apresentado
nesse trabalho, pelos professores, que ensinam as cônicas, se justiﬁca, uma vez que a
Matemática é uma ciência dedutiva e, não, descritiva.

Referências Bibliográﬁcas

[1] BOLDRINI, J. L.; COSTA, S. I. R.; RIBEIRO, V. L. F. F.; WETZLER, H. G. Álgebra

Linear. Harbra, São Paulo, 1980.

[2] HEFEZ, A.; DE SOUZA FERNANDEZ, C. Introducao a Algebra Linear. SBM, Rio de

Janeiro, 2012.

[3] HEGENBERG, L. Matrizes, Vetores e Geometria Analítica. Almeida Neves, Rio de

Janeiro, 1971.

[4] LIMA, E. L. Álgebra Linear. Impa, Rio de Janeiro, 2008.

[5] LIPSCHUTZ, S. Álgebra Linear. MacGraw-Hill, Rio de Janeiro, 1980.

[6] MORGADO, A. C.; JÚDICE, E. D.; WAGNER, E.; LIMA, E. L.; DE CARVALHO, J. B. P.;

CARNEIRO, J. P. Q.; GOMES, M. L. M.; CARVALHO, P. C. P. Exame de textos:

Análise de livros de Matemática para o ensino médio. SBM, Rio de Janeiro, 2001.

[7] NACHBIN, L. Introducão a Álgebra. MacGraw-Hill, Rio de Janeiro, 1971.

[8] PENNEY, D. E.; C.H. EDWARDS, J.

Introducão à Álgebra Linear. LTC-Livros

T’ecnicos e Cient’iﬁcos Editora S.A, Rio de Janeiro, 1998.

