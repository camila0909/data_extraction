Diagonalização de matrizes e suas aplicações

Carla Andrucioli Carnesecca

Dissertação de Mestrado do Programa de Mestrado Proﬁssional em
Matemática em Rede Nacional (PROFMAT)

UNIVERSIDADE DE SÃO PAULOInstituto de Ciências Matemáticas e de ComputaçãoSERVIÇO DE PÓS-GRADUAÇÃO DO ICMC-USP

Data de Depósito:

Assinatura: ______________________

Carla Andrucioli Carnesecca

Diagonalização de matrizes e suas aplicações

Dissertação apresentada ao Instituto de Ciências
Matemáticas e de Computação – ICMC-USP,
como parte dos requisitos para obtenção do título
de Mestre em Ciências – Mestrado Proﬁssional em
Matemática em Rede Nacional.
EXEMPLAR DE
DEFESA

Área de Concentração: Mestrado Proﬁssional em
Matemática em Rede Nacional

Orientadora: Profa. Dra. Katia Andreia Gonçalves
de Azevedo

USP – São Carlos
Junho de 2019

Ficha catalográfica elaborada pela Biblioteca Prof. Achille Bassi e Seção Técnica de Informática, ICMC/USP, com os dados inseridos pelo(a) autor(a)                                       Bibliotecários responsáveis pela estrutura de catalogação da publicação de acordo com a AACR2:                                        Gláucia Maria Saia Cristianini - CRB - 8/4938                                        Juliana de Souza Moraes - CRB - 8/6176A289dAndrucioli Carnesecca, Carla   Diagonalização de matrizes e suas aplicações /Carla Andrucioli Carnesecca; orientadora KátiaAndreia Gonçalves de Azevedo. -- São Carlos, 2019.   113 p.   Dissertação (Mestrado - Programa de Pós-Graduaçãoem Mestrado Profissional em Matemática em RedeNacional) -- Instituto de Ciências Matemáticas e deComputação, Universidade de São Paulo, 2019.   1. Diagonalização de matriz. 2. Reconhecimento decônicas. 3. Resolução de sistemas de equaçõesdiferenciais ordinárias . 4. Mudança de base. 5.Autovalores e autovetores. I. Andreia Gonçalves deAzevedo, Kátia, orient. II. Título. Carla Andrucioli Carnesecca

Diagonalization of matrices and applications

Master dissertation submitted to the Institute of
Mathematics and Computer Sciences – ICMC-USP,
in partial
the
degree of Mathematics Professional Master’s Program.
EXAMINATION BOARD PRESENTATION COPY

the requirements for

fulﬁllment of

Concentration Area: Professional Master Degree
Program in Mathematics in National Network

Advisor: Profa.
de Azevedo

Dra.

Katia Andreia Gonçalves

USP – São Carlos
June 2019

Dedico este trabalho ao meu ﬁlho Arthur Vicente,
luz onde é escuridão, alegria onde é tristeza,
força para a conclusão desse projeto.

AGRADECIMENTOS

A Deus por estar sempre presente em minha vida, iluminando meus caminhos tornando

tudo possível.

A minha mãe Sonia por se empenhar para proporcionar melhores oportunidades na minha

vida.

Ao meu irmão Flávio por sempre me apoiar e cobrar.

A todos meus colegas do curso, pela amizade e companherismo, em especial à Lívia,

Daniele, Paula e Rosa pelos momentos de estudos, descontrações e risadas.

Ao Diego, pelas risadas, choros, desabafos e cobranças, porque sem ele não teria

conseguido terminar.Obrigada!

Agradeço aos professores do PROFMAT, pelo conhecimento compartilhado.

A Profa Dra Kátia Andreia Gonçalves de Azevedo, pela dedicação, compreenção, paci-

ência e quanta paciência que conduziu a orientação deste trabalho. Muito, muito obrigada!

Agradeço à CAPES pelo apoio ﬁnanceiro.

“Eu tentei 99 vezes e falhei, mas na centésima
tentativa eu consegui, nunca desista de seus objetivos
mesmo que esses pareçam impossíveis, a próxima
tentativa pode ser a vitoriosa.”
(Albert Einstein)

RESUMO

CARNESECCA, C. L. Diagonalização de matrizes e suas aplicações. 2019. 113 p. Disser-
tação (Mestrado em Ciências – Mestrado Proﬁssional em Matemática em Rede Nacional) –
Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, São Carlos –
SP, 2019.

O principal objetivo desse trabalho é apresentar a teoria necessária para compreender o processo
de diagonalização de operadores lineares e, consequentemente, de matrizes, como uma técnica
para resolver sistemas de equações diferenciais ordinárias lineares homogêneos com coeﬁcientes
constantes e para reconhecer cônicas não degeneradas, as elipses, hipérboles e parábolas.

Palavras-chave: Diagonalização de Operadores Lineares, Cônicas, Sistemas Lineares de
Equações Diferenciais Ordinárias.

ABSTRACT

CARNESECCA, C. L. Diagonalization of matrices and applications. 2019. 113 p. Disser-
tação (Mestrado em Ciências – Mestrado Proﬁssional em Matemática em Rede Nacional) –
Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, São Carlos –
SP, 2019.

The main goal of this work is showing theories that are necessaries to understand the process
of diagonalization of linear operators and, consequently, matrices, as a technique for solving
homogeneous linear ordinary differential equations with constant coefﬁcients and recognize
non-degenerate cones, ellipses, hyperbolas, and parabolas.

Keywords: Diagonalization of Linear Operators, Conics, Linear Systems of Ordinary Differen-
tial Equations.

LISTA DE ILUSTRAÇÕES

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

Figura 1 – Reﬂexão em torno do eixo Ox.
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Figura 2 – Projeção .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Figura 3 – Rotação .
Figura 4 – Rotação .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Figura 5 – Dilatação e contração por α em R . . . . . . . . . . . . . . . . . . . . . .
Figura 6 – Projeção de v ao longo de w.
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
Figura 7 – Elipse .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
Figura 8 – Elipse .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Figura 9 – Hipérbole .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Figura 10 – Hipérbole .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
Figura 11 – Parábola .
Figura 12 – y2 = 4px .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
Figura 13 – x2 = −4px .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Figura 14 – Parabola P e círculo C . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.

(x − 1)2
9

(x − 2)2
4

= 1 . . . . . . . . . . . . . . . . . . . . . . .
Figura 15 – A elipse
Figura 16 – {(x, y) ∈ R2 : (y + 3)2 = 8(x + 1)} . . . . . . . . . . . . . . . . . . . . . .

+

(y′ − 1)2
3
√

Figura 17 – Elipse (x′ + 2)2 +
Figura 18 – Hiperbole - H = 3x2 − 4
Figura 19 – Par de retas: x2 − 6xy − 7y2 + 10x + 2y + 9 = 0.
Figura 20 – Parábola P : x2 + 2xy + y2 − 2x + 2y + 3 = 0.

= 1 . . . . . . . . . . . . . . . . . . . . . . . .
3xy − y2 + 20y = 25. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .

47
48
49
50
51
59
81
82
84
84
86
87
87
88

90

91

93

94
96
97

LISTA DE TABELAS

Tabela 1 – Reﬂexões mais comuns em R2
. . . . . . . . . . . . . . . . . . . . . . . .
Tabela 2 – Projeções mais comum em R2
. . . . . . . . . . . . . . . . . . . . . . . .
Tabela 3 – Rotações em R3 cujos eixos de rotação são os eixos coordenados . . . . . .

48
49
50

SUMÁRIO

1

INTRODUÇÃO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

2
2.1
2.1.1
2.1.2
2.1.3

3
3.1
3.2
3.2.1
3.2.2

4

5
5.1
5.2
5.3

6
6.1

7

8

PRÉ REQUISITOS . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
Espaços Vetoriais . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
Definição e Exemplos . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
Subespaços .
.
. . . . . . . . . . . . . . . . . . 32
Dependência e Independência Linear

.

.

TRANSFORMAÇÕES LINEARES E MATRIZES . . . . . . . . . . . 41
Transformações Lineares . . . . . . . . . . . . . . . . . . . . . . . . . . 41
. . . . . . . . . . . . . . . . . . 44
Matriz de uma Transformação Linear
Operadores Lineares em R2 e em R3 . . . . . . . . . . . . . . . . . . . 47
Mudança de Base e Matrizes semelhantes . . . . . . . . . . . . . . . 51

ESPAÇO COM PRODUTO INTERNO . . . . . . . . . . . . . . . . 55

DIAGONALIZAÇÃO DE OPERADORES . . . . . . . . . . . . . . . 67
Operadores Diagonalizáveis . . . . . . . . . . . . . . . . . . . . . . . . 67
Matrizes Diagonalizáveis . . . . . . . . . . . . . . . . . . . . . . . . . . 76
Teorema Espectral para operadores simétricos . . . . . . . . . . . . . 78

CÔNICAS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
Reconhecimento de Cônicas . . . . . . . . . . . . . . . . . . . . . . . . 89

SISTEMAS DE EQUAÇÕES DIFERENCIAIS ORDINÁRIAS . . . . 99

CONCLUSÃO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111

REFERÊNCIAS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113

21

CAPÍTULO

1

INTRODUÇÃO

A Matemática é uma ciência que sempre esteve presente em nosso cotidiano, dos pri-
mórdios históricos da humanidade até os dias atuais. E. mesmo sem notarmos, está presente
em várias áreas do conhecimento. A Álgebra Linear é um ramo da Matemática que tem muita
aplicabilidade em diversos campos de estudos como a criptograﬁa, programação linear, circuitos
elétricos, produção mecânica de peças, modelos econômicos lineares, entre outros. Por ser uma
disciplina versátil nas aplicações, a Álgebra Linear tem uma grande relevância, pois contribui
para os avanços tecnológicos e cientíﬁcos. O objetivo deste trabalho é entender conceitos básicos
e essenciais de Álgebra Linear como espaços vetoriais, autovalor e autovetor de transformações
lineares, tendo como enfoque principal o processo de diagonalização de matrizes e suas aplica-
ções, em particular a identiﬁcação de cônicas não degeneradas e resolução de sistemas lineares
de equações diferenciais ordinárias com coeﬁcientes constantes. A diagonalização de matrizes
não faz parte do currículo do Ensino Médio. No entanto, são vários os conteúdos do currículo
do Ensino Médio onde esse fundamento pode ser utilizado. Um deles está relacionado com a
identiﬁcação de cônicas não degeneradas: elipse, hipérbole e parábola, como citado.

Desta forma, consideramos ser importante para os professores de Matemática conhecer
técnicas para reconhecer uma cônica não degenerada ou resolver um sistema de equações
diferenciais, ampliando sua visão sobre as possíveis aplicações desta teoria. Há vários livros
que abordam estes problemas, mas compilamos os resultados essenciais para a compreensão da
teoria.

Assim surgiu a ideia deste trabalho, que consiste em auxiliar professores da educação

básica, em sua prática, no conteúdo de Álgebra Linear.

O trabalho será apresentado da seguinte maneira.

No capítulo 2 abordamos os pré-requisitos necessários para entender onde as trans-
formações lineares estão deﬁnidas, conceitos como: espaços vetoriais, subespaços vetoriais,
dependência e independência linear, base e dimensão. O capítulo 3 trata das transformações line-

22

Capítulo 1. Introdução

ares, os operadores lineares em R2 e em R3, mudança de base e matrizes semelhantes, obtendo
os resultados necessários para o processo de diagonalização. O capítulo 4 aborda os conceitos
de espaço vetorial com produto interno e no capítulo 5, tratamos essencialmente do processo
de diagonalização de operadores lineares e, consequentemente, diagonalização de matrizes. No
capítulo 6, descrevemos as cônicas e aplicamos a teoria diagonalização de matrizes, para o
processo de identiﬁcação de cônicas não degeneradas. Por ﬁm, no capítulo 7, uma aplicação
desta teoria sobre diagonaliização é feita para resolvermos sistemas lineares homogêneos de
equações diferenciais ordinárias com coeﬁcientes constantes.

23

CAPÍTULO

2

PRÉ REQUISITOS

Neste capítulo faremos um estudo sobre os pré-requisitos necessários para o entendimento
do processo de diagonalização de operadores lineares. Os tópicos aqui estudados podem ser
encontrados em (ZANI, ), (HEFEZ A.; SOUZA FERNANDES, 2012), (LIPSCHUTZ, 1994) e
(STEINBRUCH, 1987).

2.1 Espaços Vetoriais

2.1.1 Definição e Exemplos

Nesta seção serão apresentados a deﬁnição de espaço vetorial e alguns exemplos.

Deﬁnição 1. Um espaço vetorial V é um conjunto onde estão deﬁnidas duas operações, uma
chamada adição e outra chamada multiplicação por um escalar. Os elementos do conjunto V
são chamados de vetores. A adição faz corresponder a cada par de vetores u, v ∈ V , um novo
vetor u + v ∈ V , chamado a soma de u e v. A multiplicação por escalar, a cada número α ∈ R e
a cada vetor v ∈ V , faz corresponder um vetor α · v ou αv, chamado produto de α por v. Essas
operações devem satisfazer, para quaisquer α, β ∈ R e u, v, w ∈ V , as seguintes condições :

1. u + v = v + u;

2. (u + v) + w = u + (v + w);

3. Existe um vetor 0 ∈ V , chamado vetor nulo, denotado também por(cid:126)0, tal que v + 0 = 0 + v

para todo v ∈ V ;

4. Para cada vetor v ∈ V existe um vetor −v ∈ V , chamado inverso aditivo ou simétrico de v,

tal que: −v + v = v + (−v) = 0;

5. (α + β )v = αv + β v;

24

Capítulo 2. Pré requisitos

6. (αβ )v = α(β v);

7. α(u + v) = αu + αv;

8. 1 · v = v.

Exemplo 1. V = R2 = {(x, y)/x, y ∈ R} é um espaço vetorial com as operações de adição e
multiplicação por um número real assim deﬁnidas.

(x1, y1) + (x2, y2) = (x1 + x2, y1 + y2)

α(x1, y1) = (αx1, αx2)

Vamos veriﬁcar as oito propriedades que um espaço vetorial deve satisfazer. Para isso

considere u = (x1, y1), v = (x2, y2) e w = (x3, y3) elementos de R2. Assim,

1. u + v = (x1, y1) + (x2, y2) = (x1 + x2, y1 + y2) *= (x2 + x1, y2 + y1) = (x2, y2) + (x1, y1) =

v + u

2. u + (v + w) = (x1, y1) + ((x2, y2) + (x3, y3)) = (x1, y1) + (x2 + x3, y2 + y3) =
= (x1 + (x2 + x3), y1 + (y2 + y3)) *= ((x1 + x2) + x3, (y1 + y2) + y3) =

= (x1 + x2, y1 + y2) + (x3, y3) = (u + v) + w

3. Existe (cid:126)0 = (0, 0) ∈ R2 onde 0 é o elemento neutro da adição em R tal que para todo

u = (x1, y1) ∈ R2 temos:
u +(cid:126)0 = (x1, y1) + (0, 0) = (x1 + 0, y1 + 0) *= (x1, y1) = u

4. Para todo u = (x1, y1) ∈ R2 existe −u = (−x1, −y1) ∈ R2 tal que:

u + (−u) = (x1, y1) + (−x1, −y1) = (x1 − x1, y1 − y1) *= (0, 0), onde −x1 e −y1 serão os
opostos de x1 e y1 respectivamente em R

5. α(β u) = α(β (x1, y1)) = α(β x1, β y1) = (α(β x1), α(β y1)) *= ((αβ )x1), (αβ )y1)) = (αβ )(x1, y1) =

(αβ )u.

6. (α + β )u = (α + β )(x1, y1) = ((α + β )x1, (α + β )y1)) *= (αx1 + β x1, αy1 + β y1) =

= (αx1, αy1) + (β x1, β y1) = αu + β u.

7. α(u+v) = α((x1, y1)+(x2, y2)) = α(x1 +x2, y1 +y2) = (α(x1 +x2), α(y1 +y2)) *= (αx1 +

αx2, αy1 + αy2) = = (αx1, αy1) + (αx2, αy2) = α(x1, y1) + α(x2, y2) = αu + αv.

8. 1u = 1(x1, y1) = (1x1, 1y1) *= (x1, y1) = u.

2.1. Espaços Vetoriais

25

Quando usamos o sinal

*= signiﬁca que nesta passagem estamosassumindo o fato de que
essas propriedades são válidas para os números reais. Logo, V = R2, com as operações de adição
e multiplicação deﬁnidas acima, é um espaço vetorial.

Observe que se considerarmos Rn = {(x1, · · · , xn)/xi ∈ R, i = 1, · · · , n} e as operações de
adição e multiplicação por escalar deﬁnidas por (x1, x2, · · · , xn) + (y1, y2, · · · , yn) = (x1 + y1, x2 +
y2, · · · , xn + yn) e α(x1, · · · , xn) = (αx1, · · · , αxn), com α ∈ R, usando o mesmo raciocínio,
provamos que (Rn, +, ·) é um espaço vetorial.

Exemplo 2. V = R*
multiplicação por um número real assim deﬁnidas:

+ = {x ∈ R/x > 0} é um espaço vetorial com as operações de adição e

x ⊕ y = x · y, x, y ∈ R*
+,

λ ⊙ x = xλ , x ∈ R*

+, λ ∈ R.

Vamos veriﬁcar as oito propriedades e provar que (R*

+, ⊕, ⊙) é um espaço vetorial.

1. x ⊕ y = x · y = y · x = y ⊕ x

2. (x ⊕ y) ⊕ z = (x · y) ⊕ z = ((x · y) · z) = x · (y · z) = x ⊕ (y · z) = x ⊕ (y ⊕ z)

3. Observe que x ⊕ 1 = x · 1 = x, ∀x ∈ R*

+. Logo, 1 é o elemento neutro da adição, ou seja,(cid:126)0 = 1

4. Dado x ∈ R*

+, considere o inverso aditivo −x como sendo o número

x · (

1
x

) = 1 =(cid:126)0.

1
x

. Assim, x ⊕ (−x) =

5. α ⊙ (β ⊙ x) = α(xβ ) = (xβ )α = xαβ = (αβ ) ⊙ x

6. (α + β ) ⊙ x = xα+β = xα xβ = xα ⊕ xβ = (α ⊙ x) ⊕ (β ⊙ y)

7. α ⊙ (x ⊕ y) = α ⊙ (x · y) = (x · y)α = xα · yα = xα ⊕ yα = (α ⊙ x) ⊕ (α ⊙ y)

8. 1 ⊙ x = x1 = x

Exemplo 3. Seja Mm×n(R) o conjunto das matrizes reais de ordem m × n.Se A ∈ Mm×n(R) então
A é representada por:

A =









a11
a12
a21
a22
...
...
am1 am2

· · · a1n
· · · a2n
...
. . .
· · · amn









m×n

Dadas duas matrizes A e B ∈ Mm×n(R) então a adição e a multiplicação por um escalar são

26

Capítulo 2. Pré requisitos



A+B =

deﬁnidas por:
a12
a11
a22
a21
...
...
am1 am2
















· · · a1n
· · · a2n
...
. . .
· · · amn

+









b12
b11
b22
b21
...
...
bm1 bm2


e αA :=







...

αa11 αa12
αa21 αa22

...

αam1 αam2

· · · αa1n
· · · αa2n
. . .
· · · αamn

...









· · · b1n
· · · b2n
...
. . .
· · · bmn









a11 + b11
a21 + b21
...

:=

a12 + b12
a22 + b22
...

am1 + bm1 am2 + bm2

a1n + b1n
a2n + b2n
...

· · ·
· · ·
. . .
· · · amn + bmn









, α ∈ R, onde A + B e αA ∈ Mm×n(R).







O conjunto das matrizes com a adição e multiplicação por escalar deﬁnidas acima é um

espaço vetorial. De fato,

1. A + B =









a12
a11
a22
a21
...
...
am1 am2









=

a11 + b11
a21 + b21
...

a12 + b12
a22 + b22
...

am1 + bm1 am2 + bm2

b12
b11
b22
b21
...
...
bm1 bm2







+













· · · a1n
· · · a2n
...
. . .
· · · amn
· · ·
· · ·
. . .
· · · amn + bmn

a1n + b1n
a2n + b2n
...







=















· · · b1n
· · · b2n
...
. . .
· · · bmn
b11 + a11
b21 + a21
...

=

b12 + a12
b22 + a22
...

bm1 + am1 bm2 + am2

b1n + a1n
b2n + a2n
...

· · ·
· · ·
. . .
· · · bmn + amn









= B + A.

2. (A+B)+C =

















a12
a11
a22
a21
...
...
am1 am2









· · · a1n
· · · a2n
...
. . .
· · · amn

+









b12
b11
b22
b21
...
...
bm1 bm2


















· · · b1n
· · · b2n
...
. . .
· · · bmn

+









c12
c11
c22
c21
...
...
cm1 cm2









c1n
· · ·
c2n
· · ·
...
. . .
· · · cmn

=









a11 + b11
a21 + b21
...

=

a12 + b12
a22 + b22
...

am1 + bm1 am2 + bm2

a1n + b1n
a2n + b2n
...

· · ·
· · ·
. . .
· · · amn + bmn









+







c12
c11
c22
c21
...
...
cm1 cm2









c1n
· · ·
c2n
· · ·
...
. . .
· · · cmn


=

=









(a11 + b11) + c11
(a21 + b21) + c21
...
(am1 + bm1) + cm1

(a12 + b12) + c12
(a22 + b22) + c22
...
(am2 + bm2) + cm2

· · ·
· · ·
. . .
· · ·

(a1n + b1n) + c1n
(a2n + b2n) + c2n
...
(amn + bmn) + cmn

=







2.1. Espaços Vetoriais

=









a11
a12
a21
a22
...
...
am1 am2










· · · a1n
· · · a2n
...
. . .
· · · amn









+

b11 + c11
b21 + c21
...

b12 + c12
b22 + c22
...

bm1 + cm1 bm2 + cm2




b1n + c1n
b2n + c2n
...

· · ·
· · ·
. . .
· · · bmn + cmn










+







b12
b11
b22
b21
...
...
bm1 bm2









· · · b1n
· · · b2n
...
. . .
· · · bmn

+







c12
c11
c22
c21
...
...
cm1 cm2

c1n
· · ·
c2n
· · ·
...
. . .
· · · cmn













= A + (B +C).

27









· · · a1n
· · · a2n
...
. . .
· · · amn

+









=









a11
a12
a21
a22
...
...
am1 am2

3. Existe(cid:126)0 ∈ Mm×n dada por(cid:126)0 =









0 0 · · · 0
0 0 · · · 0
...
...
. . .
0 0 · · · 0

...









, pois

A +(cid:126)0 =









a12
a11
a22
a21
...
...
am1 am2








a11
a12
a21
a22
...
...
am1 am2









· · · a1n
· · · a2n
...
. . .
· · · amn

+









0 0 · · · 0
0 0 · · · 0
...
...
. . .
0 0 · · · 0

...









· · · a1n
· · · a2n
...
. . .
· · · amn

= A.

=

m×n


=















a11 + 0 a12 + 0 · · · a1n + 0
a21 + 0 a22 + 0 · · · a2n + 0

...

...

. . .

...









=

am1 + 0 am2 + 0 · · · amn + 0

4. Dado A ∈ Mm×n(R), existe −A ∈ Mm×n(R) de tal forma que

A + (−A) =









a12
a11
a22
a21
...
...
am1 am2









· · · a1n
· · · a2n
...
. . .
· · · amn









+

−a11 −a12
−a21 −a22

...

...

−am1 −am2









· · · −a1n
· · · −a2n
. . .
· · · −amn

...

=









a11 − a11
a21 − a21
...

=

a12 − a12
a22 − a22
...

am1 − am1 am2 − am2

a1n − a1n
a2n − a2n
...

· · ·
· · ·
. . .
· · · amn − amn









=









0 0 · · · 0
0 0 · · · 0
...
...
. . .
0 0 · · · 0

...









=(cid:126)0

5. (α + β ) · A = (α + β )







a11
...
am1







=

· · · a1n

· · · amn







(α + β )a11
...
(α + β )am1

· · ·

(α + β )a1n

· · ·

(α + β )amn







28

Capítulo 2. Pré requisitos







=

αa11 + β a11
...
αam1 + β am1

· · · αa1n + β a1n

· · · αamn + β amn







= α







a11
...
am1







+ β

· · · a1n

· · · amn







a11
...
am1







· · · a1n

· · · amn

= αA + β A

6. (αβ )A = (αβ )









a11
a21
...
am1









· · · a1n
· · · a2n

· · · amn

= α









β a11
β a21
...
β am1









· · · β a1n
· · · β a2n

· · · β amn

= α(β A)

7. α(A + B) = α

















a11
a12
a21
a22
...
...
am1 am2









· · · a1n
· · · a2n
...
. . .
· · · amn

+









b11
b12
b21
b22
...
...
bm1 bm2


















· · · b1n
· · · b2n
...
. . .
· · · bmn









a11 + b11
a21 + b21
...

a12 + b12
a22 + b22
...

am1 + bm1 am2 + bm2

a1n + b1n
a2n + b2n
...

· · ·
· · ·
. . .
· · · amn + bmn









=







α(a11 + b11) α(a12 + b12)
α(a21 + b21) α(a22 + b22)

...

α(am1 + bm1) α(am2 + bm2)

· · · α(a1n + b1n)
· · · α(a2n + b2n)
. . .
· · · α(amn + bmn)

...









=

= α









=

= α

...









=

αa11 + αb11 αa12 + αb12
αa21 + αb21 αa22 + αb22

...

...

αam1 + αbm1 αam2 + αbm2











a12
a11
a22
a21
...
...
am1 am2

· · · a1n
· · · a2n
...
. . .
· · · amn

+ α













...

· · · αa1n + αb1n
· · · αa2n + αb2n
. . .
· · · αamn + αbmn
b12
· · · b1n
b11
· · · b2n
b22
b21
...
...
...
. . .
· · · bmn
bm1 bm2









= αA + αB.

















1 · a11
1 · a21
...

=

1 · a12
1 · a22
...

1 · am1 1 · am2









· · · 1 · a1n
· · · 1 · a2n
. . .
· · · 1 · amn

...

=

8. (1 · A) = 1 ·









a12
a11
a22
a21
...
...
am1 am2

· · · a1n
· · · a2n
...
. . .
· · · amn


=









a11
a12
a21
a22
...
...
am1 am2

· · · a1n
· · · a2n
...
. . .
· · · amn

= A.







Exemplo 4. Seja Pn = {a0 + a1x + a2x2 + · · · + anxn ai ∈ R} o conjunto dos polinômios de grau
menor ou igual a n. Pn é um espaço vetorial em relação as operações usuais deﬁnidas da seguinte

2.1. Espaços Vetoriais

29

forma, se Pn(x) = a0 + a1x + a2x2 + · · · + anxn e Qn(x) = b0 + b1x + b2x2 + · · · + bnxn, então

Pn(x) + Qn(x) = (a0 + b0) + (a1 + b1)x + (a2 + b2)x2 + · · · + (an + bn)xn, e

αPn(x) = αa0 + αa1x + +αa2x2 + · · · + αanxn.

As oito propriedades são facilmente veriﬁcadas observando que o polinômio nulo 0 =
0 + 0α + · · · + 0α n é o elemento neutro da adição e dado pn(x) = a0 + a1x + a2x2 + · · · + anxn,
o oposto aditivo é dado por −pn(x) = −a0 − a1x − · · · − anxn.

2.1.2 Subespaços

Deﬁnição 2. Um subconjunto W , não vazio, de um espaço vetorial V é um subespaço vetorial
de V se estiverem satisfeitas as seguintes condições.

1. 0 ∈ W ;

2. se u, v ∈ W , então u + v ∈ W ;

3. se α ∈ R e u ∈ W , então αu ∈ W .

Todo espaço vetorial V admite pelo menos dois subespaços vetoriais: o subespaço nulo
{0} e o próprio espaço vetorial V . Estes subespaços são chamados subespaços triviais. Os demais
subespaços, se existirem, são chamados subespaços próprios.

Seja V um espaço vetorial e W um subconjunto não vazio de V . É facil ver que, W é um

subespaço vetorial de V , se u + αv ∈ W , para todo α ∈ R e para todo u, v ∈ W .

Exemplo 5. Sejam V = R2 e W = {(x, y) ∈ R2; y = 2x}. W é um subespaço vetorial de V .
De fato, se u ∈ W então u = (x1, 2x1) para algum x1 ∈ R e se v ∈ W então v = (x2, 2x2), para
algum x2 ∈ R. Logo u + v = (x1, 2x1) + (x2, 2x2) = (x1 + x2, 2x1 + 2x2) = (x1 + x2, 2(x1 + x2)) e
podemos ver que u + v ∈ W , pois a segunda componente é o dobro da primeira. Dado α ∈ R,
temos αu = α(x1, 2x1) = (αx1, 2(αx1)) que também pertence a W . Além disso, (cid:126)0 ∈ W pois
(0, 0) = (0, 2 · 0). Portanto W é um subespaço vetorial.

Exemplo 6. Seja V = M2×3(R) o espaço vetorial dado por:

e seja W ⊂ V , dada por:

V = {

(cid:35)

(cid:34)

a b c
f
d e

, a, b, c, d, e, f ∈ R}

W = {

(cid:35)

(cid:34)

a b 0
f
0 e

, a, b, e, f ∈ R}

30

Capítulo 2. Pré requisitos

W é um subespaço vetorial de V . De fato, W é não vazio, pois

Se as matrizes A, B ∈ W , então

A + B =

(cid:34)

a1 b1
e1
0

(cid:35)

0
f1

+

(cid:34)

a2 b2
e2
0

(cid:35)

0
f2

=

(cid:34)

a1 + a2 b1 + b2
e1 + e2

0

(cid:34)

(cid:35)

0 0 0
0 0 0

∈ W .

(cid:35)

0
f1 + f2

∈ W

e

αA = α

(cid:34)

a1 b1
e1
0

(cid:35)

0
f 1

=

(cid:34)

αa1 αb1

(cid:35)

0

0

αe1 α f 1

∈ W

Portanto W é subespaço vetorial do espaço V .

Exemplo 7. Considere o sistema linear homogêneo:


a11x + a12y + a13z = 0

a21x + a22y + a23z = 0

a31x + a32y + a33z = 0

.

Denotando por

A =






a11 a12 a13
a21 a22 a23
a31 a32 a33




 , X =




 , 0 =


x

y

z



0


 ,
0

0

o sistema linear homogêneo pode ser escrito como AX = 0.

Seja: W = {X =


x

y

z


 , x, y, z ∈ R/AX = 0} o conjunto de todas as soluções homogêneas


do sistema dado. Vamos veriﬁcar que W é um subespaço vetorial do R3. Podemos ver que W ̸= /0,



0
 ∈ W . Também se X1, X2 ∈ W então X1 + X2 ∈ W e ∀α ∈ R temos


0

0

−→ AX1 + AX2 = A(X1 + X2) = 0 e A(αX1) = αAX1 = α · 0 = 0.

Portanto W é subespaço vetorial de V .

Como os espaços vetoriais são conjuntos é natural perguntar se a união e a interseção de

conjuntos preservam a propriedade de espaço vetorial.

1 A interseção de dois subespaços de um espaço vetorial V é um subespaço de V .

Demonstração. Sejam U e V subespaços de V . Vamos veriﬁcar se U ∩W é também um
subespaço de V . U ∩ W é um subconjunto não vazio de V , pois 0 ∈ U e 0 ∈ W , já que

pois A0 = 0, e assim 0 =

αX1 ∈ W .De fato,



X1 ∈ W ⇒ AX1 = 0



X2 ∈ W ⇒ AX2 = 0

2.1. Espaços Vetoriais

31

ambos são subespaços de V . Agora, α ∈ R e u, v ∈ U ∩ W , como u, v ∈ U e u, v ∈ W ,
segue-se que u + αv ∈ U e u + αv ∈ W , ou seja u + αv ∈ U ∩ W . Assim U ∩ W é um
subespaço de V .

2 A união de dois subespaços de um espaço vetorial V não é necessariamente um subespaço
de V . Como exemplo, podemos considerar U = {(x, y) ∈ R2; x + y = 0} e W = {(x, y) ∈
R2; x − y = 0}, subespaços de R2, o conjunto U ∪ W não é subespaço de R2, pois u =
(1, 1) ∈ U ∪W e w = (1, −1) ∈ U ∪W , mas u + w = (2, 0) /∈ U ∪W .

Deﬁnição 3. Sejam S1 e S2 subespaços de V . Deﬁnimos S = S1 + S2 como sendo o
conjunto de todos os vetores u + v tal que u ∈ S1 e v ∈ S2.

3 A soma S de dois subespaços vetoriais S1 e S2 de V é um subespaço vetorial.

Demonstração. Sejam u e w ∈ S. Então existem w1, v1 ∈ S1 e u2, v2 ∈ S2 tais que u =
u1 + u2 e v = v1 + v2. Logo, u + αv = u1 + u2 + α(v1 + α2) = (u1 + αv1) + (u2 + αv2) e
como S1 e S2 são subespaços de V, u1 + αv1 ∈ S1 e u2 + αv2 ∈ S2. Assim, u + αv ∈ S.

Deﬁnição 4. Sejam S1 e S2 dois subespaços vetorial V . W é a soma direta de S1 e S2 e representa-
se por W = S1 ⊕ S2 se, V = S1 + S2 e S1 ∩ S2 = {0}.

Exemplo 8. Sejam S1 = {(a, 0, c, 0); a, c ∈ R} e S2 = {(0, b, 0, d); b, d ∈ R} subespaços vetoriais
de R4. Então:

S1 + S2 = {(a, b, c, d); a, b, c, d ∈ R} = R4 e

S1 ∩ S2 = {(0, 0, 0, 0)}
Logo, S1 ⊕ S2 = R4 e dizemos que R4 pode ser escrito como soma direta dos subespaços

S1 e S2.

Deﬁnição 5. Seja V um espaço vetorial e sejam v1, v2, · · · , vr vetores de V . Diremos que um
vetor v de V é uma combinação linear de v1, v2, · · · , vr se existirem números reais a1, a2, · · · , ar
tais que:

v = a1v1 + a2v2 + · · · + arvr.

(2.1)

Exemplo 9. Considere os seguintes vetores do R3, v1 = (1, 2, 1), v2 = (1, 0, 2) e v3 = (1, 1, 0).
Podemos escrever v = (1, 2, 4) como uma combinação linear de v1, v2 e v3. De fato,

(1, 2, 4) = a1(1, 2, 1) + a2(1, 0, 2) + a3(1, 1, 0),

que é equivalente ao sistema


a1 + a2 + a3 = 1

2a1 + a3 = 2

a1 + 2a2 = 4

Assim, v é combinação linear de v1, v2 e v3.

. Concluímos que a1 = 2, a2 = 1 e a3 = 2.

32

Capítulo 2. Pré requisitos

Sejam v1, v2, · · · , vr vetores de um espaço vetorial V . Denotaremos por G(v1, v2, · · · , vr),
o conjunto de todas as combinações lineares de v1, v2, · · · , vr em V .O seguinte resultado é de
facil veriﬁcação.

Proposição 1. Seja W = G(v1, v2, · · · , vr) onde v1, v2, · · · , vr são vetores de um espaço vetorial
V . Valem as seguintes aﬁrmações:

1. W é um subespaço de V .

2. W é o menor subespaço de V contendo v1, v2, · · · , vr ou seja qualquer subespaço de V que

contém v1, v2, · · · , vr também contém W .

Exemplo 10. Seja V = R3. Vamos determinar o subespaço gerado pelos vetores v1 = (1, 1, 1); v2 =
(1, 1, 0) e v3 = (1, 0, 0). Considere v = (x, y, z) e suponha que v = a1v1 + a2v2 + a3v3 para algum
a1, a2, a3 ∈ R. Assim, (x, y, z) = a1(1, 1, 1) + a2(1, 1, 0) + a3(1, 0, 0) que equivale ao sistema


a1 + a2 + a3 = x

a1 + a2 = y

a1 = z

onde concluímos que a1 = z, a2 = y − z e a3 = x − y. Logo, (x, y, z) = z(1, 1, 1) + (y − z)(1, 1, 0) +
(x − y)(1, 0, 0) e concluímos que R3 ⊂ G(v1, v2, v3). Como, G(v1, v2, v3) é subespaço vetorial de
R3, temos G(v1, v2, v3) = R3.

Proposição 2. Sejam α = {v1, v2, · · · , vr} e β = {w1, w2, · · · , wm} dois conjuntos de vetores em
um espaço vetorial V . As seguintes aﬁrmações são equivalentes.

(a) G(v1, v2, · · · , vr) = G(w1, w2, · · · , wm);

(b) Cada vetor de α é uma combinação linear de vetores de β e cada vetor de β é uma combinação
linear de vetores de α.

2.1.3 Dependência e Independência Linear

Deﬁnição 6. Sejam v1, v2, · · · , vr vetores em um espaço vetorial V . Dizemos que os vetores
v1, v2, · · · , vr são linearmente independentes ou simplesmente l.i. se a equação

α1v1 + α2v2 + · · · + αrvr = 0

(2.2)

é satisfeita somente quando α1 = α2 = · · · = αr = 0. Caso exista algum αi ̸= 0 dizemos que os
vetores v1, v2, · · · , vr são linearmente dependentes ou simplesmente l. d..
O conjunto {v1, v2, · · · , vr} é dito ser independente ou dependente se os vetores v1, v2, · · · , vr são
linearmente independentes ou dependentes, respectivamente. Dizemos que 2.2 é uma combinação
linear nula de v1, · · · , vr.

2.1. Espaços Vetoriais

33

Exemplo 11. Sejam V um espaço vetorial e u, v e w ∈ V . Suponha que w é combinação linear
de u e v. Então u, v e w são linearmente dependentes. De fato, como w é combinação linear de
u, v existem α e β ∈ R tais que αu + β v = w. Assim αu + β v − w = 0.

Exemplo 12. Considere o R4 com as operações usuais. Temos que (0, 1, 0, 1), (4, 6, 2, 6) e

(2, 0, 1, 0) são linearmente dependentes, pois 3(0, 1, 0, 1) −

1
2

(4, 6, 2, 6) + 1(2, 0, 1, 0) = 0

Exemplo 13. Conside M2 com as operações usuais. Então A =

1 1
0 1
são linearmente independentes. De fato, sejam α, β , γ ∈ R tais que:

(cid:34)

(cid:35)

, B =

(cid:35)

(cid:34)

1 1
0 0

,C =

(cid:35)

(cid:34)

0 0
2 2

αA + β B + γC = 0.

, de onde concluímos que α = β = γ = 0.

Então,






α + β = 0

α + β = 0

2γ = 0

α + 2γ = 0

Proposição 3. Sejam v1, v2, · · · , vn vetores em Rn, onde, para cada i, com 1 ≤ i ≤ r, temos vi =
(ai1, ai2, · · · , ain) e considere A = [ai j]. Temos que {v1, v2, · · · , vn} é linearmente independente
se, e somente se, A é invertível.

Demonstração. Basta observar que o sistema linear k1v1 + · · · + knvn = 0 terá apenas a solução
nula.

Proposição 4. Sejam v1, v2, · · · , vr vetores em Rn. Se r > n, então os vetores v1, v2, · · · , vr são
linearmente dependentes.

Demonstração. Suponha que para cada 1 ≤ i ≤ r, vi = (ai1, · · · , ain). Consideremos a equação:

k1v1 + k2v2 + · · · + krvr = 0

(2.3)

que é equivalente ao sistema:






· · ·

a11k1 + a21k2 + · · · + ar1kr = 0

a12k1 + a22k2 + · · · + ar2kr = 0

a1nk1 + a2nk2 + · · · + arnkr = 0

O sistema dado é linearmente homogênio de n equações e r incógnitas k1, k2, · · · , kr. Como r > n
segue que o sistema tem soluções não triviais. Isto mostra que v1, v2, · · · , vr são linearmente
dependentes.

34

Capítulo 2. Pré requisitos

Proposição 5. Um conjunto ﬁnito α = {v1, · · · , vr}, com r ≥ 2, de um espaço vetorial V é
linearmente dependente se, e somente se, pelo menos um dos vetores de α pode ser escrito como
combinação linear dos outros vetores.

Deﬁnição 7. Seja α = {v1, v2, · · · , vr} um conjunto ordenado de vetores em um espaço vetorial
não nulo V . Dizemos que α é uma base de V se as seguintes condições são veriﬁcadas:

i) α é linearmente independente;

ii) V = G(α).

Exemplo 14. Considere R4 com as operações usuais. Temos que

B = {(1, 0, 1, 0), (0, 1, 0, 1), (1, 0, 0, 1), (0, 0, 1, 1)}

é uma base do R4. De fato, seja (a, b, c, d) ∈ R4 e considere α, β , γ, δ ∈ R tais que:

α(1, 0, 1, 0) + β (0, 1, 0, 1) + γ(1, 0, 0, 1) + δ (0, 0, 1, 1) = (a, b, c, d).

Temos que

α + γ = a

β = b

α + δ = c






β + γ + δ = d

e assim, α = a − d + b +
.
Assim temos que G(B) = R4. Para veriﬁcar que B é l.i., basta considerar a = b = c = d = 0 e
veriﬁcar que α = β = γ = δ = 0.

, β = b, γ = d − b −

, δ =

c − a + d − b
2

c − a + d − b
2

c − a + d − b
2

Exemplo 15. Considere o espaço vetorial M23 de todas as matrizes 2 × 3 sobre o corpo dos
números reais. Então o conjunto B forma uma base de M23, onde
(cid:34)

(cid:35)(cid:41)

(cid:40)(cid:34)

(cid:34)

(cid:34)

(cid:35)

(cid:35)

(cid:35)

(cid:35)

(cid:34)

(cid:34)

(cid:35)

B =

1 0 0
0 0 0

,

0 1 0
0 0 0

,

0 0 1
0 0 0

,

0 0 0
1 0 0

,

0 0 0
0 1 0

,

0 0 0
0 0 1

.

A demonstrção deste fato é similar ao exempolo anterior.

De modo mais geral, no espaço vetorial Mrs das matrizes r × s, seja Ei j a matriz com
1 como elemento de ordem i j, e o 0 nos demais. Então, o conjunto de todas as matrizes Ei j
formam uma base de Mrs chamada base usual de Mrs. Em particular e1 = (1, 0, 0, · · · , 0), e2 =
(0, 1, 0, · · · , 0), en = (0, 0, 0, · · · , 1) formam a base usual de Rn.

Uma base de um espaço vetorial V é um conjunto gerador no qual cada vetor de V pode
ser escrito de modo único como combinação linear desses vetores. Esse é o resultado da próxima
proposição.

2.1. Espaços Vetoriais

35

Proposição 6. Sejam α = {v1, v2, · · · , vn} um conjunto ordenado de vetores de um espaço
vetorial não nulo. As seguintes aﬁrmações são equivalentes.

i) α é uma base de V ;

ii) cada vetor de v em V pode ser escrito de modo único na forma v = a1v1 + a2v2 + · · · + anvn.

Demonstração. Suponhamos que α é uma base de V . Tomemos v ∈ V . Como α gera V , existem
números reais a1, a2, · · · , an tais que:

v = a1v1 + a2v2 + · · · + anvn.

(2.4)

Para mostrar que a combinação linear em 2.4 é única, suponhamos que existem b1, b2, · · · , bn

tais que:

v = b1v1 + b2v2 + · · · + bnvn

(2.5)

De 2.4 e 2.5 segue que:

(a1 − b1)v1 + (a2 − b2)v2 + · · · + (an − bn)vn = 0

(2.6)

Como α é linearmente independente, a equação 2.6 é satisfeita somente se (a1 − b1) = (a2 −
b2) = · · · = (an − bn) = 0. Como v ∈ V foi tomado de modo arbitrário, segue o resultado.
Suponhamos, agora, que α tem a propriedade de que cada vetor v em V pode ser escrito de
modo único como combinação linear dos espaço gerado, claramente α gera V e para mostrar
que α é linearmente independente, considere a equação k1v1 + k2v2 + · · · + knvn = 0. Como
0 = 0v1 + 0v2 + · · · + 0vn e esta forma de escrever o vetor nulo é única segue que k1 = k2 = · · · =
kn = 0.

Deﬁnição 8. Seja V um espaço vetorial gerado por um conjunto ﬁnito de vetores não nulos
α = {v1, v2, · · · , vn}, isto é, G(α) = V . Diremos que V é ﬁnitamente gerado.

Proposição 7. Seja V um espaço vetorial ﬁnitamente gerado e seja B = {v1, v2, · · · , vn} uma base
de V ̸= {0}. Então, qualquer conjunto com mais de n vetores de V é linearmente dependente.

Demonstração. De fato, seja B′ = {w1, w2, · · · , wm} um conjunto de m vetores de V , com m > n.
Queremos mostrar que B′ é LD. Sejam x1, x2, · · · , xm tais que x1w1 + x2w2 + · · · + xmwm = 0.

36

Capítulo 2. Pré requisitos

Como B é uma base de V e wi ∈ B′ ⊂ V , i = 1, . . . , m, então, existem escalares α j, β j, · · · , η j,
com j = 1, . . . , n, tais que:



w1 = α1v1 + α2v2 + · · · + αnvn


w2 = β1v1 + β2v2 + · · · + βnvn
...

wm = η1v1 + η2v2 + · · · + ηnvn.

(2.7)

Desta forma,

(α1x1 + β1x2 + · · · + η1xm)v1+

(α2x1 + β2x2 + · · · + η2xm)v2 + · · · (αnx1 + βnx2 + · · · + ηnxm)vn = 0.

Como os vetores v1, v2, · · · , vn são uma base para V então eles são l.i. Logo,


α1x1 + β1x2 + · · · + η1xm = 0


α2x1 + β2x2 + · · · + η2xm = 0
...

αnx1 + βnx2 + · · · + ηnxm = 0

.

Sendo m > n, o sistema admite mais de uma solução, além da trivial, logo B′ =

{w1, w2, · · · , wm} é l.d.

Proposição 8. Duas bases quaisquer de um espaço vetorial V ̸= {0}, V ﬁnitamente gerado, têm
o mesmo número de vetores.

Demonstração. De fato, sejam A = {v1, v2, · · · , vn} e B = {w1, w2, · · · , wm} duas bases para V .
Como A é base e B é l.i., então m ≤ n. Por outro lado, como B é base e A é l.i., então n ≤ m.
Logo, m = n.

Deﬁnição 9. Seja V um espaço vetorial ﬁnitamente gerado. Se V = {0} deﬁnimos a dimensão
de V como sendo 0. Se V ̸= {0} e ﬁnitamente gerado, deﬁnimos a dimensão de V como sendo
o número de elementos de uma base qualquer de V . Usaremos o simbolo dimV para designar
a dimensão de V . Se um espaço vetorial V não é ﬁnitamente gerado dizemos que V possui
dimensão inﬁnita.

Como exemplos, podemos citar, dim R = 1, dim Rn = n, dim M2×2 = 4, dim Mm×n =

m × n e dim Pn = n + 1.

Teorema 1. Seja V um espaço vetorial de dimensão n. Qualquer conjunto de vetores L.I. em V
é parte de uma base, isto é, pode ser completado até formar uma base de V .

2.1. Espaços Vetoriais

37

Demonstração. Suponha que esse conjunto contenha r vetores linearmente independentes.
Como r < n existe ur+1 ∈ V tal que u1, · · · , ur, ur+1 são l.i., pois caso contrário os vetores
u1, · · · , ur formariam uma base de V ; o que é impossível pois dimV = n > r. Se r + 1 = n então
u1, · · · , ur, ur+1 formam uma base de V . Se r + 1 < n então é possível encontrar ur+2 ∈ V tal que
u1, · · · , ur, ur+1, ur+2 são l.i., pois caso contrário a sequência u1, · · · , ur, ur+1 seria uma base de
V ; o que é impossível pois dimV = n > r + 1. Repetindo os argumentos acima, encontramos
vetores ur+1, ur+2, · · · , ur+k, onde r + k = n, de forma que u1, · · · , ur, ur+1, · · · , ur+k são l.i. e,
como dimV = n = r + k segue que esta sequência de vetores é uma base de V que contém os
vetores u1, · · · , ur.

Exemplo 16. Sejam v1 = (1, 0, 2) e v2 = (0, −1, 3), podemos completar o conjunto {v1, v2} de
modo a formar uma base do R3. De fato, sabemos que dim R3 = 3, logo devemos acrescentar um
vetor v3 = (a, b, c) ̸= α1v1 + α2v2 = (α1, −α2, 2α1 + 3α2), pois v3 nao pode ser uma combinação
linear de v1 e v2. Existem inﬁnitos vetores possíveis, por exemplo, podemos escolher v3 =
(2, −1, 0). Assim, {v1, v2, v3} é uma base do R3.

Proposição 9. Seja V um espaço vetorial de dimensão ﬁnita. Se W é um subespaço de V , então
W tem também dimensão ﬁnita e dimW ≤ dimV . Além disso, se dimW = dimV , então W = V .

Como exemplo, suponha V = R3. Então, se S ⊂ R3, temos dim S = 0, 1, 2 ou 3 e

(1) dim S = 0 ⇒ S = {0}

(2) dim S = 1 ⇒ S é uma reta, passando pela origem.

(3) dim S = 2 ⇒ S é um plano, passando pela origem.

(4) dim S = 3 ⇒ S = R3.

Deﬁnição 10. Sejam V um espaço vetorial ﬁnitamente gerado e B uma base de V formada pelos
vetores u1, · · · , un. Como B é uma base de V , todo elemento de u ∈ V se escreve como α1u1 +
· · · + αnun, com os coeﬁcientes α1, · · · , an ∈ R. Pela proposição 6, os coeﬁcientes α1, · · · , an são
unicamente determinados pelo vetor u. Estes coeﬁcientes são denominados coordenas de u com
relação à base B. Representaremos as coordenadas de u com relação à base B como

[u]B =







α1
...
αn







B

ou simplesmente por

quando B estiver subentendida.

[u]B =













α1
...
αn

38

Capítulo 2. Pré requisitos

Exemplo 17. Vamos mostrar que os vetores (1, 1, 1), (0, 1, 1) e (0, 0, 1) formam uma base de
R3 e encontrar as coordenadas de u = (1, 2, 0) ∈ R3 com relação à essa base. Já sabemos que
dimR3 = 3. Para veriﬁcar se os vetores acima formam uma base de V , basta veriﬁcar se eles são
l.i.. Vemos que estes vetores são de fato l.i. pois a matriz











1 0 0
1 1 0
1 1 1

possui determinante igual a 1 ̸= 0. Agora,

(1, 2, 0) = α(1, 1, 1) + β (0, 1, 1) + γ(0, 0, 1) = (α, α + β , α + β + γ)

que é equivalente ao sistema


α = 1

α + β = 2

α + β + γ = 0

cuja única solução é α = 1, β = 1 e γ = −2. Desse modo, as coordenadas de u = (1, 2, 0) com
relação à base B são dadas por

[u]B =








1
1
−2




B

Deﬁnição 11. Seja V um espaço vetorial e U um subespaço vetorial de V . O complemento
ortogonal de U é o conjunto

U ⊥ = {v ∈ V ; < u, v >= 0, ∀u ∈ U}.

Proposição 10. U ⊥ é um subespaço vetorial de V .

Demonstração. Temos 0 ∈ U ⊥ pois < 0, u >= 0 ∀u ∈ U. Se v, w ∈ U ⊥ e α ∈ R então para todo
u ∈ U, temos:

< v + αw, u >=< v, u > +α < w, u >= 0

Portanto, v + αw ∈ U ⊥.

Observação 1. Se V têm dimensão ﬁnita então u ∈ U ⊥ se, e somente se, u é ortogonal a todos
os vetores de uma base qualquer de U.

Exemplo 18. Encontre U ⊥ se U = (cid:8)(x, y, z) ∈ R3/x − y − z = 0(cid:9)

Temos (x, y, z) ∈ U se (x, y, z) = (y + z, y, z) = y(1, 1, 0) + z(1, 0, 1).

2.1. Espaços Vetoriais

39

Logo (1, 1, 0) e (1, 0, 1) formam uma base para U.

Assim, (x, y, z) ∈ U ⊥ se < (x, y, z), (1, 1, 0) >= 0 e < (x, y, z), (1, 0, 1) >= 0




x + y = 0



x + z = 0

⇐⇒ (x, y, z) = x(1, −1, −1)

Assim:

U ⊥ = [(1, −1, −1)] .

Temos o seguinte resultado para complemento ortogonal:

Teorema 2. Se W é um subespaço de V ,então

V = W ⊕W ⊥.

41

CAPÍTULO

3

TRANSFORMAÇÕES LINEARES E
MATRIZES

3.1 Transformações Lineares

No capítulo anterior, estudamos espaços vetoriais e as propriedades dos vetores, elemen-
tos destes espaços, como, por exemplo, os conceitos de dependência linear e independência linear.
Neste capítulo, vamos estudar as transformações entre espaços vetoriais, mas não qualquer uma,
vamos estudar as transformações que possuem a propriedade de associar à soma de dois vetores,
a soma dos resultados obtidos pela aplicação da transformação em cada um desses vetores e,
associar ao produto de um escalar por um vetor, o produto deste escalar pelo resultado obtido
pela transformação aplicada a esse vetor. Descrevemos de forma mais precisa essas condições na
seguinte deﬁnição:

Deﬁnição 12. Sejam U e V espaçoes vetoriais. Dizemos que uma função T : U → V é uma
transformação linear se forem eriﬁcadas as seguintes condições:
a) T (u + v) = T (u) + T (v), ∀u, v ∈ U;
b) T (λ u) = λ T (u), ∀λ ∈ R e ∀u ∈ U.

Quando a transformação linear for de um espaço vetorial V nele mesmo, ela é chamada

de operador linear em V .

Os resultados deste capítulo podem ser encontrados em (ZANI, ), (LADEIRA, 2004),

(HEFEZ A.; SOUZA FERNANDES, 2012) e (FRENSEL K.; DELGADO, 2011).

Observação 2. Toda transformação linear T : U → V leva o vetor nulo do espaço U no vetor
nulo do espaço V . De fato, T (0U ) = T (0 · 0U ) = 0 · T (0U ) = 0V .

Podemos citar como exemplos de transformações lineares:

42

Capítulo 3. Transformações Lineares e Matrizes

Exemplo 19. A transformação nula T : U → V dada por T (u) = 0, ∀u ∈ U e a transformação
identidade T : U → U dada por T (u) = u, ∀u ∈ U.

Exemplo 20. T : C 1([a, b]; R) → C ([a, b]; R), dada por T(f)=f’. Aqui C ([a, b]; R) signiﬁca o
espaço das funções contínuas em [a, b] com valores reais e C 1([a, b]; R) signiﬁca o espaço das
funções com derivadas contínuas.

Exemplo 21. Seja A ∈ Mm×n uma matriz ﬁxada e considere T : Mn×1 → Mm×1 dada por T (X) =
AX.T assim deﬁnida é uma transformação linear.

Podemos observar que uma transformação linear ﬁca completamente determinada se

conhecermos seus valores nos elementos da base do espaço de saída, isto é:

Proposição 11. Sejam U e V espaços vetoriais e seja α = {u1, u2, . . . , un} uma base para o
espaço U. Então, toda transformação linear T : U → V ﬁca determinada conhecendo-se os
valores de T (u1), T (u2), · · · , T (un).

Demonstração. Seja u ∈ U, então existem escalares α1, α2, · · · , αn tais que u = α1u1 + α2u2 +
· · · αnun. Assim, T (u) = T (α1u1 + α2u2 + · · · αnun) = α1T (u1) + α2T (u2) + · · · αnT (un).

Deﬁnição 13. Sejam U e V espaços vetoriais. O conjunto de todas as transformações lineares de
U em V é denotado por L (U,V ). Quando U = V denotamos L (U,V ) por L (U).

Nosso objetivo é identiﬁcar o conjunto das transformações lineares L (U,V ), com
dim(U) = n e dim(V ) = m com o conjunto das matrizes de ordem m × n. Primeiramente,
observamos que se deﬁnirmos em L (U,V ) as operações de adição e multiplicação por escalar
dadas por :

T + S : UV, (T + S)(u) = T (u) + S(u), ∀u ∈ U,

λ · T : U → V, (λ · T )(u) = λ T (u), ∀λ ∈ R, ∀u ∈ U,

é um espaço vetorial. A dimensão do espaço vetorial L (U,V ) é m · n, se dim(U) = n e dim(V ) =
m.

Deﬁnição 14. Uma transformação linear T : U → V é :
1) injetora se T (u) = T (v) implicar que u = v;
2) sobrejetora se para cada v ∈ V existir um vetor u ∈ U tal que T (u) = v;
3) bijetora se for injetora e sobrejetora.

Deﬁnição 15. Dizemos que T ∈ L (U,V ) possui inversa se existir S : V → U tal que S ∘T (u) = u,
∀u ∈ U e T ∘ S(v) = v, ∀v ∈ V .Denotaremos S por T −1.

A ﬁm de que T ∈ L (U,V ) possua uma função inversa é necessário e suﬁciente que T
seja uma transformação bijetora. Neste caso, a inversa de T , T −1 é também uma transformação

3.1. Transformações Lineares

linear e T −1 ∈ L (V,U).

43

Deﬁnição 16. Diremos que uma transformação linear T : U → V é um isomorﬁsmo se ela for
uma transformação bijetora. Dizemos, neste caso, que os espaços U e V são isomorfos.

A seguir, listamos alguns resultados para ajudar-nos a provar quando uma transformação

linear é uma bijeção.

Quando T é uma transformação linear, temos um resultado bastante prático para veriﬁcar

se ela é ou não uma transformação injetora, dado pela proposição a seguir:

Proposição 12. Uma transformação linera T : U → V é injetora se, e somente se, T (u) = 0
implicar em u = 0.

Demonstração. Se supormos que T é uma transformação linear injetora, então, se T (u) = 0,
como sabemos que T (0) = 0 e T é injetora, segue que u = 0. Agora, se considerarmos T (u) =
T (v), então T (u − v) = 0 e, se por hipótese, isto implicar em u − v = 0, então u = v e T é
injetora.

Proposição 13. Seja T : U → V uma transformação linear. Temos que:
1) Se W é um subespaço vetorial de U, então T (W ) = {T (w) ∈ V : w ∈ W };
2) Se W é um subespaço vetorial de V então T −1(W ) é um espaço vetorial de U.

Deﬁnição 17. O núcleo de uma transformação linear T : UV é o subespaço vetorial de U dado
por T −1({0}), ou seja, é o conjunto {u ∈ U : T (u) = 0}. O núcleo de uma transformação linear
é indicado por N (T ) ou por Ker(T ).

O seguinte teorema relaciona a dimensão do núcleo de uma transformação linear com a

dimensão de sua imagem.

Teorema 3. Sejam U e V espaços vetoriais, com dimensão de U ﬁnita, e T : U → V uma
transformação linear. Então:

dim(U) = dim(N(T )) + dim(T (U))

Proposição 14. Seja T : U → V uma transformação linear. T é injetora se, e somente se,
N (T ) = {0}.

Corolário 1. Se U e V são espaços vetoriais de dimensão ﬁnita tais que dim(U) = dim(V ) e se
T : U → V é uma transformação linear, então as seguintes condições são equivalentes:
1) T é sobrejetora;
2) T é injetora;
3) T é bijetora;

44

Capítulo 3. Transformações Lineares e Matrizes

4) T leva bases de U em bases de V .

Proposição 15. Se T : U → V é um isomorﬁsmo e U tem dimensão ﬁnita, então V também tem
dimensão ﬁnita e dim(U) = dim(V ). Por outro lado, se V tem dimensão ﬁnita , U também terá
dimensão ﬁnita e também teremos dim(U) = dim(V ).

Proposição 16. Se U e V são espaços vetoriais de dimensão ﬁnita n, existe um isomorﬁsmo
T : U → V , isto é, os espaços são isomorfos.

Corolário 2. Dois espaços vetoriais de dimensão ﬁnita são isomorfos se, e somente se, têm a
mesma dimensão.

Corolário 3. Se U é um espaço vetorial de dimensão ﬁnita n e V é um espaço vetorial de
dimensão ﬁnita m, então L (U,V ) é isomorfo ao espaço vetorial Mm×n.

3.2 Matriz de uma Transformação Linear

Se V e W são espaço vetoriais de dimensão ﬁnita, com bases ﬁxadas, então uma transfor-
mação linear T : V → W pode ser representada por uma matriz. A vantagem de tal representação
é que muitos problemas associados às tranformações lineares entre espaços de dimensão ﬁnita
podem ser resolvidos com a teoria das matrizes.

Seja T : V → W uma transformação linear, em que dimV = n e dimW = m.

Sejam α = {v1, v2, · · · , vn} e β = {w1, w2, · · · , wm} bases de V e W respectivamente.
Como β é uma base de W , podemos determinar de modo único números reais ai j, com 1 ≤ i ≤
n, 1 ≤ j ≤ m, tais que:

T (vi) = a1iw1 + · · · + a jiw j + · · · + amiwm.

Tomemos agora v em V . Temos que v = k1v1 + · · · + knvn, em que ki ∈ R para 1 ≤ i ≤ n. Então,

T (v) = k1T (v1) + · · · + knT (vn)

= k1(a11w1 + · · · + am1wm) + · · · + kn(a1nw1 + · · · + amnwm) =

= (a11k1 + · · · + a1nkn)w1 + · · · + (am1k1 + · · · + amnkn)wm.

Logo,

[T (v)]β =

nindo por







a11k1 + · · · + a1nkn
...
am1k1 + · · · + amnkn













=

a11
...
am1

· · · a1n
...
...
· · · amn













·







k1
...
kn

= [T ]α

β · [v]α , deﬁ-

3.2. Matriz de uma Transformação Linear

45



[T ]α

a11
...
am1
α e β . Assim, temos,

β =











· · · a1n
...
. . .
· · · amn

como sendo a matriz que representa T em relação às bases

[T (v)]β = [T ]α

β · [v]α .

(3.1)

Exemplo 22. Sejam α = {(1, 1), (0, 2)} e β = {(1, 0, 1), (0, 1, 0), (1, 2, 0)}, bases de R2 e R3,
β onde T : R2 → R3 é dada por:
respectivamente. Calculemos [T ]α

T (x, y) = (2x, x − y, 2y)

Como T é uma transformação linear de R2 em R3, [T ]α

β é uma matriz 3 × 2, digamos

[T ]α

β =






a11 a12
a21 a22
a31 a32




, onde a11, a21, a31 são as coordenadas de T (1, 1) na base β e

a12, a22 e a32 são as coordenadas de T (0, 2) na base β .

T (1, 1) = a11(1, 0, 1) + a21(0, 1, 0) + a31(1, 2, 0) = (2, 0, 2) e

T (0, 2) = a12(1, 0, 1) + a22(0, 1, 0) + a32(1, 2, 0) = (0, −2, 4).

Assim,


a11 + a31 = 2

a21 + 2a31 = 0

a11 = 2

e, resolvendo o sistema temos:

[T ]α

β =






4
2
0
6
0 −4




 .

Exemplo 23. Vamos agora fazer o inverso, isto é, dada a matriz [T ]α
β vamos determinar qual
é a transformação T . Sejam α e β as bases dadas no exemplo anterior α = {(1, 1), (0, 2)} e
β = {(1, 0, 1), (0, 1, 0), (1, 2, 0)}. Queremos determinar a transformação linear T : R2 → R3 tal

que [T ]α

β =









.

1 0
1 2
0 1

Para determinar T , usaremos a expressão 3.1. Acharemos inicialmente [v]α . Ora, se

(cid:19)

(cid:18)y − x
2

(0, 2)

(x, y) ∈ R2, então (x, y) = x(1, 1) +

assim,



[(x, y)]α =





 .

x
y − x
2

46

Capítulo 3. Transformações Lineares e Matrizes

Portanto,

[T (x, y)]β =






1 0
1 2
0 1










x
y − x
2



 =










.



x
y
y − x
2

Consequentemente,

T (x, y) = x(1, 0, 1) + y(0, 1, 0) +

(cid:19)

(cid:18)y − x
2

(1, 2, 0) =

(cid:18)x + y
2

(cid:19)

, 2y − x, x

.

Proposição 17. Temos as seguintes operações de transformações lineares representadas por
matrizes:

1. Sejam T e T ′ transformações lineares de V em W , onde V e W são espaços vetoriais de

dimensão ﬁnita. Se α e β são bases de V e W , respectivamente, então:

[T + T ′]α

β = [T ]α

β + [T ′]α
β ,

[kT ]α

β = k[T ]α

β , onde k é um número real arbitrário.

2. Sejam T : V → W e S : W → U transformações lineares, em que V,W e U são espaços
vetoriais de dimensão ﬁnita. Se α, β e γ são bases de V,W e U, respectivamente, então:

[S ∘ T ]α

γ = [S]β

γ · [T ]α
β .

Teorema 4. Seja T : V → W um isomorﬁsmo, onde V e W são espaços vetoriais de dimensão
ﬁnita. Se α é uma base de V e β uma base de W , então:

[T −1]β

α = ([T ]α

β )−1

(3.2)

Corolário 4. Seja T : V → W uma transformação linear, onde V e W são espaços vetoriais de
mesma dimensão ﬁnita. Sejam α e β bases de V e W , respectivamente. Temos que T é invertível
se, e somente se, a matriz [T ]α

β é invertível.

Exemplo 24. Seja T : R2 → R2 a transformação linear dada por T (x, y) = (4x − 3y, −2x + 2y).
Vamos veriﬁcar que T é invertível e vamos encontrar T −1. Para veriﬁcarmos que T é invertível,
α onde α é uma base qualquer de R2, e usar o corolário. Se α é a base
podemos calcular [T ]α

canônica de R2, então, [T ]α

α =

(cid:32)

(cid:33)

4 −3
−2
2

. Portanto,

[T −1]α

α = ([T ]α

α )−1 =





1

3
2
1 2





Assim,

[T −1(x, y)]α = [T −1]α

α [(x, y)]α =

(cid:33)





1

3
2
1 2





(cid:32)
x
y

=






y
 .

x +

3
2
x + 2y

3.2. Matriz de uma Transformação Linear

47

Logo,

T −1(x, y) =

(cid:18)

x +

3
2

(cid:19)

y, x + 2y

.

3.2.1 Operadores Lineares em R2 e em R3

Dentre os operadores lineares mais importantes em R2 e em R3 estão os que produzem

reﬂexão, projeções, rotações e homotetias.

∙ Reﬂexões:

Podemos considerar o operador linear T : R2 → R2, chamado de reﬂexão em torno do
eixo Ox, que transforma cada vetor v = (x, y) ∈ R2 em sua imagem simétrica em relação ao eixo
Ox.

Figura 1 – Reﬂexão em torno do eixo Ox.

Escrevendo w = T (v) = (w1, w2), obteremos as equações w1 = x = 1x + 0y e w2 = −y =

0x − 1y. Assim se α denota a base canônica de R2, segue que:

[T (v)]α =

(cid:33)

(cid:32)
0
1
0 −1

[v]α

48

Capítulo 3. Transformações Lineares e Matrizes

Tabela 1 – Reﬂexões mais comuns em R2

Operador

Reﬂexão em torno do eixo Oy

Reﬂexão em torno da reta y = x

Reﬂexão em torno do plano xOy

Reﬂexão em torno do plano yOz

Reﬂexão em torno do plano xOz

(cid:40)




Equações
(cid:40)
w1 = −x
w2 = y
w1 = y
w2 = x
w1 = x
w2 = y
w3 = −z
w1 = −x
w2 = y
w3 = z
w1 = x
w2 = −y
w3 = z























Matriz [T ]α
α
(cid:19)
(cid:18)−1 0
1
0
(cid:19)
(cid:18)0 1
1 0





0
1 0
0 1
0
0 0 −1





−1 0 0
1 0
0
0 1
0





1
0
0
0 −1 0
1
0
0

∙ Projeções:

Podemos considerar o operador linear T : R2 → R2 que transforma cada vetor v = (x, y) ∈

R2 em sua projeção ortogonal sobre o eixo Ox.

Figura 2 – Projeção

Escrevendo w = T (v) = (w1, w2), obteremos as equações w1 = x = 1x + 0y e w2 = 0 =

0x + 0y.

Assim se α denota a base canônica de R2, temos:

[T (v)]α =

(cid:33)

(cid:32)

1 0
0 0

[v]α

Em geral uma projeção ou projeção ortogonal de R2 ou R3 é um operador linear que
transforma cada vetor em sua projeção ortogonal sobre alguma reta ou algum plano que passa
pela origem.

3.2. Matriz de uma Transformação Linear

49

Tabela 2 – Projeções mais comum em R2

Operador

Projeção sobre o eixo Oy

Projeção sobre o plano xOy

Projeção sobre o plano yOz

Projeção sobre o plano xOz




Equações
(cid:40)
w1 = 0
w2 = y
w1 = x
w2 = y
w3 = 0
w1 = 0
w2 = y
w3 = z
w1 = x
w2 = 0
w3 = z











Matriz [T ]α
α
(cid:19)
(cid:18)0 0
0 1

























1 0 0
0 1 0
0 0 0

0 0 0
0 1 0
0 0 1

1 0 0
0 0 0
0 0 1

∙ Rotação:

Considere o operador linear T : R2 → R2 que rotaciona cada vetor v = (x, y) ∈ R2 de um

ângulo ﬁxado θ .

Figura 3 – Rotação

T é chamado de rotação por θ em R2. Escrevendo w = T (v) = (w1, w2) segue da

trigonometria que:

e

x = r cos ∅ y = r sin ∅

w1 = r cos(θ + ∅), w2 = r sin(θ + ∅)

(3.3)

(3.4)

onde r é o comprimento de v e ∅ é o ângulo entre v e o eixo Ox positivo no sentido anti -

horário. Aplicando as identidades trigonométricas em 3.4 temos:






w1 = r cos θ cos ∅ − r sin θ sin ∅
w2 = r sin θ cos ∅ + r cos θ sin ∅

Substituindo 3.3 nas expressões acima, temos:

50

Capítulo 3. Transformações Lineares e Matrizes




w1 = x cos θ − y sin θ



w2 = x sin θ + y cos θ

Assim se α denota a base canônica de R2, obteremos:

[T (v)]α =

(cid:34)

cos θ − sin θ
cos θ
sin θ

(cid:35)

[v]α

Em geral a rotação de vetores em R3 é feita em relação a uma reta partindo da origem,
chamada eixo de rotação. À medida que um vetor gira em torno do eixo de rotação, ele varre
uma porção de um cone.

Figura 4 – Rotação

O ângulo de rotação que é medido na base do cone é descrito no sentido horário ou anti -

horário, em relação a um ponto de vista ao longo do eixo de rotação olhando para a origem.

Assim como em R2, os ângulos são positivos se gerados por rotações no sentido anti -

horário e negativos se gerados por rotações no sentido horário.

Tabela 3 – Rotações em R3 cujos eixos de rotação são os eixos coordenados

Operador
Rotação anti - horária em torno

do eixo Ox por um ângulo θ

Rotação anti - horária em torno

do eixo Oy por um ângulo θ

Rotação anti - horária em torno

do eixo Oz por um ângulo θ

∙ Homotetias:

Equações

Matriz [T ]α
α






w1 = x
w2 = y cos θ − z sin θ
w3 = y sin θ + z cos θ






w1 = x cos θ + z sin θ
w2 = y
w3 = −x sin θ + z cos θ

0


1
0 cos θ − sin θ

cos θ
0 sin θ

0





cos θ
0

0 sin θ
1
− sin θ 0 cos θ

0














w1 = x cos θ − y sin θ
w2 = x sin θ + y cos θ
w3 = z







cos θ − sin θ 0
0
sin θ

1
0

cos θ
0

3.2. Matriz de uma Transformação Linear

51

Sabemos que a multiplicação por um escalar de um vetor R2 e em R3, dependendo do

valor, produz no vetor uma dilatação, contração ou inversão.

Podemos representar estes efeitos geométricos por meio de operadores lineares.

Sendo o operador linear Tα : R2 → R2, dado por Tα (v) = αv, em que α ∈ R e v ∈ R2, a
oparação dilata v, se α ≥ 1, contrai v, se 0 ≤ α < 1; inverte o sentido de v, se α < 0. No caso
particular de α = −1 o operador Tα é chamado reﬂexão em torno da origem. Isso vale também
para o R3.

Figura 5 – Dilatação e contração por α em R

3.2.2 Mudança de Base e Matrizes semelhantes

Deﬁnição 18. Dado um espaço vetorial V arbitrário de dimensão ﬁnita e duas bases α e β de V ,
podemos obter uma relação entre as matrizes [v]α e [v]β de um vetor v em V , usando para isto, o
operador identidade em V , usando a expressão:

[v]β = [Iv]α

β · [v]α ∀ v ∈ V.

A matriz [Iv]α

β é chamada matriz mudança de base de α para β , pois pela igualdade
acima, ela nos permite obter as coordenadas de um vetor v em V em relação à base β uma vez
conhecidas suas coordenadas na base α.

Exemplo 25. Considerando as bases α = {(1, 0, 1), (1, 1, 1), (1, 1, 2)} e β = {(1, 0, 0), (0, 1, 0), (0, 0, 1)}

52

Capítulo 3. Transformações Lineares e Matrizes

e encontraremos [Iv]α

β . Precisamos resolver

(1, 0, 0) = a11(1, 0, 1) + a21(1, 1, 1) + a31(1, 1, 2)

(0, 1, 0) = a12(1, 0, 1) + a22(1, 1, 1) + a32(1, 1, 2)

(0, 0, 1) = a13(1, 0, 1) + a23(1, 1, 1) + a33(1, 1, 2)

⇐⇒

(a11 + a21 + a31, a21 + a31, a11 + a21 + 2a31) = (1, 0, 0)

(a12 + a22 + a32, a22 + a32, a12 + a22 + 2a32) = (0, 1, 0)

(a13 + a23 + a33, a23 + a33, a13 + a23 + 2a33) = (0, 0, 1)

Cada linha acima representa um sistema de três equações com três incógnitas e a matriz
associada a cada um desses sistemas é a mesma, o que muda são os nomes das variáveis e o
segundo membro. Utilizando como variáveis x, y e z, basta resolvermos o seguinte sistema











1 1 1
0 1 1
1 1 2






x
y
z






 =





a
, onde a, b e c ∈ R.

b
c

O sistema acima é equivalente a











1 1 1
0 1 1
0 0 1




 =






x
y
z











a
b
c − a

cuja solução única é dada por x = a − b, y = a + b − c e z = c − a.

Tomando (a, b, c) = (1, 0, 0) obtemos (a11, a21, a31) = (1, 1, −1),

(a, b, c) = (0, 1, 0) obtemos (a12, a22, a32) = (−1, 1, 0),
(a, b, c) = (0, 0, 1) obtemos (a13, a23, a33) = (0, −1, 1)˙

Desta forma, [Iv]α

β =






1 −1
1
−1

0
1 −1
1
0




 .

Teorema 5. Sejam α e β duas bases de um espaço de dimensão ﬁnita V . Temos que a matriz
[Iv]α

β é invertível e sua inversa é a matriz [Iv]β

α , ou seja,

(cid:17)−1

(cid:16)

[Iv]α
β

= [Iv]β
α .

Teorema 6. Sejam α e β duas bases de um espaço vetorial de dimensão ﬁnita V . Se T é um
operador linear em V , então

[T ]α

α = P−1 · [T ]β

β · P

3.2. Matriz de uma Transformação Linear

53

onde P = [Iv]α
β

Se A e B são matrizes quadradas de mesma ordem, dizemos que B é semelhante a A,
quando existir uma matriz invertível P tal que B = P−1AP. É facil veriﬁcar que se uma matriz
B é semelhante a A, então A também é semelhante a B, desta forma, dizemos que A e B são
semelhantes. Pelo teorema acima temos que [T ]α

β são semelhantes.

α e [T ]β
(cid:32)

5
2
−8 −3

(cid:33)

e B =

(cid:33)

(cid:32)

1 2
0 1

são semelhantes.

Exemplo 26. Vamos veriﬁcar se as matrizes A =

Para isso devemos encontrar uma matriz invertível P tal que PA = BP. Se tal matriz P existir ela

é uma matriz quadrada de ordem 2 que pode ser denotada por

. Assim:

(cid:32)

(cid:33)

x y
t
z

(cid:32)

x y
t
z

(cid:33) (cid:32)

5
2
−8 −3

(cid:33)

=

(cid:32)

1 2
0 1

(cid:33) (cid:32)

x y
t
z

(cid:33)

Esta igualdade equivale ao sistema


4x − 8y − 2z = 0

2x − 4y − 2t = 0

4z − 8t = 0

, que admite a solução não

trivial (3, 1, 2, 1). Portanto, P =

(cid:33)

(cid:32)

3 1
2 1

e A e B são semelhantes.

55

CAPÍTULO

4

ESPAÇO COM PRODUTO INTERNO

Neste capítulo vamos apresentar a noção de produto interno em espaços vetoriais.Esta
noção generaliza a noção de produto escalar em R2 e em R3 enriquecendo a estrutura de um
espaço vetorial. Os resultados deste capítulo também podem ser encontrados em (HEFEZ A.;
SOUZA FERNANDES, 2012) e (ZANI, ).

Deﬁnição 19. Seja V um espaço vetorial. Um produto interno em V é uma função que a cada par
de vetores u e v em V associa um número real, denotado por < u, v > que satisfaz as seguintes
condições:

Para quaisquer vetores u, v e w em V e ∀ número real α,

P1. < u, v >≥ 0

P2. < u, v >= 0 se, e somente se, v = 0

P3. < u, v >=< v, u >

P4. < u + v, w >=< u, w > + < v, w >

P5. < αu, v >= α < u, v >

Exemplo 27. Sejam u = (x1, x2, · · · , xn) e v = (y1, y2, · · · , yn) em Rn. Deﬁnimos

< u, v >= x1y1 + x2y2 + · · · + xnyn.

(4.1)

Note que < u, u >= x2
n ≥ 0 e que < u, v >= x1y1 + x2y2 + · · · + xnyn == y1x1 +
y2x2 + · · · + ynxn =< v, u >, mostrando que as condições 1 e 3 da deﬁnição são satisfeitas. A
condição 2 também é satisfeita já que

2 + · · · + x2

1 + x2

< u, u >= x2

1 + x2

2 + · · · + x2

n = 0 ⇐⇒ x1 = x2 = · · · = xn = 0.

56

Capítulo 4. Espaço com Produto interno

Se w = (z1, z2, · · · , zn) então: < u + v, w >= (x1 + y1)z1 + (x2 + y2)z2 + · · · + (xn + yn)zn
= (x1z1 + x2z2 + · · · + xnzn) + (y1z1 + y2z2 + · · · + ynzn) =< u, w > + < v, w >, mostrando que
a condição 4 também é satisfeita e a condição 5 também segue, pois se k ∈ R, então:

< ku, v >= (kx1)y1 + (kx2)y2 + · · · + (kxn)yn

= k(x1y1 + x2y2 + · · · + xnyn)

= k < u, v >

Assim, 4.1 deﬁne um produto interno em Rn, chamado produto interno usual de Rn ou

produto escalar de Rn, generalizando a noção de produto escalar de R2 e de R3.

Deﬁnição 20. Seja V um espaço com produto interno. Deﬁnimos norma do vetor v de V , ou
comprimento de v, denotado por ‖v‖, como o número real

‖v‖ =

√

< v, v >.

Se ‖v‖ = 1, dizemos que v é um vetor unitário.

A distância de (u, v) entre dois vetores u e v de V é deﬁnida como:

d(u, v) = ‖u − v‖ =

√

< u − v, u − v >.

Exemplo 28. Se u = (x1, x2, · · · , xn) e v = (y1, y2, · · · , yn) são vetores em Rn, com o produto
interno usual, então

e

‖x‖ =

(cid:113)

x2
1 + · · · + x2
n

d(u, v) = ‖u − v‖ =

√

< u − v, u − v >

(cid:113)

(x1 − y1)2 + (x2 − y2)2 + · · · + (xn − yn)2.

=

Proposição 18. Seja V um espaço vetorial com um produto interno. Temos:

1. ‖αu‖ = |α| ‖u‖, para todo u ∈ V e todo α ∈ R.

2. ‖u‖ ≥ 0, para todo u ∈ V .

3. ‖u‖ = 0 se, e somente se, u = 0.

4. | < u, v > | ≤ ‖u‖ ‖v‖ para todo u, v ∈ V (desigualdade de Cauchy - Schwarz).

5. ‖u + v‖ ≤ ‖u‖ + ‖v‖, para todo u, v ∈ V (desigualdade triângular).

Proposição 19. (Identidade do Paralelogramo) Sejam u e v vetores de um espaço vetorial V com
um produto interno. Então:

57

‖u + v‖2 + ‖u − v‖2 = 2(|u‖2 + ‖v‖2).

Demonstração.

‖u + v‖2 + ‖u − v‖2 =< u + v, u + v > + < u − v, u − v >

=< u, u > + < v, v > +2 < u, v > + < u, u > + < v, v > −2 < u, v >

= 2 < u, u > +2 < v, v >= 2(‖u‖2 + ‖v‖2).

Proposição 20. Se u, v e w são vetores em um espaço com produto interno V , então:

1. d(u, v) ≥ 0.

2. d(u, v) =0 se, e somente se, u = v.

3. d(u, v) = d(v, u).

4. d(u, v) ≤ d(u, w) + d(w, v) (desigualdade triângular).

Seja V um espço vetorial com produto interno e u, v ∈ V ambos não nulos. Pela desigual-

dade de Cauchy - Schwarz, ver proposição (18), temos:

ou ainda

− ‖u‖ ‖v‖ ≤< u, v >≤ ‖u‖ ‖v‖

−1 ≤

< u, v >
‖u‖ ‖v‖

≤ 1

Desta forma, existe um único número real θ ∈ [0, π] tal que:

cos θ =

< u, v >
‖u‖ ‖v‖

Este número θ é chamado de ângulo entre os vetores u e v.

Exemplo 29. Sabe-se que ‖u‖ = ‖v‖ = 1 e ‖u − v‖ = 2. Calcule o ângulo entre u e v.

Como ‖u − v‖ = 2, então:

4 = ‖u − v‖2 =< u − v, u − v >= ‖u‖ + ‖v‖ − 2 < u, v >= 2 − 2 < u, v >

Assim, < u, v >= 1 e cos θ =

< u, v >
‖u‖ ‖v‖

= −1, ou seja, θ = π.

58

Capítulo 4. Espaço com Produto interno

Deﬁnição 21. Seja V um espaço com produto interno. Dizemos que u, v ∈ V são ortogonais se
< u, v >= 0 e, neste caso, denotamos por u ⊥ s.

Dizemos que um conjunto S = {u1, · · · , un} ⊂ V é ortogonal se ui ⊥ u j quando i ̸= j.

Dizemos que um conjunto S = {u1, · · · , un} ⊂ V é ortonormal se for ortogonal e também

(cid:13)
(cid:13) = 1, j = 1, 2, · · · , n.

(cid:13)
(cid:13)u j

Dizemos que u ∈ V é ortogonal a um subconjunto não vazio S de V se u for ortogonal a

todos os elementos de S. Neste caso usaremos a deﬁnição de u ⊥ S.

Exemplo 30. Seja um vetor arbitrário u = (x1, x2, · · · ,xn) ∈ Rn. Então um vetor v = (y1, y2, · · · , yn) ∈
Rn é ortogonal a u se < u, v >= x1y1 + x2y2, · · · + xnyn = 0.

Observação 3. É fácil veriﬁcar que

1. Se u = 0 ou v = 0, então u ⊥ v. Se u ̸= 0 e v ̸= 0 então u ⊥ v se, e somente se o ângulo entre u e

v =

π
2

.

2. Se S = {u1, · · · , un} ⊂ V é um conjunto ortogonal com u j ̸= 0, j = 1, 2, · · · , n, então S′ =

(cid:26) u1
‖u1‖

, · · · ,

un
‖un‖

(cid:27)

é um conjunto ortonormal.

Proposição 21. Todo conjunto ortogonal de vetores não nulos de V é linearmente independente.

Demonstração. Se

α1u1 + · · · + αnun = 0
então tomando o produto interno do vetor acima com u1 e lembrando que < u1, u1 >= ‖u1‖2 = 1
e < u j, u1 >= 0 se j = 2, 3, · · · , n, obteremos:

(4.2)

α1 = α1 < u1, u1 > + · · · + α < un, u1 >=< 0, u1 >= 0

isto é, α1 = 0 e 4.2 ﬁca α2u2 + · · · + αnun = 0.

Tomando o produto interno do vetor acima com u2, obtemos como acima, que α2 = 0.

Repetindo o processo chegamos à conclusão que a única possibilidade para 4.2 é α1 =

· · · = αn = 0.

Observação 4. A proposição acima continua válida se S for apenas um conjunto ortogonal com
elementos não nulos.

Deﬁnição 22. Se V é um espaço vetorial com produto interno de dimensão n e se {u1, · · · , un} é
conjunto ortonormal, então diremos que estes vetores formam uma base ortonormal de V .

Proposição 22. Sejam V um espaço euclidiano que possui uma base ortonormal dada por
{u1, · · · , un}. Então, se u ∈ V temos

59

u =< u, u1 > u1 + · · · + < u, un > un.

Demonstração. Como u1, · · · , un formam uma base de V , existem α1, · · · , αn ∈ R tais que v =
α1v1 + · · · + αnvn. Tomando o produto interno de V com v1, temos que,

< v, v1 >= α1 < v1, v1 > + · · · + αn < vn, v1 >= α1, pois a base é ortogonal. O resultado

segue tomando o produto interno de v por v2, v3, etc.

Se β = {v1, v2, · · · , vn} é uma base ortogonal de V normalizando cada um dos vetores de

β , obteremos a base ortonormal α de V , onde α =

(cid:26) v1
‖v1‖

,

v2
‖v2‖

, · · · ,

(cid:27)

.

vn
‖vn‖

Proposição 23. Seja w um vetor não nulo de V . Se v ∈ V , então:

k =

< v, w >
< w, w >

=

< v, w >
‖w‖2

é o único número real tal que v′ = v − kw é ortogonal a w.

A projeção de v ao longo de w, é denotado por pro jw(v) e é deﬁnida por:

pro jw(v) = kw =

< v, w >
< w, w >

w.

Figura 6 – Projeção de v ao longo de w.

Proposição 24. Suponhamos que {w1, w2, · · · , wr} seja um conjunto ortogonal de vetores não
, 1 ≤ i ≤ r, são os únicos números reais tais que o
nulos de V . Se v ∈ V , então ki =

< v, wi >
‖wi‖2

vetor

v′ = v − k1w1 − k2w2 − · · · − krwr

é ortogonal aos vetores w1, w2, · · · , wr.

60

Capítulo 4. Espaço com Produto interno

Demonstração. Para i = 1, 2, · · · , r e usando < wi, w j >= 0 para i ̸= j, temos:

< v − k1w1 − k2w2 − · · · − krwr, wi >=< v, wi > −ki < wi, wi >=
< v, wi >
< wi, wi >

< wi, wi >= 0

=< v, wi > −

Com estes resultados, podemos aﬁrmar o seguinte teorema:

Teorema 7. Um espaço vetorial V com produto interno possui uma base ortogonal.

Demonstração. Seja {v1, v2, · · · , vn} uma base para um espaço V com produto interno. Podemos
obter uma uma base ortogonal {w1, w2, · · · , wn} para V da maneira descrita a seguir. Considere

w1 = v1

w2 = v2 −

< v2, w1 >
‖w1‖2 w1

w3 = v3 −

< v3, w2 >
‖w2‖2 w2

< v3, w1 >
‖w1‖2 w1 −
...
< vn, w2 >
‖w2‖2 w2 − · · · −

< vn, wn >
‖wn‖2 wn.

wn = vn −

< vn, w1 >
‖w1‖2 w1 −

Pela proposição 24, o conjunto {w1, w2, · · · , wn} é um conjunto ortogonal. Além disso,
como o conjunto {v1, v2, · · · , vn} é linearmente independente, cada vetor wi é não nulo. Assim,
o conjunto {w1, w2, · · · , wn} é um conjunto ortogonal de vetores não nulos de V . Como, por
deﬁnição, n = dimV , {w1, w2, · · · , wn} é base ortogonal de V . A normalização de cada wk garante
uma base ortonormal para V .

Exemplo 31. Para encontrar uma base ortonormal para W = {(x, y, z) ∈ R3, x−2y = 0}, notamos,
primeiramente, que (x, y, z) ∈ W se

(x, y, z) = (2y, y, z) = y(2, 1, 0) + z(0, 0, 1).

Assim, (2, 1, 0) e (0, 0, 1) formam uma base de W . A base ortonormal para W será dada
pelos vetores u1 e u2, considerando u1 = (0, 0, 1) pois é um vetor unitário e pelo processo anterior,
u2 é a projeção ortogonal unitária de (2, 1, 0) sobre u1, isto é:

u2 =

(2, 1, 0)− < (2, 1, 0), (0, 0, 1) > (0, 0, 1)
‖(2, 1, 0)− < (2, 1, 0), (0, 0, 1) > (0, 0, 1)‖

=

(2, 1, 0)
‖2, 1, 0‖

=

(cid:18) 2
√
5

,

1
√
5

(cid:19)

, 0

.

61

Operadores em Espaços com Produto Interno

Agora, vamos mostrar a existência do operador adjunto de um operador linear e apartir
deste, introduzir as noções de operadores simétricos e operadores ortogonais. Suponhamos que
V é um espaço com produto interno de dimensão ﬁnita n > 0.

Mostraremos que existe um isomorﬁsmo entre V e L (V, R). Dado um vetor v ∈ V , a ele

associamos de modo natural um funcional linear em V , como segue:

φv : V → R

u → < u, v >

De fato φv é um funcional linear, pois, para cada u1, u2 ∈ V e todo α ∈ R temos:

φv(u1 + αu2) =< u1 + αu2, v >=< u1, v > +α < u2, v >= φv(u1) + αφv(u2).

Logo, cada v ∈ V deﬁne um funcional linear φv em V , ou seja, um elemento de L (V, R).

Suponhamos, agora, que φv = φv′, isto é, < u, v >=< u, v′ > para todo u ∈ V . Logo
< u, v − v′ >= 0, para todo u ∈ V . Portanto, v − v′ é ortogonal a todos os vetores de V o
que acarreta que v = v′. Desta forma, a função v ↦→ φv, onde φv(u) =< u, v >, (u ∈ V ), é um
isomorﬁsmo entre V e L (V, R).

Teorema 8. Dado o operador linear T em V , existe um único operador linear T * em V tal que:

< T (v), w >=< v, T *(w) >, ∀v, w ∈ V.

Demonstração. Tome w ∈ V . Como a função deﬁnida por v ↦→< T (v), w > é um funcional linear
em V , segue do resultado anterior que existe um único vetor w′ ∈ V tal que:
< T (v), w >=< v, w′ >, para todo v ∈ V. Basta deﬁnir T *(w) = w′.

Se v1, · · · ,vn é uma base ortonormal de V , então,

w′ =< w′, v1 > v1 + . . . + < w′, vn > vn =< T (v1), w > v1 + . . . < t(vn), w > vn.

Como T *(w) = w′, podemos ver pela igualdade anterior que T * é linear.

Deﬁnição 23. O operador T * acima deﬁnido é chamado de operador adjunto de T .

Assim, o teorema anterior aﬁrma que todo operador linear T , em um espaço com o

produto interno de dimensão ﬁnita, possui um operador adjunto T *.

Proposição 25. Para toda base ortonormal α de V e para todo operador linear T em V , temos
que

[T *]α

α = (cid:2)[T ]α

α

(cid:3)t .

62

Capítulo 4. Espaço com Produto interno

Para a prova desta proposição precisamos do lema a seguir.

Lema 1. Seja α = {v1, · · · , vn} uma base ortonormal de V . Se A = [ai j]n×n é a matriz que
representa um operador T em V , com relação à base α (ou seja, A = [T ]α

α ), então:

ai j =< T (v j), vi > para todos i, j, 1 ≤ i, j ≤ n.

Demonstração do Teorema: Considere as matrizes [T ]α

α = [ai j]n×n e [T *]α

α = [bi j]n×n.

Pelo lema 1,

ai j =< T (v j), vi > e bi j =< T *(v j), vi >, para todos i, j, 1 ≤ i, j ≤ n.

Logo,

bi j =< T *(v j), vi >=< vi, T *(v j) >=< T (vi), v j >= a ji, para todos i, j, com1 ≤ i, j ≤ n.

Deﬁnição 24. Um operador linear T : V → V é dito operador simétrico quando T * = T .

Pela proposição 25, observamos que se T é um operador simétrico em V , então para toda

base ortonormal α de V temos:

[T ]α

α )t
α = ([T ]α

Assim T : V → V é simétrico se, e somente se, [T ]α

α é uma matriz simétrica.

Exemplo 32. Seja T : R3 → R3 o operador linear deﬁnido por T (x, y, z) = (2x − y + z, −x + y +
3z, x + 3y) Se α é a base canônica de R3, então:

[T ]α

α =








1 −1 1

−1
3
1

0
3
1

é uma matriz simétrica, e portanto, T é um operador simétrico.

Deﬁnição 25. Um operador linear T : V → V é dito ser um operador ortogonal quando

T *T = T T * = Iv.

Dizemos que um operador T em V preserva norma, preserva distância, ou preserva
produto interno, quando para todos u, v ∈ V, se tenha ‖T (v)‖ = ‖v‖ , d(T (u), T (v)) = d(u, v), ou
< T (u), T (v) >=< u, v >, respectivamente.

Teorema 9. Seja T : V → V um operador linear. As seguintes aﬁrmações são equivalentes:

i. T é ortogonal.

63

ii. T preserva a norma.

iii. T preserva a distância.

iv. T preserva o produto interno.

v. T transforma toda base ortonormal numa base ortonormal.

vi. T transforma alguma base ortonormal numa base ortonormal.

Demonstração. (i) ⇒ (ii)

Se v ∈ V , então:

‖T (v)‖2 =< T (v), T (v) >=< v, T *(T (v)) >=< v, Iv(v) >=< v, v >= ‖v‖2 .

(ii) ⇒ (iii)

Se v, u ∈ V , então d(T (v), T (u)) = ‖T (v) − T (u)‖ = ‖T (v − u)‖ = ‖v − u‖ = d(v, u).

(iii) ⇒ (iv)

Se v, u ∈ V , então d(T (v + u), 0) = d(v + u, 0) ou seja, ‖T (v + u)‖2 = ‖v + u‖2

Note que:

‖T (v + u)‖ =< T (v), T (v) > +2 < T (v), T (u) > + < T (u), T (u) > e

‖v + u‖2 =< v, v > +2 < v, u > + < u, u >

Como:

< v, v >= (d(v, 0))2 = (d(T (v), 0))2 =< T (v), T (v) >

o mesmo valendo para u, temos que < T (v), T (u) >=< v, u >, como desejado.

(iv) ⇒ (i)

Se v, u ∈ V , então:

< v, u >=< T (v), T (u) >=< v, T *(T (u)) >,

mostrando que para todo u, v ∈ V ,

< v, (T *T − Iv)(u) >= 0

Temos que (T *T − Iv)(u) = 0, para todo u ∈ V , o que acarreta que T *T = Iv, logo T é

ortogonal.

(i) ⇒ (v)

Seja v1, v2, · · · , vn uma base ortonormal de V . Então

64

Capítulo 4. Espaço com Produto interno

< T (vi), T (v j) >=< vi, v j >=

(cid:33)

(cid:32)

0 se i ̸= j
1 se i = j

Logo, o conjunto T (v1), T (v2), · · · , T (vn) é ortonormal e consequentemente, linearmente

independente. Como dimV = n, concluímos que esse conjunto é uma base de V .

(v) ⇒ (vi)

Esta implicação é óbvia.

(vi) ⇒ (iv)

Seja {v1, v2, · · · , vn} uma base ortonormal de V tal que {T (v1), T (v2), · · · , T (vn)} tam-

bém é uma base ortonormal de V . Sejam v e u em V com

v = a1v1 + a2v2 + · · · + anvn e u = b1v1 + b2v2 + · · · + bnvn, então

< v, u >=

n
∑
i=1

n
∑
j=1

aib j < vi, v j >=

n
∑
i=1

n
∑
j=1

aib j.

(4.3)

Por outro lado, temos

e

donde

T (v) = a1T (v1) + · · · + anT (vn)

T (u) = b1T (v1) + · · · + bnT (vn),

< T (v), T (u) >=

n
∑
i=1

n
∑
j=1

aib j < T (vi), T (u j) >=

n
∑
i=1

n
∑
j=1

aib j.

(4.4)

Assim de 4.3 e 4.4 concluímos que

< T (v), T (u) >=< v, u > .

Deﬁnição 26. Uma matriz quadrada A tal que AtA = I é chamada de matriz ortogonal.

Proposição 26. Seja A = [ai j]n×n uma matriz. São equivalentes

i. A é ortogonal.

ii. As colunas de A formam um conjunto ortonormal em Rn.

iii. As linhas de A formam um conjunto ortonormal em Rn,

Demonstração. (i) ⇔ (ii) Chamaremos AtA = [bi j]n×n.Pela deﬁnição de produto de matrizes, o
elemento bi j é dado por:

bi j = a1ia1 j + a2ia2 j + · · · + anian j =< (a1i, a2i, · · · , ani), (a1 j, a2 j, · · · , an j) >

65

Assim,

AtA = In se, e somente se,

< (a1i, a2i, · · · , ani), (a1 j, a2 j, · · · , an j) >=




0 se i ̸= j



1 se i = j.

(i) ⇔ (iii) Basta utilizar o fato que A é ortogonal se, e somente se, At é ortogonal, que as linhas
de At são as colunas de A e aplicar o que foi provado acima.

Teorema 10. Se α e β são bases ortonormais de V , então a matriz de mudança de base [Iv]α
uma matriz ortogonal.

β é

Demonstração. Sejam α = {v1, v2, · · · , vn} e β = {w1, w2, · · · , wn}. Suponhamos [Iv]α
Para 1 ≤ i ≤ n temos que:

β = [ai j].

vi = a1iw1 + a2iw2 + · · · + aniwn

Como vi e v j são ortogonais, quando i ̸= j, então:

0 =< vi, v j >= a1ia1 j + a2ia2 j + · · · + anian j

=< (a1i, a2i, · · · , ani), (a1 j, a2 j, · · · , an j) >

pois β é ortonormal e concluímos que as colunas de [Iv]α
Vejamos agora que cada coluna de [Iv]α
então:

β formam vetores ortogonais em Rn.
β formam um vetor unitário em Rn. De fato, se 1 ≤ i ≤ n,

já que β é ortonormal. Assim, as colunas de [Iv]α
matriz ortogonal.

1 =< vi, vi >= a2

2i + · · · + a2
ni,

1i + a2
β formam vetores unitários em Rn e [Iv]α

β é uma

Agora vamos mostrar a relação entre os operadores ortogonais e as matrizes ortogonais.

Sejam dados um espaço vetorial V , com uma base α = {v1, v2, · · · , vn} e uma matriz
quadrada A = [ai j] de ordem n. Podemos, como já vimos, associar à matriz A um operador linear
TA, deﬁnido como se segue

TA(v) = (a11x1 + · · · + · · · + a1nxn, · · · , an1x1 + · · · + annxn),

66

Capítulo 4. Espaço com Produto interno

onde x1, · · · , xn são coordenadas de v relativamente à base α, ou seja

v = x1v1 + · · · + xnvn

Proposição 27. Sejam α uma base ortonormal de V , T um operador linear em V e A ∈ Mn×m

i. T é ortogonal se, e somente se, [T ]α

α é ortogonal.

ii. A é ortogonal se, e somente se, TA é ortogonal.

67

CAPÍTULO

5

DIAGONALIZAÇÃO DE OPERADORES

5.1 Operadores Diagonalizáveis

Vimos anteriormente que uma operador linear T : V → V , onde V tem dimensão ﬁnita,
pode ser representada por uma matriz. Sendo as matrizes diagonais as mais simples do ponto
de vista das operações matriciais, queremos saber se para todo operador T existe uma base α
de V tal que [T ]α
α seja uma matriz diagonal. A resposta é que nem sempre existe tal base. Por
exemplo, o operador T em R2, cuja matriz na base canônica é dada por:

A =

(cid:33)

(cid:32)

0 0
1 0

não admite uma tal representação. De fato, se fosse possível achar uma base α tal que a matriz
de T nesta base fosse diagonal, teríamos PAP−1 = C onde P é uma matriz 2 × 2 invertível e C
uma matriz diagonal. Como A2 = 0, isto acarretaria que:

C2 = (PAP−1)2 = PA2P−1 = 0

Logo C = 0, o que implica que A = 0; uma contradição.

Deﬁnição 27. Dizemos que um operador deﬁnido sobre um espaço vetorial V de dimensão ﬁnita
é diagonalizável, quando for possível representá-lo por uma matriz diagonal em alguma base de
V .

Nosso objetivo, então, é obter resultados para garantir se um operador é diagonalizável
ou não. Para isso, iniciamos com a deﬁnição de autovalor e autovetor associados a um opera-
dor linear T . Para descrever o conteúdo deste capítulo utilizamos a bibliograﬁa (HEFEZ A.;
SOUZA FERNANDES, 2012) e (ZANI, ).

Deﬁnição 28. Seja T : V → V um operador linear. Um número real α será dito um autovalor de
T se existir um vetor não nulo v, chamado de autovetor de T associado a α, tal que T (v) = αv.

68

Capítulo 5. Diagonalização de Operadores

Exemplo 33. Seja T : R2 → R2 o operador linear dado por T (x, y) = (4x − y, 2x + y). Vamos
determinar α ∈ R e v = (x, y) ∈ R2, não nulo, tais que T (x, y) = α(x, y), ou seja (4x − y, 2x + y) =
α(x, y).

Resolvendo o sistema




4x − y = αx



2x + y = αy

, temos que α1 = 3 e α2 = 2 são os autovalores

de T . Vamos agora calcular os autovetores de T . Primeiramente, para α = 2, fazemos:




4x − y = 2x



2x + y = 2y

.

Assim, y = 2x e o conjunto solução da equação T (x, y) = 2(x, y) é dado por {(x, 2x); x ∈
R}. Logo, os autovetores de T associados a α = 2 são os vetores da forma (x, 2x) em que x ∈ R
e x ̸= 0.

Agora, vamos calcular os autovetores de T associados ao autovalor α = 3, fazemos:




4x − y = 3x



2x + y = 3y

.

Assim x = y, e o conjunto solução dessa equação é dado por {(x, x); x ∈ R}. Logo, os

autovalores de T associados a α = 3 são os vetores da forma (x, x) em que x ∈ R e x ̸= 0.

O exemplo a seguir nos mostra que nem todo operador linear possui autovalores e

autovetores.

Exemplo 34. Seja T : R2 → R2 o operador linear dado por T (x, y) = (−y, x). Se α ∈ R e
v = (x, y) ∈ R2, v ̸= 0, são tais que T (x, y) = α(x, y) então:

Equivalentemente,

(−y, x) = α(x, y).




αx = −y



αy = x

Assim y(α 2 + 1) = 0 e como α ∈ R a equação y(α 2 + 1) = 0 é veriﬁcada apenas se
y = 0, o que implicaria x = 0. Como v não é o vetor nulo, isso não pode ocorrer. Concluímos
que T não tem autovalor e nem autovetores.

Proposição 28. Seja T : V → V um operador linear e sejam c1, c2, · · · , cr autovalores distintos
de T . Se v1, v2, · · · , vr são autovetores associados aos autovalores c1, c2, · · · , cr, respectivamente,
então v1, v2, · · · , vr é linearmente independente.

5.1. Operadores Diagonalizáveis

69

Demonstração. A prova é feita por indução sobre r. O resultado é valido para r = 1 pois se
T : V → V é um operador linear com autovalor c1 e se v1 é um autovetor de T associado a c1,
então v1 é linearmente independente, pois v1 ̸= 0. Suponhamos agora o resultado válido para
r − 1, vamos provar para r, r ≥ 2.

Considere a equação

a1v1 + a2v2 + · · · + arvr = 0

(5.1)

onde a1, a2, · · · , ar são números reais. Aplicando T em 5.1, e observando que T (v j) = c jv j,
1 ≤ j ≤ r, obtemos

a1(c1v1) + a2(c2v2) + · · · + ar(crvr) = 0

(5.2)

Por outro lado, T possui pelo menos um autovalor não nulo, que podemos supor cr ̸= 0. Multi-
plicando 5.1 por cr, obtemos:

a1(crv1) + a2(crv2) + · · · + ar(crvr) = 0

(5.3)

De 5.2 e 5.3

a1(c1 − cr)v1 + a2(c2 − cr)v2 + · · · + ar−1(cr−1 − cr)vr−1 = 0

(5.4)

Pela hipótese de indução {v1, v2, · · · , vr−1} é linearmente independente. Portanto, de 5.4 segue:

a j(c j − cr) = 0, 1 ≤ j ≤ r − 1.

(5.5)

Como os autovalores c1, c2, · · · , cr são todos distintos, de 5.5 obtemos que a j = 0 para todo
1 ≤ j ≤ r − 1. Substituindo os valores em 5.1, concluímos também que ar = 0, já que vr ̸= 0.
Portanto {v1, v2, · · · , vr} é independente.

Corolário 5. Seja T : V → V um operador linear. Se dimV = n e T possui n autovalores distintos,
então V possui uma base formada por autovetores de T .

Demonstração. Pela proposição acima, n autovalores distintos implicam na existência de um
conjunto de autovetores v1, v2, · · · , vn linearmente independente. Como G(v1, v2, · · · , vn) ⊂ V e
dim(v1, v2, · · · , vn) = n = dimV , temos que G(v1, v2, · · · , vn) = V . Logo {v1, v2, · · · , vn} é uma
base de V .

Deﬁnição 29. Seja A uma matriz quadrada de ordem n. A matriz tIn − A, onde In é a matriz
identidade de ordem n e t uma indeterminada é chamada de matriz característica de A. O
determinante dessa matriz que é um polinômio em t, é o polinômio característico de A, denotado
por PA(t).

70

Capítulo 5. Diagonalização de Operadores

Exemplo 35. Seja

(cid:32)

4 −1
1
2

(cid:33)

, o polinômio característico de A será dado pelo determinante da

matriz característica de A, tI2 − A =

(cid:32)

t − 4
−2

1
t − 1

(cid:33)

, dado por

PA(t) = det

(cid:32)

t − 4
−2

(cid:33)

1
t − 1

= (t − 4)(t − 1) + 2 = t2 − 5t + 6.

Observamos que as raízes do polinômio deste exemplo, ou seja, os números reais t0 tais

que PA(t0) = 0, são os autovalores dados no exemplo 33.

Existe uma relação entre autovalores de um operador e as raízes do polinômio caracterís-

tico de alguma matriz associada a ele.

Teorema 11. Seja T : V → V um operador linear e seja α = {v1, v2, · · · , vn} uma base de V .
Então:

(i) v é um autovetor de T associado a t0 se, e somente se, v é uma solução não trivial do sistema

linear AX = 0, onde A = t0In − [T ]α
α

(ii) t0 ∈ R é um autovalor de T se, e somente se, t0 é uma solução do polinômio característico da

matriz [T ]α

α , ou seja, P[T ]α

α

(t0) = 0

Demonstração. (i) Seja t0 um autovalor de T e v um autovetor de T associado a t0. Como
[T (v)]α = [T ]α

α [v]α e T (v) = t0v, temos:

[t0v]α = [T ]α

α [v]α

t0In[v]α = [T ]α

α [v]α .

Equivalentemente,

(t0In − [T ]α

α )[v]α = 0

(5.6)

(ii) Considere o sistema linear AX = 0, onde A = t0In − [T ]α

α . De (i) segue que AX = 0
tem uma solução não trivial, a saber [v]α , já que v não é um vetor nulo. o que implica que A não é
α . Reciprocamente, se t0 ∈ R
invertível. Assim, P[T ]α
(t0) = 0. Portanto, o sistema linear AX = 0, onde A = t0In − [T ]α
é uma raiz de P[T ]α
α ,
tem uma solução X1 =

(t0) = 0, provando que t0 é uma raíz de P[T ]α

não nula, pois det A = 0. Como AX1 = 0,

α , então P[T ]α
(cid:16)
[x1 x2

· · · xn]t(cid:17)

α

α

(t0In − [T ]α

α )X1 = t0X1 − [T ]α

α X1 = 0

[t0v]α = t0[v]α = [T ]α

α [v]α = [T (v)]α

(5.7)

(5.8)

5.1. Operadores Diagonalizáveis

71

pois pela construção de v, X1 = [v]α . Obtemos que [T (v)]α = [t0v]α ,isto é, as coordenadas dos
vetores T (v) e t0v na base α são iguais. Consequentemente, estes vetores são iguais, ou seja,
T (v) = t0v. Como, por construção v ̸= 0, segue-se que t0 é um autovalor de T e v um autovetor
de T associado a t0.

Exemplo 36. Vamos refazer o exemplo 33, utilizando o teorema acima. Reconsidere o operador
linear T : R2 → R2 dado por T (x, y) = (4x − y, 2x + y) e seja α a base canônica de R2.

P[T ]α

α

(t) = det

(cid:32)

t − 4
−2

(cid:33)

1
t − 1

= t2 − 5t + 6t2 − 5t + 6 = 0 ⇐⇒ t1 = 2; t2 = 3

O teorema acima nos mostra que t1 e t2 são os únicos autovalores de T . Para determinarmos os
autovetores de T associados a t1, devemos resolver o sistema

ou seja,

(cid:32)

t1 − 4
−2

1
t1 − 1

(cid:33) (cid:32)

x1
x2

(cid:33)

(cid:33)

(cid:32)
0
0

=

(cid:32)

−2 1
−2 1

(cid:33) (cid:32)

x1
x2

(cid:33)

(cid:33)

(cid:32)
0
0

=

que equivale a equação −2x1 + x2 = 0 assim, o autoespaço de T associado a t1 é {(x, 2x); x ∈
R ̸= 0}. Agora vamos determinar, da mesma forma, os autovetores associados ao autovalor
t2 = 3 Assim:

(cid:32)

t2 − 4
−2

1
t2 − 1

(cid:33) (cid:32)

(cid:33)

(cid:33)

(cid:32)
0
0

=

x1
x2

ou seja,

(cid:32)

−2 1
−2 2

(cid:33) (cid:32)

x1
x2

(cid:33)

(cid:33)

(cid:32)
0
0

=

equivale à equação linear −x1 + x2 = 0 assim, o autoespaço de T associado a t2 é {(x, x); x ∈
R ̸= 0}.

O teorema a seguir é um dos mais importantes teoremas da Álgebra Linear, o chamado

Teorema de Cayley - Hamilton.

Teorema 12. Seja A ∈ Mn e seja PA(t) o polinômio característico de A. Então, PA = 0, onde 0 é
a matriz nula de M(n).

Uma consequência imediata do Teorema de Cayley - Hamilton é que a potência An, de
uma matriz A ∈ Mn, pode ser escrita como combinação linear das potências de A com expoentes
menores do que n, pois se PA(t) = tn + bn−1tn−1 + · · · + b1t + b0, então PA(A) = 0, o que equivale
a

An = −bn−1An−1 − · · · − b1A + b0In

72

Capítulo 5. Diagonalização de Operadores

Exemplo 37. Considere a matriz A =

(cid:32)

(cid:33)
3
1
−1 0

seu polinômio característico será:

PA(t) = det(tIn − A) = det

(cid:35)

(cid:34)

t − 1 −3
t − 0

1

= t2 − t + 3.

Pelo Teorema de Cayley - Hamilton, PA(A) = 0. Vamos calcular A3.

A2 − A + 3I2 = 0, ou seja, A2 = A − 3I2

A3 = AA2 = A(A − 3I2) = A2 − 3A = −2A − 3I2.

Para A4, temos A4 = AA3 = A(−2A − 3I2) = −2A2 − 3A = −2(A − 3I2) − 3A = −5A +

6I2.

Este procedimento mostra que, em geral, se A ∈ M2, então para todo m ∈ N, a matriz Am

se escreve como combinação linear de I2 e A.

Retornando ao nosso objetivo inicial, dado um operador linear T : V → V , queremos

obter, se possível, uma base α de V na qual a matriz [T ]α

α seja uma matriz diagonal.

Teorema 13. Um operador linear T : V → V admite uma base β em relação à qual a matriz [T ]β
β
é diagonal se, e somente se, essa base β for formada por autovetores de T .

Demonstração. Suponhamos que β = {v1, v2, · · · , vn} é uma base de V tal que [T ]β
digamos

β é diagonal,

[T ]β

β =









0
α1
0 α2
...
...
0
0









· · ·
0
· · ·
0
...
. . .
· · · αn.

(5.9)

para cada 1 ≤ j ≤ n; T (v j) = 0v1 + · · · + 0v j−1 + α jv j + 0v j+1 + · · · + 0vn = α jv j, segue que α j
é um autovalor de T e v j é autovetor de T associado a α j. Portanto, β é uma base formada de
autovetores de T .
Suponhamos agora que β = {u1, u2, · · · , un} é uma base de V formada por autovetores de
T . Existem, então, números reais b1, b2, · · · , bn tais que, para cada 1 ≤ j ≤ n, T (u j) = b ju j.
Observamos que os b js não são necessariamente todos distintos. Pela deﬁnição de [T ]β
β e pelo

5.1. Operadores Diagonalizáveis

fato da base ser autovetores,

[T ]β

β =

ou seja, [T ]β

β é uma matriz diagonal.









b1
0
...
0

73

(5.10)

0
b2
...
0









· · ·
0
· · ·
0
...
. . .
· · · bn

Na demonstrção do teorema acima ﬁca claro que, se o operador linear T tem uma
representação por uma matriz diagonal [T ]β
β são
dadas pelos autovalores de T . Mais ainda, a ordem em que os autovalores aparecem na diagonal
principal da matriz é a mesma em que seus respectivos autovetores são dados na base β . O
polinõmio caracteristico de T tem a forma PT (λ ) = (λ1 − λ ) · · · (λn − λ ), onde os números reais
λ1, · · · , λn são todos os autovalores de T .

β , então as entradas da diagonal principal de [T ]β

Exemplo 38. O operador linear T : R2 → R2 dado por T (x, y) = (4x − y, 2x + y) é diagonalizável.
De fato, vimos anteriormente que os autovalores de T são 2 e 3 e os autovetores associados
aos autovalores são {(x, 2x); x ∈ R, x ̸= 0} e {(x, x); x ∈ R, x ̸= 0} respectivamente, então uma
representação diagonal para T é dada por:

[T ]β

β =

(cid:32)

2 0
0 3

(cid:33)

, onde β = {(1, 2), (1, 1)}.

Uma outra representação diagonal de T é dada por:

[T ]β

β =

(cid:33)

(cid:32)

3 0
0 2

, sendo β = {(1, 1), (1, 2)}.

Deﬁnimos a multiplicidade algébrica de um autovalor λ de T , como sendo sua multiplici-
dade como raiz do polinômio característico de T . A multiplicidade geométrica de um autovalor λ
de T é, por deﬁnição, a dimensão do espaço gerado (ou autoespaço) pelos autovetores associados
a λ , o qual denotaremos por V (λ ).

Teorema 14. Sejam U um espaço vetorial de dimensão ﬁnita e T ∈ L (U): Então, T é diagona-
lizável se, e somente se, os seus autovalores λ1, · · · , λn forem tais que

U = V (λ1) ⊕ · · · ⊕V (λn)

Demonstração. Se U = V (λ1) ⊕ · · · ⊕V (λn), então podemos formar uma base B de U formada
por bases B j de V (λ j); j = 1, · · · , n. Como cada elemento de B j é um autovetor de T , segue por
deﬁnição que T é diagonalizável.Reciprocamente, se T for diagonalizável existe uma base B de
U formada por autovetores de T . Como cada autovetor está associado a algum autovalor de T ,

74

Capítulo 5. Diagonalização de Operadores

vemos que cada elemento de B está contido em algum V (λ j). Desta forma, a soma de todos os
subespaços próprios de T contém B e, portanto, é o próprio U. Logo a soma é direta, ou seja,
U = V (λ1) ⊕ · · · ⊕V (λn).

O seguinte teorema nos fornece uma forma de veriﬁcar se T é diagonalizável, conhecendo

as multiplicidades algébricas e geométricas de seus autovalores.

Teorema 15. Sejam U um espaço vetorial de dimensão ﬁnita e T ∈ L (U). Então T é diagonali-
zável se e somente se ambas condições forem veriﬁcadas

1. para cada autovalor de T as suas multiplicidades algébrica e geométrica são iguais,

2. a soma das multiplicidades geométricas de todos os autovalores de T coincide com a dimensão

de U.

Corolário 6. Sejam U um espaço vetorial de dimensão n e T ∈ L (U): Se

pT (λ ) = (λ1 − λ ) · · · (λn − λ )

onde λ1, · · · , λn ∈ R são distintos entre si então T é diagonalizável.

Demonstração. Como os autovalores de T são dois a dois distintos, vê-se que as raízes de
pT (λ ), são todas simples, isto é, têm multiplicidade um. Desta forma, se λ é um autovalor de T
então a sua multiplicidade algébrica é um. Como dimV (λ ) ≥ 1, segue-se que a multiplicidade
geométrica de λ é um, ou seja, igual à sua multiplicidade algébrica.

Exemplo 39. Vamos veriﬁcar se T : R4 → R4 dada por T (x, y, z,t) = (x + y, y, 2z + t, 2z + t) é
diagonalizável. A matriz T em relação a base canônica é dada por:

















1 1 0 0
0 1 0 0
0 0 2 1
0 0 2 1

e seu polinômio característico é:

pT (λ ) = det









λ − 1
0
0
0

1
λ − 1
0
0

0
0
λ − 2
2









0
0
1
λ − 1

= λ (λ − 3)(λ − 1)2 = 0

onde, λ = 0 ou λ = 3 ou λ = 1 são as raízes. Vamos procurar os autovetores associados aos
autovalores encontrados.

5.1. Operadores Diagonalizáveis

75

Para λ = 0, temos que v = (x, y, z,t) é autovetor associado a λ = 0 se

T (v) = λ v ⇐⇒ (x + y, y, 2z + t, 2z + t) = (0, 0, 0, 0)

ou seja, (x, y, z,t) = z(0, 0, 1, −2), com z ∈ R*. Logo, [(0, 0, 1, −2)] é o espaço gerado pelo
autovetor associado a λ = 0.

Para λ = 3, T (v) = λ v ⇐⇒ (x+y, y, 2z+t, 2z+t) = (3x, 3y, 3z, 3t), ou seja, v = (x, y, z,t) =
t(0, 0, 1, 1), com t ∈ R*. Logo, [(0, 0, 1, 1)] é o espaço gerado pelo autovetor associado a λ = 3.

Para λ = 1, T (v) = λ v ⇐⇒ (x+y, y, 2z+t, 2z+t) = (x, y, z,t) e v = (x, y, z,t) = x(1, 0, 0, 0),

com x ∈ R*. Logo, [(1, 0, 0, 0)] é o espaço gerado pelo autovetor associado a λ = 1. Como a
multiplicidade algébrica do autovalor 1 é dois e a sua multiplicidade geométrica é um, vimos
que T não é diagonalizável.

Exemplo 40. Vamos veriﬁcar se T : R3 → R3 dada por T (x, y, z) = (x + z, y + z, x + y + 2z) é

diagonalizável. Com relação à base canônica, a matriz de T é dada por





. Assim,





1 0 1
0 1 1
1 1 2

pT (λ ) = det






1 − λ
0
1

0
1 − λ
1

1
1
2 − λ




 = λ (1 − λ )(λ − 3).

Desta forma, vemos que pT (λ ) apresenta todas as raízes reais e simples e, pelo corolário 6,
segue-se que T é diagonalizável.

Vamos encontrar uma base de autovetores para este operador. Para o autovalor λ = 0,
temos que resolver T (x, y, z) = (0, 0, 0), o que nos fornece o autovetor u = (1, 1, −1). Para o
autovalor λ = 1, temos o autovetor v = (1, −1, 0) e para o autovalor λ = 3 temos o autovetor
w = (1, 1, 2).

A matriz de T com relação à base formada por u, v e w é dada por









 .

0 0 0
0 1 0
0 0 3

O próximo exemplo será utilizado nas aplicações envolvendo cônicas.

Exemplo 41. Seja T : R2 → R2 cuja matriz com relação a alguma base é dada por

A =

(cid:32)

a b
b c

(cid:33)

.

Vamos mostrar que T é diagonalizável. O polinômio característico de T é dado por

pT (λ ) = λ 2 − (a + c)λ + ac − b2

76

Capítulo 5. Diagonalização de Operadores

Vemos que pT (λ ) apresenta duas raízes reais simples, isto é, com multiplicidade um, se, e
somente se, o discriminante (a + c)2 − 4(ac − b2) for positivo.
Temos que (a + c)2 − 4(ac − b2) = a2 + c2 − 2ac + 4b2 = (a − c)2 + 4b2 > 0 se, e somente se,
a ̸= c ou b ̸= 0. Portanto, se a ̸= c ou b ̸= 0 as multiplicidades algébrica e geométrica de cada
um dos autovalores de T (as raízes de pT (λ )) coincidem e, portanto, T é diagonalizável.

Se a = c e b = 0 então vê-se claramente que T é diagonalizável pois, neste caso, A é

diagonal.

5.2 Matrizes Diagonalizáveis

Toda matriz A = (ai j) ∈ Mm×n deﬁne uma transformação linear TA : Rn → Rm deﬁnida
por TA(v) = (a11x1 + · · · + · · · + a1nxn, · · · , an1x1 + · · · + annxn), onde x1, · · · , xn são coordenadas
de v relativamente à base α = {v1, v2, . . . , vn}. Em particular, se A é uma matriz quadrada de
ordem n, então A deﬁne um operador linear TA em Rn. Dizemos que a matriz A é diagonalizável,
se o operador linear TA for diagonalizável. Desta forma, existe uma representação diagonal D,
onde D = [TA]β
β , para o operador TA com relação a alguma base β de V . Como [T ]α
α = A onde α
denota a base canônica de Rn, assim, se P = [IRn]β

α , temos:

D = P−1AP.

Teorema 16. Uma matriz A ∈ Mm×n é diagonalizável se, e somente se, existe uma matriz P
invertível de ordem n tal que P−1AP é uma matriz diagonal.

Demonstração. Consideremos β = {v1, v2, · · · , vn}, onde v j é o vetor formado pela j-ésima
coluna de P. Seja α a base canônica de Rn.

Assim,

ou, equivalentemente,

[TA]β

β = [IRn]α

β [TA]α

α [IRn]β
α ,

[TA]β

β = P−1AP

(5.11)

(5.12)

já que [IRn]β
que [TA]β

α = P pela maneira como β foi tomada. Como P−1AP é uma matriz diagonal, segue-se

β é uma matriz diagonal. Portanto, TA é diagonalizável e. assim, A também é.




Exemplo 42. Vamos veriﬁcar se a matriz A =




Seja α a base canônica no R3. Então:

2
1 0
0 1
3
0 0 −1


 é diagonalizável.

p[TA]α

α

= pA(t) = det






t − 1
0
0

0

−2
t − 1 −3
t + 1

0




 = 0

5.2. Matrizes Diagonalizáveis

77

para t = 1 ou t = −1. O autoespaço associado ao autovalor t = 1 é o conjunto solução do sistema
linear:






0 0 −2
0 0 −3
2
0 0







x

y

z








 =







0
0
0

ou seja, é o conjunto {(x, y, 0); x, y ∈ R}. O autoespaço associado ao autovalor t = −1 é o
conjunto solução do sistema linear:






−2
0 −2
0 −2 −3
0
0
0






 =










x

y

z






0
0
0

ou seja, é o conjunto {(−z,
, −1)}
temos que β é uma base de R3 formada por autovetores de TA. Assim TA é diagonalizável. A
matriz

z, z); z ∈ R}. Se considerarmos β = {(1, 1, 0), (1, 0, 0), (1,

−3
2

3
2

P =






1 1
1
3
1 0
2
0 0 −1






é uma matriz que diagonaliza A, no sentido que D = P−1AP. Assim,







D =

0

1

3
2
1 −1 −1
0 −1
0












2
1 0
3
0 1
0 0 −1












1 1

1
3
2
0 0 −1

1 0












=






0
1 0
0
0 1
0 0 −1

Observamo que o cálculo da potência m da matriz A, isto é, Am, ﬁca bastante simpliﬁcado
quando A é diagonalizável. De fato, se A ∈ Mn) e se P ∈ Mn é invertível, então é fácil veriﬁcar
que:

(P−1AP)m = P−1AmP

Logo, se A é diagonalizável e se P−1AP = D é uma matriz diagonal, temos que:

ou, equivalente,

Dm = P−1AmP,

Am = PDmP−1.

Vejamos um exemplo deste cálculo.

78

Capítulo 5. Diagonalização de Operadores

Exemplo 43. Vamos calcular a matriz A50 onde A =

(cid:33)

(cid:32)
1
2
0 −1

.

Vamos veriﬁcar que A é diagonalizável e encontrar uma matriz P que diagonaliza A. Para

isso, façamos

det(tI − A) = det

(cid:33)

(cid:32)

t − 1 −2
t + 1

0

= (t − 1)(t + 1)

que se anula para t = 1 ou t = −1. Como as raízes são simples, temos que A será diagonalizável
(cid:32)
1
0
0 −1

e a matriz diagonal correspondente será D =

(cid:33)

.

Temos que v1 = (1, 0) é um autovetor para t = 1 e v2 = (1, −1) é um autovetor para

t = −1. Assim, P =

(cid:33)

(cid:32)
1
1
0 −1

e, como D50 = I2 segue-se que

A50 = P−1D50P = P−1I2P = I2.

5.3 Teorema Espectral para operadores simétricos

Vimos que T : V → V é um operador diagonalizável se, e somente se, existe uma base de
V formada por autovetores de T . Agora, veremos que se V é um espaço com produto interno
e se T : V → V é um operador simétrico, então existe uma base ortonormal de V formada por
autovetores de T . Em particular, todo operador simétrico é diagonalizável. Este resultado é
conhecido como Teorema Espectral e é um dos resultados mais importantes da Algebra linear.

Todos os resultados que foram provados até o momento que envolvem sistemas lineares
e determinantes são válidos sobre um corpo arbitrário K. Utilizamos K = R. Neste ponto
precisaremos considerar K = C.

Dado um operador linear T : Rn → Rn podemos estendê-lo a um operador TC : Cn → Cn
do seguinte modo: se z = x + iy ∈ Cn, onde x, y ∈ Rn, deﬁne-se TC(z) = T (x) + iT (y). Os
polinômios característicos de T e de TC coincidem, mas TC pode possuir mais autovalores e
autovetores do que T .

Proposição 29. Seja V um espaço vetorial de dimensão ﬁnita sobre R. Se T : V → V é um
operador simétrico e α uma base de V , então, todas as raízes do polinômio característico P[T ]α
em C são números reais.

α

Teorema 17. (Teorema Espectral) Seja V um espaço vetorial de dimensão ﬁnita sobre R. Se
T : V → V é um operador simétrico, então existe uma base ortonormal β de V tal que [T ]β
β é
diagonal.

5.3. Teorema Espectral para operadores simétricos

79

Demonstração. A prova será feita por indução sobre a dimensão de V . Denotaremos a matriz
[T ]α
α por A. Se dimV = 1, o resultado é óbvio. Suponhamos que n ≥ 1 e que o resultado é valido
para espaços de dimensão n. Seja V um espaço vetorial tal que dimV = n + 1. Seja α uma base
de V e seja c uma raiz complexa do polinômio PA.Pela proposição acima c ∈ R. Portanto é um
autovalor de T . Seja v um autovetor unitário de T associado a c. Consideremos os subespaços.

W = {w ∈ V ; < w, v >= 0}

Note que W = G(v)⊥. Aﬁrmamos que T (W ) ⊂ W . De fato, seja w ∈ W . Como T é um operador
simétrico, temos que: < T (w), v >=< w, T (v) >=< w, cv >= c < w, v >= c 0 = 0, donde T (w) ∈
W . Assim podemos considerar o operador restrição.

S = T |W ∈ L (W,W )

que é também um operador simétrico. Além disso, como dim G(v) = 1, segue que dimW = n.
Assim podemos aplicar a hipótese de indução ao operador S para garantir a existência de
uma base ortonormal {v1, v2, · · · , vn} de W formada por autovetores de S. Consequentemente,
β = {v, v1, · · · , vn} é uma base ortonormal de V formada por autovetores de T . Daí, [T ]β
β é
diagonal.

Teorema 18. Se A ∈ Mn é simétrica, então existe uma matriz ortogonal P ∈ Mn tal que P−1AP =
(PtAP) é diagonal.

Demonstração. Seja A ∈ Mn uma matriz simétrica. Então o operador TA ∈ L (Rn, Rn) também
é simétrico. Pelo Teorema Espectral, existe uma base ortonormal β de Rn tal que [TA]β
β = D é
diagonal. Se α é base canônica de Rn, então:

D = [TA]β

β = [IRn]α

β [TA]α

α [IRn]β

α = P−1AP

sendo P = [IRn]β
seja, P−1 = Pt

α . Como α e β são bases ortonormais, segue que P é uma matriz ortonormal, ou

81

CAPÍTULO

6

CÔNICAS

De maneira geral, uma cônica é o conjunto de pontos P = (x, y) no plano tais que

Ax2 + Bxy +Cy2 + Dx + Ey + F = 0

onde A, B,C, D, E, F ∈ R, com A, B,C não simultaneamente nulos. Esse capítulo se propõe ao
estudo das cônicas não degeneradas, os elementos principais e características gerais. Também
haverá uma breve discussão sobre cônicas degeneradas. A bibliograﬁa usada para este capítulo
pode ser encontrada em (FRENSEL K.; DELGADO, 2011), (HEFEZ A.; SOUZA FERNANDES,
2012) e (GOMEZ KATIA R. FRENSEL, 2017).

Elipse

Deﬁnição 30. Uma elipse, E , de focos F1 e F2, é o conjunto do plano que consiste de todos os
pontos P cuja soma das distâncias a F1 e F2 é igual a uma constante 2a > 0, maior do que a
distância entre os focos 2c (cid:62) 0. Ou seja.

E = {Pd(P, F1) + d(P, F2) = 2a}

0 ≤ c < a; d(F1, F2) = 2c

Figura 7 – Elipse

82

Capítulo 6. Cônicas

Os pontos F1 e F2 chamam-se Focos e a medida do segmento F1F2 é chamada de distância
focal que será representada por 2c. A reta a qual os focos pertencem é um eixo de simetria
da curva que intercepta a elipse nos pontos A1 e A2 (que chamam-se de vértices da elipse). O
segmento A1A2 é chamado eixo maior da elipse. No ponto médio dos focos se encontra o centro
da elipse e passando uma reta perpendicular por ele, tem-se outro eixo de simetria da curva. Esse
eixo intercepta a elipse nos pontos B1 e B2. O segmento determinado por esses pontos é chamado
eixo menor e sua medida será representado por 2b. Do triângulo retângulo formado decorre a
relação a2 = b2 + c2 e, portanto, sempre se tem a > b.

Figura 8 – Elipse

Chama-se de excentricidade ao quociente entre as distâncias entre focos e a distância

entre vértices, isto é

e =

d(F1, F2)
d(A1, A2

.

Observa-se que, da deﬁnição, para a elipse e < 1. Considere A1 e A2 pontos onde a elipse
intercepta o eixo maior. Observa-se que d(A1, A2) = 2a. De fato, sejam

d(A1, F1) = x, d(F1, F2) = 2c e d(F2, A2) = y.

Como o A1 está na elipse tem-se que, da equação,

d(A1, F1) + d(A1, F2q) = 2a.

Utilizando d(A1, F2) = d(A1, F1) + d(F1, F2) tira-se que 2x + 2c = 2a. Analogamente, tem-se
2y + 2c = 2a. Subtraindo estas duas equações, vê-se que x = y, ou seja, d(A1, F1) = d(A2, F2).
Logo,

d(A1, A2) = d(A1, F1) + d(F1, F2) + d(F2, A2) = x + 2c + y = 2a.

Exemplo 44. Os vértices de uma elipse são os pontos (4, 0) e (−4, 0), e seus focos são os
pontos (3, 0) e (−3, 0). Vamos determinar a equação da elipse. Como F1 = (−3, 0) e F2 =
(3, 0), a reta focal é o eixo −OX, A1 = (−4, 0) e A2 = (4, 0) são os vértices sobre a reta focal
= (0, 0) é o centro da elipse, a = d(C, A1) = d(C, A2) = 4,
(cid:96). Então, C =

√

√

√

a2 − c2 =

42 − 32 =

7 Logo, a equação da elipse é

f1 + f2
2
c = d(C, F1) = d(C, F2) = 3 e b =
E :

A1 + A2
2

= 1

=

+

x2
16

y2
7

83

Exemplo 45. A equação de uma elipse é

E : x2 + 4y2 + 2x − 12y + 6 = 0.

Vamos determinar a equação da elipse E na forma canônica, o seu centro, os seus vértices, os
seus focos e a sua excentricidade.

Completando os quadrados na equação de E, temos:

E : (x2 + 2x) + 4(y2 − 3y) = −6

E : (x2 + 2x + 1)4(y2 − 3y +

9
4

) = −6 + 1 + 4 ·

9
4

= 4

E : (x + 1)2 + 4(y −

)2 = 4

3
2

E :

(x + 1)2
4

+ (y −

)2 = 1.

3
2

2 ), a = 2, b = 1 e, portanto, c2 = a2 − b2 = 22 − 12 = 3, ou seja c =

sendo esta última equação a forma canônica de E . Dessa equação obtemos que o centro da elipse
é C = (−1, 3
3. A reta focal
de E é (cid:96) : y = 3
2, paralela ao eixo OX, e a reta não-focal é a reta vertical (cid:96)′ : x = −1, paralela ao
); os vértices sobre a
1
2

) e os vértices sobre a reta não-focal são B1 = (−1,

eixo −OY . Os focos da elipse são F1 = −1 −

reta focal são A1 = (−3,

) e F2 = (−1 +

) e A2 = (1,

3
2

3
2

3
2

3
2

√

√

√

3,

3,

)

e B2 = (−1, 5

2 )4 e a excentricidade de E é e =

√
3
2

.

Exemplo 46. Vamos determinar se a equação 25x2 + 9y2 − 225 = 0 representa uma elipse ou
uma elipse degenerada. Caso seja uma elipse, vamos determinar seus principais elementos.

x2
9

+

y2
25

= 1representa

Como 25x2 + 9y2 = 225, dividindo por 225, obtemos a equação

uma elipse com:

∙ a = 5, b = 3 e c == 4.

∙ centro: C = (0, 0).

∙ reta focal: (cid:96) = eixo − OY : x = 0.

∙ reta não-focal: (cid:96)′ = eixo − OX : y = 0.

∙ vértices sobre a reta focal: A1 = (0, −5) e A2 = (0, 5).

∙ vértices sobre a reta não-focal: B1 = (−3, 0) e B2 = (3, 0).

∙ focos: F1 = (0, −4) e F2 = (0, 4).

84

Hipérbole

Capítulo 6. Cônicas

Deﬁnição 31. Uma hipérbole, H , de focos F1 e F2, é o conjunto do plano que consiste de todos
os pontos P tais que o módulo da diferença das distâncias de F1 a F2 é igual a uma constante
2a > 0, menor do que a distância entre os focos 2c (cid:62) 0.

H = {P/d(P, F1) − d(P, F2)| = 2a}

0 (cid:54) a < c; d(F − 1, F2) = 2c

Figura 9 – Hipérbole

Os pontos F1 e F2 chamam-se Focos e a medida do segmento F1F2 é chamada de distância
focal, que será representada por 2c. A reta na qual os focos estão localizados é um eixo de simetria
da curva que intercepta a hipérbole nos pontos A1 e A2, chamados de vértices. A distância entre
os vértices será representada por 2a. O segmento A1A2 é chamado eixo real da hipérbole. No
ponto médio dos focos encontra-se o centro da hipérbole, e passando uma reta perpendicular por
ele, tem-se o outro eixo de simetria da curva. Esse eixo intercepta a hipérbole nos pontos B1 e
B2. O segmento determinado por esses pontos é chamado eixo conjugado (ou imaginário) e será
representado com medida 2b.

Figura 10 – Hipérbole

Do triângulo retângulo formado obtém-se a relação c2 = a2 + b2. Chama-se de excentri-

cidade o quociente entre as distâncias focais e as distâncias entres os vértices, isto é

e =

d(F1, F2)
d(A1, A2

.

Observa-se que, da deﬁnição, para a hipérbole e > 1. Considere A1 e A2 pontos onde a hipérbole
intercepta o eixo real. Observa-se que d(A1, A2) = 2a. De fato, sejam

d(A1, F1) = x, d(F1, F2) = 2c e d(F2, A2) = y.

85

Como o A1 está na hipérbole, tem-se da equação,

d(A1, F1) − d(A2, F2) = 2a.

Utilizando que d(F1, F2) = d(F1, A1) + d(A1, A2) tem-se que

2c = 2x + 2a.

Analogamente prova-se que 2c = 2y − 2a. Subtraindo estas duas equações se vê que x = y, ou
seja, d(F1, A1) = d(A2, F2). Logo,

d(A1, A2) = d(F1, F2) − [d(F1, A1) + d(A2, F2)] = 2c − [x − y] = 2c − 2x = 2a.

8, 0) e (
F1 + f2
2

√

√

√

Exemplo 47. Vamos determinar a equação da hipérbole equilátera com focos nos pontos
8, 0), temos que o centro da hipérbole
(−

8, 0). Como F1 = (−
= (0, 0) e a reta focal é o eixo −OX. Sendo a hipérbole equilátera, temos a = b.

8, 0) e F2 = (

é C =

√

√

8 e c2 + a2 = b2, obtemos que a2 = 4. Logo, a = b = 2 e H :

Como c =
= 1, é a
equação da hipérbole. Além disso, A1 = (−2, 0) e A2 = (2, 0) são os vértices, B1 = (0, −2) e
B2 = (0, 2) são os vértices imaginários e x = ±y são as assíntotas da hipérbole H .

−

x2
4

y2
4

Exemplo 48. Vamos mostrar que a excentricidade de qualquer hipérbole equilátera é

√

2.

Como a = b e c2 = a2 + b2, temos que c2 = 2a2, ou seja, c =
√

2.

√

2a
a

=

√

2a. Logo, e =

c
a

=

Exemplo 49. Vamos determinar se a equação 9x2 − 25y2 − 225 = 0 representa uma hipérbole
ou uma hipérbole degenerada. Caso seja uma hipérbole, vamos determinar seus principais
elementos.

x2
25

−

y2
9

= 1, que

Como 9x2 − 25y2 = 225, dividindo por 225, encontramos a equação

representa uma hipérbole com

∙ a = 5, b = 3 e c =

√

34.

∙ centro: C = (0, 0).

∙ reta focal: (cid:96) = eixo − OX : y = 0.

∙ reta não-focal: (cid:96) = eixo − OY : x = 0.

86

Capítulo 6. Cônicas

∙ vértices: A1 = (−5, 0) e A2 = (5, 0).

∙ vértices imaginários (na reta não-focal): B1 = (0, −3) e B2 = (0, 3).
√

√

∙ focos: F1 = (−

34, 0) e F2 = (

34, 0).

∙ assíntotas: y = ±

3
5

x, ou seja 3x ± 5y = 0.

Parábola

Deﬁnição 32. Sejam L uma reta no plano e F um ponto no plano não pertencente a L . A
parábola P de diretriz L e foco F é o conjunto que consiste de todos os pontos P do plano que
são equidistantes do ponto F e da reta L .

P = {P/d(P, F) = d(P, L )}

Toma-se também um número positivo 4p, chamado de parâmetro que será a distância
entre o foco e a diretriz. Na parábola se encontra o vértice no ponto médio do foco e da interseção
da diretriz com o eixo da parábola.

Figura 11 – Parábola

Exemplo 50. Considere uma parábola que no sistema catesiano tem seu foco no eixo das
abscissas, isto é, F(p, 0) e a diretriz de equação x = −p. Deduz-se a equação reduzida da
parábola nesse caso partindo da deﬁnição:

d(P, F) = d(P, −q)

(cid:113)

(x − p)2 + y2 = ‖x + p‖

(x − p)2 + y2 = (x + p)2

x2 − 2px + y2 = x2 + 2px + p2

−2px + y2 = 2px

y2 = 4px.

87

Nessas condições, se F está a direita de V , a equação da parábola será da forma

y2 = 4px,

e, se F está à esquerda de V , a equação da parábola será da forma

y2 = −4px,

De modo análogo, obtém-se a equação da parábola quando o vértice está na origem do sistema e
o foco no eixo das ordenadas. Nesse caso tem-se

quando F está acima de V e,

quando F está abaixo de V .

x2 = 4py,

x2 = −4py,

Figura 12 – y2 = 4px

Figura 13 – x2 = −4px

Se a parábola não estiver com seu vértice localizado na origem do sistema mas se o eixo

que contém o foco for paralelo a abscissa tem-se a equação

(y − y0)2 = 2p(x − x0).

E se o eixo que contém o foco estiver paralelo ao eixo da ordenada tem-se a equação

(x − x0)2 = 2p(y − y0)

88

Capítulo 6. Cônicas

Exemplo 51. Vamos determinar a equação da parábola P com vértice V na origem, cujo foco é
F = (3, 0). Temos p = d(V, F) = 3 e reta focal = eixo −OX. Como o foco F está à direita do
vértice, temos que a diretriz é L : x = −3 e a equação da parábola é P : y2 = 12x.

Exemplo 52. Um círculo C com centro no ponto C = (4, −1) passa pelo foco F da parábola
P : x2 = −16y. Mostre que C é tangente à diretriz L de P.

A reta focal da parábola P é o eixo −OY , o vértice é a origem, e o foco está abaixo da
diretriz. Então, F = (0, −4) e L : y = 4, pois 4p = 16. A equação do círculo é C : (x − 4)2 +
(y + 1)2 = r2. Sendo F = (0, −4) ∈ C, temos r = 5. Logo L é tangente a C, pois d(C, L ) =
d((4, −1), L ) = | − 1 − 4| = 5 = raio de C.

Figura 14 – Parabola P e círculo C

Exemplo 53. Vamos veriﬁcar se a equação x2 − 8y = 0 representa uma parábola ou uma parábola
degenerada. Caso seja uma parábola, vamos determinar seus principais elementos.

Como x2 = 8y, a equação representa uma parábola, com

∙ vértice: V = (0, 0).

∙ reta focal = eixo −OY : x = 0.

∙ parâmetro: p = 2.

∙ foco: F = (0, 2), acima da diretriz.

∙ diretriz: L : y = −2.

Cônicas Degeneradas

Como se vê, a elipse, a hipérbole e a parábola têm equações que serão representadas na
forma Ax2 + Bxy +Cy2 + Dx + Ey + F = 0. Entretanto, nem toda equação dessa forma, representa
uma das curvas já citadas. Há equações que representam uma única solução, ou seja, um ponto;
ou ainda, um par de retas. Essas representações são as cônicas degeneradas, que se verá a seguir.

1. Par de retas: o conjunto solução de uma equação do segundo grau que pode ser fatorada na forma
(a1x+b1y+c1)(a2x+b2y+c2) = 0, onde a1, a2, b1, b2, c1, c2 são reais e a1 ̸= 0 ou b1 ̸= 0, a2 ̸= 0
ou b2 ̸= 0 representa um par de retas, podendo ser paralelas, concorrentes ou mesmo coincidentes.

6.1. Reconhecimento de Cônicas

89

Exemplo 54. A equação x2 + 2xy + y2 − 1 = 0 representa um par de retas paralelas, pois pode
ser fatorada como (x + y + 1)(x + y − 1) = 0.

2. Um ponto: o conjunto solução de uma equação do segundo grau que pode ser escrito na forma
k1(x − x0)2 + (y − y0)2 = 0 com k1 ̸= 0 e k2 ̸= 0 representa um ponto, pois só o ponto (x0, y0)
satisfaz essa equação.

Exemplo 55. A equação x2 + y2 = 0 representa um ponto, o (0, 0).

6.1 Reconhecimento de Cônicas

Agora mostraremos como por meio de Teorema Espectral é possível fazer o reconheci-

mento de cônicas. Considere a equageral do segundo grau nas duas variáveis x e y.

ax2 + bxy + cy2 + dx + ey + f = 0

onde a, b, c, d, e e f são números reais dados. É possível mostrar que a equação acima representa
uma cônica ou uma reta ou duas retas ou um ponto ou nenhum lugar geométrico em R2. Vejamos
alguns exemplos

Exemplo 56. Vejamos que lugar geométrico em R2 cada uma das equações abaixo representa.

1. x2 + y2 + 1 = 0,

2. 2x2 + 4y2 = 0,

3. x2 − 9 = 0,

4. 4x2 + 9y2 − 8x − 36y + 4 = 0,

5. y2 + 6y2 − 8x + 1 = 0

1. Está equação representa nenhum lugar geométrico em R2, pois:

{(x, y) ∈ R2; x2 + y2 + 1 = 0} = {(x, y) ∈ R2; x2 + y2 = −1} = /0

2. Esta equação representa a origem do plano cartesiano, pois:

2x2 + 4y2 = 0

equivale à equação x2 = −2y2, que é veriﬁcada somente se x = y = 0;

3. Esta equação depresenta duas retas do R2.Mais precisamente, as retas x = 3 e x = −3;

90

Capítulo 6. Cônicas

4. Esta equação representa uma elipse.De fato, (x, y) ∈ R2 tal que 4x2 + 9y2 − 8x − 36y + 4 = 0 que

é equivalente à

4(x2 − 2x) + 9(y2 − 4y) = −4

Completando os quadrados temos:

ou seja

4(x − 1)2 + 9(y − 2)2 = 36

(x − 1)2
9

+

(y − 2)2
4

= 1

que é a equação reduzida da elipse de centro (1, 2) e eixo maior e menor medindo 6 e 4
respectivamente.

Figura 15 – A elipse

(x − 1)2
9

+

(x − 2)2
4

= 1

5. Esta equação representa uma parabola. De fato, (x, y) ∈ R2 tal que y2 + 6y − 8x + 1 = 0.Que é

equivalente à:

Completando os quadrados

(y2 + 6y) = 8x − 1

(y + 3)2 = 8(x + 1)

que é a equação reduzida da parabola de vértice (−1, −3) e parâmetro 2.

6.1. Reconhecimento de Cônicas

91

Figura 16 – {(x, y) ∈ R2 : (y + 3)2 = 8(x + 1)}

Observamos que em todos os exemplos anteriores o termo xy, o chamado termo misto
da equação não aparece. A técnica usualmente utilizada nestes casos é a técnica de completar
quadrados. Porém em equações em que o termo misto aparece, precisamos de uma algebra mais
avançada para reduzirmos a equação dada.

Exemplo 57. Vamos determinar o lugar geométrico em R2 representado pela equação:

2x2 + 2xy + 2y2 + 7

2x + 5

√

√

2y + 10 = 0?

Para respondermos a esta pergunta vamos usar o Teorema Espectral. Primeiramente a equação
acima equivale a equação matricial.

(cid:16)

x y

(cid:17)

(cid:32)

2 1
1 2

(cid:33)

(cid:33) (cid:32)
x
y

(cid:16)
7

+

√

2 5

(cid:17)

√
2

(cid:33)

(cid:32)
x
y

+

(cid:16)

(cid:17)

(cid:17)
(cid:16)
0

=

10

Sendo A =

(cid:32)

2 1
1 2

(cid:33)

. Como A é uma matriz simétrica e pelo Teorema Espectral, A é

ortogonalmente diagonalizável.

Assim vamos encontrar os autovalores.

O polinômio característico:

det(tI2 − A) = det

(cid:33)

(cid:32)

t − 2 −1
t − 2
−1

= (t − 2)2 − 1 = t2 − 4t + 3 =⇒ t1 = 3 t2 = 1

Agora os autovetores associados aos autovalores t1 e t2

t1 = 3 =⇒

(cid:32)

1 −1
−1
1

(cid:33) (cid:32)
x
y

(cid:33)

(cid:33)
(cid:32)
0
0

=

=⇒ x = y

t2 = 1 =⇒

(cid:32)

−1 −1
−1 −1

(cid:33)

(cid:33) (cid:32)
x
y

=

(cid:33)

(cid:32)
0
0

=⇒ x = −y

92

Assim os vetores

sendo x =

(cid:18)

(cid:19)

unitários v1 =

1
√
2

Capítulo 6. Cônicas

(cid:19)

(cid:18) 1
√
2

,

1
√
2

e o vetor unitário v2 =

(cid:19)

(cid:18) −1
√
2

,

1
√
2

são autovetores de t1 e t2, respectivamente.

Assim β = {v1, v2} é uma base ortonormal de R2 formada por autovetores de TA. Seja
α onde α é a base canônica de R2. Chame D = P−1AP.

P = [IR2]β

Temos: P =

(cid:32) 1√
2
1√
2

(cid:33)

− 1√
2
1√
2

e D =

(cid:32)

3 0
0 1

(cid:33)

.

Como A = PDPt, já que P−1 = Pt, segue que

(cid:16)

x y

(cid:17)

P

(cid:32)

3 0
0 1

(cid:33)

Pt

(cid:33)

(cid:32)
x
y

(cid:16)
7

+

√

2 5

√
(cid:17)
2

(cid:33)

(cid:32)
x
y

(cid:16)

10

(cid:17)

(cid:17)

(cid:16)
0

=

+

.

Observamos que Pt

Chamaremos [v]β de

(cid:33)

(cid:32)
x
y
(cid:32)

x′
y′

= [IR2]α

β vα .

(cid:33)

. Substituindo na equação acima obtemos

x′ y′(cid:17)
(cid:16)

P

(cid:32)

3 0
0 1

(cid:33)

Pt

(cid:33)

(cid:32)

x′
y′

(cid:16)
7

+

√

2 5

√
(cid:17)
2

(cid:32) 1√
2
1√
2

(cid:33)

+

(cid:33) (cid:32)

x′
y′

− 1√
2
1√
2

ou seja

3x′2 + y′2 + 12x′ − 2y′ + 10 = 0

(cid:16)

(cid:17)

(cid:17)
(cid:16)
0

=

10

Com a mudança da base canônica α para a base β , reduzimos a primeira equação a última
equação, que não apresenta termos mistos x′′y′

Agora vamos reduzir a ultima equação completando os quadrados

equivale a equação

ou seja

3x′2 + y′2 + 12x′ − 2y′ + 10 = 0

3(x′ + 2)2 + (y′ − 1)2 = 3

(x′ + 2)2 +

(y′ − 1)2
3

= 1

Portanto a primeira equação representa uma elipse. para esboçarmos o gáﬁco dessa elipse,
precisamos considerar as novas coordenadas x′ e y′. Assim nesse sistema de coordenadas, a
elipse tem centro (−2, 1), semi-eixo menor medindo 1 e semi-eixo maior medindo
3, sendo
este semi-eixo paralelo ao eixo y′.

√

6.1. Reconhecimento de Cônicas

93

Figura 17 – Elipse (x′ + 2)2 +

(y′ − 1)2
3

= 1

Exemplo 58. Considere a seguinte equação quadratica

x2 − 6xy − 7y2 + 10x + 2y + 9 = 0.

A matriz associada a forma quadrática é

A =

(cid:32)

1 −3
−3 −7

(cid:33)

.

Os autovalores t1 e t2 podem ser encontrados calculando as raízes do polinômio característico,
que neste caso é dado por

(t − 1)(t + 7)(3)2 = t2 + 6t − 17 =⇒ t1 = −8,

t2 = 2.

Agora os autovalores associados.

t1 = −8 =⇒

(cid:32)

1 −3
−3 −7

(cid:33)

(cid:33) (cid:32)
x
y

= −8

(cid:33)

(cid:32)
x
y

⇐⇒




x − y = −8x



−3x − 7y = −8y

=⇒ x = x,

y = 3x.

t2 = 2 =⇒

(cid:32)

1 −3
−3 −7

(cid:33)

(cid:33) (cid:32)
x
y

= 2

(cid:33)

(cid:32)
x
y

⇐⇒




x − y = 2x



−3x − 7y = 2y

=⇒ x = x,

y = −

1
3

x.

Portanto os autovetores s e r associados respectivamente a t1 e t2, são dados por

(cid:34)

s =

x
3x

(cid:35)

(cid:34)

, r =

(cid:35)

.

x
− 1
3 x

Para x = 1, temos que os autovetores normalizados associados respectivamente a t1 e t2 são

v1 =

(cid:35)

(cid:34) √
10
10
√
3

10

10

, v2 =

(cid:34) 3

√

10

(cid:35)

10
√

10
10

−

.

94

Capítulo 6. Cônicas

Assim β = {v1, v2} é uma base ortonormal de R2 formada por autovetores de TA. Seja P = [IR2]β
α
onde α é a base canônica de R2. Chame A = P−1DP. Então:

A matriz P que diagonaliza A ortogonalmente é

P =

(cid:32) √
10
10
√
10
10 −

3

√
3

10

10
√

10
10

(cid:33)

.

Sendo D = P−1AP, temos que D =

(cid:32)

(cid:33)

−8 0
2
0

. Como A = P−1DP e P−1 = Pt, temos

x′ y′(cid:17)
(cid:16)

(cid:32)

(cid:33) (cid:32)

−8 0
2
0

x′
y′

(cid:33)

(cid:16)

10 2

(cid:17)

=

√
3

10

10
√

10
10

(cid:32) √
10
10
√
10
10 −
√

3

√

(cid:33)

(cid:33) (cid:32)

x′
y′

(cid:17)
(cid:16)
9

+

= 0

=⇒ −8x′2 + 2y′2 +

10
5
Com a mudança da base canônica α para a base β , reduzimos a primeira equação a última
equação, que não apresenta termos mistos xy′.

y′ + 9 = 0

x′ +

10

14

5

8

−8x”2 + 2y”2 = 0 ⇐⇒ y = ±4x.

Neste caso a cônica é um par de retas. Como representado na ﬁgura 18.

Figura 18 – Hiperbole - H = 3x2 − 4

√

3xy − y2 + 20y = 25.

Exemplo 59. Considere a seguinte equação quadratica

3x2 − 4

√

3xy − y2 + 20y = 25.

6.1. Reconhecimento de Cônicas

95

A matriz associada a forma quadrática é

(cid:32)

A =

√
(cid:33)
3

.

3
√
−2

−2
3 −1

Os autovalores t1 e t2 podem ser encontrados calculando as raízes do polinômio característico,
que neste caso é dado por

(t − 3)(t + 1)(−2

√

3)2 = t2 − 2t − 15 =⇒ t1 = −3,

t2 = 5.

Agora os autovalores associados.

t1 = −3 =⇒

(cid:32)

2

3 −2
√

3 −1

√
(cid:33) (cid:32)
x
3
y

(cid:33)

= −3

(cid:33)

(cid:32)
x
y

⇐⇒




3x − 2
√

√

3y = −3x



−2

3x − y = −3y

=⇒ x = x, y =

√

3x.

t2 = 5 =⇒

(cid:32)

√
3

3 −2
√
2

3 −1

(cid:33)

(cid:33) (cid:32)
x
y

= 5

(cid:33)

(cid:32)
x
y

⇐⇒




3x − 2
√

√

3y = 5x



−2

3x − y = 5y

=⇒ x = x, y = −

√
3
3

x.

Portanto os autovetores s e r associados respectivamente a t1 e t2, são dados por

(cid:34)

s =

x
√
3x

(cid:35)

(cid:34)

, r =

(cid:35)

.

x
√
3
3 x

−

Para x = 1, temos que os autovetores normalizados associados respectivamente a t1 e t2 são
(cid:35)

(cid:35)

v1 =

, v2 =

.

(cid:34) 1
2√
3
2

(cid:34) √
3
2
− 1
2

Assim β = {v1, v2} é uma base ortonormal de R2 formada por autovetores de TQ. Seja P = [IR2]β
α
onde α é a base canônica de R2. Chame A = P−1DP. Então A matriz P que diagonaliza A
ortogonalmente é

(cid:33)

.

√
(cid:32) 1
3
2
2
√
3
2 − 1
2
(cid:33)

P =

(cid:32)

−3 0
5
0

Sendo D = P−1AP, temos que D =

. Como A = P−1DP e P−1 = Pt, temos

(cid:16)
x′ y′(cid:17)

(cid:32)

(cid:33) (cid:32)

−3 0
5
0

(cid:33)

(cid:16)

0 20

=

(cid:17)

√
(cid:32) 1
3
2
2
√
3
2 − 1
2

(cid:33)

+

(cid:33) (cid:32)

x′
y′

x′
y′′

(cid:16)

(cid:17)

−25

= 0

=⇒ −3x′2 + 5y′2 + 10

3x′ − 10y′ − 25 = 0

√

Com a mudança da base canônica α para a base β , reduzimos a primeira equação a última
equação, que não apresenta termos mistos xy′.

−3x”2 + 5y”2 = 5 ⇐⇒ −

x”2
(cid:18)(cid:113) 5

3

(cid:19)2 +

y”2
12 = 1.

96

Capítulo 6. Cônicas

Neste caso a cônica é uma hipérbole cuja a diretriz é paralela ao eixo x”. Como representado na
ﬁgura 19.

Figura 19 – Par de retas: x2 − 6xy − 7y2 + 10x + 2y + 9 = 0.

Exemplo 60. Considere a seguinte equação quadratica

x2 + 2xy + y2 + −2x + 2y + 3 = 0.

A matriz associada a forma quadrática é

A =

(cid:32)

1 1
1 1

(cid:33)

.

Os autovalores t1 e t2 podem ser encontrados calculando as raízes do polinômio característico,
que neste caso é dado por

(t − 1)(t − 1) − 12 = t2 − 2t =⇒ t1 = 2,

t2 = 0.

Agora os autovalores associados.

t1 = 2 =⇒

(cid:32)

1 1
1 1

(cid:33)

(cid:33) (cid:32)
x
y

= 2

(cid:33)

(cid:32)
x
y

⇐⇒




x + y = 2x



x + y = 2y

t2 = 0 =⇒

(cid:32)

1 1
1 1

(cid:33)

(cid:33) (cid:32)
x
y

= 0

(cid:33)

(cid:32)
x
y

⇐⇒




x + y = 0



x + y = 0

=⇒ x = x, y = x.

=⇒ x = x, y = −x.

Portanto os autovetores s e r associados respectivamente a t1 e t2, são dados por

r =

(cid:34)

x
x

(cid:35)

(cid:34)

, r =

(cid:35)

.

x
−x

6.1. Reconhecimento de Cônicas

97

Para x = 1, temos que os autovetores normalizados associados respectivamente a t1 e t2 são

v1 =

(cid:34) 1√
2
1√
2

(cid:35)

(cid:34)

, v2 =

(cid:35)

.

1√
2
− 1√
2

Assim β = {v1, v2} é uma base ortonormal de R2 formada por autovetores de TQ. Seja P = [IR2]β
α
onde α é a base canônica de R2. Chame A = P−1DP. Então A matriz P que diagonaliza A
ortogonalmente é

P =

(cid:32) 1√
2
1√
2

(cid:33)

.

1√
2
− 1√
2

Sendo D = P−1AP, temos que D =

(cid:32)

2 0
0 0

(cid:33)

. Como A = P−1DP e P−1 = Pt, temos

x′ y′(cid:17)
(cid:16)

(cid:33)

(cid:32)

2 0
0 0

(cid:33) (cid:32)

x′
y′′

(cid:16)

−2 2

=

(cid:17)

(cid:32) 1√
2
1√
2

(cid:33)

(cid:33) (cid:32)

x′
y′

1√
2
− 1√
2

(cid:17)
(cid:16)
3

+

= 0

=⇒ −3x′2 + 5y′2 + 10

3x′ − 10y′ − 25 = 0

√

Com a mudança da base canônica α para a base β , reduzimos a primeira equação a última
equação, que não apresenta termos mistos xy′.

= y”2 ⇐⇒ x”2 =

√

2y”.

x”2
√
2

Neste caso a cônica é uma hipérbole cuja a diretriz é paralela ao eixo x”. Como representado na
ﬁgura 20.

Figura 20 – Parábola P : x2 + 2xy + y2 − 2x + 2y + 3 = 0.

99

CAPÍTULO

7

SISTEMAS DE EQUAÇÕES DIFERENCIAIS
ORDINÁRIAS

Neste capítulo, introduziremos o conceito de sistemas de equações diferenciais ordinárias
lineares e, utilizando os resultados de diagonalização de operadores, exibiremos uma forma de
encontrar as soluções de tais sistemas quando a matriz é diagonalizável. A bibliograﬁa usada
para este capítulo pode ser encontrada em (LADEIRA L. A. C.; JUNIOR, 2011), (LADEIRA,
2004) e (BOYCE, 2002)

Primeiramente, vamos descrever o que é uma equação diferencial ordinária. Podemos
dizer, sem muito formalismo, que uma equação diferencial é uma relação que envolve uma
"função incógnita"e suas derivadas ou diferenciais. Assim, são exemplos de equações diferenciais

∙ ˙y(t) = f (t), onde ˙y(t) =

dy
dt

,

∙ ¨y(t) + y(t) = 0,

∙ M(x, y)dx + N(x, y)dy = 0.

Deﬁnição 33. Uma equação diferencial ordinária (E.D.O.) é uma equação diferencial na qual a
função incógnita depende apenas de uma variável.

As equações do exemplo acima são equações diferenciais ordinárias. A ordem de uma
equação diferencial é a ordem da mais alta derivada da função incógnita. Uma solução de uma
equação diferencial é uma função deﬁnida num intervalo que, juntamente com suas derivadas,
satisfaz a equação diferencial dada. Por exemplo, a função y(t) = sint é uma solução da E.D.O.

de segunda ordem ¨y + y = 0, pois

d2 sint
dt2 + sint = − sint + sint = 0.

100

Capítulo 7. Sistemas de Equações Diferenciais Ordinárias

Um sistema de equações diferenciais ordinárias de primeira ordem pode, geralmente, ser

escrito sob a forma:



˙x1 = F1(t, x1, x2, · · · , xn)


˙x2 = F2(t, x1, x2, · · · , xn)
...

˙xn = Fn(t, x1, x2, · · · , xn)

(7.1)

Uma solução de um sistema de equações diferenciais em um intervalo J é constituída
por n funções x1(t), x2(t), · · · , xn(t) tais que, cada uma destas funções são diferenciáveis em J e
satisfazem o sistema (7.1) para todo t ∈ J.

Sistemas de equações diferenciais ocorrem em muitas aplicações como circuitos elétricos,
mistura química de vários ingredientes, crescimento de duas ou mais populações interadas,
vibrações de estruturas, etc.

Como exemplo, podemos citar o par de funções x1(t) = sint e x2(t) = cost como solução

do sistema




˙x1 = x2,



˙x2 = −x1.

Um problema de valor inicial (PVI) para uma equação diferencial ordinária pode ser

colocado da seguinte maneira:




˙y(t) = f (t, y(t))



y(t0) = y0.

Um PVI para um sistema de equações diferenciais de primeira ordem é dado por:






˙x1 = F1(t, x1, x2, · · · , xn)

˙x2 = F2(t, x1, x2, · · · , xn)
...
˙xn = Fn(t, x1, x2, · · · , xn)
1, x2(t0) = x0
x1(t0) = x0

2, · · · , xn(t0) = x0
n

(7.2)

(7.3)

onde x0

1, x0

2, · · · , x0

n ∈ R são as condições iniciais do sistema.

Existem questões fundamentais a serem respondidas referentes a um PVI de uma equação
diferencial ordinária e, consequentemente, a um sistema de equações diferenciais ordinárias com

101

condições iniciais, como, por exemplo, se a equação tem solução e, se esse é o caso, se a solução
encontrada é única.

Vamos observar alguns exemplos.

Exemplo 61. Seja f : [a, b] → R uma função contínua. Pelo Teorema Fundamental do Cálculo
temos que a função F(t) = (cid:82) t
a f (s)ds, com a ≤ t ≤ b e F(a) = 0 é uma primitiva para a função f
e assim, F(t) é uma solução do problema de valor inicial




˙y(t) = f (t)



y(a) = 0

.

Este PVI possui uma solução, mas surge a pergunta, F(t) é a única solução para este PVI?
Neste caso a resposta é positiva, pois, se G(t) for uma outra solução, temos que G′(t) = f (t) =
F ′(t) e isso implica que (F − G)′(t) = 0, ou seja, (F − G)(t) =constante. Mas, (F − G)(a) =
F(a) − G(a) = 0 − 0 = 0. Portanto, G(t) = F(t) para todo t ∈ [a, b].

Exemplo 62. No entanto, há problemas de valor inicial que possuem mais de uma solução. O
problema de valor inicial




˙y(t) = |y|

1
2

não têm unicidade de soluções, pois y1 ≡ 0 é uma solução e



y(0) = 0

y2(t)






t2
4 ,t ≤ 0
− t2

4 ,t < 0

também é solução. Portanto, temos duas soluções para o problema.

Para respondermos se um PVI tem solução, podemos utilizar o Método de Picard. Para

entendermos o método, consideramos o PVI




˙y(t) = f (t, y)



y(t0) = y0

,

onde f é uma função deﬁnida num aberto A de R2. Suponhamos que f (t, x) seja uma função
contínua em (t, x) e continuamente derivável em x. Obsevamos que y(t) é solução do PVI se, e
somente se, y(t) é solução da equação integral

y(t) = y0 +

(cid:90) t

t0

f (s, y(s))ds.

Considere agora a sequência yn(t), dada da seguinte forma:

102

Capítulo 7. Sistemas de Equações Diferenciais Ordinárias

y0(t) = y0
(cid:90) t

y1(t) = y0 +

f (s, y0(s))ds,

y2(t) = y0 +

yn(t) = y0 +

t0
(cid:90) t

t0
...
(cid:90) t

t0

f (s, y1(s))ds,

f (s, yn−1(s))ds

As funções yn(t) são chamadas iteradas de Picard. Pode-se mostrar que yn(t) → y(t),
quando n → ∞, para t num intervalo conveniente. A função limite y(t) será a solução do PVI.
Este processo é conhecido por Método de Picard.

Para exempliﬁcar o método, vamos considerar o PVI




˙y = y



y(0) = 1.

Observamos que, neste caso, f (t, y) = y, t0 = 0 e y0 = 1. A equação integral equivalente

ao PVI dado é:

y(t) = 1 +

(cid:90) t

0

y(s)ds

Considerando, inicialmente, y0(t) = 1, obtemos, pelo processo de interação a próxima
0 1ds = 1 + t. Usando y1(t) no integrando, encontramos a próxima função,

0(1 + s)ds = 1 + t + t2

2!. Continuando o processo, obtemos:

função, y1(t) = 1 + (cid:82) t
y2(t) = 1 + (cid:82) t

0 y1(s)ds = 1 + (cid:82) t
(cid:90) t

y3(t) = 1 +

y2(s)ds = 1 +

0

(cid:90) t

0

(1 + s +

s2
2!

)ds = 1 + t +

t2
2!

+

t3
3!

yn(t) = 1 +

(cid:90) t

0

yn−1(s)ds = 1 +

(cid:90) t

0

(1 + s +

s2
2!

+ · · · +

sn−1
(n − 1)!

)ds = 1 + t +

t2
2!

+ · · · +

tn
n!

Como et = limn→∞(1 +t + t2

2! +· · ·+ tn

n! ) = limn→∞ yn(t), vemos que as iteradas de Picard,

yn(t), convergem para a função y(t) = et, que é solução do PVI dado.

Observação 5. Pode acontecer de a solução de um PVI não estar deﬁnida para todo t ∈ R.
Por exemplo, a função y(t) = tan(t + /4) é solução de ˙y = 1 + y2, y(0) = 1, mas está deﬁnida
somente no intervalo ( −3π

4 , π

4 ).

Por este fato, não podemos esperar que as iteradas de Picard convirjam para todo t. Para
sabermos onde as iteradas de Picard convergem, tentamos encontrar um intervalo no qual todas
as yn(t) são uniformemente limitadas, isto é, para quais valores de t existe uma constante k > 0
tal que |yn(t)| ≤ k para todo t ∈ (a, b). Este é o conteúdo do próximo lema.

103

Lema 2. Sejam a, b ∈ R e consideremos o retângulo

R = {(t, y)/t0 ≤ t ≤ to + a e |y − y0| ≤ b}.

Deﬁna M = max{| f (t, y)|, (t, y) ∈ R} e α = min{a, b
t0 ≤ t ≤ t0 + α.

M }. Então: |yn(t) − y0| ≤ M|t − t0|, para

O próximo teorema nos apresenta as condições para a existência e unicidade de soluções

para o PVI (7.2).

Teorema 19. (Existência e Unicidade Local). Suponha que f e ∂ f
∂ y sejam funções contínuas no
retângulo R e suponha ainda que M e α são considerados como no lema anterior. Então o PVI




˙y = f (t, y)



y(t0) = y0

possui uma e somente uma solução y(t) no intervalo t0 ≤ t ≤ t0 + α.

Exemplo 63. Consideramos a equação ˙y = y2 + cost2 com y(0) = 0. As funções f (t, y) =
y2 + cost2 e ∂ f
∂ y (t, y) = 2y, são contínuas em todo R2 e se M = max{| f (t, y)|, (t, y) ∈ R} =
max{|y2 + cost2|, |y| ≤ b e 0 ≤ t ≤ a} = b2 + 1, vemos, pelo teorema (19), que y(t) existe para
}. Como, a priori, podemos tomar qualquer valor de a, temos
0 ≤ t ≤ α, em que α = min{a,
b2+1 e este por sua vez atinge valor máximo 1
que o valor α será
2 . Portanto, a
solução y(t) existe e é única para 0 ≤ t ≤ 1
2 .

2 , logo α = 1

b
b2+1

b

Se cada uma das funções F1, · · · , Fn em (7.3) for linear em x1, · · · , xn, então dizemos que
o sistema de equações diferenciais é linear. O sistema mais geral de n equações lineares de 1a
ordem possui a forma:


˙x1 = a11(t)x1 + · · · + a1n(t)xn + g1(t)

...

˙xn = an1(t)x1 + · · · + ann(t)xn + gn(t)

(7.4)

Se g j(t) = 0 para todo 1 ≤ j ≤ n, então dizemos que o sistema de equações diferen-
ciais lineares acima é homogêneo. Caso contrário, ele é não homogêneo. Para simpliﬁcar a
representação do sistema, usaremos a notação matricial

A(t) =







a11(t)
...
an1(t)

· · · a1n(t)

...

¨
· · · ann(t)







, g(t) =













g1(t)
...
gn(t)

e x(t) =







x1(t)
...
xn(t)



.





104

Capítulo 7. Sistemas de Equações Diferenciais Ordinárias

Assim, o sistema (7.4) pode ser expresso na forma compacta

˙x = A(t)x + g(t)

(7.5)

onde x = (x1, · · · , xn)t.

Nosso principal objetivo neste capítulo é utilizar os resultados obtidos sobre diagonaliza-
ção de operadores para encontrar as soluções do sistema (7.4) quando A(t) = A e g j(t) = 0, para
j = 1, . . . , n. Para isso, vamos descrever a teoria necessária para descrevermos essas soluções.

Teorema 20. (Existência e Unicidade de Soluções para Sistemas). Suponha que as funções ai j(t)
e gi(t), 1 ≤ i, j ≤ n, sejam contínuas num intervalo J. Então dados t0 ∈ J e x0 ∈ Rn, existe uma
única solução x(t) de (7.5), deﬁnida em J, tal que x(t0) = x0.

1(t), · · · , x1

Proposição 30. Se u(t) = (x1
n(t)) e v(t) = (x2
n(t)) são soluções do sistema
homogêneo ˙x = A(t)x, então qualquer combinação linear c1u(t) + c2v(t), em que c1 e c2 são
constantes arbitrárias, também é solução do sistema linear homogêneo. Ou seja, o conjunto S de
todas as soluções do sistema linear homogêneo é um espaço vetorial.

1(t), · · · , x2

Teorema 21. Sejam x1(t), · · · , xk(t), soluções do sistema linear homogêneo ˙x = A(t)x e seja
t0 ∈ J. Então x1(t), · · · , xk(t) são soluções linearmente independentes se, e somente se, os vetores
x1(t0), · · · , xk(t0) são linearmente independentes em Rn.

Demonstração. Suponhamos que x1(t), · · · , xk(t) sejam linearmente dependentes. Então, exis-
tem constantes c1, · · · , ck não todas nulas, tais que c1x1(t) + · · · + ckxk(t) = 0, para todo t ∈ J.

Logo, c1x1(t0) + · · · + ckxk(t0) = 0, com constantes c1, · · · , ck não todas nulas. Portanto,

x1(t0), · · · , xk(t0) são linearmente dependentes em Rn.

Reciprocamente, suponhamos que x1(t0), · · · , xk(t0) sejam linearmente dependentes em
Rn. Então, existem constantes c1, · · · , ck não todas nulas, tais que c1x1(t0) + · · · + ckxk(t0) = 0.
Temos que a função u(t) = c1x1(t) + · · · + ckxk(t),em que c1, · · · , ck são as constantes dadas
acima, satisfaz o sistema linear homogêneo, pois é uma combinação linear de soluções. Além
disso, u(t0) = 0. Portanto, pelo Teorema (19), u(t) = 0 para todo t. Logo, x1(t), · · · , xk(t) são
soluções linearmente dependentes.

Teorema 22. A dimensão do espaço S de todas as soluções do sistema linear homogêneo
˙x = A(t)x é n.

Demonstração. Vamos mostrar que sistema linear homogêneo possui n soluções linearmente in-
dependentes. Para isto, consideremos os vetores do Rn, e1 = (1, 0, 0, . . . , 0)t, e2 = (0, 1, 0, . . . , 0)t

, . . . , en = (0, 0, 0, . . . , 1) e os PVI’s






˙x = A(t)x
xi(t0) = ei, i = 1, · · · , n e to ∈ J.

105

(7.6)

Pelo Teorema (20), temos que cada PVI possui uma única solução xi(t). Como os
vetores e1, · · · , en são linearmente independentes em Rn, segue que x1(t), · · · , xn(t) são soluções
linearmente independentes do sistema de equações diferenciais linear homogêneo. Resta mostrar
que qualquer solução deste sistema pode ser escrita como combinação linear de x1(t), · · · , xn(t).

Seja x(t) uma solução do sistema de equações diferenciais linear homogêneo tal que
x(t0) = (c1, · · · , cn)t. Com estas constantes c1, · · · , cn, construímos a função u(t) = c1x1(t) +
· · · + cnxn(t). Temos que u(t) satisfaz o sistema linear homogêneo pois é combinação linear de
soluções e além disso

u(t0) = c1x1(t0) + · · · + cnxn(t0) = c1e1 + c2e2 + · · · + cnen = (c1, c2, . . . , cn)t = x(t0).

Novamente, pelo Teorema (19), u ≡ x. Portanto,

x(t) = c1x1(t) + · · · + cnxn(t)

Observação 6. O teorema anterior diz que se conhecermos n soluções linearmente independentes
x1(t), · · · , xn(t) do sistema linear homogêneo, então toda solução deste sistema será da forma
x(t) = c1x1(t) + · · · + cnxn(t). Por esta razão, esta expressão é chamada solução geral do sistema
linear homogêneo.

Deﬁnição 34. Dizemos que uma matriz n × n, X(t) é matriz solução do sistema ˙x = A(t)x, se
cada coluna de X(t) é solução do sistema.

Exemplo 64. Temos que X(t) =

x1(t) =

(cid:33)

(cid:32)

et
0

e x2(t) =

(cid:33)

(cid:32)

0
e2t

são soluções do sistema.

(cid:32)

(cid:33)

et
0
0 e2t

é uma matriz solução de ˙x =

(cid:33)

(cid:32)

1 0
0 2

x, pois

Deﬁnição 35. Dizemos que uma matriz n × n, X(t) é matriz fundamental (M.F.) para o sistema
˙x = A(t)x se X(t) é uma matriz solução e det X(t) ̸= 0 para todo t no intervalo de existência. Ou
seja, suas colunas são soluções linearmente independentes de ˙x = A(t)x.

No exemplo anterior X(t) =

1 0
0 2
vimos acima, ela é matriz solução e além disso det X(t) = e3t ̸= 0 para todo t.

é uma M.F. de ˙x =

et
0
0 e2t

(cid:32)

(cid:33)

(cid:32)

(cid:33)

x pois, como

Teorema 23. Se X(t) é uma M.F. do sistema linear homogêneo, então a solução geral será dada
por x(t) = X(t)c, em que c = (c1, . . . , cn)t.

106

Capítulo 7. Sistemas de Equações Diferenciais Ordinárias

Com o auxílio do próximo resultado:

Lema 3. Se X(t) é uma matriz solução do sistema linear homogêneo em algum intervalo J e se
t0 ∈ J, então

det X(t) = det X(t0) exp(

(cid:90) t

t0

trA(s)ds)

onde trA(s) = soma dos elementos da diagonal principal de A(s).

podemos provar o seguinte teorema:

Teorema 24. Seja X(t) uma matriz solução do sistema de equações diferenciais linear homogê-
neo em J. X(t) é M.F. se, e somente se, det X(t0) ̸= 0 para algum t0 ∈ J.

Vamos considerar o sistema ˙x = Ax, onde A = (ai j), i, j = 1, 2, · · · , n é uma matriz
constante e a partir da teoria descrita anteriormente vamos construir a solução geral do sistema.
Para tanto, vamos procurar por soluções da forma x(t) = eλtv, em que o número λ e o vetor
constante v = (v1, v2, · · · , vn)t não nulo devem ser determinados. Substituindo a solução no
sistema, obtemos λ eλtv = Aeλtv ou, equivalentemente, Av = λ v.

Logo, temos uma solução de ˙x = Ax se, e somente se, λ é um autovalor de A e v é um
autovetor associado a λ . A natureza dos autovalores e autovetores associados determinam a
natureza da solução do sistema.

Proposição 31. Seja A uma matriz quadrada n × n e sejam λ1, . . . , λn os autovalores de A,
reais e distintos, com v1, v2, . . . vn autovetores associados a esses autovalores. Se os autovetores
forem linearmente independentes, então as soluções x1(t) = eλ1tv1, . . . xn(t) = eλntvn são soluções
linearmente independentes de ˙x = Ax.

Demonstração. Só precisamos mostrar que as soluções são linearmente independentes e, para
isso, vemos que, calculadas em t0 = 0, temos os vetores v1, . . . , vn que são linearmente indepen-
dentes e usamos o teorema anterior.

Teorema 25. Se A ∈ Mn for simétrica, então a solução geral do sistema linear homogêneo
˙x = Ax é dada por x(t) = c1eλ1(t)v1 + . . . + cneλntvn, onde λ1, λ2 . . . , λn são os autovalores de
A (não necessariamente distintos) e v1, v2, . . . , vn são os autovetores de A associados aos seus
respectivos autovalores.

Demonstração. De fato, A sendo simétrica, todos os autovalores serão reais e, mesmo que sejam
repetidos, haverá uma base ortonormal de autovetores associados a esses autovalores, ou sejam,
os autovetores serão linearmente independentes e a solução geral do sistema será dada como
combinação linear das funções eλi(t)vi, para i = 1, . . . , n.

Exemplo 65. Vamos considerar o sistema ˙x =

107






0 1 1
1 0 1
1 1 0




 x, onde, obviamente, A é simétrica.

Os autovalores da matriz são, calculando det(A − λ I) = 0, λ1 = 2 e λ2 = −1, com multiplicidade

algébrica igual a 2. O autovetor associado ao autovalor λ1 é v1 =




associados ao autovalor λ2 são v2 =






1
0
−1






 e v3 =







.

0
1
−1

Portanto, a solução geral do sistema é dada por





1
1
1


 e os autovetores

x(t) = c1


 e2t + c2












1
1
1

1
0
−1


 e−t + c3








 e−t.


0
1
−1

Vamos, agora, olhar para as soluções dos sistema ˙x = Ax utilizando a matriz fundamental,
fazendo uma associação com as soluções da equação ˙x = ax, com x(0) = x0, dadas por x(t) =
x0eat.

Para isso, consideramos a matriz solução Φ(t) do sistema ˙x = Ax com x(0) = x0, obtida

como no teorema (22) considerando Φ(0) = I.

Comparando a equação com o sistema, podemos sugerir que tal matriz Φ(t) tenha carater

exponencial.

Podemos veriﬁcar que Φ(t) = eAt, one A é a matriz do sistema. Tem-se várias considera-
ções a fazer sobre essa aﬁrmação. Primeiro, qual o signiﬁcado de eAt? Deﬁnimos eAt da seguinte
forma:

eAt = I +

∞
∑
n=1

Antn
n!

É possível mostrar que cada elemento dessa soma de matrizes converge para todo t
quando n → ∞ e mais, que eAt satisfaz a equação ˙x = Ax , com x(0) = I e, pela unicidade de
soluções Φ(t) = eAt. Mais ainda, é possível mostrar que essa função matricial tem as mesmas
propriedades "convenientes"que tem a função exponencial real.

Quando a matriz A é diagonalizável, o cálculo desta exponencial é relativamente mais
simples. A ideia que está por trás disso é transformar o sistema ˙x = Ax em um sistema ˙y = Dy,
onde D é diagonal e, dessa forma, as n equações que aparecem no sistema são desacopladas.

Se A for diagonalizável, o que é o caso quando A é simétrica, temos que existe uma base
de autovetores associados aos autovalores de A tal que D = P−1AP. onde P é formada pelos

108

Capítulo 7. Sistemas de Equações Diferenciais Ordinárias

autovetores e D é diagonal, contendo em sua diagonal principal os autovaloes de A. Desta forma,
se deﬁnirmos x = Py, temos

P ˙y = APy

e,

˙y = (P−1AP)y = Dy.

(7.7)

Uma matriz fundamental para o sistema (7.7) é a matriz diagonal

eDt =









eλ1t
0
...
0

0
eλ2t
...
0

. . .
. . .

0
0
...
. . . eλnt









Logo, uma matriz fundamental para o sistema ˙x = Ax é dada por

Φ(t) = PeDt.

Desta forma, podemos observar que resolver um sistema de equações diferenciais com

essas condições e diagonalizar uma matriz estão estritamente relacionados.

Vamos fazer um exemplo para descrever todo esse processo.

Exemplo 66. Consideramos o sistema ˙x = Ax, do exemplo (65), que já apresentamos a solução
geral, mas vamos utilizar a matriz exponencial para resolvê-lo. Obviamente, A é diagonalizável,



2
0
0 −1
0

0
0
0 −1






, P =




0
1
1
1
1
0
1 −1 −1




 e

pois é simétrica e temos D = P−1AP, onde D =




P−1 = 1
3






1
1
1
2 −1 −1
2 −1
−1




.

Observamos que

D2 =






22
0
0

0
(−1)2
0

0
0
(−1)2


, D3 =







23
0
0

0
(−1)3
0

0
0
(−1)3


,. . . , Dn =







2n
0
0

0
(−1)n
0

0
0
(−1)n




,

e,

eDt = I +

∞
∑
n=1

1
n!






(2t)n
0
0

0
(−1t)n
0

0
0
(−1t)n






 =




e2t
0
0

0
e−t
0

0
0
e−t




 .

Portanto, a solução será dada por

109

Φ(t) =






0
1
1
1
1
0
1 −1 −1











e2t
0
0

0
e−t
0

0
0
e−t






 =




e−t
e2t
0
e2t
e−t
0
e2t −e−t −e−t




 ,

conforme foi também demonstrado no exemplo (65).

111

CAPÍTULO

8

CONCLUSÃO

O domínio dos conceitos matemáticos, das demonstrações, das deﬁnições é importante
para a construção de novos conceitos e isso permite ao estudante a validação de intuições na
construção de técnicas aplicadas em diversas situações.

A Matemática, diante disso, tem um papel importante no Ensino Médio, pois cabe a
ela a apresentação de novas informações e instrumentos que deem condições ao estudante de
continuar aprendendo.

Entende-se a importância do estudo das matrizes, determinantes e sistemas lineares.

No entanto, os conhecimentos adquiridos pelos alunos ﬁcam limitados em sua maioria a

cálculos abstratos.

Portanto, a perspectiva é que o trabalho aqui apresentado seja usado para futuras aplica-
ções no último ano do Ensino Médio no estudo das cônicas. Isso representará um ganho para os
alunos uma vez que serão recordados conceitos de matrizes, determinantes e sistemas lineares no
enfoque da geometria analítica. Além disso, buscar técnicas para a identiﬁcação de cônicas de
modo geral aumenta a quantidade de conteúdos adquiridos pelos alunos, possibilitando, assim,
uma visão geral do conteúdo de cônicas que, sem dúvida, aumentarão seus conhecimentos de
Matemática de um modo geral.

113

REFERÊNCIAS

BOYCE, R. C. D. W. E. Equações Diferenciais Elementares e Problema de Valores de
Contorno. [S.l.]: LTC, 2002. Citado na página 99.

FRENSEL K.; DELGADO, J. Geometria Analítica. [S.l.]: UFMA, 2011. Citado nas páginas
41 e 81.

GOMEZ KATIA R. FRENSEL, L. S. C. J. J. D. Geometria Analítica. [S.l.]: SBM, 2017. Citado
na página 81.

HEFEZ A.; SOUZA FERNANDES, C. Introdução à Álgebra Linear. [S.l.]: SBM, 2012. ISBN
978-85-85818-61-6. Citado nas páginas 23, 41, 55, 67 e 81.

LADEIRA, L. A. C. Álgebra Linear e Equações Diferenciais. [S.l.]: ICMC, 2004. Citado nas
páginas 41 e 99.

LADEIRA L. A. C.; JUNIOR, H. C. Notas de Aula : Equações Diferenciais Ordinárias. [S.l.],
2011. Citado na página 99.

LIPSCHUTZ, S. Álgebra Linear. [S.l.]: São Paulo: Makron Books, 1994. Citado na página 23.

STEINBRUCH, A. Álgebra Linear. [S.l.]: São Paulo: Pearson Makron Books, 1987. Citado
na página 23.

ZANI, S. L. Álgebra Linear. [S.l.]: ICMC- USP. Citado nas páginas 23, 41, 55 e 67.

UNIVERSIDADE DE SÃO PAULOInstituto de Ciências Matemáticas e de Computação