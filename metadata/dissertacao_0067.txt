         Mestrado Profissional em Matemática em Rede Nacional                  PROFMAT              DISSERTAÇÃO DE MESTRADO                    REDES NEURAIS NO ENSINO BÁSICO                            ANDRÉ OLIVEIRA MARTINS                                                            Maceió, 22 de janeiro de 2021 UNIVERSIDADE FEDERAL DE ALAGOAS

INSTITUTO DE MATEMÁTICA

MESTRADO EM MATEMÁTICA EM REDE NACIONAL

ANDRÉ OLIVEIRA MARTINS

REDES NEURAIS NO ENSINO BÁSICO

Maceió

2020

André Oliveira Martins

Redes Neurais no Ensino Básico

Dissertação apresentada ao Programa de Mes-
trado Proﬁssional em Matemática em Rede Na-
cional (PROFMAT) do Instituto de Matemática
da Universidade Federal de Alagoas, como re-
quisito parcial para obtenção do grau Mestre em
Matemática.

Orientador: Prof. Dr. Márcio Henrique Batista da Silva

Maceió

2020

Catalogação na fonteUniversidade Federal de AlagoasBiblioteca CentralDivisão de Tratamento TécnicoBibliotecária: Taciana Sousa dos Santos – CRB-4 – 2062    M386r     Martins, André Oliveira.                         Redes neurais no ensino básico / André Oliveira Martins. – 2020.                         126 f. : il., figs. e tabs. color.                         Orientador: Márcio Henrique Batista da Silva.                         Dissertação (Mestrado Profissional em Matemática) – Universidade                   Federal de  Alagoas. Instituto de Matemática. Mestrado Profissional em                      Matemática em Rede Nacional. Maceió, 2021.                          Bibliografia: f. 115-116.                          Apêndices: f. 117-126.                          1. Matemática. 2. Redes neurais artificiais. 3. Ensino básico. 4. Python                     (Linguagem de programação de computador). I. Título.                                                                                                               CDU: 51: 004.8               Folha de Aprovação   ANDRÉ OLIVEIRA MARTINS   Redes Neurais no Ensino Básico   Dissertação submetida ao corpo docente do Programa de Mestrado Profissional em Matemática em Rede Nacional (PROFMAT) do Instituto de Matemática da Universidade Federal de Alagoas e aprovada em 22 de janeiro de 2021.   ___________________________________________________________ Prof. Dr. Márcio Henrique Batista da Silva – UFAL (Orientador)   Banca Examinadora:  ___________________________________________________________ Profa. Dra. Gabriela Albuquerque Wanderley  – UFPB (Examinadora Externa)   ___________________________________________________________ Prof. Dr. Márcio Silva Santos – UFPB (Examinador Externo)      AGRADECIMENTOS

Agradeço Primeiramente, a Deus;

Aos meus pais, Laurentino Martins e Cremilda Gonzaga, que me ensinaram o real valor

da família e os sentimentos de luta e coragem que hoje carrego e me conduzem em cada decisão;

As minhas irmãs Vanessa Martins e Valéria Martins pelo apoio, carinho e palavras de

motivação;

A todos os colegas do curso, em particular Denise, Gabriel e Vitor pela amizade e apoio

durante curso.

Agradeço ao corpo docente do PROFMAT da Universidade Federal de Alagoas em

especial ao Prof. Dr. Márcio Henrique Batista da Silva pela dedicação, paciência e prontidão

durante a realização deste trabalho;

Aos membros da Banca Examinadora, por aceitarem o convite de avaliar o resultado

desta caminhada;

A todos, os meus sinceros agradecimentos.

“If you change the way you look at things, the
things you look at change.”

Wayne Dyer

RESUMO

Neste trabalho, propomos uma formação continuada para professores de matemática
focada em Redes Neurais Artiﬁciais, uma área de inteligência artiﬁcial, que vem crescendo
muito nos últimos anos. O trabalho apresenta como as redes neurais artiﬁciais podem ser
empregadas na educação básica, com o intuito de inserir os alunos em um projeto de matemática
e tecnologias inovadoras. O trabalho tem como tema central a construção de uma rede neural
para reconhecimento de dígitos manuscritos com códigos em linguagem Python.

Palavras-chave: Matemática; Redes Neurais; Ensino Básico; Python.

ABSTRACT

In this work, we propose a training and developement of skills for mathematics teachers
focus on Artiﬁcial Neural Networks, an area of artiﬁcial intelligence, which has attracted many
scholars in the recent years. Introduce work presents how the artiﬁcial neural networks can be
used in secondary education, in order to introduce students in a project of mathematics and
innovative technologies. The main purpose of this paper is the construction of a neural network
for the recognition of handwritten digits with codes in Python language.

Keywords: Mathematics; Neural Network; High School; Python.

SUMÁRIO

INTRODUÇÃO .

. .

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9

1

1.1

1.2

1.3

1.4

1.5

NOÇÕES BÁSICAS DE PYTHON . . . . . . . . . . . . . . . . . . . . .

11

Instalando o Anaconda . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

11

O Interpretador Python . . . . . . . . . . . . . . . . . . . . . . . . . . . .

19

Variáveis .

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

19

Operações matemáticas no Python . . . . . . . . . . . . . . . . . . . . . .

22

Funções

.

.

.

.

.

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

23

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

23

1.5.1

input

.

.

1.6

1.6.1

1.6.2

1.7

1.7.1

1.7.2

1.8

1.9

1.10

1.11

1.12

1.13

Ferramentas de controle de ﬂuxo . . . . . . . . . . . . . . . . . . . . . . .

23

Comandos if, else e elif

. . . . . . . . . . . . . . . . . . . . . . . . . . . .

23

Comandos for e while . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

24

Funções

. .

.

.

.

A função range() .

A função zip()

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

25

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

25

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

26

Deﬁnindo funções .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

27

Funções lambda .

Listas .

Tuplas

.

.

.

.

. .

Dicionários .

.

.

.

.

.

.

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

28

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

28

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

29

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

30

Valores e operadores booleanos . . . . . . . . . . . . . . . . . . . . . . . .

30

1.13.1

Or

.

1.13.2

And .

1.13.3

Not

.

.

.

.

.

.

.

1.14

Classes .

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

31

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

32

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

32

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

33

1.15

1.16

Comandos importantes . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

34

Bibliotecas cientíﬁcas do Python . . . . . . . . . . . . . . . . . . . . . . .

34

1.16.1

NumPy .

1.16.2

Pandas .

.

.

.

.

.

.

. .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

34

. .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

34

1.16.3

Matplotlib . .

. .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

34

2

2.1

2.2

2.2.1

2.2.2

2.2.3

2.2.4

2.3

2.4

2.5

MATEMÁTICA PARA REDES NEURAIS . . . . . . . . . . . . . . . .

36

Vetores .

.

.

.

. .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

36

Matrizes .

. .

. .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

37

Soma de matrizes

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

38

Multiplicação de matrizes por um número . . . . . . . . . . . . . . . . . .

39

Multiplicação de matrizes . . . . . . . . . . . . . . . . . . . . . . . . . . .

39

O produto Hadamard . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

40

Funções reais de uma variável real

. . . . . . . . . . . . . . . . . . . . . .

40

Funções vetoriais

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

42

Funções de várias variáveis . . . . . . . . . . . . . . . . . . . . . . . . . .

43

2.5.1

Derivadas parciais .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

44

2.6

3

3.1

3.2

3.3

3.3.1

3.3.2

3.3.3

3.3.4

Máximos e mínimos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

45

NOÇÕES DE APRENDIZADO DE MÁQUINA . . . . . . . . . . . . . .

47

Inteligência artiﬁcial . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

47

Aprendizado de máquina

. . . . . . . . . . . . . . . . . . . . . . . . . . .

47

Categorias de aprendizado de máquina . . . . . . . . . . . . . . . . . . . .

47

Aprendizado de máquina supervisionado . . . . . . . . . . . . . . . . . . .

48

Aprendizado de máquina não supervisionado . . . . . . . . . . . . . . . . .

48

Aprendizado de máquina semi-supervisionado . . . . . . . . . . . . . . . .

48

Aprendizado de máquina por reforço . . . . . . . . . . . . . . . . . . . . .

49

3.3.5

3.3.6

3.3.7

3.4

3.5

3.6

4

4.0.1

4.0.2

4.0.3

4.0.4

4.0.5

4.1

4.2

4.3

4.4

4.5

5

5.1

5.2

Aprendizado online ou em lote . . . . . . . . . . . . . . . . . . . . . . . .

49

Aprendizado baseado em instâncias . . . . . . . . . . . . . . . . . . . . . .

49

Aprendizado baseado em modelo . . . . . . . . . . . . . . . . . . . . . . .

49

Sobreajuste e sub-ajuste . . . . . . . . . . . . . . . . . . . . . . . . . . . .

49

Sobreajuste . .

. .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

50

Sub-ajuste . .

. .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

50

REDES NEURAIS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

52

Neurônio biológico . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

53

Neurônio artiﬁcial

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

53

O termo bias .

. .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

55

Funções de ativação . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

56

Perceptrons .

.

. .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

59

Treinamento de um perceptron de camada única . . . . . . . . . . . . . . .

60

Construção de um perceptron de camada única código à código em Python .

64

Redes multicamada com alimentação para frente . . . . . . . . . . . . . . .

70

Treinamento de redes neurais multicamada . . . . . . . . . . . . . . . . . .

72

Construção de uma rede neural código à código em Python . . . . . . . . .

84

ROTEIRO DE APRESENTAÇÃO DE REDES NEURAIS NO ENSINO

BÁSICO .

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

95

Interpretação geométrica da derivada a partir da taxa média de variação . . .

95

Deep learning e a visão computacional

. . . . . . . . . . . . . . . . . . . .

99

5.2.1

Treinamento do modelo . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104

CONSIDERAÇÕES FINAIS .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114

REFERÊNCIAS .

. .

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115

A

B

C

NOÇÕES DE NUMPY . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117

NOÇÕES DE PANDAS . . . . . . . . . . . . . . . . . . . . . . . . . . . 120

NOÇÕES DE MATPLOTLIB . . . . . . . . . . . . . . . . . . . . . . . . 124

INTRODUÇÃO

9

Diante da necessidade de uma participação ativa no acelerado, processo das transfor-

mações do mundo tecnológico, a qual tem sido discutida como um dos desaﬁos presentes na

educação e na prática dos educadores, a escola vem sendo cobrada por uma modernização na

prática educativa para se adequar a realidade atual de seus alunos, tanto social como proﬁssional.

O Novo Ensino Médio apresenta mudanças importantes para o ensino, sendo inseridas

nas escolas a partir de 2021 e espera-se que as modiﬁcações estejam em prática até 2022. O

Novo Ensino Médio Traz como propostas que se tenha menos aulas expositivas e mais atividades

como projetos, oﬁcinas e atividades práticas signiﬁcativas, que sejam determinantes na formação

técnica e proﬁssional dos alunos.

Percebe-se que tem ocorrido muitas transformações na educação, visto que o modelo

tradicional de ensino já não se ajusta à sociedade atual que tem uma demanda diferente exigida

principalmente pelo mercado de trabalho, colocando assim em destaque a necessidade de inserir

novas práticas pedagógicas.

Nos dias atuais, as pessoas entram em contato com os mais diversos tipos tecnologias,

desde muito cedo, antes mesmo de iniciar o processo de alfabetização, desse modo muitos alunos

já chegam à escola bem familiarizados e tem grande facilidade de assimilar conhecimentos

inovadores.

A tecnologia se insere cada vez mais rápido aproximando-se das diversas realidades e

propõe mudanças para a sociedade moderna. Assim, o ensino e aprendizagem, tem ganhado

redeﬁnições que possibilitam o acesso ao conhecimento e informações de forma mais ampla.

A inteligência artiﬁcial está cada vez mais presente nas mais diversas áreas de atuação

como segurança, saúde, além de estar presente no nosso dia a dia nas atividades ﬁnanceiras,

proﬁssionais e nos mais diversos aparelhos domésticos como: TVs, media centers, câmeras

de segurança, smartphones e computadores conectados à internet. Com a educação não tem

sido diferente. Grandes mudanças já se inseriram nas escolas para proporcionar um ensino

aprendizagem mais signiﬁcativo. Contudo há ainda uma necessidade de buscar alternativas

pedagógicas, para aumentar o uso desta tecnologia.

Desse modo, o presente trabalho tem por objetivo facilitar o acesso a um conhecimento

mais amplo e signiﬁcativo, utilizando sistemas de Inteligência Artiﬁcial como redes neurais e

deep learning. A relevância da temática se dá devido a necessidade da implantação da tecnologia

no ambiente educacional. Desenvolvendo assim uma nova prática pedagógica nas escolas.

10

11

1 NOÇÕES BÁSICAS DE PYTHON

O Python é uma linguagem de programação muito intuitiva e de fácil aprendizado. Tal

linguagem possui uma sintaxe concisa e clara, além de contar com muitas bibliotecas para análise

de dados, tornando-a assim a linguagem mais adequada para machine learning.

1.1

Instalando o Anaconda

O Anaconda é um pacote, de código aberto, que agrega todas as ferramentas para análise

de dados, em Python, em um único arquivo. Estão presentes no Anaconda o próprio Python, o

Jupyter Notebook, a IDE Spyder e todas as bibliotecas que utilizaremos aqui, como o Numpy,

MatPlotlib e Scikit-learn. Por sua praticidade, ao trazer tudo que precisamos em único arquivo,

vamos instalar e usar o Anaconda, ao invés de instalar cada uma dessas ferramentas por vez.

O Anaconda está disponível para download gratuito no site https://www.anaconda.com/ ,

veja a imagem abaixo da página inicial do site.

Figura 1 – Página inicial do site https://www.anaconda.com

Fonte: https://www.anaconda.com . Acesso: AGO, 2020.

Para fazer o download devemos clicar na guia Products e em seguida em Individual

edition, veja a imagem abaixo.

Figura 2 – Caminho para download do Anaconda

12

Fonte: https://www.anaconda.com . Acesso: AGO, 2020.

Ao clicar em tal item, abre-se uma nova página e nesta encontraremos as opções de

download e devemos baixar a versão adequada ao sistema operacional do computador em uso.

Vai ser baixado a versão atual do Anaconda que traz o Python na versão 3.8.

Figura 3 – Página de download do Anaconda

Fonte: https://www.anaconda.com . Acesso: AGO, 2020.

Após o download, clique duas vezes com o botão esquerdo do mouse no arquivo baixado

para iniciar o processo de instalação. A primeira tela de instalação traz algumas informações,

clique em Next.

Figura 4 – Tela de instalação 1

13

Fonte: Instalação do Anaconda.

Em seguida, será mostrado os termos de licença, clique em I agree para aceitar.

Figura 5 – Tela de instalação 2

Fonte: Instalação do Anaconda.

Para concluir a instalação basta seguir as telas abaixo.

Figura 6 – Tela de instalação 3

14

Fonte: Instalação do Anaconda.

Figura 7 – Tela de instalação 4

Fonte: Instalação do Anaconda.

15

Figura 8 – Tela de instalação 5

Fonte: Instalação do Anaconda.

Figura 9 – Tela de instalação 6

Fonte: Instalação do Anaconda.

16

Figura 10 – Tela de instalação 7

Fonte: Instalação do Anaconda.

Figura 11 – Tela de instalação 8

Fonte: Instalação do Anaconda.

Figura 12 – Tela de instalação 9

17

Fonte: Instalação do Anaconda.

Com o Anaconda instalado temos agora, muitas ferramentas a nossa disposição e, entre

elas, o Jupyter Notebook que é uma ferramenta que permite trabalhar com Python de forma

simples e interativa. Para abrir o Jupyter Notebook, podemos procurá-lo na pasta do Anaconda

ou abrir o Anaconda Navigator e clicar em lunche da opção Jupyter Notebook.

Figura 13 – Anaconda Navigator

Fonte: Elaborado pelo autor.

O Jupyter Notebook vai abrir no navegador padrão do computador, veja sua aparência na

imagem abaixo.

Figura 14 – Tela inicial do Jupyter Notebook

18

Fonte: Elaborado pelo autor.

Nesta página, podemos criar um novo arquivo Jupyter Notebook ou procurar um arquivo

salvo anteriormente em uma pasta no computador. Para criar um novo arquivo clique em New e

escolha Python 3. O arquivo será aberto em uma nova guia do navegador com o nome Untitled,

basta clicar em cima deste nome para renomear para o nome desejado.

Figura 15 – Novo arquivo do Jupyter Notebook

Fonte: Elaborado pelo autor.

19

Perceba que temos apenas uma célula e podemos criar o primeiro código nesta célula e

rodar o código clicando em Run ou pelo atalho shift + enter, o código será rodado e aparecerá

uma nova célula. Podemos também, adicionar células clicando e +.

Figura 16 – Rodando código em célula do Jupyter Notebook

Fonte: Elaborado pelo autor.

1.2 O Interpretador Python

O Interpretador, instalado junto com o Python, é um programa com a capacidade de

lê o código-fonte e traduzi-lo para linguagem do dispositivo. Os exemplos tratados aqui que

misturam códigos e saídas serão formatados como no interpretador Python, apesar de terem sido

feitos no Jupyter Notebook. Para ﬁcar mais legível, as linhas de código serão antecedidas de

>>> ou ... para blocos indentados e as saídas não serão precedidas de nenhum símbolo.

1.3 Variáveis

Variáveis são formas de armazenar dados para usá-los posteriormente. Para deﬁnir uma

variável no Python devemos digitar o seu nome, o sinal de igualdade e o valor a ser armazenado.

Os principais tipos de variáveis:

Numéricas: Quando armazena números inteiros (int) ou pontos ﬂutuantes (ﬂoat), isto é,

números não inteiros.

Exemplo 1.1.

1 >>> a = 5

2 >>> b = 3 . 5

20

String: É uma sequência de caracteres que deve ser delimitada por aspas simples ( ’...’ )

ou aspas duplas ( "...") possibilitando a utilização de textos no Python.

Exemplo 1.2.

1 >>> s t r = ’O P y t h o n é uma l i n g u a g e m de p r o g r a m a ç ã o c u r v a de a p r e n d i z a d o r á

p i d o ’

2 >>> a = ’A l a r a n j a é r i c a em v i t a m i n a C ’

Podemos concatenar strings com o operado (+) e repeti-las com o operador (∗).

Exemplo 1.3.

1 >>> a = ’ Eu ’

2 >>> b = ’ Tu ’

3 >>> 3 * ( a + b )

4

’ EuTuEuTuEuTu ’

Podemos também indexar (subscrever) e fatiar strings utilizando índices que da esquerda

para a direita se inicia do 0 e da direita para esquerda inicia de -1.

Exemplo 1.4.

1 >>> p a l a v r a = ’ Matem á t i c a ’

2 >>> p a l a v r a [ 0 ]

3

’M’

4 >>> p a l a v r a [ − 1 ]

5

’ a ’

6 >>> p a l a v r a [ 0 : 3 ]

7

’ Mat ’

8 >>> p a l a v r a [ 2 : 6 ]

9

’ tem á ’

10 >>> p a l a v r a [ : 6 ]

11

’ Matem á ’

21

As strings do Python são imutáveis, isto é, atribuir a uma posição indexada resulta em

um erro.

Exemplo 1.5.

1 >>> p a l a v r a = ’ Matem á t i c a ’

2 >>> p a l a v r a [ 0 ] = ’ P ’

3 T r a c e b a c k ( most

r e c e n t

c a l l

l a s t ) :

4

5

6

7

F i l e " < i p y t h o n − i n p u t −36− b f 5 c c 9 f 1 1 8 5 f > " ,

l i n e 1 ,

i n <module >

p a l a v r a [ 0 ] = ’ h ’

8 T y p e E r r o r :

’ s t r ’ o b j e c t d o e s n o t

s u p p o r t

i t e m a s s i g n m e n t

Desse modo, em caso de necessidade de uma string diferente temos que criar uma nova.

A função len() retorna o comprimento da string.

Exemplo 1.6.

1 >>> p a l a v r a = ’ Matem á t i c a ’

2 >>> l e n ( p a l a v r a )

3

10

Variáveis devem ser iniciadas por letras minúsculas

O sinal (=) é usado para atribuir um valor a uma variável, por exemplo se criarmos a

variável a e atribuirmos o valor 10, isto é, a = 10 Obs.: Dentro do Python a igualdade matemática

é representada por dois sinais de igualdade, isto é, ==.

22

1.4 Operações matemáticas no Python

O interpretador do Python funciona como uma calculadora e, desse modo, podemos

utilizar as operações matemáticas de maneira simples. Utilizamos, em Python, os operadores +,

−, ∗ e / de maneira usual e os parênteses (()) para agrupar as expressões.

Exemplo 1.7.

1

2 >>> 2 + 3

3

5

4 >>> ( 4 0 − 4 * 5 ) / 2
1 0 . 0

5

6 >>> 15 / 7

7

2 . 1 4 2 8 5 7 1 4 2 8 5 7 1 4 3 # D i v i s ã o s e m p r e r e t o r n a um p o n t o f l u t u a n t e .

8 >>> 5**2

9

25

Obs.: Comentários em Python são introduzidos pelo caractere # e se estende até o ﬁnal

da linha física.

Utilizando o caractere (/) teremos sempre como retorno um ponto ﬂutuante (ﬂoat), para

fazer uma divisão e receber um inteiro devemos utilizar o operador (//) e para calcular o resto de

uma divisão devemos usar o (%).

Exemplo 1.8.

1 >>> 8 / / 5

2

1

3 >>> 8 % 5

4

3

No Python para calcular potências devemos usar o operador (**).

Exemplo 1.9.

1 >>> 3 ** 2
9

2

23

Para obter como resultado um número inteiro na divisão devemos operar com números

inteiros, caso queiramos como resultado ponto ﬂutuantes, devemos operar com pontos ﬂutuantes.

1.5 Funções

1.5.1

input

A função input é destinada para interagir com o usuário, isto é, solicita dados que podem

ter o formato numérico ou de string. O programa para e espera a digitação da informação seguida

do ENTER.

Exemplo 1.10.

1 >>> nome = i n p u t ( ’ Qual é o s e u nome ? ’ )

2 >>> p r i n t ( nome ,

" , S e j a bem− v i n d o ! " )

3 nome , S e j a bem− v i n d o !

1.6 Ferramentas de controle de ﬂuxo

1.6.1 Comandos if, else e elif

O comando if, se em português, é utilizado para executar determinada ação somente

quando a condição dada for verdadeira. Em alguns casos é necessário executar ações de forma

alternativa e para isso utilizamos o o comando elif que é a abreviação de else if, senão se

em português, temos ainda, else, senão em português, usado executar determinada ação caso

nenhuma das anteriores forem executadas.

Exemplo 1.11.

24

1 i d a d e = i n t ( i n p u t ( ’ Q u a n t o s a n o s voc ê tem ? : ’ ) )

2 >>> i f

i d a d e < 1 8 :

3 . . .

p r i n t ( ’ Voc ê n ã o é a d u l t o . ’ )

4 >>> e l i f

i d a d e >= 18 and i d a d e < 6 5 :

5 . . .

p r i n t ( ’ Voc ê é a d u l t o . ’ )

6 >>> e l s e :

7 . . .

p r i n t ( ’ Voc ê é i d o s o ’ )

1.6.2 Comandos for e while

Os comandos for e while são métodos de iteração em Python, o comando for em Python

itera sobre os itens de qualquer sequência, na mesma ordem em que aparece na sequência, por sua

vez, o comando while repete a instrução do seu bloco enquanto o condicional de seu cabeçalho

for verdadeiro.

Exemplo 1.12.

1 >>> s e q = [ 1 , 2 , 3 ]

2 >>> f o r i t e m i n s e q :

3 . . .

p r i n t ( ’ Ol á ’ )

4 Ol á

5 Ol á

6 Ol á

Exemplo 1.13.

1 >>>x = 1

2 >>> w h i l e x < 5 :

3 . . .

p r i n t ( ’ Al ô ’ )

4

x = x + 1

5 >>> e l s e :

6 . . .

p r i n t ( ’ ’ End w h i l e )

7

8 Al ô

25

9 Al ô

10 Al ô

11 Al ô

12 End w h i l e

1.7 Funções

1.7.1 A função range()

A função range() gera progressões aritméticas, o que é muito útil caso se queira iterar

sobre sequências numéricas. O ponto de parada fornecido não é incluído na lista. range (start,

stop, step). Veja abaixo o signiﬁcado dos parâmetros:

1. start é o valor do parâmetro inicial, se não for fornecido será 0;

2. stop é o valor do parâmetro ﬁnal;

3. step é o valor do parâmetro incremento, se não for fornecido será 1.

Exemplo 1.14.

1 >>> range ( 0 , 2 0 , 3 )

2 >>>b = l i s t ( range ( 0 , 2 0 , 3 ) )

3 >>> p r i n t ( b )

4

[ 0 , 3 , 6 , 9 , 1 2 , 1 5 , 1 8 ]

Exemplo 1.15.

1 >>> range ( 0 , 5 0 , 3 )

2 >>>a = l i s t ( range ( 0 , 5 0 , 3 ) )

3 >>> p r i n t ( a )

4

[ 0 , 3 , 6 , 9 , 1 2 , 1 5 , 1 8 , 2 1 , 2 4 , 2 7 , 3 0 , 3 3 , 3 6 , 3 9 , 4 2 , 4 5 , 4 8 ]

26

Exemplo 1.16.

1 >>>a = range ( 1 , 1 0 )

2 >>> f o r

i

i n range ( 1 , 1 0 ) :

3 . . .

p r i n t ( i )

4

5

6

7

8

9

10

11

12

1

2

3

4

5

6

7

8

9

1.7.2 A função zip()

A função zip() cria uma lista de tuplas, de tal modo que, a i-ésima tupla é formada pelos

i-ésimos elementos de cada um de seus argumentos.

Exemplo 1.17.

1 >>>x = [ 1 , 2 , 3 ]

2 >>>y = [ ’ a ’ ,

’ b ’ ,

’ c ’ ]

3 >>> f o r

t

i n z i p ( x , y ) :

4 . . .

p r i n t ( t )

5 ( 1 ,

’ a ’ )

6 ( 2 ,

’ b ’ )

7 ( 3 ,

’ c ’ )

Para visualizar as tuplas geradas pela função zip(), em forma de lista, devemos usar a

função interna list.

Exemplo 1.18.

27

1 >>>x = [ 1 , 2 , 3 ]

2 >>>y = [ ’ a ’ ,

’ b ’ ,

’ c ’ ]

3 >>> l i s t ( z i p ( x , y ) )

4

[ ( 1 ,

’ a ’ ) ,

( 2 ,

’ b ’ ) ,

( 3 ,

’ c ’ ) ]

1.8 Deﬁnindo funções

Para deﬁnir uma função devemos iniciar pelo comando def, seguido do nome da função

e da lista de parâmetros formais entre parênteses. Os comandos que formam o corpo da função

devem ser indentados e começam na linha seguinte e, por ﬁm, a instrução return retorna um

valor da função e ﬁnaliza a execução. Veja os exemplos abaixo.

Exemplo 1.19.

1 >>> d e f

f i b ( n ) : # A f u n ç ão que i m p r i m e a s e q u ê n c i a d o s n ú meros de F i b o n a c c i

m e n o r e s que n .

a , b = 0 , 1 # D e f i n i m o s o v a l o r

i n i c i a l p a r a a e b .

w h i l e a < n :

p r i n t ( a , end = ’

’ ) # I m p r i m e o t e r m o ’ a ’ da s e q u ê n c i a e n q u a n t o a

f o r menor que n . o comando end =’

’

i n s e r e um e s p a ç o e n t r e o s

t e r m o s da

s e q u ê n c i a .

a , b = b , a+b # I t e r a f a z e n d o a=b e b=a+b de f o r m a r e c u r s i v a .

2 . . .

3 . . .

4 . . .

5 . . .

6

7 # Agora chamaremos a f u n ç ão como d e f i n i d a

8 >>> f i b ( 2 0 0 )

9

0 1 1 2 3 5 8 13 21 34 55 89 144

Exemplo 1.20.

1 >>> d e f media ( a , b ) :

2 . . .

r e t u r n ( a+b ) / 2

3 >>> media ( 5 , 6 )

4

5 . 5

1.9 Funções lambda

As funções lambda reduzem o tamanho do código tornando o processo de programação

28

mais prático.

Exemplo 1.21.

1 >>> a = lambda x : x **4
2 >>> a ( 3 )

3

81

1.10 Listas

Listas são estruturas de dados compostas usadas para agrupar itens. As listas (list) tem

seus elementos separados por vírgula, entre colchetes.

Exemplo 1.22.

1 >>> p r i m o s = [ 2 , 3 , 5 , 7 , 1 1 , 1 1 , 1 3 ]

2 >>> p r i m o s

3

[ 2 , 3 , 5 , 7 , 1 1 , 1 1 , 1 3 ]

De modo análogo as strings, as listas podem ser fatiadas e concatenadas retornando uma

nova lista contendo os elementos solicitados.

Exemplo 1.23.

1 >>> p r i m o s = [ 2 , 3 , 5 , 7 , 1 1 , 1 1 , 1 3 ]

2 >>> p r i m o s [ 1 : 5 ]

3

[ 3 , 5 , 7 , 1 1 ]

4 >>> p r i m o s + [ 1 7 , 1 9 , 2 3 ]

5

[ 2 , 3 , 5 , 7 , 1 1 , 1 1 , 1 3 , 1 7 , 1 9 , 2 3 ]

As listas diferem das strings, pois são mutáveis, isto é, há possibilidade de alteração de

29

elementos individuais de uma lista.

Exemplo 1.24.

1 >>> p r i m o s = [ 2 , 3 , 5 , 7 , 1 1 , 1 1 , 1 3 ]

2 >>> p r i m o s [ 2 ] = 29

3 >>> p r i m o s

4

[ 2 , 3 , 2 9 , 7 , 1 1 , 1 1 , 1 3 ]

Podemos também adicionar novos elementos no ﬁnal da lista, para isso, basta usar o

método .append()

Exemplo 1.25.

1 >>> p r i m o s = [ 2 , 3 , 5 , 7 , 1 1 , 1 1 , 1 3 ]

2 >>> p r i m o s . a p p e n d ( 1 7 )

3 >>> p r i m o s

4

[ 2 , 3 , 5 , 7 , 1 1 , 1 1 , 1 3 , 1 7 ]

Podemos criar listas contendo outras listas.

Exemplo 1.26.

1 >>> p r i m o s = [ 2 , 3 , 5 , 7 , 1 1 , 1 1 , 1 3 ]

2 >>> l e t r a s = [ ’ a ’ ,

’ b ’ ,

’ c ’ ]

3 >>> l i s t a = [ p r i m o s ,

’ c a s a ’ ,

’ c a r r o ’ ,

l e t r a s ]

4 >>> l i s t a

5

[ [ 2 , 3 , 5 , 7 , 1 1 , 1 1 , 1 3 ] ,

’ c a s a ’ ,

’ c a r r o ’ ,

[ ’ a ’ ,

’ b ’ ,

’ c ’ ] ]

1.11 Tuplas

As Tuplas tuple são sequências imutáveis, diferentemente da lista não há possibilidade de

adicionar ou remover elementos. Uma tupla tem a forma de uma sequência de dados separados

por vírgulas e podem vir ou não entre parênteses.

Exemplo 1.27.

1 >>> t = ’ c a s a ’ , 1 0 ,

’ o l á ! ’

2 >>> t [ 2 ] # O b s e r v e que a c o n t a g e m d o s e l e m e n t o s de uma t u p l a come ç a do 0 da

30

e s q u e r d a p a r a a d i r e i t a .

3 ’ o l á ! ’

4 >>> t

5 ( ’ c a s a ’ , 1 0 ,

’ o l á ! ’ )

6 t [ 1 ] = 4 # N o t e que s e t e r n a r m o s m o d i f i c a r um e l e m e n t o de uma t u p l a t e r e m o s

um e r r o .

7 T r a c e b a c k ( most

r e c e n t

c a l l

l a s t ) :

8

F i l e " < s t d i n > " ,

l i n e 1 ,

i n <module >

9 T y p e E r r o r :

’ t u p l e ’ o b j e c t d o e s n o t

s u p p o r t

i t e m a s s i g n m e n t

1.12 Dicionários

Os dicionários possuem uma estrutura similar as listas, onde cada um de seus elementos

são uma combinação de chaves e valores. Em Python, os dicionários são criados utilizando

chaves.

Exemplo 1.28.

1 >>> a l i m e n t o s = { ’ F e i j ã o ’ : 5 . 8 0 ,

’ A r r o z ’ : 3 . 5 0 ,

’ Tomate ’ : 3 . 3 0 ,

’ Banana ’ :

2 . 4 0 }

2 >>> a l i m e n t o s [ ’ F e i j ã o ’ ]

3

5 . 8

1.13 Valores e operadores booleanos

No Python, uma variável pode assumir dois valores booleanos True (verdadeiro) ou False.

Pode ser muito útil para comparação de variáveis.

Exemplo 1.29.

31

1 >>> a = 2

2 >>> b = 3

3 >>> a > b

4

F a l s e

O Python suporta três operadores booleanos básicos que são or, and e not que são

equivalentes respectivamente a disjunção (∨), conjunção (∧) e a negação (¬). Cada um dos

operadores booleanos segue as regras estabelecidas pela sua tabela verdade.

1.13.1 Or

O operador or só resulta false se seus dois operadores são false. O valor lógico do

operador or é deﬁnido pela tabela verdade:

Tabela 1 – Tabela verdade do or

p

q

p or q

V V

V F

F V

F

F

V

V

V

F

Fonte: Elaborado pelo autor.

Exemplo 1.30.

1 >>> a = F a l s e

2 >>> b = F a l s e

3 >>> a or b

4

F a l s e

1.13.2 And

O operador end só resulta True se seus dois operadores forem True. O valor lógico do

operador and é deﬁnido pela tabela:

32

Tabela 2 – Tabela verdade do and

p

q

p and q

V V

V F

F V

F

F

V

F

F

F

Fonte: Elaborado pelo autor.

Exemplo 1.31.

1 >>> a = T r u e

2 >>> b = T r u e

3 >>> a e b

4

T r u e

1.13.3 Not

O operador not é utilizado para alterar o seu valor lógico de uma sentença, dando ideia

contrária. Segue abaixo a tabela do operador not.

Tabela 3 – Tabela verdade do not

p not p

V

F

F

V

Fonte: Elaborado pelo autor.

Exemplo 1.32.

1 >>> a = T r u e

2 >>> n o t a

3

F a l s e

33

Os Operadores padrões de comparação em Python são < (menor que), > (maior que), ==

(igual), <= (menor ou igual), >= (maior ou igual) e !=(diferente).

No Python temos a indentação como principal característica, usada para agrupar os

comandos em blocos.

1.14 Classes

Podemos pensar uma classe como sendo um construtor de um objeto ou projeto para

criação de objetos. Variáveis dentro de uma classe são chamados de atributos, já funções, quando

dentro de classes, são chamadas de métodos.

Exemplo 1.33.

1 >>> c l a s s Exemplo :

2 . . .

3 . . .

4 . . .

5 . . .

6 . . .

7 . . .

8 . . .

9 . . .

d e f _ _ i n i t _ _ ( s e l f , x , y ) :

s e l f . a = x

s e l f . b = y

d e f

s e q ( s e l f , n ) :

w h i l e

s e l f . a < n :

p r i n t ( s e l f . a , end = ’

’ )

s e l f . a ,

s e l f . b = s e l f . b ,

s e l f . a + s e l f . b

p r i n t ( )

10 >>> z = Exemplo ( 0 , 1 )

11 >>> z . s e q ( 2 0 0 )

12

0 1 1 2 3 5 8 13 21 34 55 89 144

O método __init__ deﬁne os valores iniciais da classe, que podem ser usados por outros

métodos.

1.15 Comandos importantes

34

• break é um comando que tem como função sair de imediato do laço de repetição mais

interno, seja for ou while.

• pass é um comando que não faz nada e é usado quando a sintaxe exige e não é necessária

nenhuma ação ou, temporariamente, quando se está decidindo ou desenvolvendo o código

para ocupar seu lugar.

1.16 Bibliotecas cientíﬁcas do Python

1.16.1 NumPy

O NumPy é uma biblioteca cientíﬁca do Python feita para trabalhar com álgebra linear.

A classe array do Numpy é chamada de ndarray e oferece mais funcionalidades que a biblioteca

array padrão do Python que trabalha apenas com matrizes unidimensionais.

Exemplo 1.34.

1 >>> import numpy a s np

1.16.2 Pandas

O Pandas é uma ferramenta construída em linguagem Python para manipulação e análise

de dados em código aberto. Devemos importá-lo usando o código abaixo

Exemplo 1.35.

1 >>> import p a n d a s a s pd

1.16.3 Matplotlib

É uma biblioteca muito utilizada em machine learning para visualização de dados através

de plotagem dos mais variados tipos de gráﬁcos. Para utilizamos essa biblioteca precisamos

importar com o camando import da biblioteca matplotlib a parte de plotar gráﬁcos pyplot

35

utilizando o comando como no exemplo abaixo

Exemplo 1.36.

1 >>> import m a t p l o t l i b . p y p l o t a s p l t

36

2 MATEMÁTICA PARA REDES NEURAIS

As redes neurais são modelos computacionais que modelam o cérebro humano, tais

modelos são baseados em cálculos matemáticos, onde se usa fortemente conhecimentos de

álgebra linear, como operações com matrizes, cálculo com funções de várias variáveis e máximos

e mínimos. Assim, neste capítulo, veremos os principais conceitos matemáticos utilizados na

construção de uma rede neural.

2.1 Vetores

Um vetor

→
v , do ponto de vista geométrico, é o conjunto de segmentos orientados munidos

de mesmo módulo, direção e sentido.

• Módulo: É o comprimento do vetor

→
v denotado por

(cid:13)
(cid:13)
(cid:13)

→
v

(cid:13)
(cid:13)
(cid:13);

• Direção: Deﬁnimos a direção, como sendo a reta suporte que contém seus representantes,

isto é, dois vetores

→
u e

→
v têm a mesma direção se possuírem representantes na mesma

reta suporte;

• Sentido: Podemos deﬁnir sentido, em segmentos orientados com mesma direção, desse
→
u e

→
v tem sentidos contrários.

→
BA dizemos que

→
AB e

modo, se

→
u =

→
v =

Figura 17 – Representação de um vetor no plano cartesiano

y

4

→
v = (2, 4)

O

2

x

Fonte: Elaborada pelo autor.

Trataremos, neste trabalho, vetores como um ponto em algum espaço de dimensão ﬁnita. Nesse

ponto de vista, vetores são excelentes ferramentas para representar dados numéricos.

Podemos realizar, com vetores, operações de adição, subtração e produto por um número

real. Dados os vetores

→
u = (u1, u2, u3) e

→
v = (v1, v2, v3)

37

→
u +

→
v = (u1 + v1, u2 + v2, u3 + v3)

→
u −

→
v = (u1 − v1, u2 − v2, u3 − v3)

•

•

• k

→
u = k (u1, u2, u3) = (ku1, ku2, ku3)

Exemplo 2.1. Sejam

→
u = (1, 3, 2),

→
v = (2, 0, 1) e k=5,

→
u +

→
v = (1 + 2, 3 + 0, 2 + 1) = (3, 3, 3)

→
u −

→
v = (1 − 2, 3 − 0, 2 − 1) = (−1, 3, 1)

•

•

• 5

→
u = 5 (1, 3, 2) = (5 · 1, 5 · 3, 5 · 2) = (5, 15, 10)

2.2 Matrizes

Deﬁnição 2.1. Uma matriz A do tipo m × n é uma lista de números aij, onde 1 ≤ i ≤ m e

1 ≤ j ≤ n, dispostos em m linhas e n colunas, de forma que o elemento aij está localizado no

cruzamento da i-ésima linha com a j-ésima coluna.

A =













a11

a12

. . . a1n

a22
a21
...
...
am1 am1

. . . a2n
...
. . .
. . . amn













Podemos criar uma matriz em Python usando a biblioteca NumPy de acordo com o

código abaixo.

1 >>> a = np . a r r a y ( [ [ 1 , 2 , 3 ] ,

[ 4 , 5 , 6 ] ] ) # C r i a um a r r a y com 2 l i n h a e 3

c o l u n a s .

2 >>> a

3 . . .

a r r a y ( [ [ 1 , 2 , 3 ] ,

4

[ 4 , 5 , 6 ] ] )

38

Vamos tratar as linhas e as colunas de uma matriz como vetores linha e coluna respecti-

vamente.

2.2.1 Soma de matrizes

Deﬁnição 2.2. Sejam A e B duas matrizes do mesmo tipo m × n, a soma A + B é deﬁnida como

a soma elemento a elemento.

A + B =

=

























a11

a12

. . . a1n

b11

b12

a22
a21
...
...
am1 am1

. . . a2n
...
. . .
. . . amn

b21
...
bm1

b22
...
bm1













+













. . .

. . .
. . .

. . .













b1n

b2n
...
bmn

a11 + b11

a12 + b12

a21 + b21
...

a22 + b22
...

am1 + bm1 am1 + bm1

. . .

. . .
. . .

a1n + b1n

a2n + b2n
...

. . . amn + bm1













Exemplo 2.2. Sendo A =

soma A + B










2

1

4

4 −2 0

6

0

1










e B =



















7 0 5

1 0 1

3 8 2

A + B =










9

1

9

5 −2 1

9

8

3










matrizes do mesmo tipo 3 × 3 a

2.2.2 Multiplicação de matrizes por um número

Seja A uma matriz m × n e k um número, o produto de k por M é deﬁnido como

multiplicação k por cada elemento da matriz A.

39

kA = k













a11

a12

. . . a1n

a21
a22
...
...
am1 am1

. . . a2n
...
. . .
. . . amn













=













ka11

ka12

. . . ka1n

ka21
...

ka22
...

. . . ka2n
...
. . .

kam1 kam1

. . . kamn













Exemplo 2.3. Seja A =



















2

1

4 −2

6

0

o produto 3A é

3A =










6

3

12 −6

18

0










2.2.3 Multiplicação de matrizes

Considere duas matrizes A = (aij)m×n e B = (bjk)n×p. O produto das matrizes A e B é

uma matriz C tal que

cik = ai1b1k + ai2b2k + . . . + ainbnk =

n
X

j=1

aijbjk

Onde 1 ≤ i ≤ m e 1 ≤ k ≤ p.

Observe que o produto usual de matrizes AB só existe se o número de colunas de A for

igual ao número de linhas de B. Desse modo, a matriz AB é uma matriz com o mesmo número

de linhas de A e mesmo número de colunas de B.

Exemplo 2.4. Dadas as matrizes A =






0 1 4

4 0 1



 e B =



















1

0

2

O produto de A por B, existe,

pois as matrizes são respectivamente dos tipos 2 × 3 e 3 × 1, fazendo C = AB






AB =

0 · 1 + 1 · 0 + 4 · 2

4 · 1 + 0 · 0 + 1 · 2



 =











8

6

40

2.2.4 O produto Hadamard

O produto Hadamard de matrizes do mesmo tipo, também conhecido como produto

de Schur, é muito utilizado em diferentes áreas em diversas aplicações. Dadas duas matrizes

A = (aij) e B = (bij), do mesmo tipo m × n, o produto Hadamard de A por B é deﬁnido por

A (cid:12) B = aijbij

Exemplo 2.5. Dadas as matrizes A =

1 3 4
A (cid:12) B, de A por B, existe, pois, as matrizes são do mesmo tipo 2 × 3.

1 1 5






2 4 0

0 2 0



 e B =









 O produto Hadamard

A (cid:12) B =






2 · 0 4 · 2 0 · 0

1 · 1 1 · 3 5 · 4



 =











0 8

0

1 3 20

2.3 Funções reais de uma variável real

Deﬁnição 2.3. Dado o conjunto X ⊂ R uma função de X em R, isto é, f : X → R é chamada

de função real de uma variável real.

Exemplo 2.6. A função f : R → R deﬁnida por f (x) = x2 + 2x − 1 é uma função real de uma

variável real.

Deﬁnição 2.4. Sejam f uma função e a ∈ Df um ponto de seu domínio. Quando existir o limite

lim
x→a

f (x) − f (a)
x − a

= L 6= ±∞

fazendo h = x − a temos x = a + h teremos outra expressão da derivada

lim
h→0

f (a + h) − f (a)
h

= L 6= ±∞

diremos que L é a derivada de f em a e escrevemos:

f 0 (a) = L

A próximas proposições trazem as regras de derivação que permitem o cálculo de

derivadas sem o uso da deﬁnição.

Proposição 2.1. Sejam f e g funções deriváveis em a e k uma constante. Então:

41

(a) dy

dx (k) = 0;

(b) (kf )0 (a) = kf 0 (a);

(c) (f + g)0 (a) = f 0 (a) + g0 (a);

(d) (f g)0 (a) = f 0 (a) g (a) + f (a) g0 (a);

(e)

(cid:17)0

(cid:16) f
g

(a) = f 0(a)g(a)−f (a)g0(a)

[g(a)]2

.

Demonstração: Vamos demonstrar apenas o item (d), os outros itens são análogos. Seja

h(x) = f (x)g(x), da deﬁnição de derivada temos

h0(a) = lim
x→a

h (x) − h (a)
x − a

= lim
x→a

= lim
x→a

= lim
x→a

f (x) g (x) − f (a) g (a)
x − a

f (x) g (x) − f (a) g (x) + f (a) g (x) − f (a) g (a)
x − a

" f (x) − f (a)
x − a

g(x) + f (a)

g (x) − g (a)
x − a

#

= f 0(a)g(a) + f (a)g0(a)

Proposição 2.2. Se f (x) = xn onde n é um inteiro positivo, então

f 0(x) = nxn−1

Demonstração: Utilizaremos a fórmula abaixo

xn − an = (x − a)(xn−a + xn−aa + · · · xan−2 + an−1)

que pode ser veriﬁcada aplicando indução sobre n. Pela deﬁnição da derivada temos

f 0(a) = lim
x→a

f (x) − f (a)
x − a

= lim
x→a

xn − an
x − a

= lim
x→a

(xn−a + xn−aa + · · · xan−2 + an−1)

= an−a + an−aa + · · · aan−2 + an−1

= nan−1

42

Proposição 2.3. Seja g e f duas funções reais de uma variável real tais que, g é derivável em

x e f é derivável em g(x). Então a função composta h = f ◦ g deﬁnida por h(x) = f (g(x)) é

derivável em x e h0(x) é dada por

h0(x) = f 0(g(x))g0(x)

Exemplo 2.7. Seja h(x) =

(cid:16)

(cid:17)2

x2 + 1

. Se ﬁzermos f (u) = u2 e g(x) = x2 + 1, então f 0(u) = 2u

e g0(x) = 2x segue pela regra da cadeia que

h0(x) = 2(x2 + 1)2x = 4x(x2 + 1) = 4x3 + 4x

2.4 Funções vetoriais

Uma função vetorial é uma função que tem como domínio o conjunto dos números reais

e como imagem um conjunto de vetores.

Deﬁnição 2.5. Dados os conjuntos I ⊂ R e R3 uma função vetorial de I ⊂ R com valores em
R3 é uma aplicação σ : I → R3 tal que

σ(t) = (x(t), y(t), z(t)), t ∈ I, onde,

x(t),y(t) e z(t) são funções reais deﬁnidas em I.

Dizemos que a função σ(t) é contínua em I se σ(t) é contínua para todo t ∈ I.

Quando σ(t) é contínua em I, O vetor σ(t) = (x(t), y(t), z(t)) descreve uma curva C
em R3. Para cada t ∈ I obtemos um ponto P = (x, y, z) ∈ C, onde x = x(t), y = y(t) e z = z(t).

A equação é chamada de parametrização da curva C e as equações acima são chamadas

de equações paramétricas da curva C e a variável t é chamada de parâmetro.

Exemplo 2.8. Seja r uma reta que passa pelo ponto P0 = (1, 0, 2) e paralela ao vetor não nulo
→
u = (1, 1, 1). Sendo P = (x, y, z) ∈ r, então P P0 = t

→
u , isto é,

Segue que,

(x, y, z) = (1, 0, 2) + t (1, 1, 1)

σ(t) = (1 + t, t, 2 + t), t ∈ R

é uma parametrização da reta r. Portanto, as equações paramétricas da reta r são:

43

x = 1 + t, y = t e z = 2 + t, t ∈ R

Vamos estabelecer, na proposição abaixo, a regra da cadeia para funções vetoriais.

Proposição 2.4. Seja σ(u) uma função vetorial no intervalo I e u uma função real diferenciável

de uma variável real t, com imagem no intervalo I, então

d
dt

σ(u(t)) =

σ
du

(u(t))

du
dt

(t)

2.5 Funções de várias variáveis

Deﬁnição 2.6. Dados os conjuntos D ⊂ Rn e R. Uma função f : D ⊂ Rn → R é uma função
real de n variáveis associa a cada n-upla (x1, x2, . . . , xn) ∈ D ⊂ Rn a um único número real

y = f (x1, x2, . . . , xn).

Exemplo 2.9. A função z = f (x, y) = x4
é o R2 menos os pontos tais que x2 + y2 = 0, isto é, o ponto (0, 0).

x2+y2 é uma função de duas variáveis, onde seu domínio

Deﬁnição 2.7. Sendo f : D ⊂ Rn → R uma função de n variáveis. O gráﬁco de f , denotado por
Gf , é deﬁnido como um subconjunto de Rn+1, isto é, os pontos do gráﬁco de f são da forma

(x1, x2, . . . , xn, z), onde z = f (x1, x2, . . . , xn) com (x1, x2, . . . , xn) ∈ D, Assim,

Gf =

n
(x1, x2, . . . , xn, f (x1, x2, . . . , xn)) ∈ Rn+1|(x1, x2, . . . , xn) ∈ D

o

Desse modo, podemos visualizar somente os gráﬁcos de funções de uma e de duas
variáveis, que são subconjuntos do R2 e do R3 respectivamente, funções de três ou mais variáveis

não são visualizáveis pois são subconjuntos de quatro ou mais dimensões.

Exemplo 2.10. Veja abaixo gráﬁcos de funções de uma e de duas variáveis.

44

y

O

Figura 18 – Gráﬁcos de uma e de duas variáveis

32

12

3

−3

3

−3

x

Fonte: Elaborada pelo autor.

2.5.1 Derivadas parciais

Deﬁnição 2.8. Seja f (x, y) uma função de duas variáveis deﬁnido em um conjunto I da forma

(a1, b1) × (a2, b2) e (x0, y0) um ponto de I. A derivada parcial de f com relação a x no ponto

(x0, y0) é deﬁnida por

se o limite existir.

∂f
∂x

= lim
∆x→0

f (x0 + ∆x, y0) − f (x0, y0)
∆x

,

De modo análogo, a derivada parcial de f com relação a y no ponto (x0, y0) é deﬁnida

por

se o limite existir.

∂f
∂y

= lim
∆y→0

f (x0, y0 + ∆y) − f (x0, y0)
∆y

,

A deﬁnição das derivadas parciais para funções de n variáveis onde n ≥ 3 é idêntico ao

caso de duas variáveis.

Exemplo 2.11. Seja f (x, y) = 3x2 + y3. Calcule ∂f

∂x e ∂f
∂y .

Solução: Para calcular ∂f

∂x devemos considerar y como uma constante e derivar f com

relação a x do mesmo modo como em funções de apenas uma variável, desse modo,

∂f
∂x

= 6x

Analogamente, consideramos x constante e derivando com relação a y, temos

45

∂f
∂y

= 3y2

A próxima proposição traz a regra da cadeia para funções de duas variáveis.

Proposição 2.5. Seja z = f (x, y) uma função derivável nas variáveis x e y, onde x = g(t) e

y = h(t) são funções deriváveis de t. Então z é uma função diferenciável de t e

∂z
∂t

=

∂f
∂x

dx
dt

+

∂f
∂y

dy
dt

Deﬁnição 2.9. Seja f uma função de n variáveis (x1, x2, . . . , xn) que possui derivadas parciais
no ponto P0 = (x0

n). O vetor gradiente de f em P0, é o vetor

2, . . . , x0

1, x0

∇f (P0) =

  ∂f
∂x1

(P0), . . . ,

!

(P0)

∂f
∂xn

.

2.6 Máximos e mínimos

Deﬁnição 2.10. Sendo a ∈ X e dado um número real r > 0. A bola aberta de centro a e raio r é

o conjunto B (a, r) dos pontos cuja distância ao ponto a é menor que r. isto é,

B (a, r) = {x ∈ X, d (x, a) < r}

Deﬁnição 2.11. Seja z = f (x, y) uma função real deﬁnida em um conjunto D ⊂ R2 e (x0, y0) ∈

D. Chamamos de valor mínimo relativo de f (resp. valor máximo relativo de f ) se existir uma

bola aberta Br(x0, y0) ⊂ D de modo que

f (x, y) ≥ f (x0, y0) (resp.f (x, y) ≥ f (x0, y0))

,

Para todo (x, y) em B.

A proposição abaixo estabelece uma condição necessária para a existência de máximos e

mínimos relativos.

46

Proposição 2.6. Sejam z = f (x, y) uma função deﬁnida em D ⊂ R2 cujo interior é não vazio, e
∂x (x0, y0) e ∂f
(x0, y0) pertence ao interior de D. Se ∂f

∂y (x0, y0) existem e f (x, y) tem um máximo

ou um mínimo relativo em (x0, y0) então

∂f
∂x

(x0, y0) = 0 e

∂f
∂y

(x0, y0) = 0

.

Deﬁnição 2.12. Um ponto (x0, y0) pertencente ao interior do domínio de f (x, y) é chamado

ponto crítico de f se ∇f (x0, y0) não existe ou ∇f (x0, y0) = (0, 0)

Deﬁnição 2.13. Um ponto (x0, y0) pertencente ao interior do domínio de f (x, y) é chamado

ponto crítico de f se ∇f (x0, y0) não existe ou ∇f (x0, y0) = (0, 0)

A natureza de um ponto crítico poderá ser analisada pelo teste da segunda derivada de

acordo com a proposição abaixo.

Proposição 2.7. Seja f (x, y) uma função de classe C2 dentro da bola aberta Br(x0, y0). Supo-

nha que (x0, y0) é um ponto crítico de f (x, y), isto é, ∇f (x0, y0) = (0, 0). Fazendo

A =

∂2f
∂x2 (x0, y0), B =

∂2f
∂x∂y

(x0, y0) e C =

∂2f
∂y2 (x0, y0).

Se

(i) B2 − AC < 0 e A < 0, então f tem um valor máximo relativo em (x0, y0);

(ii) B2 − AC < 0 e A > 0, então f tem um valor mínimo relativo em (x0, y0);

(iii) B2 − AC >, então f tem um ponto de cela em (x0, y0).

47

3 NOÇÕES DE APRENDIZADO DE MÁQUINA

O aprendizado de máquina é um ramo da inteligência artiﬁcial que trabalha com modelos

capazes de reconhecer padrões a partir dos dados apresentados. Este tipo de inteligência tem uso

imprescindível em tarefas computacionais em que algoritmos explícitos não são viáveis.

3.1

Inteligência artiﬁcial

A inteligência é a capacidade de tomar a melhor decisão possível, dada a informação

disponível, com a capacidade de se adaptar a novas situações. A inteligência artiﬁcial é, por sua

vez, a capacidade das máquinas de simular os seres humanos, isto é, resolver problemas e tomar

decisões inteligentes.

3.2 Aprendizado de máquina

Também chamado de modelo preditivo ou mineração de dados é um campo da ciência

da computação que cria modelos que evoluem, isto é, aprendem a partir dos dados que absorvem

e através do reconhecimento de padrões.

Deﬁnição 3.1. Um algoritmo é uma sequência ﬁnita de instruções, onde cada uma delas são

executadas em um tempo ﬁnito, com as quais podemos transformar a entrada na saída desejada.

Exemplo 3.1. Em uma soma, a entrada são os dois valores que desejamos somar e sua respectiva

saída é o resultado da soma.

3.3 Categorias de aprendizado de máquina

Existem vários tipos de aprendizado de máquina, daí surge a necessidade de categorizá-

los tomando como base suas características. Por exemplo, se são ou não treinados com supervisão

humana, se podem ou não aprender rapidamente, se comparam dados novos com os dados

conhecidos ou se encontram padrões nos dados de treinamento e criam um modelo preditivo.

48

3.3.1 Aprendizado de máquina supervisionado

O aprendizado de máquina supervisionado ocorre quando os dados usados para treinar

o algoritmo incluem a solução desejada, classe ou rótulo ("label"), isto é, tem um conjunto de

dados rotulados que traz a resposta certa do problema.

Exemplo 3.2. O ﬁltro de spam é um exemplo aprendizado de máquina supervisionado, pois ele

é treinado com um conjunto de e-mails, onde uma parte são classiﬁcados como spam e a outra

parte como não spam, com o objetivo que o ﬁltro aprenda a classiﬁcar novos e-mails.

Podemos realizar dois tipos de tarefas de aprendizado supervisionado:

• Classiﬁcação: Quando a variável a ser predita é qualitativa, neste caso, o objetivo é fazer

a previsão de classes.

• Regressão: Ocorre quando a variável a ser predita é quantitativa.

3.3.2 Aprendizado de máquina não supervisionado

No aprendizado de máquina não supervisionado, a máquina recebe um conjunto de

entradas, mas nenhum conjunto de saídas correspondente. Nesse caso, cabe à máquina encontrar

padrões de semelhança e diferenças entre os dados e, com esses padrões, gerar novas saídas

corretas. Não existe rótulo ("label") o algoritmo aprende sem uma resposta certa.

Exemplo 3.3. A identiﬁcação dos perﬁs de clientes que compram em uma loja, onde o objetivo

é agrupá-los em várias categorias de acordo com semelhanças e diferenças.

3.3.3 Aprendizado de máquina semi-supervisionado

Em aprendizado de máquina, podemos trabalhar com conjuntos de dados onde existem

dados com rótulo e uma grande quantidade de dados sem rótulo, nesse caso, dizemos que se

trata de aprendizado de máquina semi-supervisionado. A maioria dos algoritmos de aprendizado

semi-supervionado são uma combinação dos algoritmos de aprendizado supervisionando com os

de aprendizado não supervisionado.

Exemplo 3.4. Um bom exemplo disso é a identiﬁcação de fotos em redes sociais. O algoritmo

identiﬁca a mesma pessoa em várias fotos (não supervisionado) e depois só precisa de um

49

rótulo(supervisionado).

3.3.4 Aprendizado de máquina por reforço

Em Aprendizado de máquina por reforço há interação com o ambiente dinâmico, com o

objetivo de executar ações e obter, em troca, feedbacks em termos de premiações e punições. O

aprendizado ao longo do tempo tem com o objetivo realizar as ações com as melhores estratégias.

Um bom exemplo, onde se usa aprendizado por reforço, é em robôs para que aprendam a andar.

3.3.5 Aprendizado online ou em lote

No aprendizado online o sistema aprende de modo incremental a partir de um ﬂuxo de

dados recebido, enquanto no aprendizado em lote o sistema não é capaz de aprender de forma

incremental, isto é, é treinado com a quantidade de dados disponível e geralmente feito ofﬂine, o

sistema depois de treinado roda sem aprender mais nada.

3.3.6 Aprendizado baseado em instâncias

O sistema aprende ao memorizar os exemplos e, posteriormente, generaliza para novos

casos analisando a similaridade.

3.3.7 Aprendizado baseado em modelo

No aprendizado baseado em modelo constrói-se um modelo a partir de um conjunto de

exemplos o qual é usado para fazer previsões.

3.4 Sobreajuste e sub-ajuste

Um projeto de aprendizado de máquina pode não funcionar bem, isto é, pode não ter o

desempenho esperado. Isso pode acontecer devido a dois fatores, um deles é que o algoritmo

pode ser ruim, outro é que os dados podem ser ruins. Quando se faz um treinamento com o

conjunto de dados ruim, isto é, que não é representativo, provavelmente, não haverá previsões

50

satisfatórias.

3.5 Sobreajuste

É um modelo que funciona bem com os dados de testes, mas não tem bom desempenho

com dados novos, isto é, não generaliza muito bem. O sobreajuste acontece normalmente devido

a quantidade ou a qualidade do conjunto de dados de treinamento. Para evitar o sobreajuste pode

ser necessário usar um modelo mais simples, com menos parâmetros, coletar mais dados ou

reduzir os ruídos dos dados de treinamento.

Figura 19 – Gráﬁco de um modelo com sobreajuste

y

x

Fonte: Elaborada pelo autor.

3.6 Sub-ajuste

Não funciona bem nem com os dados de treino nem com os dados novos. Normalmente

ocorre quando o modelo é muito simples para um aprendizado complexo em relação a conjunto de

dados, daí a tendência de imprecisão de suas predições. Nesse caso, para solucionar o problema

de sub-ajuste se torna necessário escolher um modelo melhor, com mais parâmetros.

Para tentar evitar problemas como sobreajuste e sub-ajuste podemos dividir o conjunto

de dados da seguinte forma: dois terços dos dados serão usados para treinar o modelo e o um

terço restante usado para testar o modelo. É comum dividir o conjunto de forma que 80% dos

dados sejam utilizados para treinar o modelo e 20% para testá-lo.

Exemplo 3.5. O gráﬁco abaixo representa um treinamento que teve um sub-ajuste.

51

Figura 20 – Gráﬁco de um modelo com sub-ajuste

y

x

Fonte: Elaborada pelo autor.

Exemplo 3.6. O gráﬁco abaixo representa um treinamento que faz boas previsões.

Figura 21 – Gráﬁco de um modelo com um bom treinamento

y

x

Fonte: Elaborada pelo autor.

52

4 REDES NEURAIS

As redes neurais artiﬁciais são modelos matemáticos que apresentam um grande poder

de processamento e que se estabelece na estrutura ﬁsiológica do cérebro humano com objetivo

de reproduzir suas funções. Se faz necessário, antes de mais nada, buscar compreender como as

redes neurais funcionam, conhecendo os estudos que proporcionaram o surgimento dessa área.

Diferentemente do que se pensa hoje em dia, a teoria das redes neurais não é nova. As

redes neurais foram desenvolvidas inicialmente pelo neuroﬁsiologista Warren McCulloch e pelo

matématico Walter Pitts, ainda na década de 1940. Eles criaram um modelo computacional

simpliﬁcado de como neurônios artiﬁciais podem trabalhar para realizar cálculos usando lógica

proposicional, construindo assim a primeira arquitetura de rede neural artiﬁcial, ainda sem

capacidade de aprendizado. Desde então, foram criadas muitas outras arquiteturas de redes

neurais.

Surgiram diversos estudos que impulsionaram a origem de vários modelos de redes

neurais que são aplicados atualmente. Porém, a aplicação das técnicas de redes neurais apontava

para algumas limitações, impossibilitando assim, a utilização na solução de alguns problemas.

Durante os anos 80 novas arquiteturas foram inventadas, juntamente com novas técnicas

de treinamento, o que provocou uma nova demanda de interesse em pesquisas na área, mas o

avanço continuou lento. Nos anos 90, novas técnicas de aprendizado de máquina foram inventadas

como o Support Vector Machine (SVM) que pôs as redes neurais em segundo plano.

A partir dos anos 2000, as redes neurais voltaram ao centro das atenções devido ao cres-

cimento do poder de processamento dos computadores além da quantidade de dados disponíveis,

o que facilitou a implementação e o treinamento dessas redes e consequentemente avanço do

Deep Learning.

As Redes Neurais Artiﬁciais ou apenas redes neurais, como foi dito, são modelos de

inteligência artiﬁcial inspirado no funcionamento do cérebro humano, elas são formadas por

células computacionais denominadas neurônio ou unidades de processamento conectadas, onde

cada neurônio é alimentado por todos os neurônios da camada imediatamente anterior. Este

modelo pode solucionar uma grande variedade de problemas como reconhecimento facial,

classiﬁcação de imagens, recomendar vídeos de acordo com o interesse dos usuários de uma

plataforma ou aprender vencer campeões de jogos como o xadrez.

53

4.0.1 Neurônio biológico

O neurônio biológico, com tamanho aproximado de 100µm, é composto por um corpo

celular com muitas extensões de ramiﬁcações chamadas dendritos, além de uma longa extensão

chamada Axônio.

Figura 22 – Neurônio biológico

Fonte: https://metodosupera.com.br/neuronios-glossario-do-cerebro/. Acesso: OUT, 2020.

Os dendritos tem como função a recepção de estímulos nervosos vindos de outros

neurônios ou do ambiente, que são transmitidos para o corpo celular, também chamado de soma,

que combina as informações e as processa, gerando um novo impulso que depende da intensidade

e frequência do impulso anterior. O novo estímulo é transportado pelo axônio ao encontro de

outro neurônio. O contato das terminações do axônio de um neurônio com os dendritos de outro é

chamada de sinapse, desse modo, o sinal do neurônio ﬂui da esquerda para a direita transmitindo

estímulos elétricos através de sinapses.

4.0.2 Neurônio artiﬁcial

As Redes neurais são compostas por unidade de processamento, inspiradas em neurônios

biológicos, denominadas neurônios artiﬁciais, dispostos em camadas e densamente interligados.

Um neurônio artiﬁcial é representado no diagrama abaixo:

CorpocelularAxônioTerminaçõesdo  a xônioDendritosFigura 23 – Neurônio artiﬁcial

54

Bias

b

Σ

Função de

ativação
σ

Saída
ˆyj

Entradas

x1

x2
...
xm

wj1

wj2
...
wjm

Pesos

Fonte: https://tex.stackexchange.com/questions/132444/diagram-of-an-artiﬁcial-neural-network.

Acesso: OUT, 2020.

Observe que, no neurônio, cada entrada está associada a um peso denominado peso

sináptico que pode assumir valores positivos ou negativos dependendo do comportamento da

conexão ser excitatório ou inibitório, respectivamente. Desse modo, o integrador representado
pele letra Σ realiza a soma das entradas ponderadas por esses pesos, isto é, uj = Pm

i=1 wjixi.

Note que a posição dos índices dos pesos wji são invertidas, o primeiro índice (j) se refere ao

neurônio e o segundo índice (i) se refere a entrada ou ao neurônio da camada anterior, no caso de

camada oculta ou camada de saída, isto é, indica de onde vem o sinal de ativação. Adicionando o

termo de bias obtemos o campo local induzido vj = uj + bj. A saída da rede é obtida aplicando

a função de ativação σ em vj. Podemos simpliﬁcar esses cálculos usando representação matricial

como segue.

uj =

m
X

i=1

wjixi = wj1x1 + wj2x2 + . . . + wjmxm = xT w

vj = uj + bj

ˆyj = σ(vj)

Portanto,

onde

55

• x = [x1, x2, . . . , xm]T é o vetor onde suas componentes são os sinais de entrada;

• w = [wj1, xj2, . . . , xjm]T é o vetor onde suas componentes são os pesos sinápticos de

ligação das unidades de entrada com neurônio j;

• uj é o sinal de saída do integrador linear devido aos sinais de entrada;

• bj é termo de bias, que é um parâmetro externo do neurônio

• vj é chamado de campo local induzido;

• σ(•) é uma função de ativação;

• ˆyj é o sinal de saída do neurônio.

4.0.3 O termo bias

O termo bias é acrescentado na equação do campo local induzido v = wjixi + bj que é

a equação de uma reta, onde wji é o coeﬁciente angular e bj é o coeﬁciente linear da reta. Em

uma reta, fazendo variar o coeﬁciente angular giramos a reta no sentido horário ou anti-horário.

Por sua vez, o coeﬁciente linear, o termo de bias bj, deﬁne se a reta para pela origem, acima ou

abaixo da origem, desse modo, o bias aumenta a liberdade de posicionamento da reta. A reta que

separa as duas classes de um conjunto de dados binários é chamada reta de decisão.

Exemplo 4.1. Veja os gráﬁcos abaixo, se não existir um bias diferente de 0 a reta não conseguirá

separar as classes, pois só poderá rotacionar em torno da origem durante o treinamento.

y

bb

Figura 24 – A importância do termo bias

y

x

x

Fonte: Elaborada pelo autor.

Podemos trabalhar, equivalentemente, com o termo bias embutido, isto é, adicionando a

entrada

56

e o seu respectivo peso é

x0 = 1

wj0 = bj

Desse modo, o vetor de entrada terá como primeira componente 1, x = [1, x1, x2, . . . , xm]T
e o vetor de pesos terá como primeira componente o termo de bias bj, w = [bj, wj1, xj2, . . . , xjm]T .

Assim podemos reescrever a equação do campo local induzido da seguinte forma.

vj =

m
X

i=0

wjixi.

4.0.4 Funções de ativação

As funções de ativação são transformações não-lineares, feitas ao longo do sinal de

entrada e enviada para a próxima camada de neurônios como uma nova entrada, em caso de

camada oculta. Essas funções deﬁnem a saída de um neurônio em um intervalo, isto é, restringem

a amplitude de um neurônio, normalmente saída de um neurônio é representado como um

intervalo normalmente [0, 1] ou [−1, 1]. As principais funções de ativação são:

• Função ReLU:A função de ativação linear retiﬁcada (Rectiﬁed Linear Unit - ReLU) é

uma função linear por partes que se tornou a mais popular função de ativação na atualidade,

sendo padrão em algumas redes neuras devido a facilidade do treinamento e do desempenho

alcançado.

φ(u) =






u,

0,

se u ≥ 0

se u < 0

57

Figura 25 – Gráﬁco da função de ativação ReLu

y
5

O

−5

Fonte: Elaborada pelo autor.

• Função Sigmoide: A função sigmoide deﬁnida por

σ(u) =

1
1 + exp(−au)

.

Veja o gráﬁco abaixo da função sigmóid para a = 1.

Figura 26 – Gráﬁco da função de ativação sigmoid

y

1

0.5

O

−5

φ

x

5

σ

x

5

Fonte: Elaborada pelo autor.

Onde a é o seu parâmetro de inclinação. A função sigmoide tem propriedades importantes

que a tornam uma das funções de ativação mais utilizada em construções de redes neurais,

como ser estritamente crescente, exibir um balanceamento adequado entre comportamento

linear e não linear e ser uma função diferenciável.

58

Podemos deﬁnir a função sigmoid, em python, da seguinte maneira:

1 >>> d e f

s i g m o i d ( z ) :

2 . . .

r e t u r n 1 . 0 / ( 1 . 0 + np . exp ( − z ) )

3 >>> s i g m o i d ( 1 0 )

4 . . . 0 . 9 9 9 9 5 4 6 0 2 1 3 1 2 9 7 6

Conhecendo as principais funções de ativação, devemos escolher a que se adequa me-

lhor ao problema a ser trabalhado, não há uma regra especíﬁca para sua escolha, entretanto,

dependendo da complexidade do problema:

• A função Sigmoid é mais utilizada em problemas de classiﬁcação;

• A função ReLU é uma função de ativação muito usada, em diversos casos, más so deve

esta em neurônios de camadas ocultas;

• De modo geral, podemos usar inicialmente a função ReLU, caso não tenha os resultados

esperados, testamos outras funções de ativação.

As redes neurais são compostas por nós ou unidades, os neurônios, conectados por

ligações direcionadas e tendo duas características básicas:

• Arquitetura: É a forma que rede está disposta, isto é, o número de camadas e como se

dar a conexão entre os neurônios;

• Aprendizado: Trata das regras de ajuste dos pesos sinápticos da rede.

A ligação entre as unidades i e j tem por ﬁnalidade propagar a ativação do neurônio ai

com o neurônio hj, onde cada ligação tem um peso wji que determina a força e o sinal dessa

conexão. Para cada neurônio j é calculado a soma deu suas entradas ponderadas, pelos pesos

wji.

sj =

m
X

i=1

wjixi

59

Que é somado ao termo bias, em seguida aplica-se a função de ativação σ para obter a

saída.

aj = σ(sj + bj) = σ(

m
X

wijxi + bj)

i=1

A forma de conexão dos neurônios é de grande importância para o tipo de aprendizado

que se quer trabalhar e para isso existe duas opções relevantes quando se fala em conexão de

neurônios entre as camadas da rede:

• Redes com alimentação para frente : Também chamada Redes Feedforward (FNN), são

redes sem realimentação, isto é, o sinal tem uma única direção percorrendo a rede da

camada de entrada (input layer) para a camada de saída (output layer) sem conexão entre

neurônios da mesma camada.

• Redes recorrentes(Com retroalimentação): Diferentemente de redes Feedforward, as

redes recorrentes incluem conexões ponderadas entre neurônios de uma mesma camada

ou recebem entradas de camadas posteriores, ou ainda, podem ter sua própria saída como

entrada. Essas conexões são chamadas de conexões de retroalimentação ou feedback.

Vamos tratar, neste trabalho, somente redes com alimentação para frente. As redes

feedforward são dispostas em camadas, de forma que cada unidade recebe entrada somente de

unidades da camada imediatamente anterior. Podemos classiﬁcar este tipo de rede neural em

dois subtipos. O primeiro são as redes de camada única ou rede perceptron, em que todos os

neurônios da camada de entrada se conectam diretamente com os neurônios da camada de saída,

o segundo tipo, as redes multicamadas são as que tem uma ou mais camadas ocultas, o que

implica que as entradas não são conectadas com as saídas da rede.

Quando uma RNA contém uma profunda pilha de camadas ocultas ela é chamada de deep

neural network(DNN). A área de estudo de Deep Learning estuda as DNNs e mais geralmente

modelos que contém pilhas profundas de cálculos.

4.0.5 Perceptrons

As redes neurais com uma única camada e com alimentação para frete, chamadas também

de rede perceptron, foram as primeiras redes neurais artiﬁciais, desenvolvidas por Rosenblatt

60

(1958). Essas redes têm como característica principal que todas as entradas são conectadas

diretamente com as saídas da rede neural. Os perceptrons construídos com apenas um neurônio

na camada de saída são limitados a classiﬁcação de problemas binários, isto é, quando temos

apenas duas classes. Aumentando a quantidade de neurônios da camada de saída podemos

realizar classiﬁcações com mais de duas classes, desde que, sejam todas linearmente separáveis,

caso contrário o perceptrons não terão resultados satisfatórios.

Exemplo 4.2. Diagrama de uma rede perceptron com m inputs (entradas) e k outputs (saídas).

Figura 27 – Rede perceptron de camada única

Camada de entrada

Camada de saída

x1

x2

x3

xm

...

...

ˆy1

ˆyk

Fonte: https://tex.stackexchange.com/questions/423745/draw-a-ﬁlled-neural-network-diagram-

with-tikz . Acesso: OUT, 2020.

4.1 Treinamento de um perceptron de camada única

Para treinar uma rede preceptron de camada única vamos utilizar o algoritmo Least Mean

Squares (LMS) Mínimos quadrados médios. Considere o conjunto T de treinamento com k

classes e n amostras:

Onde:

T =

n(cid:16)

x(1), y(1)(cid:17)

,

(cid:16)

x(2), y(2)(cid:17)

, . . . ,

(cid:16)

x(n), y(n)(cid:17)o

(1)

(cid:21)T

é um vetor m-dimensional;

61

(cid:20)
1 , x(j)
x(j)

2 , . . . , x(j)

m

(cid:20)
1 , y(j)
y(j)

2 , . . . , y(j)

k

(cid:21)T

é um vetor k-dimensional.

• x(j) =

• y(j) =

Para treinar o perceptron, vamos utilizar o termo de bias embutido, isto é, como primeira

componente do vetor de pesos, desse modo acrescentaremos 1 como primeira componente

do vetor de entrada. Assim, para o i-ésimo neurônio, um vetor de entrada é da forma x(j) =
(cid:21)T
(cid:20)
1, x(j)

e o vetor de pesos é da forma w(j) =

(cid:20)
i0 = bi, w(j)
w(j)

i2 , . . . , w(j)

2 , . . . , x(j)

i1 , w(j)

1 , x(j)

(cid:21)T

im

m

.

Sendo (cid:15)i(j) o erro do i-ésimo neurônio da camada de saída, ao ser inserido na rede o

j-ésimo exemplo do conjunto de treinamento, é deﬁnido por

i = y(j)
(cid:15)(j)

i − ˆy(j)

i

.

(2)

Onde ˆy(j)

i

, dado em função de wir, é a predição da rede para a j-ésimo exemplo do

conjunto de treinamento, é deﬁnido por

ˆy(j)
i =

m
X

r=0

w(j)
ir x(j)
r .

(3)

Desse modo a soma E(n) dos erros de todos os neurônios da camada de saída da rede,

ao passarmos o j-ésimo exemplo do conjunto de teste é

E(j) =

1
2

k
X

i=1

(j)

(cid:15)2
i

E(j) =

1
2

k
X

(cid:18)

i=1

i − ˆy(j)
y(j)

i

(cid:19)2

.

(4)

(5)

Onde k é o número de neurônios da camada de saída. Sendo n o número de amostras do

conjunto de treinamento podemos deﬁnir o erro total ET da rede após a apresentação de todo o

conjunto de treinamento como:

ET =

1
n

n
X

j=1

E(j)

(6)

Ou ainda,

ET =

1
n

n
X

k
X

(cid:18)

i − ˆy(j)
y(j)

i

(cid:19)2

.

j=1

i=1

62

(7)

Onde ET , a função de custo da rede, é uma função de várias variáveis dadas pelos pesos

sinápticos e os níveis de bias introduzidos dentro dos vetores de pesos. A atualização dos pesos

ocorre a cada apresentação de um exemplo do conjunto de treinamento a rede, um a um, até a

apresentação completa do todo o conjunto de treinamento, que recebe o nome de uma época.

Utilizando a regra da cadeia, podemos derivar a função de custo E(j) com relação aos

pesos sináptico w(j)

ir , assim

∂E(j)
∂w(j)
i

=

∂E(j)
∂(cid:15)(j)
i

∂(cid:15)(j)
i
∂ ˆy(j)
i

∂ ˆy(j)
i
∂w(j)
ir

.

(8)

Vamos determinar agora cada uma das derivadas do lado direito da equação (8) Derivando

ambos os lados da equação (4) com relação (cid:15)i, temos:





((cid:15)(j)

i )2





k
X

i=1

∂
∂(cid:15)(j)
i


∂E(j)
∂(cid:15)(j)
i

=

=

1
2

1
2

∂
∂(cid:15)(j)
i

((cid:15)(j)

1 )2 +

∂
∂(cid:15)(j)
i

((cid:15)(j)

2 )2 + · · · +

∂
∂(cid:15)(j)
i

((cid:15)(j)

i )2 + · · · +





((cid:15)(j)

k )2

∂
∂(cid:15)(j)
i

(9)



= (cid:15)(j)
i

.

Derivando ambos os lados da equação (2) obtemos:

∂(cid:15)(j)
i
∂ ˆy(j)
i

(cid:18)
i − ˆy(j)
y(j)

i

(cid:19)

−

∂ ˆy(j)
i
∂ ˆy(j)
i

=

=

∂
∂ ˆy(j)
i
∂y(j)
i
∂ ˆy(j)
i

= −1.

Por ﬁm derivando a equação (3), com relação a wir, obtemos

∂ ˆy(j)
i
∂w(j)
ir

=

∂
∂w(j)
ir
= x(j)
r .

!

w(j)
ir x(j)
r

  m
X

r=0

(10)

(11)

Substituindo de (9) a (11) em (8) obtemos:

∂E
∂w(j)
ir

= (cid:15)(j)
i

· (−1) · xr

= −(cid:15)(j)

i x(j)
r .

Assim, a variação do peso wir pode ser deﬁnida como

∆w(j)

ir = −η

∂E(j)
∂w(j)
ir
i x(j)
r .

= η(cid:15)(j)

63

(12)

(13)

Onde η é a taxa de aprendizado da rede. Observe que o sinal negativo na equação (13) é

usado para inverter a mudança do gradiente que aponta para crescimento da curva, buscando

então a direção para a mudança de peso que reduza o valor do erro da rede. Para a correção do

valor do peso wir referente do J-ésimo exemplo do conjunto de treinamento para o (j + 1)-ésimo

utilizaremos a seguinte equação.

w(j+1)
ir

= w(j)

ir + ∆w(j)
ir + η(cid:15)(j)

ir
i x(j)
r .

= w(j)

(14)

As redes perceptrons, redes de uma única camada, são limitadas e só podem ser utilizadas

em classiﬁcação de objetos linearmente separáveis. Para entender melhor, considere um conjunto

de dados com duas classes que podem ser representados no plano, pois bem, essas classes são

linearmente separáveis se existir uma reta que separe os objetos de uma classe dos objetos da

outra classe. veja a ﬁgura abaixo.

Figura 28 – Classes linearmente separáveis

y

64

x

Fonte: Elaborada pelo autor.

Para resolver problemas não-lineares seja regressão ou classiﬁcação é necessário a

introdução de camadas ocultas. É consenso que o uso de apenas uma camada oculta é suﬁciente

para resolver a maioria de dos problemas, visto que, redes com apenas uma camada oculta são

capazes de aproximar qualquer função não-linear.

4.2 Construção de um perceptron de camada única código à código em Python

Vamos apresentar, agora, os códigos, em Python, de um Perceptron de camada única para

classiﬁcação binária. Para isso vamos utilizar o conjunto MNIST, que contém 60000 imagens de

dígitos manuscritos.

No código abaixo importamos as bibliotecas e o módulos que serão utilizados para

construção do perceptron.

1 import numpy a s np

2 import p a n d a s a s pd

3 import m a t p l o t l i b . p y p l o t a s p l t

4 import p i c k l e #Mó d u l o u s a d o p a r a s e r i a l i z a r o b j e t o s p a r a que possam s e r

s a l v o s em um a r q u i v o e c a r r e g a d o s em um programa m a i s

t a r d e .

5 import g z i p #Mó d u l o com f u n ç ão de c o m p a c t a r e d e s c o m p a c t a r a r q u i v o s .

6 import

random # E s t e mó d u l o i m p l e m e n t a g e r a d o r e s de n ú meros p s e u d o a l e a t ó r i o s

.

65

Os códigos abaixo, carregam o arquivo mnist.pkl.gz, que contém o conjunto MNIST,

que está na mesma pasta onde se localiza o arquivo do Jupyter Notebook da rede perceptron, em

seguida faz o tratamento dos dados para que estejam prontos para serem inseridos na rede.

1 d e f

l o a d _ d a t a ( ) :

f = g z i p . open ( ’ m n i s t . p k l . gz ’ ,

’ r b ’ ) # Abre o a r q u i v o compactado , o modo

p a d r ão de a r g u m e n t o ’ r b ’ .

t r a i n i n g _ d a t a , v a l i d a t i o n _ d a t a ,

t e s t _ d a t a = p i c k l e . l o a d ( f ,

e n c o d i n g = "

l a t i n 1 " ) # O mé t o d o p i c k l e . l o a d l e r o s o s d a d o s

f . Ao d e f i n i r a c o d i f i c a ç ã

o p a r a l a t i n 1 p e r m i t e i m p o r t a r o s d a d o s d i r e t a m e n t e .

f . c l o s e ( ) # f e c h a o a r q u i v o f .

r e t u r n ( t r a i n i n g _ d a t a , v a l i d a t i o n _ d a t a ,

t e s t _ d a t a )

2

3

4

5

6

7 # I m p o r t a o c o n j u n t o de d a d o s c a r r e g a d o na f u n ç ão l o a d _ d a t a

8 t r a i n i n g _ d a t a , v a l i d a t i o n _ d a t a ,

t e s t _ d a t a = l o a d _ d a t a ( )

9

10 # C o n v e r t e o s a r r a y de r ó t u l o s de modo que s e o r ó t u l o é 5 m o d i f i c a p a r a 1 ,

c a s o c o n t r á r i o 0 .

11 Y = ( t r a i n i n g _ d a t a [ 1 ] == 5 ) . a s t y p e ( np . i n t )

12 y _ v a l i d _ b i n a r y = ( v a l i d a t i o n _ d a t a [ 1 ] == 5 ) . a s t y p e ( np . i n t )

13 y _ t e s t _ b i n a r y = ( t e s t _ d a t a [ 1 ] == 5 ) . a s t y p e ( np . i n t )

14

15 # Renomeia e s e p a r a o s c o n j u n t o s de d a d o s d o s

r e s p e c t i v o s

r ó t u l o s .

16 x _ t r a i n _ b i n a r y = t r a i n i n g _ d a t a [ 0 ]

17 x _ v a l i d _ b i n a r y = v a l i d a t i o n _ d a t a [ 0 ]

18 x _ t e s t _ b i n a r y = t e s t _ d a t a [ 0 ]

19

20 # C o n v e r t e o s a r r a y que r e p r e s e n t a m a s

i m a g e n s de 28 x28 p i x e l s p a r a um uma

a r r e y de uma l i n h a de 784 p i x e l s

21 Z = [ np . r e s h a p e ( x ,

( 7 8 4 ) )

f o r x i n x _ t r a i n _ b i n a r y ]

22 v a l i d a t i o n _ i n p u t s = [ np . r e s h a p e ( x ,

( 7 8 4 ) )

f o r x i n x _ v a l i d _ b i n a r y ]

23 t e s t _ i n p u t s = [ np . r e s h a p e ( x ,

( 7 8 4 ) )

f o r x i n x _ t e s t _ b i n a r y ]

24

25 X = np . a r r a y ( Z ) # C r i a um a r r a y de a r r a y de 784 p i x e l

26 y = Y # Renomeia o c o n j u n t o que c o n t ém o s

r ó t u l o s

do c o n j u n t o de

t r e i n a m e n t o .

Podemos veriﬁcar, com o código abaixo, se a forma dos dados de entrada se está de

acordo com a camada de entrada rede que terá 784 neurônios.

1 # M o s t r a a f o r m a d o s d a d o s de t r e i n o .

2 X . s h a p e

3 ( 5 0 0 0 0 , 7 8 4 )

66

Abaixo, construímos a rede perceptron através do construtor de objetos class.

1 c l a s s P e r c e p t r o n ( o b j e c t ) :

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

d e f _ _ i n i t _ _ ( s e l f ,

s i z e s ) : #O mé t o d o ( f u n ç ã o ) _ _ i n i t _ _ é chamado de

c o n s t r u t o r e é u t i l i z a d o p a r a a r m a z e n a r v a r i á v e i s g l o b a i s que s e r ão

u t i l i z a d o s p o r o u t r o s mé t o d o s . O p a r m e t r o s i z e s é um o b j e t o do t i p o

l i s t a que d e t e r m i n a o nú mero de n e u r ô n i o d a s camadas da r e d e .

s e l f . s i z e s = s i z e s

s e l f . b i a s e s = np . z e r o s ( 1 ) # D e f i n e o s b i a s

i n i c i a i s

s e l f . w e i g h t s = np . z e r o s ( 7 8 4 ) # D e f i n e o s p e s o s

i n i c i a i s

s e l f . e p o c a s = e p o c a s

s e l f . r a t e = r a t e

d e f

f i t ( s e l f , X, y ) : # T r e i n a a r e d e .

s e l f . e r r o r s = [ ]

# C r i a um c o n j u n t o v a z i o , onde s e r á armazenado o nú

mero de c l a s s i f i c a ç õ e s e r r a d a s .

f o r

i

i n range ( s e l f . e p o c a s ) : # N o t e que n e s t e c ó d i g o x += n −> x = x +

n .

e r r = 0

f o r x i ,

t a r g e t

i n z i p (X, y ) :

d e l t a _ w = s e l f . r a t e * ( t a r g e t − s e l f . p r e d i c t ( x i ) ) # \ D e l t a w r i ( j )

=\ e t a \ e p i s i l o n x_ { i }

s e l f . w e i g h t s += d e l t a _ w * x i #w_ { j +1} = w_ { j } + \ D e l t a w x_ { i }
s e l f . b i a s e s += d e l t a _ w # b_ { j +1}=b_ { j } + \ D e l t a w

e r r += i n t ( d e l t a _ w ! = 0 . 0 ) # != D i f e r e n t e

s e l f . e r r o r s . a p p e n d ( e r r ) #O mé t o d o . append i n s e r e o s e r r o s no

c o n j u n t o s e l f . e r r o s .

r e t u r n s e l f

67

24

25

26

27

28

29

30

d e f n e t _ i n p u t ( s e l f , X) : # C a l c u l a o campo de a t i v a ç ã o da r e d e dado o

c o n j u n t o de e n t r a d a X .

r e t u r n np . d o t (X,

s e l f . w e i g h t s ) + s e l f . b i a s e s

d e f p r e d i c t ( s e l f , X) : # A p l i c a o campo de a t i v a ç ã o na f u n ç ã o de a t i v a ç ão

degrau , de modo que s e a s a í da f o r 1 t e m o s um 5 , p o r o u t r o l a d o ,

s e a s a

í da f o r 0 tem que a r e d e p r e v e r que n ão é um 5 .

r e t u r n np . where ( s e l f . n e t _ i n p u t (X) >= 0 . 0 , 1 , 0 )

Fonte: <http://www.bogotobogo.com/python/scikit-learn/Perceptron_Model_with_Iris_DataSet.php>.

Nesse ponto, com a rede já construída, podemos treiná-la, para isso, antes, devemos

deﬁnir a taxa de aprendizado e o número de épocas de treinamento.

1 r a t e = 0 . 0 0 1 # Taxa de a p r e n d i z a d o

2 e p o c a s = 30

3 r e d e = P e r c e p t r o n ( [ 7 8 4 , 1 ] ) # s i z e s =[784 , 1 ] d e f i n e que a camada de e n t r a d a

tem 784 n e u r ô n i o e camada de s a í da tem a p e n a s 1 n e u r ô n i o .

4 r e d e . f i t (X, y )

5 r e d e . f i t (X, y ) # T r e i n a a r e d e

Podemos veriﬁcar se o treinamento foi satisfatório com o código abaixo.

1 p l t . p l o t ( range ( 1 ,

l e n ( r e d e . e r r o r s ) + 1 ) ,

r e d e . e r r o r s , m a r k e r = ’ o ’ )

2 p l t . x l a b e l ( ’ Epochs ’ )

3 p l t . y l a b e l ( ’Nú mero de

c l a s s i f i c a ç õ e s e r r a d a s ’ )

4 p l t . show ( )

5

68

Vamos testar o desempenho do perceptron com o conjunto de testes, para isso, vamos

deﬁnir a função evaluate, que vai contar o número de previsões corretas do preceptron.

1 d e f e v a l u a t e ( t e s t _ i n p u t s , y _ t e s t _ b i n a r y ) : #O mé t o d o e v a l u a t e

r e t o r n a o nú

mero de e n t r a d a s de t e s t e p a r a a s q u a i s a r e d e n e u r a l p r o d u z o r e s u l t a d o

c o r r e t o . A s a í da da r e d e , n e s t e caso , é o í n d i c e do n e u r ô n i o da camada

de s a í da com a m a i o r a t i v a ç ã o .

2

3

4

t e s t _ r e s u l t s = [ ( r e d e . p r e d i c t ( x ) , y )

f o r x , y i n z i p ( t e s t _ i n p u t s ,

y _ t e s t _ b i n a r y ) ] #O c ó d i g o np . argmax c a l c u l a o í n d i c e com m a i o r a r g u m e n t o (

a t i v a ç ão ) da s a í da g e r a d a p e l o mé t o d o f e e d f o r w a r d p a r a o s i n a l de

e n t r a d a x .

r e t u r n sum ( i n t ( x == y )

f o r x , y i n t e s t _ r e s u l t s ) # R e t o r n a o nú mero

de t u p l a s em t e s t _ r e s u l t s onde x=y ,

i s t o é , que a p r e v i s ão f o i

c o r r e t a .

5 t = e v a l u a t e ( t e s t _ i n p u t s , y _ t e s t _ b i n a r y ) # E x e c u t a a f u n ç ão no c o n j u n t o de

t e s t e s

6 t

7 9620

8 l e n ( t e s t _ i n p u t s ) #Nú mero de e l e m e n t o s do c o n j u n t o t e s t _ i n p u t s

Note que o perceptron acertou 9620 imagens de um total de 10000, isso representa uma

taxa de precisão de 96,2%.

Podemos fazer a previsão de uma única imagem do conjunto de testes e visualizar se a

previsão foi correta, veja a sequência de códigos abaixo.

1 r e d e . p r e d i c t ( t e s t _ i n p u t s [ 5 0 9 ] )

2 a r r a y ( [ 1 ] )

3

4 p l t . f i g u r e ( )

5 p l t . imshow ( t e s t _ i n p u t s [ 5 0 9 ] . r e s h a p e ( 2 8 , 2 8 ) ) # Usamos o mé t o d o . r e s h a p e p a r a

m o d i f i c a r a imagem p a r a o f o r m a t o o r i g i n a l p a r a que s e j a p o s s í v e l a

v i s u a l i z a ç ão .

6 p l t . c o l o r b a r ( )

7 p l t . g r i d ( F a l s e )

8 p l t . show ( )

69

9

Vamos testar o Perceptron com uma imagem que não pertence ao conjunto de dados

MNIST, nessa parte, podemos escrever o dígito em uma folha escanear para fazer o teste, é

importante destacar que é necessário retirar o fundo da imagem escaneada para que a imagem

não tenha ruídos.

1 from PIL import

Image # PIL é a b i b l i o t e c a p a r a t r a b a l h a r com i m a g e n s no

P y t h o n . O modulo Image do PIL t r a z

v a r i a f u n ç õ e s ú t e i s p a r a i m a g e n s .

2 from m a t p l o t l i b import

image #O mó d u l o image do m a t p l o t l i b é u s a d o p a r a

o p e r a ç õ e s bá s i c a s de c a r r e g a m e n t o ,

r e d i m e n s i o n a m e n t o e e x i b i ç ão de

i m a g e n s

3

4 image = Image . open ( ’ img5 . png ’ ) # C a r r e g a a imagem img5 ( sem f u n d o ) p r e s e n t e na

mesma p a s t a do a r q u i v o J u p y t e r N o t e b o o k da r e d e n e u r a l .

5

6 i m g _ r e s i z e = image . r e s i z e ( ( 2 8 , 2 8 ) ) # r e d i m e n s i o n a a imagem p a r a 28 x28 .

7

8 i m a g e _ c i n z a = i m g _ r e s i z e . c o n v e r t ( mode= "L" ) # C o n v e r t e a imagem p a r a e s c a l e de

c i n z a .

9

10 from numpy import a s a r r a y #O a s a r r a y é uma f u n ç ão NumPy u s a d a p a r a

c o n v e r t e r a s e n t r a d a s de um a r r a y .

11

12 d a t a = a s a r r a y ( i m a g e _ c i n z a ) # C o n v e r t e a i m a g e _ c i n z a em um a r r a y .

13

14 d a t a 1 = d a t a

/ 100 #A d i v i s ão o c o r r e p a r a que o s v a l o r e s que r e p r e s e n t a m o s

p i x e l s

v a r i e m de 0 a 1 .

15

16 img = np . r e s h a p e ( d a t a 1 ,

( 7 8 4 ) ) # R e d i m e n s i o n a a imagem p a r a f i c a r de a c o r d o

com a camada de e n t r a d a da r e d e .

70

Agora, a imagem está de acordo com a camada de entrada do Perceptron, podemos fazer

a previsão.

1 r e d e . p r e d i c t ( img )

2 a r r a y ( [ 1 ] )

3

4 p l t . f i g u r e ( )

5 p l t . imshow ( d a t a 1 )

6 p l t . c o l o r b a r ( )

7 p l t . g r i d ( F a l s e )

8 p l t . show ( )

9

4.3 Redes multicamada com alimentação para frente

As redes neurais multicamadas, também chamadas de perceptrons de múltiplas camadas

(MLP, Multilayer perceptron), tem como característica principal a existência de uma ou mais

camadas ocultas. Observe abaixo um diagrama de uma rede multicamada com alimentação para

frente.

71

Figura 29 – Redes multicamada com alimentação para frente

Camada

de entrada

Camada

oculta

Camada

oculta

Camada

de saída

I1

I2

I3

In

...

H1

...
...

Hn

. . .

H1

...

Hn

...

O1

On

Fonte:

https://tex.stackexchange.com/questions/343462/n-level-deep-networks-using-tikz.

Acesso: OUT, 2020.

Uma rede perceptron de múltiplas camadas tem as seguintes características:

• Uma função de ativação não-linear e diferenciável;

• Uma ou mais camadas ocultas, responsáveis pela realização de tarefas complexas como

extração de padrões signiﬁcativos dos dados de entrada;

• Alto grau de conectividade.

É muito importante, ao construir uma rede neural artiﬁcial, a decisão quanto ao número

de camadas a serem utilizadas. Como foi dito na seção anterior a utilização de apenas uma

camada oculta já capacita a rede para resolver uma grande parte dos problemas não-lineares.

O uso de poucas camadas ocultas pode levar a rede ao underﬁtting (sub-ajuste), isto é, não

funcionará bem nem com os dados de treino nem com os dados de testes. Por outro lado, se rede

tiver muitas camadas ocultas ocorrerá overﬁtting (sobreajute), isto é, a rede vai se ajustar aos

dados de treinamento e não fara boas generalizações. Desse modo, devemos sempre escolher o

menor número de camadas que soluciona o problema, isso deve ser obtido testando e modiﬁcando

os paramentos da rede durante o treinamento.

72

Podemos pensar as redes neurais como uma função hw,b(x) parametrizadas pelos pesos

wji e pelos bias bj, desse modo podemos expressar a saída como uma função real de várias

variáveis dadas pelos pesos e pelos bias. A partir daí temos que determinar os pesos e o bias

de modo que o erro seja mínimo e para isso podemos utilizar derivadas parciais e o método de

gradiente descendente.

Cada neurônio da camada de saída está associado a uma das classes a serem preditas.

Desse modo, podemos representar o valor de saída, para uma entrada especíﬁca, pelo vetor
ˆy = [ ˆy1, ˆy2, . . . , ˆym]T onde m é o número de neurônios da camada de saída e também o número de

classes do conjunto de dados, desse modo, a componente com maior ativação é a classe predita

pela rede. Os rótulos do problema podem também ser representados por vetores m-dimensionais,

onde a componente da classe correta vale 1 e as demais componentes vale 0.

4.4 Treinamento de redes neurais multicamada

Nesta seção, vamos apresentar a matemática por trás do treinamento de uma rede neural

profunda, utilizando o algoritmo backpropagation que fornece um método computacional eﬁ-

ciente para o treinamento de redes neurais multicamadas. O algoritmo é baseado no gradiente

descendente e, para utilizá-lo, é preciso que a função de custo seja contínua e diferenciável, para

garantir esses requisitos utilizaremos uma função de ativação contínua e diferenciável.

Até 1986, muitos pesquisadores tentaram encontrar uma maneira de treinar MLPs (Mul-

tilayer perceptron), sem sucesso. Até que David Rumelhart, Geoffrey Hinton e Ronald Williams

publicaram um artigo que introduziu o algoritmo de treinamento backpropagation usado até

hoje.

O Algoritmo backpropagation é composto por duas fases que interagem:

• forward: É a fase para frente, também chamada de propagação, onde o conjunto de

treinamento é apresentado a rede, passando pela primeira camada, sendo ponderado pelos

respectivos pesos e aplicado a função de ativação em cada neurônio produzindo um valor

de saída, que se tornam as entradas dos neurônios da camada seguinte e esse processo

ocorre camada a camada até a camada de saída produzir o valor de saída para a rede, que

é comparado com os respectivos rótulos, que são os valores desejáveis, através de uma

função de custo que mede o erro cometido pela rede ao fazer a previsão.

73

• backward É a fase para trás, também chamado de retropropagação, que utiliza o erro

encontrado na fase forward para ajustar os pesos e os bias. O ajuste é realizado da camada

de saída para a camada de entrada. Os pesos e bias são ajustados para que a resposta da

rede se aproxime da resposta correta (rótulo).

Considere o conjunto T de treinamento com k classes e n amostras

T =

n(cid:16)

x(1), y(1)(cid:17)

,

(cid:16)

x(2), y(2)(cid:17)

, . . . ,

(cid:16)

x(n), y(n)(cid:17)o

(15)

Onde:

• x(j) =

(cid:20)
1 , x(j)
x(j)

2 , . . . , x(j)

m

(cid:21)T

é um vetor m-dimensional;

(cid:20)

• y(j) =

1 , y(j)
y(j)

2 , . . . , y(j)

k

(cid:21)T

é um vetor k-dimensional.

Vamos construir e treinar uma rede com a camada de entrada (input layer), com m

entradas, três camadas ocultas (hidden layer) e a camada de saída com k neurônios.

Figura 30 – Rede neural artiﬁcial

Entradas

Camadas Ocultas

Saídas

Im

wpm

hp

hq

hr

wrq

wqp

wir

Oi

...

...

...

...

...

Fonte: Elaborada pelo autor.

x1

x2

xm

ˆy1

ˆy2

ˆyk

74

O treinamento de uma rede neural é determinar os pesos sinápticos wpm, wqp, wrq e wir

que fazem as ligações entre as camadas Im e hp, hp e hq, hq e hr e hr e Oi respectivamente e os

termos de viés, os bias, bp, bq, br e bi de modo que o erro total, medido por uma função de custo,

seja mínimo ou satisfatório, isto é, que a rede tenha uma boa precisão nas predições para que foi

treinada. Considere que σp, φq, ζr e ξi são respectivamente as funções de ativação dos neurônio

das camadas hp, hq, hr e Oi. Note que cada neurônio pode ter uma função de ativação especíﬁca.

Para treinar o perceptron, vamos utilizar o termo de bias embutido, isto é, como primeira

componente do vetor de pesos, desse modo acrescentaremos 1 como primeira componente do

vetor de entrada de todos os neurônios da rede. Assim, por exemplo, para o p-ésimo neurônio da
camada hp, um vetor de entrada é da forma x(j) =
(cid:21)T

e o vetor de pesos é

2 , . . . , x(j)

(cid:20)
1, x(j)

1 , x(j)

(cid:21)T

m

(cid:20)

da forma w(j) =

p0 = bp, w(j)
w(j)

p1 , w(j)

p2 , . . . , w(j)

pm

.

Para iniciar o treinamento, com algoritmo Backpropagation, devemos escolher uma

função de custo, utilizaremos o MSE (Mean Squared Error) Erro médio quadrático, amplamente

utilizado em aprendizado supervisionado. Seja (cid:15)i(j) o erro do i-ésimo neurônio da camada de

saída Oi ao ser inserido na rede o j-ésimo exemplo do conjunto de treinamento, deﬁnido por

(cid:15)(j)
i = y(j)

i − ˆy(j)

i

.

(16)

Assim a soma E(j) dos erros de todos os neurônios da camada de saída da rede ao

passarmos o j-ésimo exemplo do conjunto de teste é

E(j) =

1
2

k
X

i=1

2

(cid:15)(j)
i

E(j) =

1
2

k
X

(cid:18)

i=1

i − ˆy(j)
y(j)

i

(cid:19)2

.

(17)

(18)

Onde k é o número de neurônios da camada de saída. Sendo n o número de amostras

do conjunto de treinamento deﬁnimos o erro total ET da rede após a apresentação de todo o

conjunto de treinamento como

ET =

1
n

n
X

j=1

E(j),

(19)

ou ainda,

ET =

1
n

n
X

k
X

(cid:18)

i − ˆy(j)
y(j)

i

(cid:19)2

,

j=1

i=1

75

(20)

onde ET , a função de custo da rede, é uma função de várias variáveis dadas pelos pesos

sinápticos e os níveis de bias. A atualização dos pesos ocorre a cada apresentação de um exemplo

do conjunto de treinamento a rede, um a um, até a apresentação completa de todo o conjunto

de treinamento que recebe o nome de época. Por simpliﬁcação de notação, omitiremos o índice

(j) que indica a apresentação do j-ésimo exemplo do conjunto de treinamento a rede, por
exemplo, utilizaremos ˆyi em vez de ˆy(j)

. Deixado claro que, todos os cálculos abaixo se referem

i

a apresentação do j-ésimo exemplo do conjunto de treinamento, note que

ˆyi = ξ (uOi (wir)) ,

(21)

Onde uOi, dado em função de wir , é chamado de campo de ativação do i-ésimo neurônio da
camada de saída Oi, deﬁnido por

uOi(wir) =

R
X

r=0

wirhr.

Substituindo (21) em (18) temos

E =

k
X

i=1

(yi − ξ (uOi (wir)))2.

(22)

(23)

Utilizando a regra da cadeia, podemos derivar a função de custo E com relação aos pesos

sinápticos wir, assim

∂E
∂wir

=

∂E
∂(cid:15)i

∂(cid:15)i
∂ ˆyi

∂ ˆyi
∂uOi

∂uOi
∂wir

.

(24)

Vamos determinar agora cada uma das derivadas do lado direito da equação (24) Deri-

vando ambos os lados da equação (17) com relação (cid:15)i, temos que

76



((cid:15)i)2



k
X





i=1

((cid:15)1)2 +

∂
∂(cid:15)i

((cid:15)2)2 + · · · +

∂
∂(cid:15)i

((cid:15)i)2 + · · · +

∂
∂(cid:15)i

((cid:15)k)2

(25)

∂E
∂(cid:15)i

=

=

∂
∂(cid:15)i
∂
∂(cid:15)i

= 2(cid:15)i.

Derivando ambos os lados da equação (16) obtemos:

∂(cid:15)i
∂ ˆyi

=

=

∂
∂ ˆyi
∂yi
∂ ˆyi

= −1.

(yi − ˆyi)

−

∂ ˆyi
∂ ˆyi

Derivando ambos os lados da equação (21), com relação a, obtemos:

∂ ˆyi
∂uOi

(ξ (uOi))

=

∂
∂uOi
= ξ0 (uOi) .

Por ﬁm derivando a equação (22), com relação a wir, obtemos

∂uOi
∂wir

=

∂
∂wir



wirhr







R
X

r=0

= hr.

Substituindo de (25) a (28) em (24) obtemos:

∂E
∂wir

= 2(cid:15)i · (−1) · ξ0 (uOi) hr

= −2(cid:15)i · ξ0 (uOi) hr.

(26)

(27)

(28)

(29)

Assim, a variação do peso wir pode ser deﬁnida como

∆wir = −η

∂E
∂wir
∂E
∂(cid:15)i

= −η

∂(cid:15)i
∂ ˆyi

∂ ˆyi
∂uOi
= 2η(cid:15)i · ξ0 (uOi) hr.

∂uOi
∂wir

77

(30)

Onde η é a taxa de aprendizado da rede. Observe que o sinal negativo na equação (30) é

usado para inverter a direção do gradiente que que aponta para crescimento da curva, buscando

então a direção para a mudança de peso que reduza o valor do erro da saída da rede. Podemos

reescrever a equação (30) da seguinte forma:

∆wir = 2ηδihr,

(31)

onde δi é o gradiente local deﬁnido como sendo a derivada parcial do erro E com relação ao

campo de ativação uOi, isto é

δi =

∂E
∂uOi

= (cid:15)i · ξ0 (uOi) .

(32)

Para a correção do valor do peso wir referente ao J-ésimo exemplo do conjunto de

treinamento para o (j + 1)-ésimo utilizaremos a seguinte equação

wir(j + 1) = wir(j) + ∆wir(j)

= wir(j) + 2ηδihr.

(33)

Vamos derivar a função de custo E com relação aos pesos wrq. Note que para isso

devemos escrever E em função de wrq que é um peso pertencente a uma camada oculta e,

diferente do caso anterior, a atualização deste peso depende do erro de todos os neurônios que

estão a sua direita, isto é, todos os neurônios da camada de saída. Desse modo, Observe que:

uhr =

Q
X

q=0

wrqhq

(34)

daí,

hr = ζ (uhr (wrq)) .

Substituindo (35) em (21) obtemos:

ˆyi = ξ (uOi (ζ (uhr (wrq)))) .

Substituindo (36) em (23):

E =

1
2

n
X

i=1

(yi − ξ (uOi (ζ (uhr (wrq)))))2.

Derivando E em relação a wrq obtemos:

∂E
∂wrq

=

=

=

∂
∂wrq
n
X

(cid:15)i

i=1
n
X

i=1

(cid:15)i

 1
2

n
X

i=1

(yi − ξ (uOi (ζ (uhr (wrq)))))2

!

∂(cid:15)i
∂wrq
∂(cid:15)i
∂ ˆyi

∂ ˆyi
∂uOi

∂uOi
∂hr

∂hr
∂uhr

∂uhr
∂wrq

.

78

(35)

(36)

(37)

(38)

Observe que já calculamos algumas destas derivadas vamos calcular as que faltam.

Derivando a equação (22) em relação a hr temos:

∂uOi
∂wir

=

∂
∂wir



wirhr







R
X

r=0

= wir.

(39)

A derivada parcial de hr, com relação ao campo de ativação uhr , é obtida diferenciando

ambos os lados da equação (35):

∂hr
∂uhr

= ζ 0 (uhr ) .

(40)

Para determinar ∂uhr
∂wrq

devemos derivar a equação (34), desse modo:

∂uhr
∂wrq

=

∂
∂wrq



wrqhq



Q
X





q=0

= hq.

Substituindo de (25) a (27) e de (39) a (41) em (38) obtemos:

∂E
∂wrq

=

n
X

i=1

(cid:15)i · (−1) · ξ0 (uOi) wirζ 0 (uhr ) hq

= −

n
X

i=1

δiwirζ 0 (uhr ) hq

= −ζ 0 (uhr ) hq

n
X

i=1

δiwir.

Desse modo, a variação do peso wrq é deﬁnida como

∆wrq = −η

∂E
∂wrq

= ηζ 0 (uhr ) hq

n
X

i=1

δiwir

= ηδrhq,

79

(41)

(42)

(43)

onde δr = ζ 0 (uhr ) Pn
exemplo do conjunto de treinamento é feita de acordo com a equação abaixo.

i=1 δiwir. Assim a correção dos pesos wrq com referência ao j-ésimo

wrq(j + 1) = wrq(j) + ∆wrq(j)

= wrq(j) + ηδrhq.

(44)

O próximo passo é derivar o erro com relação aos pesos wqp, de modo análogo aos casos

anteriores devemos escrever a função do custo E em função dos pesos wqp

Assim temos que:

uhq (wqp) =

P
X

p=0

wqphp,

(45)

daí segue que:

hq = φ

(cid:16)

(cid:17)
uhq (wqp)

.

Substituindo (46) em (36)

ˆyi = ξ

(cid:16)

uOi

(cid:16)

(cid:16)

ζ

uhr

(cid:16)

(cid:16)

φ

uhq (wqp)

(cid:17)(cid:17)(cid:17)(cid:17)(cid:17)

.

Substituindo (47) em (18):

E =

1
2

n
X

(cid:16)

i=1

yi − ξ

(cid:16)
uOi

(cid:16)

ζ

(cid:16)

uhr

(cid:16)

(cid:16)

φ

(cid:17)(cid:17)(cid:17)(cid:17)(cid:17)(cid:17)2

uhq (wqp)

.

Derivando E em relação a wqp obtemos:

∂E
∂wqp

=

=

=

∂
∂wqp
n
X

(cid:15)i

i=1
n
X

i=1

(cid:15)i

 1
2

n
X

(cid:16)

i=1

yi − ξ

(cid:16)
uOi

(cid:16)

ζ

(cid:16)

uhr

(cid:16)

(cid:16)

φ

(cid:17)(cid:17)(cid:17)(cid:17)(cid:17)(cid:17)2

uhq (wqp)

!

∂(cid:15)i
∂wqp
∂(cid:15)i
∂ ˆyi

∂ ˆyi
∂wqp

.

Vamos determinar ∂ ˆyi
∂wqp

. Para isso vamos derivar a equação (47) com relação a wqp

∂uOi
∂wqp
R
X

wirζ 0 (uhr )

∂uhr
∂wqp

∂ ˆyi
∂wqp

= ξ0 (uOi)

= ξ0 (uOi)

= ξ0 (uOi)

r=0
R
X

r=0

wirζ 0 (uhr ) wrqφ0 (cid:16)

uhq

(cid:17)

hp.

80

(46)

(47)

(48)

(49)

(50)

A derivada parcial de hq com relação ao campo de ativação uhq é obtida diferenciando

ambos os lados da equação (46) obtemos:

∂hq
∂uhq

= φ0 (cid:16)

(cid:17)

.

uhq

(51)

Para determinar

∂uhq
∂wqp

devemos derivar a equação (45), desse modo:

∂uhq
∂wqp

=

∂
∂wqp



wqphp







P
X

p=0

= hp.

Substituindo de (25) a (27), de (39) a (41) e de (50) a (52) em (49) obtemos:

∂E
∂wqp

n
X

=

(cid:15)i · (−1) · ξ0 (uOi)

R
X

r=0

wirζ 0 (uhr ) wrqφ0 (cid:16)

uhq

(cid:17)

hp

i=1
= −φ0 (cid:16)

(cid:17)

hp

uhq

= −φ0 (cid:16)

(cid:17)

hp

uhq

R
X

r=0
R
X

r=0

ζ 0 (uhr )

n
X

i=1

δiwirwrq

δrwrq.

Desse modo, a variação do peso wqp é deﬁnida como

∆wqp = −η

∂E
∂wqp

= ηφ0 (cid:16)

(cid:17)

hp

uhq

R
X

r=0

δrwrq

= ηδqhp.

81

(52)

(53)

(54)

Onde δq = φ0 (cid:16)

uhq

(cid:17) PR

r=0 δrwrq. Assim a correção dos pesos wqp com referência ao

j-ésimo exemplo do conjunto de treinamento é feita de acordo com a equação abaixo.

wqp(j + 1) = wqp(j) + ∆wqp(j)

= wqp(j) + ηδphp.

(55)

Por ﬁm, devemos derivar a função de custo E em relação aos pesos wpm. Escrevendo o

erro E em função de wpm, para isso, note que



uhp = σ



M
X

m=0


 ,

wpmIm

(56)

daí segue que

hq = σ

(cid:16)

(cid:17)
uhp (wpm)

.

Substituindo (57) em (47)

ˆyi = ξ

(cid:16)

uOi

(cid:16)

(cid:16)

ζ

uhr

(cid:16)

(cid:16)

φ

uhq

(cid:16)

(cid:16)

σ

uhp (wpm)

(cid:17)(cid:17)(cid:17)(cid:17)(cid:17)(cid:17)(cid:17)

.

Substituindo (58) em (18)

82

(57)

(58)

E =

1
n

n
X

(cid:16)

i=1

(cid:16)

ξ

uOi

(cid:16)

(cid:16)

ζ

uhr

(cid:16)

(cid:16)

φ

uhq

(cid:16)

(cid:16)
σ

uhp (wpm)

(cid:17)(cid:17)(cid:17)(cid:17)(cid:17)(cid:17)(cid:17)

(cid:17)2

,

− yi

(59)

Derivando E parcialmente com relação aos pesos wpm de modo análogo aos casos anteriores

obtemos

∂E
∂wpm

= −σ0 (cid:16)

(cid:17)

Im

uhp

Q
X

q=0

δqwqp.

Segue que a variação do peso wpm é dada por

∆wpm = −η

∂E
∂wpm

= ησ0 (cid:16)

(cid:17)

Im

uhp

Q
X

q=0

δqwqp

= ηδpIm.

(60)

(61)

Onde δp = φ0σ0 (cid:16)

uhp

(cid:17) PQ

q=0 δqwqp. Assim a correção dos pesos wpm com referência ao

j-ésimo exemplo do conjunto de treinamento é feita de acordo com a equação abaixo.

wpm(j + 1) = wpm(j) + ∆wpm(j)

= wpm(j) + ηδpIm.

(62)

Podemos generalizar esse treinamento para redes neurais com um número qualquer de

camadas ocultas. Desse modo, o algoritmo Backpropagation poderá ser sintetizado da forma

abaixo.

83

1. Inicialização: Temos que criar os pesos e bias iniciais.

2. Input x: Apresente os exemplos de treinamento de uma época à rede neural.

3. Feedforward: Para cada neurônio j, da camada l, l = 2, 3, . . . , L, onde L é a profundidade

da rede, isto é, o número de camadas da rede, calcular o campo local induzido

vl
j(n) =

m
X

i=1

ji · yl−1
wl
i

(n),

onde yl−1

i

(n) é o sinal de saída do neurônio i da camada l − 1, na apresentação do

n − ésimo exemplo de treinamento, wl
yl−1
0

j0 = bl
ativação, a saída do neurônio j da camada l é

(n) = +1 e wl

ji é o peso do neurônio j da camada l. Observe que,
j(n) que é o bias do neurônio j da camada l. Sendo σ a função de

yl
j = σ(vl

j(n)).

Note que y0

j (n) = xj, isto é, a j − ésima componente do vetor de entrada x(n). Se o

neurônio j pertence a camada de saída l = L, então

calcular o sinal de erro

yL
j (n) = ˆyj,

(cid:15)j(n) = yj − ˆyj

, onde yj é a j − ésima componente do vetor de rótulos.

4. Retropropagação: Para cada l = L, L − 1, L − 2, . . . , 2, Calcular o gradiente local, deﬁnido

por:

δl
i =






φ0 (cid:16)

vl
j

L

(cid:17)

j · φ0 (cid:16)
(cid:15)i
(cid:17) Pn
i=1 δl+1

vL
j
i wl+1
ij

,

,

se j pertence a camada de saída;

se j pertence a camada oculta.

5. Gradiente descendente: Para cada l = L, L − 1, L − 2, . . . , 2 atualize os pesos de acordo

com a equação,

.

ji(n + 1) = wl
wl

ji(n) + ηδl

jyl−1
i

Note que os bias também são atualizados por esta equação, visto que, wl

j0 = bl
j.

84

4.5 Construção de uma rede neural código à código em Python

Vamos construir uma rede neural para identiﬁcar dígitos manuscritos e utilizaremos o

conjunto de dados MNIST para seu treinamento. O MNIST é formado por imagens digitalizadas

de dígitos manuscritos de 28x28 pixels, em escala de cinza onde cada pixel vai ser representado

por uma escala de 0 a 1, de modo que 0 representa o branco e 1 representa o preto e os

valores intermediários representam tons de cinza que se escurecem a medida que se aproxima

de 1. As imagens não tem ruídos, isto é, não tem o fundo. A imagem será convertida para

uma sequência 28x28=784 pixels, isto é, os pixels serão alinhados, desse modo, as entradas

da serão representadas por vetores de 784 dimensões onde cada uma de suas componentes

representa um pixel da imagem. Já a saída desejada pode ser representada como uma função

vetorial y = f (x) onde x é a entrada da rede, por exemplo, se x representa um 4 temos que

y(x) = (0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0)T .

O conjunto MNIST poderá ser baixado no endereço https://github.com/mnielsen/neural-

networks-and-deep-learning/archive/master.zip

Em python, a construção da rede neural será em forma de uma classe que chamaremos

de RedeNeural.

1 >>># I m p o r t s

2 >>> import

random

3 >>> import numpy a s np

4

5 >>> c l a s s R e d e N e u r a l ( o b j e c t ) :

6

7

8

9

10

11

d e f _ _ i n i t _ _ ( s e l f ,

s i z e s ) : # s i z e s é um o b j e t o do t i p o l i s t a que

c o n t ém o nú mero de n e u r ô n i o n a s

r e s p e c t i v a s camadas

s e l f . n u m _ l a y e r s = l e n ( s i z e s )

s e l f . s i z e s = s i z e s

s e l f . b i a s e s = [ np . random . r a n d n ( y , 1 )

f o r y i n s i z e s [ 1 : ] ] # Gera o s

b i a s a l e a t o r i a m e n t e u s a n d o a f u n ç ã o Numpy ’ np . random . r a n d n ’ p a r a g e r a r

d i s t r i b u i ç õ e s g a u s s i a n a s com 0 de mé d i a e 1 de d e s v i o p a d r ã o .

s e l f . w e i g h t s = [ np . random . r a n d n ( y , x )

f o r x , y i n z i p ( s i z e s

[ : − 1 ] ,

s i z e s [ 1 : ] ) ] # Gera o s p e s o s a l e a t o r i a m e n t e u s a n d o a f u n ç ão Numpy ’

np . random . r a n d n ’ p a r a g e r a r d i s t r i b u i ç õ e s g a u s s i a n a s com 0 de mé d i a e 1

de d e s v i o p a d r ão .

d e f

f e e d f o r w a r d ( s e l f , a ) : # a d i c i o n a o mé t o d o f e e d f o r w a r d a c l a s s e

N e t w o r k que dada a e n t r a d a p a r a r e d e r e t o r n a a s a í da c o r r e s p o n d e n t e ,

85

i s t o é , a p l i c a a equa ç ão a ’ = \ p h i ( wa+b ) .

f o r b , w i n z i p ( s e l f . b i a s e s ,

s e l f . w e i g h t s ) :

a = s i g m o i d ( np . d o t (w, a ) +b ) # A f u n ç ã o np . d o t

c a l c u l a o

p r o d u t o e s c a l a r de w e a .

r e t u r n a

d e f SGD( s e l f ,

t r a i n i n g _ d a t a , e p o c h s , m i n i _ b a t c h _ s i z e ,

e t a ,

t e s t e _ d a t a = None ) :

t r a i n i n g _ d a t a = l i s t ( t r a i n i n g _ d a t a )

n = l e n ( t r a i n i g _ d a t a )

i f

t e s t _ d a t a :

t e s t e _ d a t a = l i s t ( t e s t _ d a t a ) # C r i a uma l i s t a com o s

e l e m e n t o s do c o n j u n t o de t r e i n a m e n t o . O a r g u m e n t o t e s t _ d a t a é o p c i o n a l ,

s e f o r

f o r n e c i d o o programa a v a l i a r á a r e d e ap ó s cada p e r í odo de

t r e i n a m e n t o e m o s t r a r á o p r o g r e s s o p a r c i a l .

I s t o é ú t i l p a r a r a s t r e a r o

p r o g r e s s o , mas d e i x a o a p r e n d i z a d o m a i s

l e n t o .

n _ t e s t = l e n ( t e s t _ d a t a )

f o r

j

i n range ( e p o c h s ) :

random . s h u f f l e ( t r a i n i n g _ d a t a ) # A f u n ç ã o random . s h u f f l e

r a n d o m i z a o s e l e m e n t o s do c o n j u n t o de t r e i n a m e n t o .

m i n i _ b a t c h e s = [ t r a i n i n g _ d a t a [ k : k+ m i n i _ b a t c h _ s i z e ]

f o r k i n

range ( 0 , n , m i n i _ b a t c h _ s i z e ) ] # d i v i d e o c o n j u n t o de t r e i n a m e n t o em

l o t e s m e n o r e s de tamanho c o n v e n i e n t e p a r a que o t r e i n a m e n t o s e j a m a i s

r á

p i d o e e f i c i e n t e .

f o r m i n i _ b a t c h i n m i n i _ b a t c h e s :

s e l f . u p d a t e _ m i n i _ b a t c h ( m i n i _ b a t c h ,

e t a ) # e t a é a t a x a de

a p r e n d i z a g e m , a t u a l i z a o s p e s o s e o s b i a s com uma ú n i c a i t e r a ç ão da

d e c i d a de g r a d i e n t e .

i f

t e s t _ d a t a : # Se o ’ t e s t _ d a t a ’

f o r a c r e s c e n t a d o no mé t o d o SGD

v a i p r i n t a r o p r o g r e s s o do a p r e n d i z a d o .

p r i n t ( " Epoch {} : {} / {} " . format ( j , s e l f . e v a l u a t e ( t e s t _ d a t a

) , n _ t e s t ) ) ;

e l s e : # Caso não s e j a a c r e s c e n t a d o o ’ t e s t _ d a t a ’ ao mé t o d o SGD o

p r o g r e s s o do t r e i n a m e n t o n ão s e r á m o s t r a d o ,

s o m e n t e a f i n a l i z a ç ão .

p r i n t ( " Epoch {} f i n a l i z a d a " . format ( j ) )

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

86

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

52

53

54

55

56

57

58

59

d e f u p d a t e _ m i n i _ b a t c h ( s e l f , m i n i _ b a t c h ,

e t a ) : # U t i l i z a o m i n i _ b a t c h p a r a

a t u a l i z a r o s p e s o s e b i a s , u t i l i z a n d o a d e c i d a do g r a d i e n t e . O

m i n i _ b a t c h é uma l i s t a de t u p l a s

( x , y ) ,

i n t o é , um s u b c o n j u n t o do

c o n j u n t o de t r e i n a m e n t o ,

e t a é a t a x a de a p r e n d i z a d o da r e d e .

n a b l a _ b = [ np . z e r o s ( b . s h a p e )

f o r b i n s e l f . b i a s e s ] # C r i a um l i s t a de

a r r a y s com e n t r a d a s

t o d a s 0 , com a mesma f o r m a d o s a r r a y de b i a s .

n a b l a _ w = [ np . z e r o s (w . s h a p e )

f o r w i n s e l f . w e i g h t s ] # C r i a um l i s t a

de a r r a y s com e n t r a d a s

t o d a s 0 , com a mesma f o r m a d o s a r r a y de p e s o s .

f o r x , y i n m i n i _ b a t c h :

d e l t a _ n a b l a _ b , d e l t a _ n a b l a _ w = s e l f . b a c k p r o p ( x , y )

n a b l a _ b = [ nb+dnb f o r nb , dnb i n z i p ( n a b l a _ b , d e l t a _ n a b l a _ b ) ] #

n a b l a _ b=nb+dnb , nb em n a b l a _ b e dnb em d e l t a _ n a b l a _ b

n a b l a _ w = [ nw+dnw f o r nw , dnw i n z i p ( nabla_w , d e l t a _ n a b l a _ w ) ] #

nabla_w=nw+dnw , nw em nabla_w e dnw em d e l t a _ n a b l a _ w

s e l f . w e i g h t s = [w− ( e t a / l e n ( m i n i _ b a t c h ) ) *nw f o r w, nw i n z i p ( s e l f .

w e i g h t s , n a b l a _ w ) ]

s e l f . b i a s e s = [ b − ( e t a / l e n ( m i n i _ b a t c h ) ) * nb f o r b , nb i n z i p ( s e l f .

b i a s e s , n a b l a _ b ) ]

d e f b a c k p r o p ( s e l f , x , y ) : #O mé t o d o b a c k p r o p r e t o r n a a ( n a b l a _ b , nabla_w

)

r e p r e s e n t a n d o o g r a d i e n t e p a r a a f u n ç ão de c u s t o C_x . n a b l a _ b e

nabla_w s ão l i s t a s de camadas de m a t r i z e s numpy ,

s e m e l h a n t e s a ‘ s e l f .

b i a s e s ‘ e ‘ s e l f . w e i g h t s ‘ . " " "

n a b l a _ b = [ np . z e r o s ( b . s h a p e )

f o r b i n s e l f . b i a s e s ]

n a b l a _ w = [ np . z e r o s (w . s h a p e )

f o r w i n s e l f . w e i g h t s ]

a c t i v a t i o n = x # D e f i n e a s a t i v a ç õ e s

f e i t a s no mé t o d o f e e d f o r w a r d

como x .

a c t i v a t i o n s = [ x ] # C r i a a l i s t a ’ a c t i v a t i o n s ’ p a r a a r m a z e n a r a s

a t i v a ç õ e s .

# L i s t a p a r a a r m a z e n a r

t o d o s o s v e t o r e s

z , camada p o r camada

87

60

61

62

63

64

65

66

67

68

69

70

71

72

73

74

75

76

77

78

79

80

81

82

83

84

85

z s = [ ] # C r i a uma l i s t a p a r a a r m a z e n a r o s campos de a t i v a ç ão ( z =

wa + b ) camada a camada .

f o r b , w i n z i p ( s e l f . b i a s e s ,

s e l f . w e i g h t s ) :

z = np . d o t (w,

a c t i v a t i o n ) +b # C a l c u l a o campo de a t i v a ç ão .

z s . a p p e n d ( z ) # A d i c i o n a o campo de a t i v a ç ã o z c a l c u l a d o na l i n h a

i m e d i a t a m e n t e acima à l i s t a ’ z s = [ ] ’ .

a c t i v a t i o n = s i g m o i d ( z ) # a p l i c a a f u n ç ã o de a t i v a ç ão s i g m o i d ao

campo de a t i v a ç ã o .

a c t i v a t i o n s . a p p e n d ( a c t i v a t i o n ) # A d i c i o n a a s a t i v a ç ã o c a l c u l a d a

na l i n h a acima à l i s t a ’ a c t i v a t i o n s ’

# Backward p a s s

d e l t a = s e l f . c o s t _ d e r i v a t i v e ( a c t i v a t i o n s [ − 1 ] , y ) * s i g m o i d _ p r i m e ( z s

[ − 1 ] ) # d e f i n e d e l t a como

n a b l a _ b [ − 1 ] = d e l t a

n a b l a _ w [ − 1 ] = np . d o t ( d e l t a ,

a c t i v a t i o n s [ − 2 ] . t r a n s p o s e ( ) )

# Aqui ,

l = 1 s i g n i f i c a a ú l t i m a camada de n e u r ô n i o s ,

l = 2 é a

s e g u n d a e a s s i m p o r d i a n t e .

f o r

l

i n range ( 2 ,

s e l f . n u m _ l a y e r s ) :

z = z s [ − l ]

s p = s i g m o i d _ p r i m e ( z )

d e l t a = np . d o t ( s e l f . w e i g h t s [ − l + 1 ] . t r a n s p o s e ( ) , d e l t a ) * s p
n a b l a _ b [ − l ] = d e l t a

n a b l a _ w [ − l ] = np . d o t ( d e l t a ,

a c t i v a t i o n s [ − l − 1 ] . t r a n s p o s e ( ) )

r e t u r n ( n a b l a _ b , n a b l a _ w )

d e f e v a l u a t e ( s e l f ,

t e s t _ d a t a ) : #O mé t o d o e v a l u a t e

r e t o r n a o n ú mero de

e n t r a d a s de t e s t e p a r a a s q u a i s a r e d e n e u r a l p r o d u z o r e s u l t a d o c o r r e t o

. A s a í da da r e d e , n e s t e caso , é o í n d i c e do n e u r ô n i o da camada de s a í da

com a m a i o r a t i v a ç ão .

t e s t _ r e s u l t s = [ ( np . argmax ( s e l f . f e e d f o r w a r d ( x ) ) , y )

f o r ( x , y )

i n

t e s t _ d a t a ] #O c ó d i g o np . argmax c a l c u l a o í n d i c e com m a i o r a r g u m e n t o ( a t i v a

ç ão ) da s a í da g e r a d a p e l o mé t o d o f e e d f o r w a r d p a r a o s i n a l de e n t r a d a x .

r e t u r n sum ( i n t ( x == y )

f o r ( x , y )

i n t e s t _ r e s u l t s ) # R e t o r n a o nú

mero de t u p l a s em t e s t _ r e s u l t s onde x=y ,

i s t o é , que a p r e v i s ão f o i

c o r r e t a .

88

86

87

88

89

d e f

c o s t _ d e r i v a t i v e ( s e l f , o u t p u t _ a c t i v a t i o n s , y ) : # D e f i n e o mé t o d o

c o s t _ d e r i v a t i v e como s e n d o a s a t i v a ç õ e s de s a í da menos o s

r e s p e c t i v o s

r ó

t u l o s ( p r e v i s ã o c o r r e t a ) .

r e t u r n ( o u t p u t _ a c t i v a t i o n s −y )

90 # Fun ç ão de A t i v a ç ão S i g m o i d e

91 d e f

s i g m o i d ( z ) :

r e t u r n 1 . 0 / ( 1 . 0 + np . exp ( − z ) )

92

93

94 # Fun ç ão p a r a r e t o r n a r a s d e r i v a d a s da f u n ç ão S i g m o i d e

95 d e f

s i g m o i d _ p r i m e ( z ) :

96

r e t u r n s i g m o i d ( z ) *(1 − s i g m o i d ( z ) )

Fonte: http://deeplearningbook.com.br/construindo-uma-rede-neural-com-linguagem-python/

Acesso em: 6 de set. 2020

As imagens do conjunto de dados MNIST são arrays numpy 28x28. Vamos utilizar os

códigos abaixo para converter cada array 28x28 para 784x1 que é a forma adequada para a

camada de entrada da rede que terá 784 neurônios.

1 import p i c k l e

2 import g z i p

3 import numpy a s np

4

5 d e f

l o a d _ d a t a ( ) :

f = g z i p . open ( ’ m n i s t . p k l . gz ’ ,

’ r b ’ )

t r a i n i n g _ d a t a , v a l i d a t i o n _ d a t a ,

t e s t _ d a t a = p i c k l e . l o a d ( f ,

e n c o d i n g = "

l a t i n 1 " ) # O mé t o d o p i c k l e . l o a d l e r o s d a d o s

f . Ao d e f i n i r a c o d i f i c a ç ão

p a r a l a t i n 1 p e r m i t e i m p o r t a r o s d a d o s d i r e t a m e n t e .

f . c l o s e ( ) # f e c h a o a r q u i v o f .

r e t u r n ( t r a i n i n g _ d a t a , v a l i d a t i o n _ d a t a ,

t e s t _ d a t a )

6

7

8

9

10

11 d e f

l o a d _ d a t a _ w r a p p e r ( ) :

12

13

t r _ d , va_d ,

t e _ d = l o a d _ d a t a ( ) # Armazena o s c o n j u n t o s de t r e i n a m e n t o ,

v a l i d a ç ão e t e s t e s n a s v a r i á v e i s

t r _ d , va_d e t e _ d r e s p e c t i v a m e n t e .

t r a i n i n g _ i n p u t s = [ np . r e s h a p e ( x ,

( 7 8 4 , 1 ) )

f o r x i n t r _ d [ 0 ] ] #Muda o

f o r m a t o d a s

i m a g e n s c o n j u n t o de t r e i n a m e n t o de 28 x28 p a r a 784 x1 ,

e n f i l e i r a o s p i x e l s .

14

t r a i n i n g _ r e s u l t s = [ v e c t o r i z e d _ r e s u l t ( y )

f o r y i n t r _ d [ 1 ] ] # C r i a o

89

c o n j u n t o de v e t o r e s

c o l u n a com o s

r ó t u l o s do c o n j u n t o de t r e i n a m e n t o . Ex

. :

s e r ó t u l o é 1 c r i a o v e t o r

( 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ) ^ T .

15

t r a i n i n g _ d a t a = z i p ( t r a i n i n g _ i n p u t s ,

t r a i n i n g _ r e s u l t s ) # C r i a o c o n j u n t o

t r a i n i n g _ d a t a a t r a v é s da f u n ç ã o i n t e r n a z i p que r e t o r n a uma l i s t a de

t u p l a onde a i −é s i m a t u p l a é f o r m a d a p e l o s

i −é s i m o s e l e m e n t o s d o s

a r g u m e n t o s de z i p que s ã o t r a i n i n g _ i n p u t s e t r a i n i n g _ r e s u l t s .

16

v a l i d a t i o n _ i n p u t s = [ np . r e s h a p e ( x ,

( 7 8 4 , 1 ) )

f o r x i n va_d [ 0 ] ] #Muda o

f o r m a t o d a s

i m a g e n s c o n j u n t o de v a l i d a ç ã o de 28 x28 p a r a 784 x1 ,

e n f i l e i r a

o s p i x e l s .

17

v a l i d a t i o n _ d a t a = z i p ( v a l i d a t i o n _ i n p u t s , va_d [ 1 ] ) # C r i a o c o n j u n t o

v a l i d a t i o n _ d a t a a t r a v é s da f u n ç ã o i n t e r n a z i p que r e t o r n a um l i s t a de

t u p l a onde a i −é s i m a t u p l a é f o r m a d a p e l o s

i −é s i m o s e l e m e n t o s d o s

a r g u m e n t o s de z i p que s ã o v a l i d a t i o n _ i n p u t s e va_d [ 1 ] .

18

t e s t _ i n p u t s = [ np . r e s h a p e ( x ,

( 7 8 4 , 1 ) )

f o r x i n t e _ d [ 0 ] ] #Muda o f o r m a t o

d a s

i m a g e n s c o n j u n t o de t e s t e s de 28 x28 p a r a 784 x1 ,

e n f i l e i r a o s p i x e l s

.

19

t e s t _ d a t a = z i p ( t e s t _ i n p u t s ,

t e _ d [ 1 ] ) # C r i a o c o n j u n t o t e s t _ d a t a a t r a v é

s da f u n ç ão i n t e r n a z i p que r e t o r n a um l i s t a de t u p l a onde a i −é s i m a

t u p l a é f o r m a d a p e l o s

i −é s i m o s e l e m e n t o s d o s a r g u m e n t o s de z i p que s ão

t e s t _ i n p u t s e t e _ d [ 1 ] .

20

r e t u r n ( t r a i n i n g _ d a t a , v a l i d a t i o n _ d a t a ,

t e s t _ d a t a ) # R e t o r n a o s

c o n j u n t o s

t r a i n i n g _ d a t a ,

v a l i d a t i o n _ d a t a ,

t e s t _ d a t a no f o r m a t o de imagem

adequado p a r a a r e d e r e c e b e r em s u a camada de e n t r a d a 784 x1 p i x e l s .

21

22 d e f v e c t o r i z e d _ r e s u l t ( j ) :

23

24

25

e = np . z e r o s ( ( 1 0 , 1 ) )

e [ j ] = 1 . 0

r e t u r n e

Nesse ponto, podemos escolher o número de camadas e neurônios por camada, para

treinar os dados MNIST que foi carregado pelo método load_data_wrapper() no código anterior.

Note que, também no código anterior, convertemos as imagens de 28x28 para 784x1, isto é,

enﬁleiramos os pixels de cada imagem do conjunto de dados, desse modo, a rede terá três camada

onde camada de entrada terá 784 neurônios(número de pixels da imagem) e camada de saída terá

10 neurônios(um neurônio para cada resultado possível), por sua vez a camada oculta poderemos

testar quantidades de neurônios de modo a melhorar o desempenho da rede.

90

1 t r a i n i n g _ d a t a , v a l i d a t i o n _ d a t a ,

t e s t _ d a t a = l o a d _ d a t a _ w r a p p e r ( ) # C a r r e g a o s

c o n j u n t o de t r e i n a m e n t o ,

v a l i d a ç ão e de t e s t e s a t r a v é s do mé t o d o ’

l o a d _ d a t a _ w r a p p e r ( ) ’

j á c o n v e t i d o s .

2 t r a i n i n g _ d a t a = l i s t ( t r a i n i n g _ d a t a ) # I n s e r e o s d a d o s de t r e i n a m e n t o em uma

l i s t a .

3

4 n e t = Network ( [ 7 8 4 , 3 0 , 1 0 ] ) # D e f i n e o n ú mero de camadas e n e u r ô n i o s p o r

camada .

5 n e t . SGD( t r a i n i n g _ d a t a , 1 0 , 1 0 , 3 . 0 ,

t e s t _ d a t a = t e s t _ d a t a ) # T r e i n a a r e d e onde

t r a i n i n g _ d a t a ,

e p o c h s =10 , m i n i _ b a t c h _ s i z e =10 , e t a = 3 . 0 ,

t e s t _ d a t a =

t e s t _ d a t a .

6 . . . Epoch 0 : 9029 / 10000

7 . . . Epoch 1 : 9180 / 10000

8 . . . Epoch 2 : 9317 / 10000

9 . . . Epoch 3 : 9331 / 10000

10 . . . Epoch 4 : 9398 / 10000

11 . . . Epoch 5 : 9383 / 10000

12 . . . Epoch 6 : 9393 / 10000

13 . . . Epoch 7 : 9407 / 10000

14 . . . Epoch 8 : 9385 / 10000

15 . . . Epoch 9 : 9423 / 10000

Observe que em 10 épocas a rede atinge 94, 23% de precisão com o conjunto de testes.

Podemos agora, com a rede treinada, fazer predições de imagens especíﬁcas. Vamos utilizar o

conjunto de validação validation_data para testar a rede com uma única imagem.

1 v a l i d a t i o n _ d a t a = l i s t ( v a l i d a t i o n _ d a t a ) # I n s e r e o c o n j u n t o de v a l i d a ç ão em

uma l i s t a .

Vamos deﬁnir a função predict para predizer uma única imagem através do método

feedforward que gera a saída da rede ao ser apresentada uma imagem de entrada.

1 p r e d i c t = n e t . f e e d f o r w a r d

91

Agora podemos escolher uma imagem do conjunto de validação e testar a rede neural

com uma determinada imagem.

1 img = v a l i d a t i o n _ d a t a [ 8 7 ] [ 0 ] # O c o n j u n t o v a l i d a t i o n _ d a t a é uma l i s t a de

t u p l a s , onde cada t u p l a é da f o r m a ( x , y )

s e n d o x uma imagem 784 x1 e y o

r ó t u l o de 0 a 9 . D e s s e modo ,

v a l i d a t i o n _ d a t a [ 8 6 ] [ 0 ] é a t u p l a da 87

p o s i ç ão e o [ 0 ]

i n d i c a que e s t a m o s

t r a t a n d o de x .

1 >>> p r e d i c t ( img )

2 . . . a r r a y ( [ [ 1 . 6 4 9 9 5 9 6 3 e − 0 8 ] ,

3

4

5

6

7

8

9

10

11

[ 9 . 3 6 3 1 2 7 1 4 e − 0 4 ] ,

[ 8 . 9 2 4 3 0 4 3 5 e − 0 3 ] ,

[ 9 . 3 1 6 6 2 8 7 1 e − 0 1 ] ,

[ 6 . 9 3 0 5 8 3 7 5 e − 1 0 ] ,

[ 9 . 2 1 3 6 0 9 4 9 e − 0 5 ] ,

[ 4 . 4 3 2 6 8 5 5 6 e − 0 7 ] ,

[ 1 . 9 0 3 0 7 8 0 5 e − 0 6 ] ,

[ 5 . 5 9 1 0 4 4 9 1 e − 0 4 ] ,

[ 7 . 4 7 9 8 5 0 1 4 e − 0 5 ] ] )

Para determinar qual o neurônio com maior ativação podemos a função numpy argmax

como no código abaixo.

1 >>>np . argmax ( p r e d i c t ( img ) ) # R e t o r n a a p o s i ç ão da m a i o r c o m p o n e n t e da a r r a y ,

i s t o é , a p r e d i ç ão da r e d e .

2 . . . 3

Podemos veriﬁcar se predição está correta através da veriﬁcação do seu rótulo

1 >>> r = v a l i d a t i o n _ d a t a [ 8 7 ] [ 1 ] #O 87 i n d i c a a p o s i ç ão da t u p l a na l i s t a e o 1

i n d i c a o r ó t u l o y . Obs f o r m a t o d a s

t u p l a s

( x , y ) .

2 >>> r

3 . . . 3

92

Podemos também utilizar a biblioteca matplotlib para visualizar a imagem predita. Para

isso, devemos antes, mudar o formato da imagem de acordo com o código abaixo.

1

2 img2 = v a l i d a t i o n _ d a t a [ 8 7 ] [ 0 ] . r e s h a p e ( 2 8 , 2 8 ) # O c o n j u n t o v a l i d a t i o n _ d a t a é

uma l i s t a t u p l a s , onde cada t u p l a é da f o r m a ( x , y )

s e n d o x uma imagem

784 x1 e y o r ó t u l o de 0 a 9 . D e s s e modo ,

v a l i d a t i o n _ d a t a [ 8 7 ] [ 0 ] é a

t u p l a da 87

p o s i ç ão e o [ 0 ]

i n d i c a que e s t a m o s

t r a t a n d o de x . A f u n ç ão

r e s h a p e muda o f o r m a t o da imagem de 784 x1 p a r a 28 x28 p a r a p e r m i t i r a

v i s u a l i z a ç ão da imagem .

1 import m a t p l o t l i b . p y p l o t a s p l t

1 p l t . f i g u r e ( )

2 p l t . imshow ( img )

3 p l t . c o l o r b a r ( )

4 p l t . g r i d ( F a l s e )

5 p l t . show ( )

6

Vamos agora testar a rede com uma imagem que não pertence ao conjunto de dados

MNIST. Podemos escrever um digito, escanear e testar se a rede consegue fazer a predição. Para

isso vamos utilizar o pacote Python PIL além do Matplotlib já importado.

1 >>>from PIL import

Image

Agora vamos utilizar o PIL para importar a imagem escaneada e nomeada como igm1

salva na mesma pasta do arquivo Jupyter notebook que estamos construindo a rede neural.

1 >>> image = Image . open ( ’ img4 . png ’ )

93

Para visualizar a imagem importada utilizamos os códigos abaixo.

1 >>> p l t . f i g u r e ( )

2 >>> p l t . imshow ( image )

3 >>> p l t . c o l o r b a r ( )

4 >>> p l t . g r i d ( F a l s e )

5 >>> p l t . show ( )

6

Vamos utilizar uma sequência de códigos para preparar a imagem de acordo com as

exigências da entrada da rede, isto é, vamos converter a imagem para o mesmo formato das

imagens do conjunto MNIST e depois convertê-la para o formato de entrada 784x1.

1 >>> image . s i z e # M o s t r a o f o r m a t o da imagem c a r r e g a d a .

2 . . . ( 9 3 , 1 0 3 )

1 >>> i m g _ r e s i z e = image . r e s i z e ( ( 2 8 , 2 8 ) ) # C o n v e r t e a imagem p a r a o f o r m a t o 28

x28 mesma d i m e n s ão d a s

i m a g e n s do c o n j u n t o MNIST .

2 >>> i m g _ r e s i z e . s i z e # M o s t r a a s n o v a s d i m e n s õ e s da imagem .

3 . . . ( 2 8 , 2 8 )

1 >>> i m a g e _ c i n z a = i m g _ r e s i z e . c o n v e r t ( mode= "L" ) # C o n v e r t e a imagem p a r a e s c a l a

de c i n z a

94

1 >>>from numpy import a s a r r a y # i m p o r t a o p a c o t e a s a r r a y do Numpy , que tem

f u n ç ão de c o n v e r t e r uma imagem p a r a a r r a y .

1 >>> d a t a = a s a r r a y ( i m a g e _ c i n z a ) # C o n v e r t e a imagem p a r a um a r r a y numpy .

1 >>> d a t a . s h a p e # M o s t r a o f o r m a t o de a r r a y numpy .

2 . . . ( 2 8 , 2 8 )

1 >>>img = np . r e s h a p e ( d a t a ,

( 7 8 4 , 1 ) ) # C o n v e r t e o a r r a y numpy de a c o r d o com a s

e x i g ê n c i a s de e n t r a d a da r e d e ,

i s t o é , c o n v e r t e de 28 x28 p a r a 784 x1 .

Agora, que a imagem já foi tratada e está de acordo com as exigências da rede, podemos

fazer a predição.

1 >>> p r e d i c t ( img )

2 . . . a r r a y ( [ [ 1 . 4 9 0 1 8 5 6 0 e − 0 8 ] ,

3

4

5

6

7

8

9

10

11

[ 8 . 1 9 5 2 1 7 0 9 e − 0 2 ] ,

[ 8 . 4 4 0 5 3 4 9 8 e − 0 9 ] ,

[ 3 . 8 8 9 6 3 4 6 6 e − 0 8 ] ,

[ 7 . 1 5 1 3 7 5 9 3 e − 0 1 ] ,

[ 4 . 5 4 8 0 3 5 7 2 e − 0 7 ] ,

[ 5 . 8 0 7 2 4 5 6 7 e − 0 3 ] ,

[ 3 . 3 4 1 0 3 7 7 6 e − 0 3 ] ,

[ 2 . 3 4 0 2 2 6 1 1 e − 0 4 ] ,

[ 5 . 9 7 0 8 8 4 8 4 e − 0 3 ] ] )

12 >>>np . argmax ( p r e d i c t ( img ) )

13 . . . 4

95

5 ROTEIRO DE APRESENTAÇÃO DE REDES NEURAIS NO ENSINO BÁSICO

É proposto para o novo ensino médio que a escola converse com a realidade atual e

com os caminhos a serem seguidos pelos alunos em sua trajetória proﬁssional, trazendo o que é

essencial para o trabalho e para a vida em sociedade. A inteligência artiﬁcial está cada vez mais

presente na vida das pessoas e nas mais variadas áreas como saúde, segurança e educação, sendo

utilizada para resolver diversos tipos problemas.

Trabalhar aplicações relevantes nas aulas de matemática desperta a curiosidade e o

interesse dos alunos para com o tema estudado e o processo de ensino-aprendizagem se torna

mais atraente e signiﬁcativo. Nesse sentido Moran destaca,

Alunos curiosos e motivados facilitam enormemente o processo, estimulam as
melhores qualidades do professor, tornam-se interlocutores lúcidos e parceiros
de caminhada do professor-educador. Alunos motivados aprendem e ensinam,
avançam mais, ajudam o professor a ajudá-los melhor. (MORAN, 2000, p.17).

Nessa perspectiva, apresentaremos aos professores do ensino médio uma proposta de

ensino para educação básica, em aulas de matemática, com a temática redes neurais e Deep

Learning, tendo como pressuposto que a ciência de dados está presente no nosso dia a dia e que

os proﬁssionais na área encontram-se em número reduzido, enﬁm, evidenciar que existe um

mercado de atuação proﬁssional muito promissor a ser desbravado.

5.1

Interpretação geométrica da derivada a partir da taxa média de variação

Para treinar uma rede neural devemos encontrar os pesos e os bias que minimizam a

sua função de custo, para isso, vamos utilizar os conceitos de máximos e mínimos do cálculo

diferencial, através do algoritmo gradiente descendente.

Para que os alunos compreendam o algoritmo utilizado para treinar as redes neurais

podemos trabalhar o conceito de derivada partir de sua interpretação geométrica, a inclinação da

reta tangente a uma curva, feito a partir de aproximações de retas secantes e da taxa de variação

média da função.

Considere uma curva que seja gráﬁco de uma função y = f (x) e P = (x0, f (x0)), um

ponto de f , onde será traçado a reta tangente. Atribuindo a x um acréscimo ∆x = h, temos

que a variável y sofrerá um acréscimo ∆y, assim, saímos do ponto P (x, y) para um ponto

Q = (x + ∆x, y + ∆y). Veja o gráﬁco abaixo.

Figura 31 – Reta secante

96

y

y0 + ∆y

y0

Q
Q

∆y

P
P

x0

x0 + h

x

∆x

Fonte: https://tex.stackexchange.com/questions/460632/tikz-and-secant-line-diagram. Acesso:

OUT, 2020.

Observe que o coeﬁciente angular da reta secante r =

∆x. Mantendo ﬁxo o
ponto P e fazendo Q variar na direção de P , temos que h vai se aproximando de zero, mas não

↔
P Q é m = ∆y

zero. Veja o gráﬁco abaixo.

Figura 32 – Aproximação de uma reta tangente por retas secantes

Q1

Q2

p

Q3

Fonte: https://tex.stackexchange.com/questions/460632/tikz-and-secant-line-diagram. Acesso:

OUT. 2020.

Como Q se aproxima de P , más diferente de P , temos uma posição limite para Q.

Informalmente, podemos deﬁnir a reta tangente a f em P , como sendo a reta r =

↔
P Q quando Q

97

está na sua posição limite e a derivada de f no ponto P é a inclinação ou o coeﬁciente angular

da reta tangente a f em P .

Exemplo 5.1. seja y = f (x) = x2 e P = (2, 4) um ponto de f e um acréscimo ∆x = h e o
acréscimo de de y ∆y = (2 + h)2 − 4 = 4 + 4h + h2 − 4. Segue que o coeﬁciente angular da reta

secante é

m =

∆y
∆x

=

4h − h2
h

Para determinar a derivada, basta fazer ∆x se aproximar de 0, isto é, fazer h se aproximar de 0,

o que implica que a reta secante se aproxima da reta tangente, e por sua vez, o coeﬁciente da reta

secante se aproxima do coeﬁciente da reta tangente. Note que

m =

∆y
∆x

=

h(4 − h)
h

Como h tende a 0, mas nunca é igual a 0, temos:

m = lim
∆x→0

∆y
∆x

= lim
h→0

h(4 − h)
h

= lim
h→0

(4 − h) = 4

Portanto a derivada de f (x) = x2 em x = 2 é 4, isto signiﬁca que em x = 2 a função f tem

uma taxa de crescimento igual a 4, desse modo, se nosso objetivo é encontrar o mínimo de f ,

tratando-a como uma função de custo, devemos reduzir x para minimizar f .

No exemplo anterior, foi mostrado a interpretação geométrica da derivada para uma

função com uma variável, más a função de custo nas redes neurais, tem como variáveis muitos

pesos e bias. Vamos ver como isso funciona como duas variáveis, a interpretação para n variáveis

é análoga. Considere a superfície z = f (x, y) representada no gráﬁco abaixo.

Figura 33 – Graﬁco de uma função de duas variáves

z

x

y

Fonte: https://tex.stackexchange.com/questions/479814/a-diagram-about-partial-derivatives-of-

fx-y. Acesso: OUT, 2020.

As funções de custos costumam ter gráﬁcos irregulares com muitos vales, mas para ﬁns

98

de exemplo, essa superfície é satisfatória. A interpretação geométrica das derivadas parciais de
z = f (x, y) são análogas a de uma variável. A derivada parcial de com relação a x, ∂f

∂x no ponto
(x0, y0) é a inclinação da reta tangente a a curva gerada pela interseção do plano y = y0 com a
superfície f , do mesmo modo, a derivada parcial de com relação a y, ∂f

∂y no ponto (x0, y0) é a
inclinação da reta tangente a curva gerada pela interseção do plano x = x0 com a superfície f .

Veja os gráﬁcos abaixo.

Figura 34 – Interpetração geométrica da derivada parcial em funções de duas variáves

Inclinação na direção de x
z

∂f
∂x (x0, y0)

Inclinação na direção de y
z

∂f
∂y (x0, y0)

x0

x

x0

x

y0

y

y0

y

(x0, y0)

(x0, y0)

Fonte: https://tex.stackexchange.com/questions/479814/a-diagram-about-partial-derivatives-of-

fx-y. Acesso: OUT, 2020.

Exemplo 5.2. Considere a função z = f (x, y) = x2 + y2 + 10. Para calcular a derivada parcial
com relação a x, ∂f

∂x , devemos considerar y = y0 uma constante e derivar com relação a x do

mesmo modo como em função real de uma variável real.

Analogamente temos:

∂f
∂x

= 2x

∂f
∂y

= 2y

Onde ∂f

∂x = 2x e ∂f

∂y = 2y são as taxas de variação da superfície nas direções dos eixos x

e y respectivamente.

99

5.2 Deep learning e a visão computacional

Vamos treinar um modelo de rede neural para reconhecimento de imagens, neste caso

imagens de dígitos manuscritos. Para tal vamos usar o keras é uma API de alto nível para

construir e treinar modelos no TensorFlow que é uma biblioteca completa de código aberto

para machine learning. Para usar o TensorFlow; disponível para Ubunto, Windows, macOS e

Raspberry Pi; é necessário fazer sua instalação com o gerenciador de pacotes pip do Python

usando os códigos abaixo

1 # R e q u e r o p i p m a i s

r e c e n t e

2 p i p i n s t a l l −− u p g r a d e p i p

3

4 # V e r s ão e s t á v e l a t u a l p a r a CPU e GPU

5 p i p i n s t a l l

t e n s o r f l o w

6

7 # Se p r e f e r i r

t e n t e tamb ém a c o m p i l a ç ão de v i s u a l i z a ç ão ( i n s t á v e l )

8 p i p i n s t a l l

t f − n i g h t l y

Com o Tensorﬂow instalado, devemos importar as bibliotecas a serem utilizadas para

treinar o modelo que neste caso temos o TensorFlow e a API Keras do TensorFlow.

1 >>> import

t e n s o r f l o w a s

t f # i m p o r t e

t e n s o r f l o w como t f

2 >>> from t e n s o r f l o w import k e r a s # do t e n s o r f l o w i m p o r t e k e r a s

Além das Bibliotecas auxiliares

1 >>> import numpy a s np # i m p o r t e numpy como np

2 >>> import m a t p l o t l i b . p y p l o t a s p l t # i m p o r t e m a t p l o t l i b . p y p l o t como p l t

O próximo passo é importar o conjunto de dados MNIST que se encontra na API keras.

1 >>> ( t r a i n _ i m a g e s ,

t r a i n _ l a b e l s ) ,

( t e s t _ i m a g e s ,

t e s t _ l a b e l s ) = t f . k e r a s .

d a t a s e t s . m n i s t . l o a d _ d a t a ( )

100

Com o código acima é carregado o conjunto de dados na forma de quatro NumPy arrays

onde:

• Os arrays train_images e train_labels: É o conjunto de treinamento, dividido em duas

numpy arrays. A primeira com as classes e a segunda com os rótulos;

• Os arrays test_images e test_labels : É o conjunto de teste conﬁgurado da mesma forma

do conjuto de treinamento.

No conjunto MNIST, as imagens são arrays NumPy de 28x28, com os pixels variando

entre 0 e 255. As labels (rótulos) são arrays de inteiros variando de 0 a 9, onde cada valor tem

uma classe que corresponde a respectivo dígito conforme a tabela abaixo.

Figura 35 – Tabela de correspondencia entre classes e rótulos

Classe Label(rótulo)

Zero

Um

Dois

Três

Quatro

Cinco

Seis

Sete

Oito

Nove

0

1

2

3

4

5

6

7

8

9

Fonte: Elaborada pelo autor.

Cada imagem tem apenas um rótulo (label) que representa a qual classe representa. Como

os nomes das classes não estão incluídas no conjunto de dados, podemos armazenar os nomes

das classes para uso posterior com o código abaixo.

1 >>> c l a s s _ n a m e s = [ ’ Z e r o ’ ,

’Um’ ,

’ D o i s ’ ,

’ Tr ê s ’ ,

’ Q u a t r o ’ ,

2

’ C i n c o ’ ,

’ S e i s ’ ,

’ S e t e ’ ,

’ O i t o ’ ,

’ Nove ’ ]

Com os dados importados, podemos agora, explorar os dados e veriﬁcar a sua forma com

101

o comando .shape

1 >>> t r a i n _ i m a g e s . s h a p e

2 . . .

( 6 0 0 0 0 , 2 8 , 2 8 )

Assim o conjunto tem 60000 imagens, onde cada imagem é representada por um NumPy

array de 28x28

Com o camando len() podemos veriﬁcar o número de elemento do conjunto train_labels

que é o conjunto de rótulos do conjunto de treinamento.

1 >>> l e n ( t r a i n _ l a b e l s )

2 . . . 60000

Do mesmo modo, veriﬁcamos as conﬁgurações do conjunto de testes

1 >>> t e s t _ i m a g e s . s h a p e

2 . . .

( 1 0 0 0 0 , 2 8 , 2 8 )

3 >>> l e n ( t e s t _ l a b e l s )

4 . . . 10000

Note que o conjunto dos rótulos é uma numpy array de inteiros como dito antes, veja.

1 >>> t r a i n _ l a b e l s

2 . . .

a r r a y ( [ 5 , 0 , 4 ,

. . . , 5 , 6 , 8 ] , d t y p e = u i n t 8 )

A próxima fase é o pré-processamento dos dados, se escolhermos uma das imagens do

conjunto de treinamento, por exemplo, a número 5 veremos que os pixels estão variando entre 0

e 255

1 >>> p l t . f i g u r e ( ) # c r i a uma nova f i g u r a ou a t i v a uma f i g u r a e x i s t e n t e .

2 >>> p l t . imshow ( t r a i n _ i m a g e s [ 5 ] ) # E x i b e o s d a d o s como uma imagem em 2D .

3 >>> p l t . c o l o r b a r ( ) # A d i c i o n a uma b a r r a de c o r e s a um g r á f i c o .

4 >>> p l t . g r i d ( F a l s e ) # C o n f i g u r a a s

l i n h a s de g r a d e : Com l i n h a s

( T r u e )

sem

l i n h a ( F a l s e ) .

5 >>> p l t . show # M o s t r a a f i g u r a

102

6

Observe, na imagem acima, que a tonalidade dos pixels das imagens do conjunto MNIST

varia de 0 a 255. Antes do treinamento do modelo, é conveniente escalar os valores dos pixels

entre 0 e 1, para isso, dividimos os valores por 255.0. É importante observar que o conjunto de

treinamento e o conjunto de testes devem ser pré-processados da mesma maneira.

1 >>> t r a i n _ i m a g e s = t r a i n _ i m a g e s

/ 2 5 5 . 0

2 >>> t e s t _ i m a g e s = t e s t _ i m a g e s

/ 2 5 5 . 0

Podemos analisar se os dados estão prontos para construção e treinamento da rede neural,

para isso, vamos utilizar o código abaixo para visualizar as 16 primeiras imagens do conjunto de

treinamento com os respectivos nomes das classes à qual pertence.

1 >>> p l t . f i g u r e ( f i g s i z e = ( 1 0 , 1 0 ) ) # C r i e ou a t i v e uma f i g u r a .

e x i s t e n t e / f i g s i z e

( f l o a t , f l o a t ) : d e t e r m i n a a l a r g u r a e a l t u r a da imagem em p o l e g a d a s .

2 >>> f o r

i

i n range ( 1 6 ) : # p a r a i em r a n g e ( 2 5 ) = [ 0 , 2 , . . . , 2 4 ]

3 . . .

4 . . .

5 . . .

6 . . .

7 . . .

p l t . s u b p l o t ( 5 , 5 , i + 1 ) # D i v i d e a imagem em 25 i m a g e n s .

p l t . x t i c k s ( [ ] ) # r e t i r a a e s c a l a do e i x o x .

p l t . y t i c k s ( [ ] ) # r e t i r a a e s c a l a do e i x o y .

p l t . g r i d ( F a l s e )

p l t . imshow ( t r a i n _ i m a g e s [ i ] , cmap= p l t . cm . b i n a r y ) # E x i b e o s d a d o s como

uma imagem em 2D .

t r a i n _ i m a g e s [ i ] : i −é s i m a imagem de t r a i n \ _ i m a g e s

8 . . .

p l t . x l a b e l ( c l a s s _ n a m e s [ t r a i n _ l a b e l s [ i ] ] ) , cmap : c o l o r m a p d e f i n e a

c o r em e s c a l a de c i n z a .

9 >>> p l t . show ( ) # M o s t r a a f i g u r a

103

10

Para construir o modelo de uma rede neural devemos, primeiramente, fazer a conﬁguração

das camadas e, por ﬁm, compilar o modelo. Como vimos, ao encadear as camadas, o aprendizado

é a otimização dos pesos sinápticos a ﬁm de minimizar a função de custo. Usando tf.keras temos

modelos prontos que fazem praticamente todo o trabalho automaticamente.

Vamos utilizar, para treinar a rede neural, o modelo sequencial do keras que é apropriado

para uma pilha simples de camadas onde cada camada tem exatamente um tensor de entrada e

um tensor de saída.

Deﬁnição 5.1. Tensores são arrays multidimensionais, que vão ﬂuindo pelos neurônios ou nós

da rede neural.

1 >>>model = k e r a s . S e q u e n t i a l ( [

k e r a s . l a y e r s . F l a t t e n ( i n p u t _ s h a p e = ( 2 8 , 2 8 ) ) ,

k e r a s . l a y e r s . Dense ( 1 2 8 ,

a c t i v a t i o n = ’ r e l u ’ ) ,

k e r a s . l a y e r s . Dense ( 1 0 ,

a c t i v a t i o n = ’ s o f t m a x ’ )

2 . . .

3 . . .

4 . . .

5 . . . ] )

104

A primeira camada da rede neural, tf.keras.layers.Flatten, transforma o formato da

imagem atual que é um array de duas dimensões (28x28 pixels) em um array de uma única

dimensão de (28*28=784 pixels), isto é, enﬁleira os pixels. Observe que esta camada só tem esta

função de formatação dos dados, não tem parâmetros para aprender.

As duas outras camadas keras.layers.Dense com 128 e 10 nós (ou neurônios) respectiva-

mente são full connected (totalmente conectadas). A segunda camada com 10 nós, retorna um

array de 10 probabilidades, onde cada uma delas indica a probabilidade de que a imagem de

testada pertença a cada uma das 10 classes, que somadas retorna resultado 1.

O próximo passo é compilar o modelo, mas é necessário fazer algumas conﬁgurações

adicionais.

1 >>>model . c o m p i l e ( o p t i m i z e r = ’ adam ’ ,

2 . . .

3 . . .

l o s s = t f . k e r a s . l o s s e s . S p a r s e C a t e g o r i c a l C r o s s e n t r o p y (

f r o m _ l o g i t s = T r u e ) ,

m e t r i c s = [ ’ a c c u r a c y ’ ] )

Onde:

• Optimizer É como o modelo atualiza os pesos de acordo com a função de perda;

• loss ou função loss (função de perda) mede a precisão do modelo em cada passo do

treinamento, com objetivo minimizá-lo.

• Metrics Usada para monitoramento dos passos de treinamento e teste. No caso de accuracy

se trata da fração das imagens que foram classiﬁcadas corretamente.

5.2.1 Treinamento do modelo

Para treinar uma rede neural artiﬁcial devemos seguir os passos abaixo:

1. Alimentar a rede com os dados de treinamento, que neste caso são as arrays train_imagens

e train_labels;

2. Durante o treinamento o modelo aprende a associar as imagens aos respectivos rótulos;

3. É feito predições com conjunto de teste que neste caso é a array test_imagens

105

4. É feito a veriﬁcação da precisão das das predições com os rótulos do conjunto de teste,

que neste caso é a array test_labels

Para iniciar o treinamento usamos o método model.ﬁt()

1 >>> model . f i t ( t r a i n _ i m a g e s ,

t r a i n _ l a b e l s ,

e p o c h s = 15 ) #A r e d e é a l i m e n t a d a

com o c o n j u n t o de t r e i n a m e n t o ( t r a i n _ i m a g e s ,

t r a i n _ l a b e l s ) p o r 15 é p o c a s

.

2 . . .

l o s s : 0 . 2 0 3 6 − a c c u r a c y : 0 . 9 2 3 8 # Obtendo 92 ,38\% de p r e c i s ã o

Agora, vamos avaliar a desempenho do modelo com o conjunto de testes.

1 >>> t e s t _ l o s s ,

t e s t _ a c c = model . e v a l u a t e ( t e s t _ i m a g e s ,

t e s t _ l a b e l s , v e r b o s e

= 2 ) # v e r b o s e =2 d e t a l h a o p r o c e s s o como uma l i n h a de l o g p o r é poca

2 . . . 3 1 3 / 3 1 3 − 1 s − l o s s : 0 . 3 3 8 7 − a c c u r a c y : 0 . 8 8 4 3

Observe que a accuracy teve uma performance um pouco menor com o conjunto de

teste em relação ao conjunto de treinamento. Essa diferença entre a accuracy do conjunto de

treinamento e o conjunto de teste representa um overﬁtting (sobreajute).

Como o modelo já está treinado, vamos usá-lo para fazer algumas predições de imagens.

1 >>> p r o b a b i l i t y _ m o d e l = t f . k e r a s . S e q u e n t i a l ( [ model ,

2

t f . k e r a s . l a y e r s . S o f t m a x ( ) ] )

1 >>> p r e d i c t i o n s = p r o b a b i l i t y _ m o d e l . p r e d i c t ( t e s t _ i m a g e s )

Com este código, estamos usando o modelo treinado para fazer a predição de todas as

imagens do conjunto de testes. como exemplo, podemos veriﬁcar qual a décima predição como a

seguir:

1 >>> p r e d i c t i o n s [ 1 0 ]

2 . . .

a r r a y ( [ 9 . 9 9 9 9 8 0 9 e −01 , 8 . 1 8 7 0 3 6 1 e −16 , 1 . 9 3 0 0 8 6 7 e −06 , 2 . 5 6 6 0 8 9 0 e −11 ,

3

4

5 . 4 1 4 8 4 6 3 e −18 , 4 . 0 9 3 0 5 2 3 e −10 , 2 . 9 5 3 2 3 6 5 e −10 , 4 . 9 8 4 6 0 8 7 e −09 ,

7 . 2 1 5 2 1 2 0 e −13 , 9 . 8 5 4 1 4 3 1 e − 1 0 ] , d t y p e = f l o a t 3 2 )

106

Observe que a predição é um array com dez números, que indica a conﬁança do modelo

para qual classe a imagem pertence, O maior valor será a classe predita.

1 >>> np . argmax ( p r e d i c t i o n s [ 1 0 ] )

2 . . . 0

Podemos veriﬁcar se o modelo acertou a predição consultando o conjunto test_labelscom

o seguinte código.

1 >>> t e s t _ l a b e l s [ 1 0 ]

2 . . . 0

O que mostra que a rede neural fez a previsão correta. Podemos fazer um gráﬁco de

barras indicando as probabilidades de cada classe para uma determinada imagem utilizando os

códigos a seguir.

1 >>> d e f p l o t _ i m a g e ( i , p r e d i c t i o n s _ a r r a y ,

t r u e _ l a b e l ,

img ) :

2 . . .

t r u e _ l a b e l ,

img = t r u e _ l a b e l [ i ] ,

img [ i ]

3 . . .

p l t . g r i d ( F a l s e )

4 . . .

p l t . x t i c k s ( [ ] )

5 . . .

p l t . y t i c k s ( [ ] )

6

7 . . .

p l t . imshow ( img , cmap= p l t . cm . b i n a r y )

8

9 . . .

p r e d i c t e d _ l a b e l = np . argmax ( p r e d i c t i o n s _ a r r a y )

10 . . .

i f p r e d i c t e d _ l a b e l == t r u e _ l a b e l :

11 . . .

c o l o r = ’ b l u e ’

12 . . .

e l s e :

13 . . .

c o l o r = ’ r e d ’

14

15 . . .

p l t . x l a b e l ( " {} { : 2 . 0 f}% ( { } ) " . format ( c l a s s _ n a m e s [ p r e d i c t e d _ l a b e l ] ,

16

17

18

100* np . max ( p r e d i c t i o n s _ a r r a y ) ,
c l a s s _ n a m e s [ t r u e _ l a b e l ] ) ,

c o l o r = c o l o r )

107

19

20 >>> d e f p l o t _ v a l u e _ a r r a y ( i , p r e d i c t i o n s _ a r r a y ,

t r u e _ l a b e l ) :

21 . . .

t r u e _ l a b e l = t r u e _ l a b e l [ i ]

22 . . .

p l t . g r i d ( F a l s e )

23 . . .

p l t . x t i c k s ( range ( 1 0 ) )

24 . . .

p l t . y t i c k s ( [ ] )

25 . . .

t h i s p l o t = p l t . b a r ( range ( 1 0 ) , p r e d i c t i o n s _ a r r a y ,

c o l o r = " #777777 " )

26 . . .

p l t . y l i m ( [ 0 , 1 ] )

27 . . .

p r e d i c t e d _ l a b e l = np . argmax ( p r e d i c t i o n s _ a r r a y )

28

29 . . .

t h i s p l o t [ p r e d i c t e d _ l a b e l ] . s e t _ c o l o r ( ’ r e d ’ )

30 . . .

t h i s p l o t [ t r u e _ l a b e l ] . s e t _ c o l o r ( ’ b l u e ’ )

O código acima introduziu as conﬁgurações de plotagem, onde rótulos de predição

correta aparecerão na cor azul e os rótulos de predição incorretas aparecerão em vermelho.

1 >>> i = 0

2 >>> p l t . f i g u r e ( f i g s i z e = ( 6 , 3 ) )

3 >>> p l t . s u b p l o t ( 1 , 2 , 1 )

4 >>> p l o t _ i m a g e ( i , p r e d i c t i o n s [ i ] ,

t e s t _ l a b e l s ,

t e s t _ i m a g e s )

5 >>> p l t . s u b p l o t ( 1 , 2 , 2 )

6 >>> p l o t _ v a l u e _ a r r a y ( i , p r e d i c t i o n s [ i ] ,

t e s t _ l a b e l s )

7 >>> p l t . show ( )

8

1 >>> i = 149

2 >>> p l t . f i g u r e ( f i g s i z e = ( 6 , 3 ) )

3 >>> p l t . s u b p l o t ( 1 , 2 , 1 )

4 >>> p l o t _ i m a g e ( i , p r e d i c t i o n s [ i ] ,

t e s t _ l a b e l s ,

t e s t _ i m a g e s )

5 >>> p l t . s u b p l o t ( 1 , 2 , 2 )

6 >>> p l o t _ v a l u e _ a r r a y ( i , p r e d i c t i o n s [ i ] ,

t e s t _ l a b e l s )

108

7 >>> p l t . show ( )

8

Podemos plotar as n primeiras predições, com seus respectivos rótulos, com as mesmas

conﬁgurações da plotagem anterior, veja o código.

1 num_rows = 3

2 n u m _ c o l s = 3

3 num_images = num_rows * n u m _ c o l s
4 p l t . f i g u r e ( f i g s i z e =(2 * 2 * num_cols , 2* num_rows ) )
5 f o r

i n range ( num_images ) :

i

6

7

8

9

p l t . s u b p l o t ( num_rows , 2* num_cols , 2* i + 1)
p l o t _ i m a g e ( i , p r e d i c t i o n s [ i ] ,

t e s t _ l a b e l s ,

t e s t _ i m a g e s )

p l t . s u b p l o t ( num_rows , 2* num_cols , 2* i + 2)
p l o t _ v a l u e _ a r r a y ( i , p r e d i c t i o n s [ i ] ,

t e s t _ l a b e l s )

10 p l t . t i g h t _ l a y o u t ( )

11 p l t . show ( )

12

Enﬁm, como o modelo já treinado e veriﬁcado no conjunto de testes podemos usá-lo

para fazer a previsão de uma única imagem especíﬁca, seja do conjunto de teste ou uma imagem

em pasta. Primeiramente vamos fazer a previsão de uma única imagem do conjunto de testes,

para isso criamos a variável img e armazenamos a segunda imagem do conjunto de testes, além

109

de veriﬁcar a sua forma.

1 >>>img = t e s t _ i m a g e s [ 1 ]

2 >>> p r i n t ( img . s h a p e )

3 . . . ( 2 8 , 2 8 )

Note que a imagem tem o tamanho de 28x28 mesmo tamanho de entrada da rede. O

tf.keras é otimizado para fazer previsões em lote de imagens a cada vez. Desse modo temos que

criar um lote com apenas uma imagem.

1 >>>img = ( np . e x p a n d _ d i m s ( img , 0 ) )

2 >>> p r i n t ( img . s h a p e )

3 . . . ( 1 , 2 8 , 2 8 )

Agora temos um lote com apenas uma imagem de acordo com as conﬁgurações de

entrada da rede e já podemos prever o rótulo dessa imagem.

1 >>> p r e d i c t i o n s _ s i n g l e = p r o b a b i l i t y _ m o d e l . p r e d i c t ( img )

2 >>> p r i n t ( p r e d i c t i o n s _ s i n g l e )

3 . . . [ [ 7 . 6 3 9 5 1 1 9 e −13 6 . 5 1 4 6 0 1 1 e −08 9 . 9 9 9 9 9 8 8 e −01 7 . 3 8 5 7 6 9 1 e −12 3 . 4 2 6 6 9 7 8 e −24

4

2 . 7 3 6 6 5 6 0 e −11 3 . 5 7 7 6 4 9 3 e −11 1 . 8 3 5 9 4 8 7 e −18 1 . 3 9 1 4 2 5 9 e −09 1 . 4 1 0 5 8 3 5 e − 1 9 ] ]

1 >>> p l o t _ v a l u e _ a r r a y ( 1 , p r e d i c t i o n s _ s i n g l e [ 0 ] ,

t e s t _ l a b e l s )

2 >>>g = p l t . x t i c k s ( range ( 1 0 ) , c l a s s _ n a m e s ,

r o t a t i o n = 4 5 )

3

1 >>>np . argmax ( p r e d i c t i o n s _ s i n g l e [ 0 ] )

2 . . . 2

110

Vamos agora fazer a predição de uma imagem localizada em uma pasta do disco local.

Para isso devemos fazer todo o tratamento da imagem de modo que ﬁque compatível com a

entrada da rede. O código abaixo importa módulos necessários para o tratamento da imagem

1 >>>from k e r a s . p r e p r o c e s s i n g . image import

I m a g e D a t a G e n e r a t o r ,

a r r a y _ t o _ i m g ,

i m g _ t o _ a r r a y ,

l o a d _ i m g

O próximo código insere a imagem na variável img a partir de um caminho do disco.

1 >>>img2 = l o a d _ i m g ( ’ img3 . png ’ )

2 >>>img2

3

1 >>> p r i n t ( img . mode )

2 . . . RGB # R e d G r e e n B l u e

Usando o comando .mode vemos que o canal de formato de pixel da imagem é RGB.

Devemos converter a imagem para o formato (28, 28) e em escala de cinza.

1 >>>from m a t p l o t l i b import

image

2 >>>from m a t p l o t l i b import p y p l o t

1 >>> i m g _ r e s i z e = img . r e s i z e ( ( 2 8 , 2 8 ) ) # R e d i m e n s i o n a a imagem p a r a ( 2 8 , 2 8 ) .

1 >>> i m a g e _ c i n z a = i m g _ r e s i z e . c o n v e r t ( mode= "L" ) # C o n v e r t e a imagem em e s c a l a

111

de c i n z a .

2 >>> i m a g e _ c i n z a

3

Como a rede foi conﬁgurada para fazer previsões em lote de imagens a cada vez, vamos

criar um lote com image_cinza.

1 >>>img2 = ( np . e x p a n d _ d i m s ( i m a g e _ c i n z a , a x i s = 0 ) ) # a x i s =0 d e t e r m i n a a p o s i ç ão

onde o e i x o s e r á i n s e r i d o

1 >>>img2 . s h a p e # R e t o r n a a f o r m a da m a t r i z ,

i s t o é , uma t u p l a r e f e r e n t e ao nú

mero de e l e m e n t o s de cada d i m e n s ã o .

2 . . . ( 1 , 2 8 , 2 8 ) # E i x o i n s e r i d o na p r i m e i r a p o s i ç ã o : a x i s =0

Podemos visualizar a imagem com o código abaixo

1 >>> p l t . f i g u r e ( )

2 >>> p l t . imshow ( img3 [ 0 ] )

3 >>> p l t . c o l o r b a r ( )

4 >>> p l t . g r i d ( F a l s e )

5 >>> p l t . show ( )

6

Note que os valores de pixel estão no intervalo de 0 a 143, devemos redimensionar esses

valores para o intervalo de 0 a 1 antes de inserir na rede neural.

112

1 >>>img3 = img3 / 1 4 3 . 0

Agora a imagem está pronta para ser predita pela rede.

1 >>> p r e d i c t = p r o b a b i l i t y _ m o d e l . p r e d i c t ( img3 )

1 >>> p r e d i c t

2 . . . a r r a y ( [ [ 9 . 4 6 6 0 9 5 6 e −01 , 1 . 8 2 3 7 3 9 7 e −04 , 4 . 8 8 2 6 2 4 7 e −02 , 1 . 5 5 3 2 5 4 5 e −06 ,

3

4

1 . 2 1 5 9 5 2 2 e −03 , 2 . 3 1 2 1 3 3 1 e −05 , 3 . 6 1 4 4 5 5 6 e −04 , 2 . 6 8 2 2 6 5 8 e −03 ,

9 . 1 4 9 3 9 2 3 e −05 , 5 . 8 9 8 2 7 0 0 e − 0 6 ] ] , d t y p e = f l o a t 3 2 )

1 >>>np . argmax ( p r e d i c t ) # C a l c u l a o m a i o r a r g u m e n t o do a r r a y p r e d i c t

2 . . . 0

Podemos visualizar a previsão através de um gráﬁco de barras que mostra as probabi-

lidades como ﬁzemos anteriormente. Em azul a previsão correta e em vermelho as previsões

incorretas, para isso temos que criar antes uma numpy array com dtype=uint8 com o rótulo

correto para img3.

1 >>> i m g 3 _ l a b e l s = np . a r r a y ( [ 0 , ] , d t y p e =np . u i n t 8 )

2 >>> i m g 3 _ l a b e l s

3 . . . a r r a y ( [ 0 ] , d t y p e = u i n t 8 )

1 >>> i = 0

2 >>> p l t . f i g u r e ( f i g s i z e = ( 6 , 3 ) )

3 >>> p l t . s u b p l o t ( 1 , 2 , 1 )

4 >>> p l o t _ i m a g e ( i , p r e d i c t [ i ] ,

i m g 3 _ l a b e l s ,

img3 )

5 >>> p l t . s u b p l o t ( 1 , 2 , 2 )

6 >>> p l o t _ v a l u e _ a r r a y ( i , p r e d i c t [ i ] ,

i m g 3 _ l a b e l s )

7 >>> p l t . show ( )

113

8

CONSIDERAÇÕES FINAIS

114

Diante de uma sociedade moderna é evidente a necessidade de uma educação de qualidade

para todos, porém tem sido um desaﬁo para os professores proporcionar aulas inovadoras. O

mundo educacional tem mudado e exige-se um proﬁssional capacitado que tenha conhecimento

multidisciplinar, capaz de interagir e manipular as novas tecnologias, que está cada vez mais

presente nas mais diversas atividades que realizamos, não sendo diferente na educação, trazendo

novas possibilidades para enriquecer as aulas.

Assim, o presente trabalho propõe aos professores de matemática a utilização de redes

neurais e deep learning no ensino básico, com a ﬁnalidade de se adequar as novas realidades e

deixar o ensino de matemática mais signiﬁcativo, visando combater o desinteresse dos alunos,

visto que, temas que envolve tecnologia são agradáveis aos estudantes tornando as aulas mais

atrativas e participativas.

A apresentação aos docentes de propostas de trabalhos de ensino-aprendizagem, com

temas atuais e relevantes, além de melhorar a qualidade do processo de ensino-aprendizagem,

pode despertar, nos alunos, o interesse na área estudada, abrindo assim, novas opções para

escolha das carreiras proﬁssionais a seguir.

A formação continuada de professores de matemática, necessita de estratégias que

ofereçam um signiﬁcado relevante e atual à construção de seus conceitos. E, nesse contexto, este

trabalho mostra que novas tecnologias têm grande potencial para enriquecer a prática pedagógica

no ensino de matemática.

Portanto, as novas práticas interdisciplinares no ensino complementam a aprendizagem

de forma relevante, faz o aluno gostar de aprender, impulsionado por conhecimentos atuais e

relevantes para seu modo de viver, transformando realidade escolar e construindo um momento

de êxito na prática pedagógica.

REFERÊNCIAS

115

ÁVILA, Geraldo. Limites e Derivadas do Ensino Médio?. Revista Professor de Matemática,

n° 60,Rio de Janeiro, p. 30-38, mai./ago.2006.

CRESPO, A. A. Estatística Fácil. 17.ed. São Paulo: Saraiva, 2002

GÉRON, Aurélien. Hands-On Machine Learning with Scikit-Learn, Keras, and

TensorFlow.2nd Edition. Sebastopol, CA: O’Reilly, 2019.

GRUS, Joel. Data Science do Zero. 2. ed. São Paulo: Novatec, 2014.

HAYKIN, Simon. Redes Neurais: Princíos e prática. 2nd Edition. Porto Alegre: Bookman,

2008.

JAMES, Gareth et al. An Introduction to Statistical Learning. New York 2013: Springer,

2013 (Corrected at 8th printing 2017).

MARTINS, F. E. Estatística e Probabilidade. 2ª.ed. São Paulo: ATLAS, 2010.

MATPLOTLIB. Matplotlib Version 3.1.2. Disponível em:

<https://matplotlib.org/3.1.1/index.html>. Acessado em: 18 de Junho, 2020.

MENEZES, Nilo Ney Coutinho. Introdução à Programação com Python 2. ed. São Paulo:

Novatec, 2014.

MORAN, José Manuel. MASSETTO, Marcos Tarciso. BEHRENS, Marilda Aparecida. Novas

Tecnologias E Mediação Pedagógica. 13.ed. Campinas-SP: Papirus, 2002.

NIELSEN, Michael A. Neural Networks and Deep Learning, Determination Press, 2015.

PINTO, Diomara. MORGADO, Maria Cândida Ferreira. Cáculo Diferencial e Integral de

Funções de Várias Variáveis. 3ª Edição. Rio de Janeiro: Editora UFRJ, 2009.

PYTHON. The Python Tutorial 3.8. Disponível em: <http://python.org/>. Acessado em: 12 de

abril, 2020.

RUSSELL, Stuart. Inteligência Artiﬁcial. Tradução da 3nd Edition. Rio de Janeiro: Elsevier,

2013.

SANTIAGO, Luiz. Entendendo a biblioteca NumPy. Disponível em:

<https://medium.com/ensina-ai/entendendo-a-biblioteca-numpy-4858fde63355>. Acessado em:

22 de Julho, 2020.

TENSORFLOW. Treinamente de Redes Neurais. Disponível em:

<https://www.tensorﬂow.org/>. Acessado em: 20 de setembro, 2020.

116

A NOÇÕES DE NUMPY

117

O Numpy é uma biblioteca cientíﬁca do Python feita para trabalhar com álgebra linear.

A classe array do Numpy é chamada de ndarray e oferece mais funcionalidades que a biblioteca

array padrão do Python que trabalha apenas com matrizes unidimensionais. O numpy traz uma

grande quantidade de funções e operações amplamente utilizadas nos algoritmos de modelos

de aprendizado de máquina, processamento de imagens e computação gráﬁca, além de tarefas

matemáticas como integração numérica, diferenciação interpolação. A biblioteca pode ser usada

como substituto do MATLAB combinado com o Scipy e Matplotlib.

Os Arrays NumPy são usados para armazenar os dados de treinamento dos mais diversos

tipos como imagens que são representadas por array multidimensional, e também os parâmetros

de aprendizado do modelo utilizado.

Como estamos trabalhando com o pacote anaconda o numpy já vem instalado e o que

temos que fazer apenas é importar o numpy com o comando abaixo.

Exemplo A.1.

1 >>> import numpy a s np # i m p o r t a a b i b l i o t e c a NumPy e r e n o m e i a como np .

O Conceito mais relevante no NumPy é o Array, chamado ndarray, que é uma tabela

de elementos todos do mesmo tipo, normalmente números, com grande número de funções e

operações, o que facilita a velocidade de escrita do código e com desempenho excelente. Veja

como criar um array NumPy no exemplo abaixo.

Exemplo A.2.

1 >>> a r r = np . a r r a y ( [ [ 1 , 2 , 3 , 4 , 5 ] ,

[ 6 , 7 , 8 , 9 , 1 0 ] ] )

2 >>> a r r

3 . . . a r r a y ( [ [ 1 ,

2 ,

3 ,

4 ,

5 ] ,

4

[ 6 ,

7 ,

8 ,

9 , 1 0 ] ] )

No exemplo acima temos um array com duas dimensões com cinco elementos.

Podemos criar arrays unidimensionais com sequencias de números com o atributo

np.arrange(), para redimensionar um array usamos o atributo .reshape, veja no exemplo abaixo.

118

Exemplo A.3.

1 >>> a r r = np . a r a n g e ( 4 )

2 >>> a r r

3 . . . a r r a y ( [ 0 , 1 , 2 , 3 ] )

4 >>> a r r _ r = np . a r a n g e ( 4 ) . r e s h a p e ( 2 , 2 )

5 >>> a r r _ r

6 . . . a r r a y ( [ [ 0 , 1 ] ,

7

[ 2 , 3 ] ] )

Com arrays NumPy podemos somar, subtrair, multiplicar e dividir, vejamos alguns

exemplos de operações com arrays.

Exemplo A.4.

1 >>> a = np . a r r a y ( [ [ 1 , 2 ] ,

[ 3 , 4 ] ] )

2 >>> b = np . a r r a y ( [ [ 5 , 6 ] ,

[ 7 , 8 ] ] )

3 >>> a+b

4 . . .

a r r a y ( [ [ 6 ,

8 ] ,

5

[ 1 0 , 1 2 ] ] )

6 >>> a −b

7 . . .

a r r a y ( [ [ 5 , 1 2 ] ,

8

[ 2 1 , 3 2 ] ] )

9 >>> a *b # N o t e que e s t a m u l t i p l i c a ç ã o é f e i t a e l e m e n t o à e l e m e n t o , não é a

u s u a l de m a t r i z e s .

10 . . .

a r r a y ( [ [ 5 , 1 2 ] ,

11

[ 2 1 , 3 2 ] ] )

12 >>> a / b

13 . . .

a r r a y ( [ [ 0 . 2

, 0 . 3 3 3 3 3 3 3 3 ] ,

14

[ 0 . 4 2 8 5 7 1 4 3 , 0 . 5

] ] )

A multiplicação de arrays usual, pode é feita com o método .dot.

Exemplo A.5.

1 >>> np . d o t ( a , b )

2 . . .

a r r a y ( [ [ 1 9 , 2 2 ] ,

3

[ 4 3 , 5 0 ] ] )

119

A função Numpy np.random.randn() é usada para criar um Numpy array aleatoriamente,

de modo que, os elemento geram distribuições gaussianas com 0 de média e 1 de desvio padrão.

Exemplo A.6.

1 >>> k = np . random . r a n d n ( 3 , 3 )

2 >>> k

3 . . . a r r a y ( [ [ − 1 . 2 3 6 8 9 5 5 2 ,

1 . 1 1 8 2 7 6 0 2 ,

0 . 3 1 9 7 6 8 2 9 ] ,

4

5

[ 0 . 0 9 5 3 6 1 5 5 , − 1 . 8 6 5 9 9 7 4 2 , − 1 . 0 0 5 2 8 9 7 2 ] ,

[ − 0 . 7 5 9 3 7 3 7 6 , − 0 . 7 9 0 4 8 8 7 8 , − 0 . 3 7 1 6 2 4 0 2 ] ] )

6 >>>np . mean ( k ) # C a l c u l a a mé d i a a r i t m é t i c a do e l e m e n t o do a r r a y Numpy .

7 . . . − 0 . 4 9 9 5 8 4 8 2 0 0 9 2 1 9 1 7 4

8 >>>k . s t d ( ) # C a l c u l a o d e s v i o p a d r ã o d o s e l e m e n t o do a r r a y Numpy .

9 . . . 0 . 8 4 8 3 7 8 3 0 0 2 3 4 3 6 0 3

Podemos calcular o produto escalar de matrizes em python usando a função numpy.dot().

Exemplo A.7.

1 >>>v = np . a r r a y ( [ ( 1 , 2 ) ,

( 2 , 2 ) ] )

2 >>>v

3 . . . a r r a y ( [ [ 1 , 2 ] ,

4

[ 2 , 2 ] ] )

5 >>>w = np . a r r a y ( [ ( 2 , 2 ) ,

( 1 , 2 ) ] )

6 >>>w

7 . . . a r r a y ( [ [ 2 , 2 ] ,

8

[ 1 , 2 ] ] )

9 >>>u = np . d o t ( v , w)

10 . . . a r r a y ( [ [ 4 , 6 ] ,

11

[ 6 , 8 ] ] )

B NOÇÕES DE PANDAS

120

O Pandas é a mais completa biblioteca Python usada para manipulação, leitura e visuali-

zação de dados, sendo fundamental para se trabalhar com análise de dados. O Pandas já vem

instalado e o que temos que fazer apenas é importar o Pandas com o comando abaixo.

Exemplo B.1.

1 >>> import p a n d a s a s pd # i m p o r t a a b i b l i o t e c a Pandas e r e n o m e i a como pd .

Em Pandas, destacamos duas estruturas de dados:

• Séries são objetos unidimensionais, uma lista de valores, semelhante à um array, que

possui índice, chamado index.

1 >>> s = pd . S e r i e s ( [ 2 , 4 , 6 , 8 , 1 0 ] )

2 >>> s

3 0

4 1

5 2

6 3

7 4

2

4

6

8

10

8 d t y p e :

i n t 6 4

Note que, como não deﬁnimos os índices, foi gerado o padrão. Veja, no código abaixo,

como podemos deﬁnir índices.

1 >>> i d a d e s = pd . S e r i e s ( [ 5 , 4 , 6 , 9 , 1 5 ] ,

i n d e x = [ ’ M a r i a ’ ,

’ R a f a e l ’ ,

’ J ú l i a ’

,

’ C a r l o s ’ ,

’ Marcos ’ ] )

2 >>> i d a d e s

3 M a r i a

4 R a f a e l

5 J ú l i a

6 C a r l o s

5

4

6

9

7 Marcos

15

8 d t y p e :

i n t 6 4

121

A função do index facilitar a referência do valor usando o seu rótulo.

1 >>> i d a d e s [ ’ M a r i a ’ ]

2 10

Outras funcionalidades proporcionadas pela estrutura Series é que seus métodos fornecem

dados estatísticos sobre seu valores, como média .mean(), variância .var() e desvio padrão

.std(). Podemos utilizar o código .describe() para resumir as estatísticas da Series.

1 >>> i d a d e s . d e s c r i b e ( )

2 c o u n t

5 . 0 0 0 0 0 0

3 mean

4 s t d

5 min

6 25\%

7 50\%

8 75\%

9 max

7 . 8 0 0 0 0 0

4 . 4 3 8 4 6 8

4 . 0 0 0 0 0 0

5 . 0 0 0 0 0 0

6 . 0 0 0 0 0 0

9 . 0 0 0 0 0 0

1 5 . 0 0 0 0 0 0

10 d t y p e :

f l o a t 6 4

• DataFrame: É uma estrutura bidimensional, análoga a uma planilha. veja o código abaixo.

1 >>> d f = pd . DataFrame ( { ’ Aluno ’ : [ ’ M a r i a ’ ,

’ R a f a e l ’ ,

’ J ú l i a ’ ,

’ C a r l o s ’ ,

’ Marcos ’ ] ,

’ N o t a s ’ : [ 6 . 5 , 7 . 0 , 9 . 0 , 8 . 5 , 7 . 5 ] ,

’ F a l t a s ’

: [ 1 , 0 , 3 , 2 , 5 ] } )

2 >>> d f

3

Podemos acessar os valores de uma coluna pelo respectivo nome.

122

1 >>> d f [ ’ N o t a s ’ ]

2 0

3 1

4 2

5 3

6 4

6 . 5

7 . 0

9 . 0

8 . 5

7 . 5

7 Name : Notas , d t y p e :

f l o a t 6 4

Podemos, também, utilizar o método .describe() para visualizar o resumo dos dados

estatísticos do DataFrame.

1 >>> d f . d e s c r i b e ( )

2

Podemos utilizar o método loc para resgatar dados do DatraFrame pelo index.

1 >>> d f . l o c [ 3 ]

2 Aluno

3 N o t a s

4 F a l t a s

C a r l o s

8 . 5

2

5 Name : 3 , d t y p e : o b j e c t

O Pandas pode, também, fornece uma extensa gama de funcionalidades para leitura e

análise de dados prontos. O Pandas pode ler diversos tipos de arquivos como csv, xlsx e html,

para isso, basta utilizar os códigos pd.read_csv, pd.read_xlsx e pd.read_html respectivamente.

Vamos carregar os dados housing.csv que mostra o preço de casas em diferentes regiões da

California.

1 >>> pd . r e a d \ _ c s v ( ’ h o u s i n g . c s v ’ )

123

2

C NOÇÕES DE MATPLOTLIB

O Matplotlib é uma biblioteca Python destinada para criar gráﬁcos em Python. Para

utilizarmos essa biblioteca devemos importá-la com o seguinte código.

124

Exemplo C.1.

1 >>> import m a t p l o t l i b . p y p l o t a s p l t

O pyplot é uma estrutura do matplotlib que o faz funcionar de modo semelhante ao

MATLAB, isto é, cada função do pyplot faz uma alteração no gráﬁco. Assim podemos criar

gráﬁcos com poucas linhas de código e de modo muito intuitivo como no exemplo abaixo.

Exemplo C.2.

1 # P r i m e i r o d e f i n i m o s a s v a r i á v e i s

2 >>> x = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 1 0 ]

3 >>>y = [ 1 , 4 , 9 , 1 6 , 2 5 , 3 6 , 4 9 , 6 4 , 8 1 , 1 0 0 ]

4

5 # I n s e r i n d o d e s c r i ç õ e s no g r á f i c o

6 >>> p l t . s c a t t e r ( x , y ) # c r i a um g r á f i c o de d i s p e r s ã o

7 >>> p l t . t i t l e ( ’ Exemplo de de T í t u l o ’ )

8 >>> p l t . x l a b e l ( ’ P r i m e i r a v a r i á v e l ’ )

9 >>> p l t . y l a b e l ( ’ Segunda v a r i á v e l ’ )

10

11 # I n s e r i uma l e g e n d a p a r a o g r á f i c o de d i s p e r s ão .

12 >>> p l t . s c a t t e r ( x , y ,

l a b e l = ’ Exemplo de Legenda ’ )

13 >>> p l t . l e g e n d ( )

14

15 # M o s t r a o g r á f i c o c r i a d o

16 >>> p l t . show

125

17

Exemplo C.3.

1 >>> p l t . b a r ( x , y ) # c r i a um g r á f i c o de b a r r a s

2 >>> p l t . show # m o s t r a o g r á f i c o

3

Podemos também construir mais de um gráﬁco em uma única ﬁgura, de uma só vez,

usando o comando plt.subplot como no exemplo abaixo.

Exemplo C.4.

1 >>>z = np . a r a n g e ( − 1 0 0 , 1 0 0 , 1 )

2 >>> f = −z **2 −4
3 >>>g = z

4 >>>h = z **4
5 >>> i = z **5
6 >>> p l t . f i g u r e ( f i g s i z e = ( 8 , 8 ) ) # d e f i n e a a l t u r a e a l a r g u r a da f i g u r a

7 >>> p l t . s u b p l o t ( 2 , 2 , 1 )

8 >>> p l t . p l o t ( z ,

f )

9 >>> p l t . s u b p l o t ( 2 , 2 , 2 )

126

10 >>> p l t . p l o t ( z , g )

11 >>> p l t . s u b p l o t ( 2 , 2 , 3 )

12 >>> p l t . p l o t ( z , h )

13 >>> p l t . s u b p l o t ( 2 , 2 , 4 )

14 >>> p l t . p l o t ( z ,

i )

15 >>> p l t . show

16

Como vimos o Matplotlib disponibiliza diversas funcionalidades para personalizar os

gráﬁcos sendo, desse modo, uma biblioteca muito utilizada para tratamento de dados.

